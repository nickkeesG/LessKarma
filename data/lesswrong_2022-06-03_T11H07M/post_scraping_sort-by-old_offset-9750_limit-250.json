{"results": [{"createdAt": null, "postedAt": "2013-10-14T01:57:13.526Z", "modifiedAt": null, "url": null, "title": "Open Thread, October 13 - 19, 2013", "slug": "open-thread-october-13-19-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.838Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4qpPkG2iegY4p643k/open-thread-october-13-19-2013", "pageUrlRelative": "/posts/4qpPkG2iegY4p643k/open-thread-october-13-19-2013", "linkUrl": "https://www.lesswrong.com/posts/4qpPkG2iegY4p643k/open-thread-october-13-19-2013", "postedAtFormatted": "Monday, October 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20October%2013%20-%2019%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20October%2013%20-%2019%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qpPkG2iegY4p643k%2Fopen-thread-october-13-19-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20October%2013%20-%2019%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qpPkG2iegY4p643k%2Fopen-thread-october-13-19-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qpPkG2iegY4p643k%2Fopen-thread-october-13-19-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 12.660714149475098px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4qpPkG2iegY4p643k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.3791536715946333e-06, "legacy": true, "legacyId": "24389", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 251, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-14T06:41:42.449Z", "modifiedAt": null, "url": null, "title": "[LINK] A Turing test for free will", "slug": "link-a-turing-test-for-free-will", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:59.825Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JBzD2GiwrujbJBQny/link-a-turing-test-for-free-will", "pageUrlRelative": "/posts/JBzD2GiwrujbJBQny/link-a-turing-test-for-free-will", "linkUrl": "https://www.lesswrong.com/posts/JBzD2GiwrujbJBQny/link-a-turing-test-for-free-will", "postedAtFormatted": "Monday, October 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20A%20Turing%20test%20for%20free%20will&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20A%20Turing%20test%20for%20free%20will%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBzD2GiwrujbJBQny%2Flink-a-turing-test-for-free-will%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20A%20Turing%20test%20for%20free%20will%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBzD2GiwrujbJBQny%2Flink-a-turing-test-for-free-will", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBzD2GiwrujbJBQny%2Flink-a-turing-test-for-free-will", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 401, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Seth_Lloyd\">Seth Lloyd</a> has posted a well-written <a href=\"http://arxiv.org/pdf/1310.3225.pdf\">pre-print</a>, proposing a self-administered Turing test for free will but also dealing with some other aspects of the free will debate. Some excerpts:</p>\n<blockquote>\n<p>... the theory of computation implies that even when our decisions arise from a completely deterministic decision-making process, the outcomes of that process can be intrinsically unpredictable, even to &ndash; especially to &ndash; ourselves. I argue that this intrinsic computational unpredictability of the decision making process is what give rise to our impression that we possess free will.</p>\n</blockquote>\n<blockquote>\n<p>It is important to note that satisifying the criteria for assigning oneself free will does not imply that one possesses consciousness. Having the capacity for self-reference is a far cry from full self-consciousness...An entity that possesses free will need not be conscious in any human sense of the word.</p>\n</blockquote>\n<blockquote>\n<p>This paper investigated the role of physical law in problems of free will. I reviewed the argument that the mere introduction of probabilistic behavior through, e.g., quantum mechanics, does not resolve the debate between compatibilists and incompatibilists. By contrast, ideas from computer science such as uncomputability and computational complexity do cast light on a central feature of free will &ndash; the inability of deciders to predict their decisions before they have gone through the decision making process. I sketched proofs of the following results. The halting problem implies that we can not even predict in general whether we will arrive at a decision, let alone what the decision will be. If she is part of the universe, Laplace&rsquo;s demon must fail to predict her own actions. The computational complexity analogue of the halting problem shows that to simulate the decision making process is strictly harder than simply making the decision. If one is a compatibilist, one can regard these results as justifying a central feature of free will. If one is an incompatibilist, one can take them to explain free will&rsquo;s central illusion that our decisions are not determined beforehand. In either case, it is more efficient to be oneself than to simulate oneself.</p>\n</blockquote>\n<div>The \"Turing\" test itself consists of the following questions:</div>\n<p>Q1: Am I a decider?</p>\n<p>Q2: Do I make my decisions using recursive reasoning?</p>\n<p>Q3: Can I model and simulate &ndash; at least partially &ndash; my own behavior and that of other deciders?</p>\n<p>Q4:&nbsp;Can I predict my own decisions beforehand?</p>\n<div>If you answer Yes, Yes, Yes and No, then you are likely to believe you have free will.</div>\n<div><br /></div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JBzD2GiwrujbJBQny", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.3794176344649714e-06, "legacy": true, "legacyId": "24390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-14T08:08:26.230Z", "modifiedAt": null, "url": null, "title": "Meetup : Sunday Brunch Club - Sunday 20th October", "slug": "meetup-sunday-brunch-club-sunday-20th-october", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:59.598Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BraydenM", "createdAt": "2013-06-10T09:17:31.294Z", "isAdmin": false, "displayName": "BraydenM"}, "userId": "KBZHqcMC6z2rPZkZK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7jdz7Ha7TW9zdS2ZY/meetup-sunday-brunch-club-sunday-20th-october", "pageUrlRelative": "/posts/7jdz7Ha7TW9zdS2ZY/meetup-sunday-brunch-club-sunday-20th-october", "linkUrl": "https://www.lesswrong.com/posts/7jdz7Ha7TW9zdS2ZY/meetup-sunday-brunch-club-sunday-20th-october", "postedAtFormatted": "Monday, October 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sunday%20Brunch%20Club%20-%20Sunday%2020th%20October&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sunday%20Brunch%20Club%20-%20Sunday%2020th%20October%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7jdz7Ha7TW9zdS2ZY%2Fmeetup-sunday-brunch-club-sunday-20th-october%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sunday%20Brunch%20Club%20-%20Sunday%2020th%20October%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7jdz7Ha7TW9zdS2ZY%2Fmeetup-sunday-brunch-club-sunday-20th-october", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7jdz7Ha7TW9zdS2ZY%2Fmeetup-sunday-brunch-club-sunday-20th-october", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s7'>Sunday Brunch Club - Sunday 20th October</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2013 07:08:36PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2105 / 5 Caravel Lane Docklands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Dear Friends,</p>\n\n<p>As we await the coming of summer, October in Melbourne is wet yet deceivingly warm. Thank heavens for food though! New friends and old are joining together for the much awaited Sunday Brunch Club. This time held our home, Isengard, in the Docklands with a lovely view of the city and harbour.</p>\n\n<p>Come out for a good time on Sunday Oct 20 for a brunch (lunch?) we have cooked up just for you. Let this be one of the myriad of joyous occasions we choose to celebrate knowing and loving each other. Let us welcome summer, as we embark on our paths, wherever they may lead.</p>\n\n<p>On to the details! \nWhere: 2105 / 5 Caravel Lane, Docklands\nGetting Here: 12 minute walk from Southern Cross station, or catch a free tram straight to our front door.\nWhen: 11am-4pm You will be provided a seating time by Thursday based on your survey responses. Please be punctual so we can make sure it all works to plan!\nSuggested Contribution: $11-$15 per person to cover food costs, all excess proceeds to support GiveWell top rated charities.\nGuests: Please limit yourself to inviting one additional guest.</p>\n\n<p>We need to receive all RSVPs via this form by 7pm on Thurs 16th October.</p>\n\n<p>Wish you well and look forward to seeing you soon!</p>\n\n<p>Your friends and hosts,</p>\n\n<p>Adrienne, Allison, Brayden, Helen, and Thomas</p>\n\n<p>Tentative \u00c0 La Carte Menu Below\nSTARTER</p>\n\n<p>Homemade Pumpkin Bread\nServed with nutmeg and rosemary butter</p>\n\n<p>MAINS</p>\n\n<p>Patatas Bravas Latkes \nApple creme drizzled sweet and savoury potato pancakes served with Gruy\u00e8re cheese and rocket</p>\n\n<p>Omelette du Fromage\nFluffy omelettes with capsicum, ham, mushrooms, tomato, parsley, and, of course, cheese.</p>\n\n<p>DESSERT\nMatcha Green Tea Crepes Roulade \nA Japanese-French fusion dusted with sugar and served with banana slices</p>\n\n<p>Orange-Blossom Mead du la Glace\nHoney-fermented wine liqueur served with vanilla ice cream</p>\n\n<p>DRINKS\nSangria Blanco\nSpanish wine punch with chopped fruit</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s7'>Sunday Brunch Club - Sunday 20th October</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7jdz7Ha7TW9zdS2ZY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3794981262345507e-06, "legacy": true, "legacyId": "24391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sunday_Brunch_Club___Sunday_20th_October\">Discussion article for the meetup : <a href=\"/meetups/s7\">Sunday Brunch Club - Sunday 20th October</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2013 07:08:36PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2105 / 5 Caravel Lane Docklands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Dear Friends,</p>\n\n<p>As we await the coming of summer, October in Melbourne is wet yet deceivingly warm. Thank heavens for food though! New friends and old are joining together for the much awaited Sunday Brunch Club. This time held our home, Isengard, in the Docklands with a lovely view of the city and harbour.</p>\n\n<p>Come out for a good time on Sunday Oct 20 for a brunch (lunch?) we have cooked up just for you. Let this be one of the myriad of joyous occasions we choose to celebrate knowing and loving each other. Let us welcome summer, as we embark on our paths, wherever they may lead.</p>\n\n<p>On to the details! \nWhere: 2105 / 5 Caravel Lane, Docklands\nGetting Here: 12 minute walk from Southern Cross station, or catch a free tram straight to our front door.\nWhen: 11am-4pm You will be provided a seating time by Thursday based on your survey responses. Please be punctual so we can make sure it all works to plan!\nSuggested Contribution: $11-$15 per person to cover food costs, all excess proceeds to support GiveWell top rated charities.\nGuests: Please limit yourself to inviting one additional guest.</p>\n\n<p>We need to receive all RSVPs via this form by 7pm on Thurs 16th October.</p>\n\n<p>Wish you well and look forward to seeing you soon!</p>\n\n<p>Your friends and hosts,</p>\n\n<p>Adrienne, Allison, Brayden, Helen, and Thomas</p>\n\n<p>Tentative \u00c0 La Carte Menu Below\nSTARTER</p>\n\n<p>Homemade Pumpkin Bread\nServed with nutmeg and rosemary butter</p>\n\n<p>MAINS</p>\n\n<p>Patatas Bravas Latkes \nApple creme drizzled sweet and savoury potato pancakes served with Gruy\u00e8re cheese and rocket</p>\n\n<p>Omelette du Fromage\nFluffy omelettes with capsicum, ham, mushrooms, tomato, parsley, and, of course, cheese.</p>\n\n<p>DESSERT\nMatcha Green Tea Crepes Roulade \nA Japanese-French fusion dusted with sugar and served with banana slices</p>\n\n<p>Orange-Blossom Mead du la Glace\nHoney-fermented wine liqueur served with vanilla ice cream</p>\n\n<p>DRINKS\nSangria Blanco\nSpanish wine punch with chopped fruit</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sunday_Brunch_Club___Sunday_20th_October1\">Discussion article for the meetup : <a href=\"/meetups/s7\">Sunday Brunch Club - Sunday 20th October</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sunday Brunch Club - Sunday 20th October", "anchor": "Discussion_article_for_the_meetup___Sunday_Brunch_Club___Sunday_20th_October", "level": 1}, {"title": "Discussion article for the meetup : Sunday Brunch Club - Sunday 20th October", "anchor": "Discussion_article_for_the_meetup___Sunday_Brunch_Club___Sunday_20th_October1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-14T18:33:12.200Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta October Meetup (Second of Two)", "slug": "meetup-atlanta-october-meetup-second-of-two", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KNkz9sxMALbhjw3Mz/meetup-atlanta-october-meetup-second-of-two", "pageUrlRelative": "/posts/KNkz9sxMALbhjw3Mz/meetup-atlanta-october-meetup-second-of-two", "linkUrl": "https://www.lesswrong.com/posts/KNkz9sxMALbhjw3Mz/meetup-atlanta-october-meetup-second-of-two", "postedAtFormatted": "Monday, October 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20October%20Meetup%20(Second%20of%20Two)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20October%20Meetup%20(Second%20of%20Two)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNkz9sxMALbhjw3Mz%2Fmeetup-atlanta-october-meetup-second-of-two%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20October%20Meetup%20(Second%20of%20Two)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNkz9sxMALbhjw3Mz%2Fmeetup-atlanta-october-meetup-second-of-two", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNkz9sxMALbhjw3Mz%2Fmeetup-atlanta-october-meetup-second-of-two", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s8'>Atlanta October Meetup (Second of Two)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1314 Hosea L Williams Dr. NE, Atlanta </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Members:  Please note the change in location from our usual meetup location.</p>\n\n<p>Come join us for the second meetup for the month of October!\nWe'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!\nPlease contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.\nAnd check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/ where you can connect with Atlanta Lesswrongers and suggest a topics for discussion at this meetup!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s8'>Atlanta October Meetup (Second of Two)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KNkz9sxMALbhjw3Mz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 1.3800782028607261e-06, "legacy": true, "legacyId": "24393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_October_Meetup__Second_of_Two_\">Discussion article for the meetup : <a href=\"/meetups/s8\">Atlanta October Meetup (Second of Two)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1314 Hosea L Williams Dr. NE, Atlanta </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Members:  Please note the change in location from our usual meetup location.</p>\n\n<p>Come join us for the second meetup for the month of October!\nWe'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!\nPlease contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.\nAnd check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/ where you can connect with Atlanta Lesswrongers and suggest a topics for discussion at this meetup!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_October_Meetup__Second_of_Two_1\">Discussion article for the meetup : <a href=\"/meetups/s8\">Atlanta October Meetup (Second of Two)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta October Meetup (Second of Two)", "anchor": "Discussion_article_for_the_meetup___Atlanta_October_Meetup__Second_of_Two_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta October Meetup (Second of Two)", "anchor": "Discussion_article_for_the_meetup___Atlanta_October_Meetup__Second_of_Two_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-14T19:08:59.461Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta:  Rationalist Movie Night!", "slug": "meetup-atlanta-rationalist-movie-night", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KHMb3TPtpC9ZnCdCS/meetup-atlanta-rationalist-movie-night", "pageUrlRelative": "/posts/KHMb3TPtpC9ZnCdCS/meetup-atlanta-rationalist-movie-night", "linkUrl": "https://www.lesswrong.com/posts/KHMb3TPtpC9ZnCdCS/meetup-atlanta-rationalist-movie-night", "postedAtFormatted": "Monday, October 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%3A%20%20Rationalist%20Movie%20Night!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%3A%20%20Rationalist%20Movie%20Night!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKHMb3TPtpC9ZnCdCS%2Fmeetup-atlanta-rationalist-movie-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%3A%20%20Rationalist%20Movie%20Night!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKHMb3TPtpC9ZnCdCS%2Fmeetup-atlanta-rationalist-movie-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKHMb3TPtpC9ZnCdCS%2Fmeetup-atlanta-rationalist-movie-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s9'>Atlanta:  Rationalist Movie Night!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1314 Hosea L Williams Dr. NE, Atlanta</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Woo! Let's do rationalist cinema, and have popcorn, then discussion, games, or going out for coffee or whatever we like.</p>\n\n<p>I'll post a poll on the facebook page soon with a short list of proposed movies, so more information coming soon.</p>\n\n<p>Please contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.</p>\n\n<p>And check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/ where you can connect with Atlanta Lesswrongers and vote for a movie.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s9'>Atlanta:  Rationalist Movie Night!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KHMb3TPtpC9ZnCdCS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 1.3801114436742355e-06, "legacy": true, "legacyId": "24394", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta___Rationalist_Movie_Night_\">Discussion article for the meetup : <a href=\"/meetups/s9\">Atlanta:  Rationalist Movie Night!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1314 Hosea L Williams Dr. NE, Atlanta</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Woo! Let's do rationalist cinema, and have popcorn, then discussion, games, or going out for coffee or whatever we like.</p>\n\n<p>I'll post a poll on the facebook page soon with a short list of proposed movies, so more information coming soon.</p>\n\n<p>Please contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.</p>\n\n<p>And check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/ where you can connect with Atlanta Lesswrongers and vote for a movie.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta___Rationalist_Movie_Night_1\">Discussion article for the meetup : <a href=\"/meetups/s9\">Atlanta:  Rationalist Movie Night!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta:  Rationalist Movie Night!", "anchor": "Discussion_article_for_the_meetup___Atlanta___Rationalist_Movie_Night_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta:  Rationalist Movie Night!", "anchor": "Discussion_article_for_the_meetup___Atlanta___Rationalist_Movie_Night_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-14T20:13:50.185Z", "modifiedAt": null, "url": null, "title": "Meetup : First Meetup in Cologne (K\u00f6ln)", "slug": "meetup-first-meetup-in-cologne-koeln", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:04.421Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Za3bzMyPPa3749td4/meetup-first-meetup-in-cologne-koeln", "pageUrlRelative": "/posts/Za3bzMyPPa3749td4/meetup-first-meetup-in-cologne-koeln", "linkUrl": "https://www.lesswrong.com/posts/Za3bzMyPPa3749td4/meetup-first-meetup-in-cologne-koeln", "postedAtFormatted": "Monday, October 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Meetup%20in%20Cologne%20(K%C3%B6ln)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Meetup%20in%20Cologne%20(K%C3%B6ln)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZa3bzMyPPa3749td4%2Fmeetup-first-meetup-in-cologne-koeln%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Meetup%20in%20Cologne%20(K%C3%B6ln)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZa3bzMyPPa3749td4%2Fmeetup-first-meetup-in-cologne-koeln", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZa3bzMyPPa3749td4%2Fmeetup-first-meetup-in-cologne-koeln", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 244, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sa'>First Meetup in Cologne (K\u00f6ln)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Starbucks Coffee, An der Hahnepooz 8 50674 Cologne\u200e</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>ETA:</strong> The meetup is going to take place on November, 10th, 15:00. I'll be there. In case you don't find the place or something, here's my <strong>number</strong>: 0157 39606835</p>\n\n<p>As far as I can tell, there never has been a Lesswrong meetup in Cologne. This is a shame, considering that Cologne has over 1 million inhabitants.</p>\n\n<p>I recently moved here from Munich (where I already attended 3 Lesswrong Meetups) to study and would like to meet folks who are also interested in Lesswrong and related topics. Regarding the content and structure of the meetup: I would suggest that at first each of us proposes some discussion topics he or she is interested in (e.g. epistemic rationality, effective altruism, far future/FAI, practical life tips, etc.) and then we choose the most popular ones. And simple socializing and getting to know each other is also great, as far as I'm concerned.</p>\n\n<p>Here is a link to a doodle survey (<a href=\"http://doodle.com/3ms7afrniqxb5i7e\" rel=\"nofollow\">http://doodle.com/3ms7afrniqxb5i7e</a>), in which you can put your favorite date. I prefer Sundays, but if nobody can attend on Sundays, we can probably change the date.</p>\n\n<p>Please, please leave a comment if you're interested in a LW meetup in Cologne, even if you can't attend one in the next weeks/months.</p>\n\n<p>And remember, (almost) everyone is welcome, especially newbies!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sa'>First Meetup in Cologne (K\u00f6ln)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Za3bzMyPPa3749td4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.380171677942051e-06, "legacy": true, "legacyId": "24395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Meetup_in_Cologne__K_ln_\">Discussion article for the meetup : <a href=\"/meetups/sa\">First Meetup in Cologne (K\u00f6ln)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Starbucks Coffee, An der Hahnepooz 8 50674 Cologne\u200e</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>ETA:</strong> The meetup is going to take place on November, 10th, 15:00. I'll be there. In case you don't find the place or something, here's my <strong>number</strong>: 0157 39606835</p>\n\n<p>As far as I can tell, there never has been a Lesswrong meetup in Cologne. This is a shame, considering that Cologne has over 1 million inhabitants.</p>\n\n<p>I recently moved here from Munich (where I already attended 3 Lesswrong Meetups) to study and would like to meet folks who are also interested in Lesswrong and related topics. Regarding the content and structure of the meetup: I would suggest that at first each of us proposes some discussion topics he or she is interested in (e.g. epistemic rationality, effective altruism, far future/FAI, practical life tips, etc.) and then we choose the most popular ones. And simple socializing and getting to know each other is also great, as far as I'm concerned.</p>\n\n<p>Here is a link to a doodle survey (<a href=\"http://doodle.com/3ms7afrniqxb5i7e\" rel=\"nofollow\">http://doodle.com/3ms7afrniqxb5i7e</a>), in which you can put your favorite date. I prefer Sundays, but if nobody can attend on Sundays, we can probably change the date.</p>\n\n<p>Please, please leave a comment if you're interested in a LW meetup in Cologne, even if you can't attend one in the next weeks/months.</p>\n\n<p>And remember, (almost) everyone is welcome, especially newbies!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Meetup_in_Cologne__K_ln_1\">Discussion article for the meetup : <a href=\"/meetups/sa\">First Meetup in Cologne (K\u00f6ln)</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Meetup in Cologne (K\u00f6ln)", "anchor": "Discussion_article_for_the_meetup___First_Meetup_in_Cologne__K_ln_", "level": 1}, {"title": "Discussion article for the meetup : First Meetup in Cologne (K\u00f6ln)", "anchor": "Discussion_article_for_the_meetup___First_Meetup_in_Cologne__K_ln_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T04:02:05.988Z", "modifiedAt": null, "url": null, "title": "Cambridge Meetup: Talk by Eliezer Yudkowsky: Recursion in rational agents", "slug": "cambridge-meetup-talk-by-eliezer-yudkowsky-recursion-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.682Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y7Ja3Mh7GN5tnuG2e/cambridge-meetup-talk-by-eliezer-yudkowsky-recursion-in", "pageUrlRelative": "/posts/y7Ja3Mh7GN5tnuG2e/cambridge-meetup-talk-by-eliezer-yudkowsky-recursion-in", "linkUrl": "https://www.lesswrong.com/posts/y7Ja3Mh7GN5tnuG2e/cambridge-meetup-talk-by-eliezer-yudkowsky-recursion-in", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20Meetup%3A%20Talk%20by%20Eliezer%20Yudkowsky%3A%20Recursion%20in%20rational%20agents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20Meetup%3A%20Talk%20by%20Eliezer%20Yudkowsky%3A%20Recursion%20in%20rational%20agents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7Ja3Mh7GN5tnuG2e%2Fcambridge-meetup-talk-by-eliezer-yudkowsky-recursion-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20Meetup%3A%20Talk%20by%20Eliezer%20Yudkowsky%3A%20Recursion%20in%20rational%20agents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7Ja3Mh7GN5tnuG2e%2Fcambridge-meetup-talk-by-eliezer-yudkowsky-recursion-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7Ja3Mh7GN5tnuG2e%2Fcambridge-meetup-talk-by-eliezer-yudkowsky-recursion-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 434, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/sb\">Talk by Eliezer Yudkowsky: Recursion in rational agents: Foundations for self-modifying AI</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">17 October 2013 04:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Stata Center room 32-123 Cambridge, MA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>On October 17th from 4:00-5:30pm, Scott Aaronson will host a talk by MIRI research fellow Eliezer Yudkowsky. Yudkowsky&rsquo;s talk will take place in MIT&rsquo;s Ray and Maria Stata Center (see image on right), in room 32-123 (aka Kirsch Auditorium, with 318 seats). There will be light refreshments 15 minutes before the talk. Yudkowsky&rsquo;s title and abstract are:</p>\n<p>Recursion in rational agents: Foundations for self-modifying AI</p>\n<p>Reflective reasoning is a familiar but formally elusive aspect of human cognition. This issue comes to the forefront when we consider building AIs which model other sophisticated reasoners, or who might design other AIs which are as sophisticated as themselves. Mathematical logic, the best-developed contender for a formal language capable of reflecting on itself, is beset by impossibility results. Similarly, standard decision theories begin to produce counterintuitive or incoherent results when applied to agents with detailed self-knowledge. In this talk I will present some early results from workshops held by the Machine Intelligence Research Institute to confront these challenges.</p>\n<p>The first is a formalization and significant refinement of Hofstadter&rsquo;s &ldquo;superrationality,&rdquo; the (informal) idea that ideal rational agents can achieve mutual cooperation on games like the prisoner&rsquo;s dilemma by exploiting the logical connection between their actions and their opponent&rsquo;s actions. We show how to implement an agent which reliably outperforms classical game theory given mutual knowledge of source code, and which achieves mutual cooperation in the one-shot prisoner&rsquo;s dilemma using a general procedure. Using a fast algorithm for finding fixed points, we are able to write implementations of agents that perform the logical interactions necessary for our formalization, and we describe empirical results.</p>\n<p>Second, it has been claimed that Godel&rsquo;s second incompleteness theorem presents a serious obstruction to any AI understanding why its own reasoning works or even trusting that it does work. We exhibit a simple model for this situation and show that straightforward solutions to this problem are indeed unsatisfactory, resulting in agents who are willing to trust weaker peers but not their own reasoning. We show how to circumvent this difficulty without compromising logical expressiveness.</p>\n<p>Time permitting, we also describe a more general agenda for averting self-referential difficulties by replacing logical deduction with a suitable form of probabilistic inference. The goal of this program is to convert logical unprovability or undefinability into very small probabilistic errors which can be safely ignored (and may even be philosophically justified).</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/sb\">Talk by Eliezer Yudkowsky: Recursion in rational agents: Foundations for self-modifying AI</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y7Ja3Mh7GN5tnuG2e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 1.3806067815050327e-06, "legacy": true, "legacyId": "24397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Talk_by_Eliezer_Yudkowsky__Recursion_in_rational_agents__Foundations_for_self_modifying_AI\">Discussion article for the meetup : <a href=\"/meetups/sb\">Talk by Eliezer Yudkowsky: Recursion in rational agents: Foundations for self-modifying AI</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">17 October 2013 04:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Stata Center room 32-123 Cambridge, MA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>On October 17th from 4:00-5:30pm, Scott Aaronson will host a talk by MIRI research fellow Eliezer Yudkowsky. Yudkowsky\u2019s talk will take place in MIT\u2019s Ray and Maria Stata Center (see image on right), in room 32-123 (aka Kirsch Auditorium, with 318 seats). There will be light refreshments 15 minutes before the talk. Yudkowsky\u2019s title and abstract are:</p>\n<p>Recursion in rational agents: Foundations for self-modifying AI</p>\n<p>Reflective reasoning is a familiar but formally elusive aspect of human cognition. This issue comes to the forefront when we consider building AIs which model other sophisticated reasoners, or who might design other AIs which are as sophisticated as themselves. Mathematical logic, the best-developed contender for a formal language capable of reflecting on itself, is beset by impossibility results. Similarly, standard decision theories begin to produce counterintuitive or incoherent results when applied to agents with detailed self-knowledge. In this talk I will present some early results from workshops held by the Machine Intelligence Research Institute to confront these challenges.</p>\n<p>The first is a formalization and significant refinement of Hofstadter\u2019s \u201csuperrationality,\u201d the (informal) idea that ideal rational agents can achieve mutual cooperation on games like the prisoner\u2019s dilemma by exploiting the logical connection between their actions and their opponent\u2019s actions. We show how to implement an agent which reliably outperforms classical game theory given mutual knowledge of source code, and which achieves mutual cooperation in the one-shot prisoner\u2019s dilemma using a general procedure. Using a fast algorithm for finding fixed points, we are able to write implementations of agents that perform the logical interactions necessary for our formalization, and we describe empirical results.</p>\n<p>Second, it has been claimed that Godel\u2019s second incompleteness theorem presents a serious obstruction to any AI understanding why its own reasoning works or even trusting that it does work. We exhibit a simple model for this situation and show that straightforward solutions to this problem are indeed unsatisfactory, resulting in agents who are willing to trust weaker peers but not their own reasoning. We show how to circumvent this difficulty without compromising logical expressiveness.</p>\n<p>Time permitting, we also describe a more general agenda for averting self-referential difficulties by replacing logical deduction with a suitable form of probabilistic inference. The goal of this program is to convert logical unprovability or undefinability into very small probabilistic errors which can be safely ignored (and may even be philosophically justified).</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Talk_by_Eliezer_Yudkowsky__Recursion_in_rational_agents__Foundations_for_self_modifying_AI1\">Discussion article for the meetup : <a href=\"/meetups/sb\">Talk by Eliezer Yudkowsky: Recursion in rational agents: Foundations for self-modifying AI</a></h2>", "sections": [{"title": "Discussion article for the meetup : Talk by Eliezer Yudkowsky: Recursion in rational agents: Foundations for self-modifying AI", "anchor": "Discussion_article_for_the_meetup___Talk_by_Eliezer_Yudkowsky__Recursion_in_rational_agents__Foundations_for_self_modifying_AI", "level": 1}, {"title": "Discussion article for the meetup : Talk by Eliezer Yudkowsky: Recursion in rational agents: Foundations for self-modifying AI", "anchor": "Discussion_article_for_the_meetup___Talk_by_Eliezer_Yudkowsky__Recursion_in_rational_agents__Foundations_for_self_modifying_AI1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T04:02:11.785Z", "modifiedAt": null, "url": null, "title": "Meetup : LW/Methods of Rationality meetup", "slug": "meetup-lw-methods-of-rationality-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.710Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fmR77s34YTopgoaNf/meetup-lw-methods-of-rationality-meetup", "pageUrlRelative": "/posts/fmR77s34YTopgoaNf/meetup-lw-methods-of-rationality-meetup", "linkUrl": "https://www.lesswrong.com/posts/fmR77s34YTopgoaNf/meetup-lw-methods-of-rationality-meetup", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%2FMethods%20of%20Rationality%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%2FMethods%20of%20Rationality%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfmR77s34YTopgoaNf%2Fmeetup-lw-methods-of-rationality-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%2FMethods%20of%20Rationality%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfmR77s34YTopgoaNf%2Fmeetup-lw-methods-of-rationality-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfmR77s34YTopgoaNf%2Fmeetup-lw-methods-of-rationality-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sc'>LW/Methods of Rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">MIT building 6, room 120 Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On Oct 18th at 7pm there will be a Less Wrong / Methods of Rationality meetup/party on the MIT campus in Building 6, room 120. There will be snacks and refreshments, and Yudkowsky will be in attendance.</p>\n\n<p><a href=\"http://intelligence.org/2013/10/01/upcoming-talks-at-harvard-and-mit/\" rel=\"nofollow\">http://intelligence.org/2013/10/01/upcoming-talks-at-harvard-and-mit/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sc'>LW/Methods of Rationality meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fmR77s34YTopgoaNf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3806068712584327e-06, "legacy": true, "legacyId": "24398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Methods_of_Rationality_meetup\">Discussion article for the meetup : <a href=\"/meetups/sc\">LW/Methods of Rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">MIT building 6, room 120 Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On Oct 18th at 7pm there will be a Less Wrong / Methods of Rationality meetup/party on the MIT campus in Building 6, room 120. There will be snacks and refreshments, and Yudkowsky will be in attendance.</p>\n\n<p><a href=\"http://intelligence.org/2013/10/01/upcoming-talks-at-harvard-and-mit/\" rel=\"nofollow\">http://intelligence.org/2013/10/01/upcoming-talks-at-harvard-and-mit/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Methods_of_Rationality_meetup1\">Discussion article for the meetup : <a href=\"/meetups/sc\">LW/Methods of Rationality meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW/Methods of Rationality meetup", "anchor": "Discussion_article_for_the_meetup___LW_Methods_of_Rationality_meetup", "level": 1}, {"title": "Discussion article for the meetup : LW/Methods of Rationality meetup", "anchor": "Discussion_article_for_the_meetup___LW_Methods_of_Rationality_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T05:49:22.771Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, weekday gathering", "slug": "meetup-moscow-weekday-gathering", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BSwgheDjN32LH6LTN/meetup-moscow-weekday-gathering", "pageUrlRelative": "/posts/BSwgheDjN32LH6LTN/meetup-moscow-weekday-gathering", "linkUrl": "https://www.lesswrong.com/posts/BSwgheDjN32LH6LTN/meetup-moscow-weekday-gathering", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20weekday%20gathering&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20weekday%20gathering%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBSwgheDjN32LH6LTN%2Fmeetup-moscow-weekday-gathering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20weekday%20gathering%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBSwgheDjN32LH6LTN%2Fmeetup-moscow-weekday-gathering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBSwgheDjN32LH6LTN%2Fmeetup-moscow-weekday-gathering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sd'>Moscow, weekday gathering</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 October 2013 07:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55.755804,37.643312</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at <strong>\u201cNeuron\u201d hackspace on Monday, 19:00.</strong> Please check <a href=\"http://umneem.org/%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8/50?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_umneem.org_notice+20131021_meet_up&amp;utm_content=20131021_meet_up&amp;utm_campaign=moscow_meetups\">this post</a> for detailed instruction how to get there. <strong>Address is Khokhlovskiy pereulok, 7/9c2</strong>.</p>\n\n<p>We will continue working with beliefs. Please take your notes from the previous meet ups if you visited any. If you do not have any list of your beliefs please think about beliefs you have, write some of them down and bring this notes with you. You will have more beliefs to work with in this case.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sd'>Moscow, weekday gathering</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BSwgheDjN32LH6LTN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3807064983519266e-06, "legacy": true, "legacyId": "24399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__weekday_gathering\">Discussion article for the meetup : <a href=\"/meetups/sd\">Moscow, weekday gathering</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 October 2013 07:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55.755804,37.643312</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at <strong>\u201cNeuron\u201d hackspace on Monday, 19:00.</strong> Please check <a href=\"http://umneem.org/%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8/50?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_umneem.org_notice+20131021_meet_up&amp;utm_content=20131021_meet_up&amp;utm_campaign=moscow_meetups\">this post</a> for detailed instruction how to get there. <strong>Address is Khokhlovskiy pereulok, 7/9c2</strong>.</p>\n\n<p>We will continue working with beliefs. Please take your notes from the previous meet ups if you visited any. If you do not have any list of your beliefs please think about beliefs you have, write some of them down and bring this notes with you. You will have more beliefs to work with in this case.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__weekday_gathering1\">Discussion article for the meetup : <a href=\"/meetups/sd\">Moscow, weekday gathering</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, weekday gathering", "anchor": "Discussion_article_for_the_meetup___Moscow__weekday_gathering", "level": 1}, {"title": "Discussion article for the meetup : Moscow, weekday gathering", "anchor": "Discussion_article_for_the_meetup___Moscow__weekday_gathering1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T16:41:22.188Z", "modifiedAt": null, "url": null, "title": "US default as a risk to mitigate", "slug": "us-default-as-a-risk-to-mitigate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:01.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bokov", "createdAt": "2010-01-11T01:11:23.480Z", "isAdmin": false, "displayName": "bokov"}, "userId": "4sgsBYAsjDHNvB7Q6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6mSSunArudYhcEok4/us-default-as-a-risk-to-mitigate", "pageUrlRelative": "/posts/6mSSunArudYhcEok4/us-default-as-a-risk-to-mitigate", "linkUrl": "https://www.lesswrong.com/posts/6mSSunArudYhcEok4/us-default-as-a-risk-to-mitigate", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20US%20default%20as%20a%20risk%20to%20mitigate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUS%20default%20as%20a%20risk%20to%20mitigate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6mSSunArudYhcEok4%2Fus-default-as-a-risk-to-mitigate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=US%20default%20as%20a%20risk%20to%20mitigate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6mSSunArudYhcEok4%2Fus-default-as-a-risk-to-mitigate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6mSSunArudYhcEok4%2Fus-default-as-a-risk-to-mitigate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 441, "htmlBody": "<p><strong>Update: Thanks everyone for the continuing thought-provoking discussion. I intend to post my decision spreadsheet, and still am looking for suggestions on where to do so. It might come in handy come February. A discussion that I find interesting has branched off on the topic of technological progress versus Malthusian Crunch, and I started <a href=\"/lw/ius/blind_spot_malthusian_crunch/\">a new article on that over here</a>.</strong></p>\n<p>&nbsp;</p>\n<p>I would like to kick off a discussion about optimal strategies to prepare for the event that the US government fails to raise the debt ceiling before the US Treasury Department's \"extraordinary measures\" are exhausted, which is estimated to happen sometime between October 17th and mid-November.</p>\n<p>This is a risk *caused* by politics, but my goal is to talk about bracing against the event itself if it happens, not the underlying politics. If you want to debate Obama-care, who is at fault, or how likely a US default actually is, please start a separate discussion.</p>\n<p>I consider this to be an indirect existential risk because if it kicks off a national or global recession, it will likely slow or halt research and philanthropic efforts at mitigating longer-term existential risks.</p>\n<p>Since there are obvious associations between unemployment/poverty and crime, civil unrest, and poor health, a global recession is likely to be to some extent a personal existential risk to those living in the United States or countries that have trade links with the United States.</p>\n<p>I notice that the <a href=\"https://www.google.com/finance?chdnp=1&amp;chdd=1&amp;chds=1&amp;chdv=1&amp;chvs=maximized&amp;chdeh=0&amp;chfdeh=0&amp;chdet=1381867200000&amp;chddm=49657&amp;chls=IntervalBasedLine&amp;cmpto=INDEXNASDAQ:.IXIC;INDEXSP:.INX&amp;cmptdms=0;0&amp;q=INDEXNASDAQ:W5000FLT&amp;ntsp=0&amp;ei=m4BdUoCFKIKQlQOu0gE\">markets do not seem to be anticipating a bad outcome</a>. But I heard one analyst advance the theory that investors simply don't believe the government can (his words) <a href=\"http://hereandnow.wbur.org/2013/08/26/business-durable-goods\">\"be that stupid\"</a>. I imagine there is more than a touch of availability bias as well-- breaching the debt ceiling might, even for fund managers who harbor no illusions about the wisdom of politicians, be up there with science-fictional scenarios like asteroid impact, peak oil, grey goo, global warming, and <del>terrorist attacks</del>. Moreover, there may be a dangerous feedback loop as the politicians in turn watch the stock indexes and conclude that \"the market says there is nothing to worry about\".</p>\n<p>So, I would like to hear what folks who are making contingency plans are doing. Especially people who have training or experience in economics and finance. What do you think the closest parallels in 20th/21st century history are for what the worst case scenario for a US government default would be like? Is there anything you would have done differently if you had known the date for the start of the 2008 recession with a +/- 2 week confidence interval, starting in two days? Or, if you did call it ahead of time, what are you glad you did?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6mSSunArudYhcEok4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 4, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "24404", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 120, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JNYchZou2xxxLsQiF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T17:16:10.993Z", "modifiedAt": null, "url": null, "title": "How do I back-up myself?", "slug": "how-do-i-back-up-myself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.161Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Locaha", "createdAt": "2012-12-26T10:59:43.142Z", "isAdmin": false, "displayName": "Locaha"}, "userId": "CnvnngECxgWgp5DQE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mTArmMXLPf8T3AAwF/how-do-i-back-up-myself", "pageUrlRelative": "/posts/mTArmMXLPf8T3AAwF/how-do-i-back-up-myself", "linkUrl": "https://www.lesswrong.com/posts/mTArmMXLPf8T3AAwF/how-do-i-back-up-myself", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20I%20back-up%20myself%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20I%20back-up%20myself%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTArmMXLPf8T3AAwF%2Fhow-do-i-back-up-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20I%20back-up%20myself%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTArmMXLPf8T3AAwF%2Fhow-do-i-back-up-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTArmMXLPf8T3AAwF%2Fhow-do-i-back-up-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>Reading a Reddit thread about the experiences of people who woke up after a long coma, got me thinking. If something similar happens to me, I might wake up with a significant portion of my memories gone (not to mention changes of personality). The question is, how do I make sure that the amnesiac future-me will continue to pursue the goals of present-me?</p>\n<p>This can be divided into 3 sub-questions:</p>\n<ol>\n<li>How to determine what goals are worth transmitting to my future-self?</li>\n<li>How to actually transmit those goals?</li>\n<li>How to persuade future-self that the goals are worth pursuing?</li>\n</ol>\n<p>Any suggestions?</p>\n<p>PS. You may ask, why am I focusing on goals? I'll just let agent Smith <a href=\"http://www.youtube.com/watch?v=hUAie-X3u8I\">speak for me</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mTArmMXLPf8T3AAwF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "24405", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T20:04:34.910Z", "modifiedAt": null, "url": null, "title": "Meetup : Bristol meetup", "slug": "meetup-bristol-meetup", "viewCount": null, "lastCommentedAt": "2018-02-11T13:13:36.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zm74gEuP5x8ngkmBk/meetup-bristol-meetup", "pageUrlRelative": "/posts/Zm74gEuP5x8ngkmBk/meetup-bristol-meetup", "linkUrl": "https://www.lesswrong.com/posts/Zm74gEuP5x8ngkmBk/meetup-bristol-meetup", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bristol%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bristol%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZm74gEuP5x8ngkmBk%2Fmeetup-bristol-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bristol%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZm74gEuP5x8ngkmBk%2Fmeetup-bristol-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZm74gEuP5x8ngkmBk%2Fmeetup-bristol-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/se\">Bristol meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 October 2013 02:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Hodgkin House, 3 Meridian Place, Bristol BS8 1JG</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll have another meetup in Bristol this upcoming <strong>Sunday, October 20</strong>, at <a rel=\"nofollow\" href=\"http://goo.gl/maps/f11Uv\">the student house where I live</a>. We'll officially start at <strong>2pm</strong> to hopefully make it reasonably convenient for everyone, but at least two of us will be there from around 12, so if you want to come earlier and hang out a bit more, let me know! (<a rel=\"nofollow\" href=\"mailto:benja.fallenstein@gmail.com\">benja.fallenstein@gmail.com</a>, or PM me.)</p>\n<p>I'll put up a LessWrong sign outside saying this, but please <strong>call me at 07463169075</strong> or (from around 2pm on) <strong>ring the buzzer marked \"Basement\"</strong> when you arrive.</p>\n<p>Also, whether or not you can attend this time, if you're interested in future meetups, please <strong><a href=\"https://groups.google.com/forum/?fromgroups#!forum/lesswrong-bristol\">join the Google group</a></strong> for organizing meetup times!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/se\">Bristol meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zm74gEuP5x8ngkmBk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.3815018681859672e-06, "legacy": true, "legacyId": "24406", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bristol_meetup\">Discussion article for the meetup : <a href=\"/meetups/se\">Bristol meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 October 2013 02:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Hodgkin House, 3 Meridian Place, Bristol BS8 1JG</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll have another meetup in Bristol this upcoming <strong>Sunday, October 20</strong>, at <a rel=\"nofollow\" href=\"http://goo.gl/maps/f11Uv\">the student house where I live</a>. We'll officially start at <strong>2pm</strong> to hopefully make it reasonably convenient for everyone, but at least two of us will be there from around 12, so if you want to come earlier and hang out a bit more, let me know! (<a rel=\"nofollow\" href=\"mailto:benja.fallenstein@gmail.com\">benja.fallenstein@gmail.com</a>, or PM me.)</p>\n<p>I'll put up a LessWrong sign outside saying this, but please <strong>call me at 07463169075</strong> or (from around 2pm on) <strong>ring the buzzer marked \"Basement\"</strong> when you arrive.</p>\n<p>Also, whether or not you can attend this time, if you're interested in future meetups, please <strong><a href=\"https://groups.google.com/forum/?fromgroups#!forum/lesswrong-bristol\">join the Google group</a></strong> for organizing meetup times!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Bristol_meetup1\">Discussion article for the meetup : <a href=\"/meetups/se\">Bristol meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bristol meetup", "anchor": "Discussion_article_for_the_meetup___Bristol_meetup", "level": 1}, {"title": "Discussion article for the meetup : Bristol meetup", "anchor": "Discussion_article_for_the_meetup___Bristol_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T20:59:30.879Z", "modifiedAt": null, "url": null, "title": "As an upload, would you join the society of full telepaths/empaths?", "slug": "as-an-upload-would-you-join-the-society-of-full-telepaths", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:30.490Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gGeHFPg9LfoRv8F9w/as-an-upload-would-you-join-the-society-of-full-telepaths", "pageUrlRelative": "/posts/gGeHFPg9LfoRv8F9w/as-an-upload-would-you-join-the-society-of-full-telepaths", "linkUrl": "https://www.lesswrong.com/posts/gGeHFPg9LfoRv8F9w/as-an-upload-would-you-join-the-society-of-full-telepaths", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20As%20an%20upload%2C%20would%20you%20join%20the%20society%20of%20full%20telepaths%2Fempaths%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAs%20an%20upload%2C%20would%20you%20join%20the%20society%20of%20full%20telepaths%2Fempaths%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgGeHFPg9LfoRv8F9w%2Fas-an-upload-would-you-join-the-society-of-full-telepaths%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=As%20an%20upload%2C%20would%20you%20join%20the%20society%20of%20full%20telepaths%2Fempaths%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgGeHFPg9LfoRv8F9w%2Fas-an-upload-would-you-join-the-society-of-full-telepaths", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgGeHFPg9LfoRv8F9w%2Fas-an-upload-would-you-join-the-society-of-full-telepaths", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p>I asked this question on IRC before and got some surprising answers.</p>\n<p>Suppose, for the sake of argument, you get cryo-preserved and eventually wake up as an upload. Maybe meat-&gt;sim transfer ends up being much easier than sim-&gt;meat or meat-&gt;meat, or something. Further suppose that you are not particularly averse to a digital-only existence, at least not enough to specifically prohibit reviving you if this is the only option. Yet further suppose that sim-you is identical to meat-you for all purposes that meat-you cared about (including all your hidden desires and character faults). Let's also preemptively assume that any other attempts to fight this hypothetical have been satisfactorily resolved, just to get this out of the way.</p>\n<p>Now, in the \"real world\", or at least in the simulation level we are at, there is no evidence that telepathy of any kind exists or is even possible. However, in the sim-world there is no technological reason it cannot be implemented in some way, for just thoughts, or just feelings, or both. There is a lot to be said for having this kind of connection between people (or sims). It gets rid of or marginalizes deception, status games, mis-communication-based biases and fallacies. On the other hand, your privacy disappears completely and so do any advantages over others the meat-you might want to retain in the digital world. And what you perceive as your faults are out there for everyone to see and feel.</p>\n<p>As a new upload, you are informed that many \"people\" decided to get integrated into the telepathic society and appear to be happy about it, with few, if any, defections. There is also the group of those who opted out, and it looks basically like your \"normal\" mundane human society. There is only a limited and strictly monitored interaction between the two worlds to prevent exploitation/manipulation.&nbsp;</p>\n<p>Would you choose to get fully integrated or stay as human-like as possible? Feel free to suggest any other alternative (suicide, start a partially integrated society, etc.).</p>\n<p>P.S. This topic has been rather extensively covered in science fiction, but I could not find a quality online discussion anywhere.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gGeHFPg9LfoRv8F9w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 7, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "24407", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 120, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-15T22:46:01.879Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 90-93", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-90-93", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QKt8RXXsmZEf3xNsP/meetup-durham-rtlw-hpmor-discussion-ch-90-93", "pageUrlRelative": "/posts/QKt8RXXsmZEf3xNsP/meetup-durham-rtlw-hpmor-discussion-ch-90-93", "linkUrl": "https://www.lesswrong.com/posts/QKt8RXXsmZEf3xNsP/meetup-durham-rtlw-hpmor-discussion-ch-90-93", "postedAtFormatted": "Tuesday, October 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2090-93&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2090-93%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQKt8RXXsmZEf3xNsP%2Fmeetup-durham-rtlw-hpmor-discussion-ch-90-93%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2090-93%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQKt8RXXsmZEf3xNsP%2Fmeetup-durham-rtlw-hpmor-discussion-ch-90-93", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQKt8RXXsmZEf3xNsP%2Fmeetup-durham-rtlw-hpmor-discussion-ch-90-93", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sf'>Durham/RTLW HPMoR discussion, ch. 90-93</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 October 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us at Fullsteam at noon this Saturday for discussion of HPMoR chapters 90-93 and metadiscussion of what we ought to do with our Saturday afternoons after we run out of HPMoR to read.</p>\n\n<p>12:00 gather at Fullsteam -- bring coffee and/or food <br />\n12:30 begin discussion <br />\n2:00ish (re)commence freestyle Saturday activities</p>\n\n<p>We'll be the ones looking like we are about to have a Thoughtful Discussion.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sf'>Durham/RTLW HPMoR discussion, ch. 90-93</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QKt8RXXsmZEf3xNsP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24409", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__90_93\">Discussion article for the meetup : <a href=\"/meetups/sf\">Durham/RTLW HPMoR discussion, ch. 90-93</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 October 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us at Fullsteam at noon this Saturday for discussion of HPMoR chapters 90-93 and metadiscussion of what we ought to do with our Saturday afternoons after we run out of HPMoR to read.</p>\n\n<p>12:00 gather at Fullsteam -- bring coffee and/or food <br>\n12:30 begin discussion <br>\n2:00ish (re)commence freestyle Saturday activities</p>\n\n<p>We'll be the ones looking like we are about to have a Thoughtful Discussion.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__90_931\">Discussion article for the meetup : <a href=\"/meetups/sf\">Durham/RTLW HPMoR discussion, ch. 90-93</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 90-93", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__90_93", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 90-93", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__90_931", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T05:55:12.029Z", "modifiedAt": null, "url": null, "title": "Bookmarklet to Hide Nested Comments", "slug": "bookmarklet-to-hide-nested-comments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:10.072Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "witzvo", "createdAt": "2012-05-10T07:45:38.575Z", "isAdmin": false, "displayName": "witzvo"}, "userId": "efsbsXkkuRESNruDe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s4ujafbmXaM3WEssj/bookmarklet-to-hide-nested-comments", "pageUrlRelative": "/posts/s4ujafbmXaM3WEssj/bookmarklet-to-hide-nested-comments", "linkUrl": "https://www.lesswrong.com/posts/s4ujafbmXaM3WEssj/bookmarklet-to-hide-nested-comments", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bookmarklet%20to%20Hide%20Nested%20Comments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABookmarklet%20to%20Hide%20Nested%20Comments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4ujafbmXaM3WEssj%2Fbookmarklet-to-hide-nested-comments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bookmarklet%20to%20Hide%20Nested%20Comments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4ujafbmXaM3WEssj%2Fbookmarklet-to-hide-nested-comments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs4ujafbmXaM3WEssj%2Fbookmarklet-to-hide-nested-comments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 478, "htmlBody": "<p>When reading comments on Less Wrong, I sometimes find myself reading reply after reply deep into a discussion when I really shouldn't be. If I had stopped and thought, I would have said, let's move on to the next thread. Similarly, I've seen comments that pose an interesting question or raise an interesting point get derailed because the first response to that post nitpicks on some issue and reply after reply delve into that. By the time I get to another child of the original comment that addresses its main point, I've exhausted myself.</p>\n<p>This is nobody's fault but my own: the nesting mechanism is sound, the default behavior is reasonable, and the show/hide features are right there, but I don't find myself using them as much as I should.</p>\n<p>As an experiment to see if I can improve my behavior I created this <strong>bookmarklet</strong>: <a href=\"javascript:var%20cl=$$('div.comment');for(var%20i=0;i&lt;cl.length;i++){a=cl[i];if(a.parentNode.parentNode.id!='comments')hidecomment(a.id.replace(/^[^_]*_/,''),a)};void(0);\">Hide Nested Comments</a></p>\n<p><strong>EDIT: </strong>The link isn't working right now. It should link to:  javascript:var%20cl=$$('div.comment');for(var%20i=0;i&lt;cl.length;i++){a=cl[i];if(a.parentNode.parentNode.id!='comments')hidecomment(a.id.replace(/^[^_]*_/,''),a)};void(0);</p>\n<p>but it doesn't. Sorry. Presumably I'm in violation of some security  policy by attempting to make a bookmarklet in a post. If you trust me, you can create a  bookmark with that as the destination yourself, but the directions below  about dragging won't work.</p>\n<p>What it does is pretty simple: <strong>it hides all comments on a post that aren't top level comments</strong>. This way, the default is that I don't see all the followups unless I decide that the subject matter is important enough that I want to wade into it. It makes it more effort to dig into subjects that interest me, but (hopefully, at least) I'll get less distracted where I shouldn't and a few more clicks won't kill me.</p>\n<p>The biggest drawback I'm aware of right now is that it means that I'll start missing interesting content that's buried under uninteresting content (arguably I'm missing most of those anyway). A fancier version might, for example, label the hidden posts with the maximum buried karma.</p>\n<p>Anyway, it's an experiment. Feel free to try it yourself and report back, or tell me why you think it's a terrible idea.<strong><br /></strong></p>\n<h3><strong>How do I try it?</strong><br /></h3>\n<p>One way is to turn on your browser's bookmark toolbar and drag the above link onto that toolbar. Click on it on the toolbar to use it. If you don't like the results, just refresh. Another way, in Firefox, is to right-click (control-click on Mac) on the link and choose bookmark this link.</p>\n<h3><strong>What's the code?</strong><br /></h3>\n<p>The javascript that makes the bookmarklet work is:</p>\n<pre>var cl=$$('div.comment'); // a list of all the comment divs on the page<br />for (var i=0; i&lt;cl.length; i++) {<br /> a=cl[i]; // take each in turn (forgot \"var\", sigh)<br /> if (a.parentNode.parentNode.id!='comments') hidecomment(a.id.replace(/^[^_]*_/,''),a); // unless it's top-level, hide it<br /> };<br />void(0);<br />// (yeah, I could use .each(...), but I didn't; it's just a hack at the moment anyway)</pre>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s4ujafbmXaM3WEssj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.382051638761067e-06, "legacy": true, "legacyId": "24413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>When reading comments on Less Wrong, I sometimes find myself reading reply after reply deep into a discussion when I really shouldn't be. If I had stopped and thought, I would have said, let's move on to the next thread. Similarly, I've seen comments that pose an interesting question or raise an interesting point get derailed because the first response to that post nitpicks on some issue and reply after reply delve into that. By the time I get to another child of the original comment that addresses its main point, I've exhausted myself.</p>\n<p>This is nobody's fault but my own: the nesting mechanism is sound, the default behavior is reasonable, and the show/hide features are right there, but I don't find myself using them as much as I should.</p>\n<p>As an experiment to see if I can improve my behavior I created this <strong>bookmarklet</strong>: <a href=\"javascript:var%20cl=$$('div.comment');for(var%20i=0;i<cl.length;i++){a=cl[i];if(a.parentNode.parentNode.id!='comments')hidecomment(a.id.replace(/^[^_]*_/,''),a)};void(0);\">Hide Nested Comments</a></p>\n<p><strong>EDIT: </strong>The link isn't working right now. It should link to:  javascript:var%20cl=$$('div.comment');for(var%20i=0;i&lt;cl.length;i++){a=cl[i];if(a.parentNode.parentNode.id!='comments')hidecomment(a.id.replace(/^[^_]*_/,''),a)};void(0);</p>\n<p>but it doesn't. Sorry. Presumably I'm in violation of some security  policy by attempting to make a bookmarklet in a post. If you trust me, you can create a  bookmark with that as the destination yourself, but the directions below  about dragging won't work.</p>\n<p>What it does is pretty simple: <strong>it hides all comments on a post that aren't top level comments</strong>. This way, the default is that I don't see all the followups unless I decide that the subject matter is important enough that I want to wade into it. It makes it more effort to dig into subjects that interest me, but (hopefully, at least) I'll get less distracted where I shouldn't and a few more clicks won't kill me.</p>\n<p>The biggest drawback I'm aware of right now is that it means that I'll start missing interesting content that's buried under uninteresting content (arguably I'm missing most of those anyway). A fancier version might, for example, label the hidden posts with the maximum buried karma.</p>\n<p>Anyway, it's an experiment. Feel free to try it yourself and report back, or tell me why you think it's a terrible idea.<strong><br></strong></p>\n<h3 id=\"How_do_I_try_it_\"><strong>How do I try it?</strong><br></h3>\n<p>One way is to turn on your browser's bookmark toolbar and drag the above link onto that toolbar. Click on it on the toolbar to use it. If you don't like the results, just refresh. Another way, in Firefox, is to right-click (control-click on Mac) on the link and choose bookmark this link.</p>\n<h3 id=\"What_s_the_code_\"><strong>What's the code?</strong><br></h3>\n<p>The javascript that makes the bookmarklet work is:</p>\n<pre>var cl=$$('div.comment'); // a list of all the comment divs on the page<br>for (var i=0; i&lt;cl.length; i++) {<br> a=cl[i]; // take each in turn (forgot \"var\", sigh)<br> if (a.parentNode.parentNode.id!='comments') hidecomment(a.id.replace(/^[^_]*_/,''),a); // unless it's top-level, hide it<br> };<br>void(0);<br>// (yeah, I could use .each(...), but I didn't; it's just a hack at the moment anyway)</pre>", "sections": [{"title": "How do I try it?", "anchor": "How_do_I_try_it_", "level": 1}, {"title": "What's the code?", "anchor": "What_s_the_code_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T10:31:27.055Z", "modifiedAt": null, "url": null, "title": "Research interests I don't currently have time to develop alone", "slug": "research-interests-i-don-t-currently-have-time-to-develop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:19.122Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DrKXZXYec8HJpsavQ/research-interests-i-don-t-currently-have-time-to-develop", "pageUrlRelative": "/posts/DrKXZXYec8HJpsavQ/research-interests-i-don-t-currently-have-time-to-develop", "linkUrl": "https://www.lesswrong.com/posts/DrKXZXYec8HJpsavQ/research-interests-i-don-t-currently-have-time-to-develop", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Research%20interests%20I%20don't%20currently%20have%20time%20to%20develop%20alone&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResearch%20interests%20I%20don't%20currently%20have%20time%20to%20develop%20alone%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrKXZXYec8HJpsavQ%2Fresearch-interests-i-don-t-currently-have-time-to-develop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Research%20interests%20I%20don't%20currently%20have%20time%20to%20develop%20alone%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrKXZXYec8HJpsavQ%2Fresearch-interests-i-don-t-currently-have-time-to-develop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrKXZXYec8HJpsavQ%2Fresearch-interests-i-don-t-currently-have-time-to-develop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 285, "htmlBody": "<p>EDIT: added the \"rights of parents\" and \"simulation hypothesis\" research interests.</p>\n<p>I've started a lot of research projects and have a lot of research interests that I don't currently have time to develop on my own. So I'm putting the research interests together on this page, and anyone can let me know if they're interested in doing any joint projects on these topics. This can range from coauthoring, to simply having a conversation about these and seeing where that goes.</p>\n<p>The possible research topics are:</p>\n<ul>\n<li><a href=\"/lw/gmx/domesticating_reduced_impact_ais/\">Reduced</a>&nbsp;<a href=\"/lw/854/satisficers_want_to_become_maximisers/\">impact</a>&nbsp;<a href=\"http://www.fhi.ox.ac.uk/utility-indifference.pdf\">AI</a> (this is one I'm certainly going to be returning to, once I have the time).</li>\n<li>The <a href=\"http://www.youtube.com/watch?v=zQTfuI-9jIo\">Fermi</a> <a href=\"https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC8QFjAA&amp;url=http%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS0094576513001148&amp;ei=4V9eUoXtAeaW0AXP4oGgCw&amp;usg=AFQjCNFkB9AlHMplmz5pGZYxoJtRhryRlw&amp;sig2=L5hMumhV0_bzRGdzJ_Mr6A&amp;bvm=bv.54176721,d.d2k\">paradox</a> and universal colonisation.</li>\n<li><a href=\"/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">Anthropic</a>&nbsp;decision <a href=\"/\">theory</a> (extending it to include quantum mechanics).</li>\n<li><a href=\"/lw/i20/even_with_default_points_systems_remain/\">How</a> to <a href=\"/lw/hv6/gains_from_trade_slug_versus_galaxy_how_much/\">divide</a> the gains from trade/to negotiate/to achieve \"fairness\"/to avoid blackmail.</li>\n<li><a href=\"http://www.youtube.com/watch?v=oAHIa651Wa0\">Oracle AI</a>&nbsp;and the (non-)differences between <a href=\"/lw/cfd/tools_versus_agents/\">tool AIs and agents</a></li>\n<li><a href=\"/lw/hvo/against_easy_superintelligence_the_unforeseen/\">General</a> <a href=\"/intelligence.org/files/PredictingAI.pdf\">assessments</a> <a href=\"/lw/gue/ai_prediction_case_study_1_the_original_dartmouth/\">of</a> <a href=\"http://fora.tv/2012/10/14/Stuart_Armstrong_How_Were_Predicting_AI\">predictions</a>&nbsp;and <a href=\"/lw/e46/competence_in_experts_summary/\">experts</a> (including <a href=\"/lw/ea8/counterfactual_resiliency_test_for_noncausal/\">counterfactual resiliency</a>).</li>\n<li>The <a href=\"http://www.aeonmagazine.com/world-views/the-strange-benefits-of-a-total-surveillance-state/\">future</a> <a href=\"http://blog.practicalethics.ox.ac.uk/2013/01/enlightened-surveillance/\">of</a> <a href=\"http://blog.practicalethics.ox.ac.uk/2013/09/how-to-get-positive-surveillance-a-few-ideas/\">surveillance</a>.</li>\n<li>The risks of <a href=\"/lw/hdw/singleton_the_risks_and_benefits_of_one_world/\">singleton world governments</a>&nbsp;and <a href=\"/lw/hna/ideas_wanted_democracy_in_an_em_world/\">mass-em worlds</a>.</li>\n<li><a href=\"/lw/hw8/caught_in_the_glare_of_two_anthropic_shadows/\">Anthropic shadows for multiple risks</a>.</li>\n<li>The potential <a href=\"/lw/hgl/the_flawed_turing_test_language_understanding_and/\">limitations</a> of the Turing test as an accurate general intelligence test.</li>\n<li><a href=\"/lw/i1h/the_idiot_savant_ai_isnt_an_idiot/\">Extending</a> <a href=\"/lw/c49/nonorthogonality_implies_uncontrollable/\">the</a> <a href=\"/lw/h0k/arguing_orthogonality_published_form/\">orthogonality thesis</a>&nbsp;to realistic AI designs.</li>\n<li><a href=\"/lw/gng/higher_than_the_most_high/\">Infinite</a>&nbsp;<a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">ethics</a>.</li>\n<li>The <a href=\"/lw/f3v/cake_or_death/\">cake or death</a> problem for value loading: whether it's fully solved or whether similar problems remain.</li>\n<li>The <a href=\"/lw/d80/malthusian_copying_mass_death_of_unhappy/\">problems</a> <a href=\"/lw/4rn/nonpersonal_preferences_of_neverexisted_people/\">with</a> <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">total utilitarianism</a>.</li>\n<li>The <a href=\"/lw/hmh/mahatma_armstrong_ceved_to_death/\">problems</a> with CEV (and potential <a href=\"/r/discussion/lw/8qb/cevinspired_models/\">solutions</a>).</li>\n<li>Whether AIXI would end <a href=\"/lw/8rl/would_aixi_protect_itself/\">protecting itself</a>&nbsp;or not.</li>\n<li>The <a href=\"http://blog.practicalethics.ox.ac.uk/2013/07/rights-of-the-parents-principles-or-practicalities/\">rights of parents versus others</a>&nbsp;<a href=\"http://blog.practicalethics.ox.ac.uk/2013/07/in-the-genetic-supermarket-should-parents-be-allowed-to-buy/\">in determining the genetic modifications of their children</a>&nbsp;(and the <a href=\"http://blog.practicalethics.ox.ac.uk/2013/05/why-are-we-not-much-much-much-better-at-parenting/\">efficiency of the markets</a> in these kinds of modifications).</li>\n<li>Exploring the consequences of the <a href=\"http://www.simulation-argument.com/\">simulation hypothesis</a>,&nbsp;as Joshua Fox <a href=\"/lw/f40/if_we_live_in_a_simulation_what_does_that_imply/\">did</a>.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DrKXZXYec8HJpsavQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 27, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "24414", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FdcxknHjeNH2MzrTj", "2qCxguXuZERZNKcNi", "svhbnSdxW3XmFXXTK", "gtGjiizupfCYCz7Lo", "7kvBxG9ZmYb5rDRiq", "nAwTGhgrdxE85Bjmg", "rPBoLRxSJ88jrthEJ", "fHSf8ACvTCvH9fFyd", "yrNW4ApXrpn2KMhxr", "YMwf9agAPTJPqgk9h", "4cD4dywgtX5pdxbHT", "8ddgsXmcqw3PLABvm", "XwxHwyLtZkQi8jtgg", "er6G2DdzevvfdZWtg", "iqBaHP38uRdHzxhPP", "npZMkydRMqAqMqbFb", "AJ3aP8iWxr6NaKi6j", "jpMwB3NKjcYXZLo8E", "PpTN7GP2FsPyHfKrs", "6bdb4F6Lif5AanRAd", "ynyemLY8YWX8rQ84f", "DfCJoqcFGBqaroc85", "pyTuR4ZbLfqpS2oMh", "vgFvnr7FefZ3s3tHp", "PBHtYurAxfm6iEpqv", "LDbnZDRidKNKQBFyY", "hCbrPFH3PFYb93dy6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T12:37:48.808Z", "modifiedAt": null, "url": null, "title": "[LINK] Spread the wings of uncertainty, the research drug version", "slug": "link-spread-the-wings-of-uncertainty-the-research-drug", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.444Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dMv4q2AMjX2xkcnTX/link-spread-the-wings-of-uncertainty-the-research-drug", "pageUrlRelative": "/posts/dMv4q2AMjX2xkcnTX/link-spread-the-wings-of-uncertainty-the-research-drug", "linkUrl": "https://www.lesswrong.com/posts/dMv4q2AMjX2xkcnTX/link-spread-the-wings-of-uncertainty-the-research-drug", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Spread%20the%20wings%20of%20uncertainty%2C%20the%20research%20drug%20version&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Spread%20the%20wings%20of%20uncertainty%2C%20the%20research%20drug%20version%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMv4q2AMjX2xkcnTX%2Flink-spread-the-wings-of-uncertainty-the-research-drug%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Spread%20the%20wings%20of%20uncertainty%2C%20the%20research%20drug%20version%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMv4q2AMjX2xkcnTX%2Flink-spread-the-wings-of-uncertainty-the-research-drug", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdMv4q2AMjX2xkcnTX%2Flink-spread-the-wings-of-uncertainty-the-research-drug", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 371, "htmlBody": "<p><em><strong>EDIT</strong>: Image now visisble!</em></p>\n<p><em>From Anders Sandberg:</em></p>\n<p>Another <a href=\"http://www.nature.com/nrd/journal/v12/n10/full/nrd4127.html\">piece</a> examining predictive performance, this time in the pharmaceutical industry. How well can industry experts predict sales?</p>\n<p>You guessed it, not very well. Not even when data really accumulated.</p>\n<p>Large pharma has less bias than small companies, but the variance still overshadows everything.</p>\n<p>&nbsp;</p>\n<blockquote>\n<p style=\"padding: 0px; line-height: 23.90625px; color: #333333; font-size: 14px; margin: 0px 0px 20px; font-family: arial, helvetica, \uff2d\uff33\uff30\u30b4\u30b7\u30c3\u30af, \uff2d\uff33\u30b4\u30b7\u30c3\u30af, Osaka, 'MS PGothic', sans-serif;\">First, most consensus forecasts were wrong, often substantially. And although consensus forecasts improved over time as more information became available, accuracy remained an issue even several years post-launch. More than 60% of the consensus forecasts in our data set were either over or under by more than 40% of the actual peak revenues (<a style=\"color: #5c7996; text-decoration: none;\" href=\"http://www.nature.com/nrd/journal/v12/n10/full/nrd4127.html#supplementary-information\" target=\"_blank\">Fig. a</a>). Although the overall median of the data set was within 4%, the distribution is wide for both under- and overestimated forecasts. Furthermore, a significant number of consensus forecasts were overly optimistic by more than 160% of the actual peak revenues of the product.</p>\n</blockquote>\n<p><img src=\"http://images.lesswrong.com/t3_iu7_0.png?v=fb3512372e65c62541b98bd5ced660dc\" alt=\"\" width=\"600\" height=\"507\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\" /></p>\n<blockquote style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">\n<p style=\"padding: 0px; line-height: 23.90625px; color: #333333; font-size: 14px; margin: 0px 0px 20px; font-family: arial, helvetica, \uff2d\uff33\uff30\u30b4\u30b7\u30c3\u30af, \uff2d\uff33\u30b4\u30b7\u30c3\u30af, Osaka, 'MS PGothic', sans-serif;\">The unanswered question in this analysis is what companies and investors ought to be doing to forecast better. We do not offer a complete answer here, but we have thoughts based on our analysis.</p>\n<p style=\"padding: 0px; line-height: 23.90625px; color: #333333; font-size: 14px; margin: 0px 0px 20px; font-family: arial, helvetica, \uff2d\uff33\uff30\u30b4\u30b7\u30c3\u30af, \uff2d\uff33\u30b4\u30b7\u30c3\u30af, Osaka, 'MS PGothic', sans-serif;\"><strong><em>Beware the wisdom of the crowd.</em></strong>&nbsp;The 'consensus' consists of well-compensated, focused professionals who have many years of experience, and we have shown that the consensus is often wrong. There should be no comfort in having one's own forecast being close to the consensus, particularly when millions or billions of dollars are on the line in an investment decision or acquisition situation.</p>\n<p style=\"padding: 0px; line-height: 23.90625px; color: #333333; font-size: 14px; margin: 0px 0px 20px; font-family: arial, helvetica, \uff2d\uff33\uff30\u30b4\u30b7\u30c3\u30af, \uff2d\uff33\u30b4\u30b7\u30c3\u30af, Osaka, 'MS PGothic', sans-serif;\"><strong><em>Broaden the aperture on what the future could look like, and rapidly adapt to new information.</em></strong>&nbsp;Much of the divergence between a forecast and what actually happens is due to the emergence of a scenario that no one foresaw: a new competitor, unfavourable clinical data or a more restrictive regulatory environment. Companies need to fight their own inertia and the tendency to make only incremental shifts in forecasting and resourcing.</p>\n<p style=\"padding: 0px; line-height: 23.90625px; color: #333333; font-size: 14px; margin: 0px 0px 20px; font-family: arial, helvetica, \uff2d\uff33\uff30\u30b4\u30b7\u30c3\u30af, \uff2d\uff33\u30b4\u30b7\u30c3\u30af, Osaka, 'MS PGothic', sans-serif;\"><strong><em>Try to improve.</em></strong>&nbsp;It appears that some companies and analysts may be better at forecasting than others (see&nbsp;<a style=\"color: #5c7996; text-decoration: none;\" href=\"http://www.nature.com/nrd/journal/v12/n10/full/nrd4127.html#supplementary-information\" target=\"_blank\">Supplementary information S1 (box)</a>). We suspect there is no magic bullet to improving the accuracy of forecasts, but the first step is conducting a self-assessment and recognizing that there may be a capability issue that needs to be addressed.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dMv4q2AMjX2xkcnTX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3824266284096632e-06, "legacy": true, "legacyId": "24415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T12:38:25.199Z", "modifiedAt": null, "url": null, "title": "Teaching rationality to kids?", "slug": "teaching-rationality-to-kids", "viewCount": null, "lastCommentedAt": "2017-08-03T09:05:53.883Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wSEGsjDPtSWkAtXce/teaching-rationality-to-kids", "pageUrlRelative": "/posts/wSEGsjDPtSWkAtXce/teaching-rationality-to-kids", "linkUrl": "https://www.lesswrong.com/posts/wSEGsjDPtSWkAtXce/teaching-rationality-to-kids", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teaching%20rationality%20to%20kids%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeaching%20rationality%20to%20kids%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSEGsjDPtSWkAtXce%2Fteaching-rationality-to-kids%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teaching%20rationality%20to%20kids%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSEGsjDPtSWkAtXce%2Fteaching-rationality-to-kids", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSEGsjDPtSWkAtXce%2Fteaching-rationality-to-kids", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<p>I'm finally getting around to reading \"Thinking, Fast and Slow\". Much of it I had already learned on LW and elsewhere. Maybe that's why my strongest impression from the book is how accessible it is. Simple sentences, clear and vivid examples, easy-to-follow exercises, a remarkable lack of references to topics not explained right away.</p>\n<p>I caught myself thinking \"This is a book I should have read as a kid\". In my first language, I think I could have managed it as early as 11 years old. Since <a href=\"http://www.ted.com/talks/james_flynn_why_our_iq_levels_are_higher_than_our_grandparents.html\">measured IQ is strongly influenced by habits of thinking</a> and <a href=\"http://intelligence.org/files/IEM.pdf\">cognitive returns can be reinvested</a>, I'm sure I would be smarter now if I had.</p>\n<p>So I have decided to buy a stack of these books and give them to kids on their, say, 12th birthdays. Then maybe Dan Dennett's \"Intuition Pumps\" a year later - and HPMOR a year after that? I would like to see more suggestions from you guys.</p>\n<p>It should be obviously better to start even earlier. So how do you teach rationality to a nine-year-old? Or a seven-year-old? Has anybody done something like that? Please name books, videos or web sites.</p>\n<p>If such media are not available, creating them should be low-hanging fruit in the quest to raise the global IQ and <a href=\"/lw/1e/raising_the_sanity_waterline/\">sanity waterline</a>. <a href=\"http://www.reddit.com/r/explainlikeimfive/&lrm;\">ELI5 writing</a> is very learnable, after all, and ELI5 type interpretations of, say, the sequences, <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">might be helpful for adults too</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wSEGsjDPtSWkAtXce", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 1.382427193417964e-06, "legacy": true, "legacyId": "24416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf", "2TPph4EGZ6trEbtku"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T13:09:44.454Z", "modifiedAt": null, "url": null, "title": "Meetup: Philadelphia-- Taleb discussion", "slug": "meetup-philadelphia-taleb-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vhHjLhNEan6wmAoxK/meetup-philadelphia-taleb-discussion", "pageUrlRelative": "/posts/vhHjLhNEan6wmAoxK/meetup-philadelphia-taleb-discussion", "linkUrl": "https://www.lesswrong.com/posts/vhHjLhNEan6wmAoxK/meetup-philadelphia-taleb-discussion", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Philadelphia--%20Taleb%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Philadelphia--%20Taleb%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvhHjLhNEan6wmAoxK%2Fmeetup-philadelphia-taleb-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Philadelphia--%20Taleb%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvhHjLhNEan6wmAoxK%2Fmeetup-philadelphia-taleb-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvhHjLhNEan6wmAoxK%2Fmeetup-philadelphia-taleb-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p><strong>When:</strong> Sunday, October 20, 1 PM</p>\n<p><strong>Where:</strong> <a href=\"http://www.namphuongphilly.com\">Nam Phuong</a> at 11 St. and Washington St. Nam Phuong is a Vietnamese restaurant notable for good food, low prices, and very little background noise.</p>\n<p><strong>Discussion topic</strong>: Taleb on <a href=\"http://edge.org/conversation/the-hard-problem\">how to think about small probabilities</a>.</p>\n<p><a href=\"https://groups.google.com/forum/#!forum/lesswrong-philadelphia\">Meetup discussion group</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vhHjLhNEan6wmAoxK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "24417", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T16:45:41.940Z", "modifiedAt": null, "url": null, "title": "[LINK] Productivity Ninja: 5 Powerful Tips For Getting More Stuff Done", "slug": "link-productivity-ninja-5-powerful-tips-for-getting-more", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WdWfyefJ54XqYhYGc/link-productivity-ninja-5-powerful-tips-for-getting-more", "pageUrlRelative": "/posts/WdWfyefJ54XqYhYGc/link-productivity-ninja-5-powerful-tips-for-getting-more", "linkUrl": "https://www.lesswrong.com/posts/WdWfyefJ54XqYhYGc/link-productivity-ninja-5-powerful-tips-for-getting-more", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Productivity%20Ninja%3A%205%20Powerful%20Tips%20For%20Getting%20More%20Stuff%20Done&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Productivity%20Ninja%3A%205%20Powerful%20Tips%20For%20Getting%20More%20Stuff%20Done%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWdWfyefJ54XqYhYGc%2Flink-productivity-ninja-5-powerful-tips-for-getting-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Productivity%20Ninja%3A%205%20Powerful%20Tips%20For%20Getting%20More%20Stuff%20Done%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWdWfyefJ54XqYhYGc%2Flink-productivity-ninja-5-powerful-tips-for-getting-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWdWfyefJ54XqYhYGc%2Flink-productivity-ninja-5-powerful-tips-for-getting-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>From the blog [Bakadesuyo](http://www.bakadesuyo.com/2013/10/productivity-ninja/):<br /><br />&gt;1) Know When You&rsquo;re At Your Best<br /><br />&gt;And plan accordingly. To be a productivity ninja focus less on time management, and more on managing your energy.<br /><br />&gt;Charlie Munger, Vice-Chairman of Berkshire Hathaway, used a system like this to make sure he was always growing.<br /><br />&gt;He identified the hours when he was at his best &mdash; and then routinely stole one of those peak hours for learning.<br /><br />&gt;&gt;Charlie Munger hit upon one strategy when he was a young lawyer. He decided that whenever his legal work was not as intellectually stimulating as he&rsquo;d like, &ldquo;I would sell the best hour of the day to myself.&rdquo; He would take otherwise billable time at the peak of his day and dedicate it to his own thinking and learning. &ldquo;And only after improving my mind &mdash; only after I&rsquo;d used my best hour improving myself &mdash; would I sell my time to my professional clients.&rdquo;</p>\n<p>&nbsp;</p>\n<p>There are four more entries, but posting them here would probably violate copyright. Anyone implement any of the suggestions listed?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WdWfyefJ54XqYhYGc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -3, "extendedScore": null, "score": 1.3826575959557561e-06, "legacy": true, "legacyId": "24418", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T19:46:57.935Z", "modifiedAt": null, "url": null, "title": "Meetup : LW meetup: Polyphasic sleep and Offline habit training", "slug": "meetup-lw-meetup-polyphasic-sleep-and-offline-habit-training", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A9pTCBq49zDkqmvqq/meetup-lw-meetup-polyphasic-sleep-and-offline-habit-training", "pageUrlRelative": "/posts/A9pTCBq49zDkqmvqq/meetup-lw-meetup-polyphasic-sleep-and-offline-habit-training", "linkUrl": "https://www.lesswrong.com/posts/A9pTCBq49zDkqmvqq/meetup-lw-meetup-polyphasic-sleep-and-offline-habit-training", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20meetup%3A%20Polyphasic%20sleep%20and%20Offline%20habit%20training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20meetup%3A%20Polyphasic%20sleep%20and%20Offline%20habit%20training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9pTCBq49zDkqmvqq%2Fmeetup-lw-meetup-polyphasic-sleep-and-offline-habit-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20meetup%3A%20Polyphasic%20sleep%20and%20Offline%20habit%20training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9pTCBq49zDkqmvqq%2Fmeetup-lw-meetup-polyphasic-sleep-and-offline-habit-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9pTCBq49zDkqmvqq%2Fmeetup-lw-meetup-polyphasic-sleep-and-offline-habit-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sg'>LW meetup: Polyphasic sleep and Offline habit training</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel House, 98 Elm St, apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Kenneth Chen, visiting from New York, will give a talk on Polyphasic Sleep. This is a technique that lets you sleep fewer hours, while potentially retaining the same level of well-being and cognitive capacity. Basically, you get a time-turner for the rest of your life, if it works for you. Ken switched to polyphasic sleep two months ago, and will share his experiences.</p>\n\n<p>Victoria will run a CFAR-style session on offline habit training. This is an effective method of installing new habits by repeated deliberate practice, as an alternative to using willpower in real time. We will brainstorm new habits, and come up with repeatable procedures to train them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sg'>LW meetup: Polyphasic sleep and Offline habit training</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A9pTCBq49zDkqmvqq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3828265345248648e-06, "legacy": true, "legacyId": "24419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_meetup__Polyphasic_sleep_and_Offline_habit_training\">Discussion article for the meetup : <a href=\"/meetups/sg\">LW meetup: Polyphasic sleep and Offline habit training</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel House, 98 Elm St, apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Kenneth Chen, visiting from New York, will give a talk on Polyphasic Sleep. This is a technique that lets you sleep fewer hours, while potentially retaining the same level of well-being and cognitive capacity. Basically, you get a time-turner for the rest of your life, if it works for you. Ken switched to polyphasic sleep two months ago, and will share his experiences.</p>\n\n<p>Victoria will run a CFAR-style session on offline habit training. This is an effective method of installing new habits by repeated deliberate practice, as an alternative to using willpower in real time. We will brainstorm new habits, and come up with repeatable procedures to train them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_meetup__Polyphasic_sleep_and_Offline_habit_training1\">Discussion article for the meetup : <a href=\"/meetups/sg\">LW meetup: Polyphasic sleep and Offline habit training</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW meetup: Polyphasic sleep and Offline habit training", "anchor": "Discussion_article_for_the_meetup___LW_meetup__Polyphasic_sleep_and_Offline_habit_training", "level": 1}, {"title": "Discussion article for the meetup : LW meetup: Polyphasic sleep and Offline habit training", "anchor": "Discussion_article_for_the_meetup___LW_meetup__Polyphasic_sleep_and_Offline_habit_training1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T20:22:10.507Z", "modifiedAt": null, "url": null, "title": "Trusting Expert Consensus", "slug": "trusting-expert-consensus", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:07.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R8YpYTq8LoD3k948L/trusting-expert-consensus", "pageUrlRelative": "/posts/R8YpYTq8LoD3k948L/trusting-expert-consensus", "linkUrl": "https://www.lesswrong.com/posts/R8YpYTq8LoD3k948L/trusting-expert-consensus", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Trusting%20Expert%20Consensus&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrusting%20Expert%20Consensus%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR8YpYTq8LoD3k948L%2Ftrusting-expert-consensus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Trusting%20Expert%20Consensus%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR8YpYTq8LoD3k948L%2Ftrusting-expert-consensus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR8YpYTq8LoD3k948L%2Ftrusting-expert-consensus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5174, "htmlBody": "<p>I recently read Bryan Caplan's <em>The Myth Of The Rational Voter. </em>It's an excellent book in a lot of ways, and one of those ways is how it got me thinking about the issue of expert consensus. It's pushed me more towards thinking that hard data on what the experts in a given field believe about their area of expertise is <em>incredibly </em>useful. Specifically, based on examples from a variety of fields (listed below the fold), I'll conclude:</p>\n<ul>\n<li>When the data show an overwhelming consensus in favor of one view (say, if the number of dissenters is less than the <a href=\"http://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/\">Lizardman's Constant</a>), this almost always ought to swamp any other evidence a non-expert might think they have regarding the issue.</li>\n<li>When a strong but not overwhelming majority of experts favor one view, non-experts should take this as strong evidence in favor of that view, but there's a greater chance that evidence could be overcome by other evidence (even from a non-expert's point of view).&nbsp;</li>\n<li>When there is only barely a majority view among experts, or no agreement at all, this is much less informative than the previous two conditions. It may indicate agnosticism is the appropriate attitude, but in many cases non-experts needn't hesitate before having their own opinion.</li>\n<li>Expert opinion should be discounted when their opinions could be predicted solely from information not relevant to the truth of the claims. This may be the only reliable, easy heuristic a non-expert can use to figure out a particular group of experts should not be trusted.</li>\n</ul>\n<div>Notable incidental conclusions relating to specific fields include:</div>\n<div>\n<ul>\n<li>Economics doesn't have the kind of overwhelming consensus you find on some issues in the natural sciences, but there still seem to be a lot of things most economists agree on.</li>\n<li>Atheists shouldn't be timid about citing the majority opinion of philosophers as evidence for atheism.</li>\n<li>Non-experts should default to thinking probably there was a historical Jesus.</li>\n<li>You should really seriously accept the mainstream scientific consensus on global warming.</li>\n</ul>\n</div>\n<div><a id=\"more\"></a>\n<h1>Evolution</h1>\n</div>\n<div>The efforts of the anti-evolution movement have had the unintended consequence of making many people, especially in the atheist/rationalist/skeptic community, especially well aware of just how strong the evidence for evolution is. Unusually good explanations of the evidence for evolution is available in <a href=\"http://www.amazon.com/Greatest-Show-Earth-Evidence-Evolution/dp/1416594795\">books</a> and <a href=\"http://talkorigins.org/\">websites</a>. And the hard data on support for evolution among scientists is pretty much what you'd expect.&nbsp;Wikipedia has a <a href=\"http://en.wikipedia.org/wiki/Level_of_support_for_evolution#Scientific_support\">good summary</a>&nbsp;of this data.&nbsp;</div>\n<div><br /></div>\n<div>Particularly noteworthy is a <a href=\"http://www.people-press.org/2009/07/09/section-5-evolution-climate-change-and-other-issues/\">Pew poll</a>&nbsp;that found that 97% of American scientists (as opposed to 61% of the general public) accept that humans and other living things have evolved over time. 87% of scientists (as opposed to a mere 32% of the general public) agree that evolution occurred due to natural processes, while only 8% of scientists claim evolution was guided by a supreme being.</div>\n<div><br /></div>\n<div>It is a little hard to know what to make of this 8% number, especially given that <a href=\"http://www.usnews.com/news/blogs/god-and-country/2009/07/16/pew-survey-a-huge-god-gap-between-scientists-and-other-americans\">a majority of scientists say they believe in God or a higher power</a>. The 8% may include so-called \"theistic evolutionists,\" who claim God worked through natural mechanisms of evolution to create life, but it may also include Michael Behe-style views that evolution <em>could not </em>have occurred through purely natural processes. The Pew poll doesn't tell us how many of each type of view is included in the 8%.&nbsp;However, other evidence suggests even Behe-style views are extremely rare among scientists, especially those with training relevant to the evolution controversy. Compare, for example, the Discovery Institute's <a href=\"http://www.dissentfromdarwin.org\">\"Dissent from Darwin\"</a> list to the National Center for Science Education's <a href=\"http://ncse.com/taking-action/project-steve\">Project Steve</a>.</div>\n<div><br /></div>\n<div>In total, based both having spent a considerable amount of time following the creation-evolution controversy, and the overwhelming support that evolution enjoys among scientists, I think those who accept the scientific consensus on evolution are safe saying something like: \"Given the overwhelming scientific consensus on evolution, it's extraordinarily unlikely that you know something all those scientists don't, or that there's a conspiracy to suppress the evidence for creationism. If you're not willing to just trust the scientists, here are some books and websites for you to read, but I'm not going to argue with you about it.\"</div>\n<h1>Economics</h1>\n<p>Expert opinion plays a central role in Caplan's <em>Myth of the Rational Voter. </em>Caplan argues that voters aren't just ill-informed about many topics, they suffer from biases that lead them to have views that <em>systematically </em>differ from those of the experts. And Caplan focuses on economics, both because it's his own area of expertise and because of it's obvious relevance for policy making.</p>\n<p>I've heard <em>The Myth of the Rational Voter </em>described as having a \"libertarian\" perspective, but while Caplan is himself a libertarian, that didn't strike me as coming across particularly strongly in the book. I say this as someone who considers himself a fan of Paul Krugman, and in fact Caplan frequently cites Krugman in support of his points.</p>\n<p>Furthermore, Caplan has hard data to back his claims up: one study found that, in 2000, 72.5% of economists mainly agreed with the claim that \"tariffs and import quotas usually reduce the general welfare of society.\" A further 20.1% said they \"agreed with provisos,\" while only 6% \"generally disagreed.\" In another example Caplan cites, nearly three-quarters of economists generally disagreed with the statement \"wage-price controls should be used to control inflation.\"</p>\n<p>However, these numbers are not as overwhelming as the numbers on scientific support for evolution. And Caplan does not mind sometimes rejecting the majority view among his colleagues. He writes:</p>\n<blockquote>\n<p>If my premise is that the economic consensus is reliable, how can I reach a conclusion that the economic consensus rejects?...</p>\n<p>This complaint would be airtight if my premise were that the economic consensus is <em>infallible. </em>But my actual premise is merely that economists, like other experts, deserve the benefit of the doubt&mdash;and that the burden of proof rests on those who would question the expert consensus. Since the rational voter assumption is part of that consensus, my responsibility as a naysayer is to refute it&mdash;which is precisely why I needed to wrote this book.</p>\n</blockquote>\n<p>I take it there's a connection between the fact that consensus in economics tends to be weaker, and Caplan's willingness to contradict the consensus on some issues. As a majority gets smaller, the chances that a scholar with the minority position might have good reasons to reject the minority view gets larger. Furthermore, when the majority view only has around 70%-75% of experts in its favor, it's also much more plausible that an informed layman could have good reasons to reject the majority view. Even though I'm not an economist, I don't hesitate to think Caplan is probably right to reject the rational voter assumption. Still, non-experts who have who know little about a debate should generally be willing to accept the conclusions of a 70+% majority.</p>\n<p>Also relevant: Krugman himself has <a href=\"http://krugman.blogs.nytimes.com/2013/01/05/ideology-and-economics/\">written</a> about the issue of consensus in economics.&nbsp;His conclusion is that \"most of what economists do is indeed fairly objective and non-ideological,\" and can produce considerable agreement among economists. However, there are some important issues in business-cycle macroeconomics where there are big, ideologically-driven disagreements. Even so, he notes that in one panel of 42 economists from top schools, 80% agreed with the statement that the 2009 American Reinvestment and Recovery Act \"significantly boosted output and employment.\"</p>\n<p>Something I almost forgot to mention, because it seems obvious to me, but probably doesn't seem obvious to other people: the views of economists probably can't be explained away as driven by political ideology. Anecdotally, people like Krugman are a problem for attempts to dismiss economists as libertarian ideologues (though I guess some people would dismiss Krugman as a neo-liberal sellout).</p>\n<p>Furthermore, Caplan argues that data from the Survey of Americans and Economists on the Economy suggests the opinions of economists mostly cannot be explained as the result of ideology or socioeconomic class. He notes that, in spite of occasional accusations of \"free-market fundamentalism\" hurled at economists, the reality is that economists are acutely aware of the weaknesses of markets.</p>\n<h1>Philosophy</h1>\n<p>This is my own field, as much as someone who's left academia can claim a field as his own. And in philosophy, we have unusually good data about what professional philosophers believe about the major disputes in their field, thanks to a <a href=\"http://philpapers.org/surveys/results.pl?affil=Target+faculty&amp;areas0=0&amp;areas_max=1&amp;grain=fine\">2009 survey</a> organized by philosophers&nbsp;David Bourget and David Chalmers through the website PhilPapers.org.</p>\n<p>Robin Hanson <a href=\"http://www.overcomingbias.com/2009/12/majoritarian-philosophy.html\">commented</a> when the survey data was released, and seemed happy to learn he agreed with the modal expert opinion on 25 out of the 30 questions surveyed. But this seemed to me, and still seems to me, the wrong reaction, given that on most questions there was <em>no </em>majority view. On the questions where there was a majority view, on about half of <em>those</em> there was still a large amount of disagreement, with less than 60% of philosophers accepting the majority view.</p>\n<p>Under such circumstances, I'm inclined to think the opinion of the experts tells us very little about what the right view is. My reaction was partly colored by experience within philosophy, which has led me to think most philosophers don't have very good reasons for their philosophical opinions and agnosticism towards particular questions is more often the right view than most philosophers would admit.</p>\n<p>It's interesting, though, to look at look at the three questions where there was <em>most </em>agreement. 81.6% of philosophers endorse non-skeptical realism about the external world, 75.1% endorse scientific realism, and 72.8% endorse atheism. Note that on all questions, there was a significant number of \"other\" answers, for example, on God, only 14.6% of philosophers endorsed theism, 5.5% were agnostic, while others endorsed various idiosyncratic views.</p>\n<p>My initial reaction to these numbers were that they were the exceptions that proved the rule. \"Non-skeptical realism about the external world,\" is basically just the claim that there's stuff (tables and chairs and trees and rocks and so on, or perhaps subatomic particles) that exists outside our minds and we can know about them. The fact that <em>only </em>about 80% of philosophers are able to agree on this seemingly commonsensical proposition just underlines how incapable of agreeing about anything philosophers are.</p>\n<p>Scientific realism, similarly, is just the view that science describes the real world, and the less-than-total agreement of philosophers on this idea is similarly disappointing. The question of God's existence, meanwhile, strikes me as having more in common with the question of whether fairies exist than most other questions on the survey.</p>\n<p>Yet as I've thought about the question more, particularly philosopher's opinions on God's existence, I've shifted towards thinking there are some questions where a strong majority of philosophers agreeing is significant. For one thing, it's striking how few theistic philosophers are interested in defending traditional arguments for the existence of God. With a few exceptions (most notably Richard Swinburne and William Lane Craig), they stick to arguing that belief in God is <em>reasonable. </em>That certainly seems worth mentioning if you're an atheist trying to persuade theists that there are no good arguments for the existence of God.</p>\n<p>Reading <em>The Myth of the Rational Voter </em>caused my views to shift further, because it made me realize the level of agreement you'll find among philosophers on the external world, science, and God is about as high as you'll find among economists on major questions of economics. This makes me think that probably atheists should <em>not </em>be shy citing strong support for atheism among philosophers as a reason to be an atheist. I wouldn't claim it's conclusive all by itself, but it does create a presumption that atheism is the most rational view, which should place a strong burden of proof on the theist to overcome.</p>\n<p>Arguably, there's also a lesson here for philosophy professors teaching undergrads (particularly in 101 classes). The standard approach to teaching philosophy is to make a show of being very even-handed in presenting any given debate, at most subtly prodding students towards the professor's preferred view. But it might be better for professors to be unapologetic about explaining to students that most philosophers are atheists and why.</p>\n<p>The value of taking the same approach to knowledge of the external world and science may be less obvious, but I suspect it may be even more important. Too many undergraduates come out of their philosophy 101 classes thinking the lesson of philosophy is that we don't really know anything, or that science is a matter of faith too. Philosophy professors should consider trying to actively fight that tendency.</p>\n<p>(What about the other issues that had relatively high levels agreement among philosophers? Professors might make a point of defending them too, though the next four most agreed-upon points from the PhilPapers survey aren't as exciting: the a priori, the analytic/synthetic distinction, the trolley problem, and cognitivism about moral judgement. At least, they're not as exciting until you connect them to other, more controversial, issues.)</p>\n<p>The broader lesson I take away from this is that you don't need to worry about looking for the kind of symptoms that Luke has cited in <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">diagnosing philosophy as a \"diseased discipline.\"</a>&nbsp;The example of philosophy suggests that when a group of experts is deeply confused about the subject-matter of their own discipline, they mostly fail to converge on any kind of strong majority agreement. That means all a non-expert needs to do is look at the data on expert opinion and if there's no strong majority agreement, they should feel pretty comfortable ignoring the experts when they feel it's necessary to do so.</p>\n<p>And yes, though I'm inclined towards agnosticism on a lot of philosophical questions, that does mean it's often OK for non-philosophers to have strong opinions on philosophical questions that philosophers themselves can't agree on. For example, I think Eliezer should feel pretty comfortable with his advocacy of one-boxing on <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb's problem</a>, even though a plurality of philosophers support two-boxing.</p>\n<h2>Philosophy vs. philosophy of religion</h2>\n<p>There's an important side-note to make here regarding the opinions of specialists in philosophy of religion. They, unlike professional philosophy as a whole, tend to be theists; 72.3% theists vs. 19.1% atheists. Some theistic philosophy-bloggers took this as vindication of theism, arguing philosophy of religion specialists know the most about the issues so they're who we should trust. On the other hand, it looks suspicious that they haven't managed to convince their colleagues in other specialties. What should we make of this?</p>\n<p>The answer becomes obvious once you know a little more about the professional dynamics surrounding philosophy of religion. Philosophy of religion is not taken very seriously in mainstream philosophy. That is not a controversial point; philosophers of religion know it and complain about it. When I was at Notre Dame (the department rated number one for philosophy of religion in the world), one of the department's philosophers of religion specifically had the job of taking the religious grad students aside and telling them they had to specialize in something other than philosophy of religion or else they would not get a job.</p>\n<p><em></em>This means atheist philosophers have basically no incentive to work in philosophy of religion. If you read the biographies of most philosophers of religion, overwhelmingly they were religious before they started studying philosophy and got into philosophy of religion because they really, really wanted to use philosophy to defend and/or elaborate on their religious beliefs. Even philosophers of religion are <a href=\"http://exapologist.blogspot.com/2013/09/recent-trend-on-exploring-bias-and.html\">beginning to notice</a> that philosophy of religion often resembles religious apologetics more than objective inquiry.</p>\n<p>I'll save the technical account of why this matters for the section on Biblical scholarship, but here I'll note that it's hard to explain away mainstream support for atheism in a parallel manner. Academics do tend to be less religious than the general population, true, but philosophers are an outlier even among other academics. A <a href=\"http://epiphenom.fieldofscience.com/2009/05/psychologists-are-least-religious-of.html\">2009 study</a> that looked at religious belief within the 20 largest academic disciplines found that only about 10 percent of academics are atheists, with psychologists being the most atheistic at 50 percent. 50 percent is extremely high compared to the general US population, but still not up to philosopher levels.</p>\n<h1>Biblical scholarship</h1>\n<p>I unfortunately don't have good data on the opinions of Biblical scholars, but I'm going to talk about it because it's an area I know a lot about (particularly historical Jesus research), and there's an important point to be made here about how non-experts can decide whether the opinions of experts are trustworthy.</p>\n<p>The point is a rather obvious one: Christian apologists like William Lane Craig often cite the (alleged) consensus of Biblical scholarship in support of their arguments, but even if the basic claims were true, it wouldn't be to find that Biblical scholars tend to have views favorable to Christianity (and Judaism) because Biblical scholars are overwhelmingly Christians and Jews. Biblical scholars tend to get their degrees from seminaries rather than secular academic departments. Atheist Biblical scholar Jacques Berlinerblau once <a href=\"/Berlinerblau once estimated you'd \">estimated</a> that at a typical meeting of a thousand members of the Society for Biblical Literature, you'd find only a couple dozen atheists (I'm not sure if he meant to include people like Bart Ehrman, who identifies as an agnostic).</p>\n<p>Non-believing Biblical scholars tend to be former believers who deconverted in part due to their studies; this is true of Bart Ehrman, Gerd L&uuml;demann, Michael Goulder, and Robert M. Price. Jacques Berlinerblau is an exception to the \"former believers\" rule. Hector Avalos is also a partial exception (his interest in the subject stems from his time a teen evangelist, but he deconverted before starting his graduate studies). I e-mailed Berlinerblau about this once, and he could think of one other scholar in the field who he didn't think had ever been a believer, but confirmed that such people are extraordinarily rare.</p>\n<p>This <em>doesn't </em>mean that outside the few dozen atheists and agnostics, Biblical scholars are all going to agree with William Lane Craig, because there are also plenty of liberal Christians like John Dominic Crossan and Marcus Borg in Biblical scholarship. It does mean that if it did turn out that most Biblical scholars had views favorable to (traditional) Christianity, it wouldn't mean much because it would be no more surprising that finding out that Muslim scholars tend to have views favorable to Islam. It's the Crossans and the Borgs that are surprising (regardless of the exact liberal Christian / conservative Christian breakdown).</p>\n<p>This common-sense argument in terms of what is and isn't surprising is reinforced by a Bayesian analysis: if the opinions of a group of experts can be predicted using information that isn't relevant to whether or not their opinions are true, knowing what their opinions are gives you no information on the claims in question. It's important to distinguish this heuristic from <em>post hoc </em>explaining away of expert opinions you don't like: conservative Christians often propose unflattering explanations for the opinions of the Crossans and the Ehrmans, but it's unlikely that they could have <em>predicted </em>their opinions using information not relevant to whether their opinions are right.</p>\n<p>So far, I probably haven't said anything that would surprise the average member of LessWrong: <em>of course </em>Jesus didn't perform any miracles or rise from the dead. So let's tackle a slightly harder question: was there a historical Jesus at all? Or, to put it somewhat more precisely: can Christianity be traced back to the followers of a single Jewish preacher who was executed by the Roman authorities in Palestine in the first half of the first century A.D.?</p>\n<p>That Christian scholars think Jesus existed isn't surprising. Even Crossan has his own version of the historical Jesus, as a wise teacher who's palatable to liberal churchgoers, that he wants to sell. Knowing the opinions of Crossan <em>et al.&nbsp;</em>may be <em>slightly </em>informative, since since many liberal Christian and Jewish scholars seem to have reconciled themselves to there being no historical Moses, so the fact that they haven't gone that route with Jesus may give us <em>some </em>information. But looking at the opinions of non-believing scholars specifically will be more informative.</p>\n<p>Though we don't have survey data because non-believing Biblical scholars are so rare, and scholars who doubt Jesus' historicity even rarer, we can make an estimate. Richard Carrier (a historical Jesus skeptic with a PhD in history from Columbia University) <a href=\"http://www.strangenotions.com/questioning-the-historicity-of-jesus/\">recently listed</a> Arthur Droge, Kurt Noll, and Thomas Thompson as people who \"agree historicity agnosticism is warranted,\" in addition to listing Thomas Brodie, Robert M. Price, and himself as being \"even more certain historicity is doubtful.\" Because historicity skeptics are so rare, and because Carrier knows the debate so well, this list is probably exhaustive at least as far as <em>publicly expressed </em>views are concerned.</p>\n<p>There are a number of difficulties here. First, it's unclear whether Berlinerblau's \"two dozen\" estimate includes agnostics. Second, categorizing Carrier's list of skeptics gets tricky in a few cases. Robert M. Price is a non-believer who's officially agnostic on whether Jesus existed, but he's written a lot about the issue and is quite confident agnosticism is the right view, justifying his inclusion on the \"more certain\" list. Brodie has <a href=\"http://freethoughtblogs.com/carrier/archives/2795\">apparently</a> tried to argue he can be a good Catholic while rejecting the idea of a historical Jesus (which, as I've already noted, is a position some people have already taken on Moses). Noll <a href=\"http://chronicle.com/article/The-Ethics-of-Being-a/47442/\">describes himself</a> as \"theistic off the job and professionally agnostic.\"</p>\n<p>Finally, Brodie has apparently been convinced there was no historical Jesus since the 1970s, but until recently was keeping quiet about it (for fear of the professional repercussions?) That raises the question of whether there might be other skeptics out there still in the closet. Still, I'd venture that probably at least 75% (Carrier's 6 divided by Berlinerblau's 24) of non-religious scholars with relevant expertise are at least fairly confident Jesus existed. Much of the remainder would be agnostic rather than full-blown mythicists. That would be strong, but not overwhelming, support for historicity.</p>\n<p>Having read both defenses of historicity by people like Ehrman, and critiques by Robert M. Price and Earl Doherty (who's an amateur, but has Carrier's endorsement), my impression is that even ignoring head counts the historicists have the better of the argument. But Carrier has said he doesn't expect most scholars to agree with him without having seen his research and arguments. He'll be presenting those in a book which is slated for release&nbsp;<a href=\"http://freethoughtblogs.com/carrier/archives/4090\">early next year</a>. I think the book <em>probably </em>won't persuade many people, but I don't think Carrier's approach is necessarily foolish. It's possible Carrier has some decisive evidence that other people just don't have access to, yet.</p>\n<h1>Climate science</h1>\n<p>Reference to global warming on LessWrong sometimes elicit negative reactions (often with references to <a href=\"/lw/9l4/politics_is_the_mindkiller_is_the_mindkiller/\">a misinterpreted version of</a>&nbsp;<a href=\"/lw/gw/\">Politics is the Mind-Killer</a>), yet on the merits global warming seems to be in the same situation as evolution. Wikipedia, again, has a <a href=\"http://en.wikipedia.org/wiki/Scientific_opinion_on_climate_change\">good summary</a> of the data on expert opinion, even better than the one for evolution.&nbsp;Highlights include:</p>\n<ul>\n<li>The <a href=\"http://www.ipcc.ch/publications_and_data/ar4/syr/en/main.html\">2007 IPCC report</a> concluded that,&nbsp;\"Most of the observed increase in global average temperatures since the mid-20th century is very likely due to the observed increase in anthropogenic GHG concentrations,\" with \"very likely\" defined as &gt;90% certainty.</li>\n<li>A survey of members of the American Meteorological Society and American Geophysical Union found that 97% agreed that global temperatures had increased during the past 100 years and 84% agreed that human-caused warming was occurring.</li>\n<li><a href=\"http://tigger.uic.edu/~pdoran/012009_Doran_final.pdf\">Other</a> <a href=\"http://www.pnas.org/content/early/2010/06/04/1003187107.full.pdf+html\">studies</a> suggest that, among the climatologists who are most active in publishing on the issue, support for the mainstream consensus on the causes of global warming is even higher: 97-98%.</li>\n</ul>\n<div>I haven't spent as much time studying the global warming debate as I have studying the creation-evolution debate, but to the extent I have studied it, the evidence seems uniformly supportive of the view that human activity has caused significant warming. Reconstructions of past temperatures seem to show that current temperatures are the warmest we've had in 2000 years and possibly much longer. Importantly, while this may or may not be the first time global temperatures have been this high, the <em>rate </em>of warming since the mid-19th century seems to be truly unprecedented.</div>\n<div><br /></div>\n<div>Michael Mann et al's so-called <a href=\"http://en.wikipedia.org/wiki/Hockey_stick_controversy\">\"hockey stick\"</a>&nbsp;graph has come under a lot of fire from skeptics, but (a) many other reconstructions have reached the same conclusion and (b) a panel formed by the National Research Council concluded that, while there were some problems with Mann et al's statistical analysis, these problems did not affect the conclusion. Furthermore, even if we didn't have the pre-1800 reconstructions, I understand that given what we know about CO2's heat-trapping properties, and given the increase in atmospheric CO2 levels due to burning fossil fuels, it would be surprising if humans <em>hadn't </em>caused significant warming.</div>\n<div><br /></div>\n<div>Sometimes I regret not knowing the climate controversy as well as I know the evolution controversy, but what I've seen so far makes me think it's extremely likely that if I did study it in greater depth, doing so would just confirm the evolution-climate change parallel. Given that, it's probably not worth my time. So lately, when people have tried to argue with me about climate change, I've refused to argue with them just as I'd refuse to argue with a creationist. While I don't know of anything that's quite the equivalent of TalkOrigins for climate change, Wikipedia has an entire set of excellent (even by Wikipedia standards) articles on climate change and I'd recommend people who want to know more start there.</div>\n<div><br /></div>\n<div>This is a good place to note how my analysis here differs from that in Vladimir_M's post <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">\"Some Heuristics for Evaluating the Soundness of the Academic Mainstream in Unfamiliar Fields.\"</a>&nbsp;Vladmir suggests \"ideological interest\" as one of two signs that a discipline may be in bad shape, and suggests climate science as an example of this. Vladmir may for all I know be right about there being a fair amount of bad research out there on the likely <em>effects </em>of climate change, but in general I think merely noticing the <em>potential </em>for ideological bias in field shouldn't greatly weaken your confidence in the opinion of a strong majority of experts.</div>\n<div><br /></div>\n<div>As discussed in the section on Biblical scholarship, the test you <em>should </em>be applying&nbsp;is whether you could predict the expert opinions on the basis of ideology, without needing to know anything about the relevant evidence. Dismissing a field merely based on the <em>potential </em>for ideological bias is an invitation to ignore any conclusion you don't like. After all, creationists have no trouble coming up with ways to explain away the scientific consensus about evolution in terms of ideology.</div>\n<h1>Other examples?</h1>\n<p>The above examples largely exhaust the cases I know about where a strong majority of experts agree on an issue, yet the issue is controversial among people outside the field. The age of the earth, the age of the universe, and the Big Bang are also technically examples because young earth creationism, but you already knew that.</p>\n<p>However, I can think of some minor examples that seem to support the principle that when there's little to no agreement among the experts, non-experts should focus on other evidence if they're going to have an opinion. For example, <a href=\"http://www.preposterousuniverse.com/blog/2013/01/17/the-most-embarrassing-graph-in-modern-physics/\">there's currently no majority view among physicists on how to interpret quantum mechanics</a>, and nothing strikes me as inherently foolish about Eliezer's <a href=\"http://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence\">advocacy of many-worlds</a>. Similarly, though I don't have data my impression is that evolutionary biologists and psychologists are divided on evolutionary psychology, but I feel no hesitation in thinking that the basic tenets of evolutionary psychology are correct.</p>\n<p>Can we find counter-examples to the heuristics suggested above? A case where, for non-obvious reasons, a strong majority of the experts in some field have converged on a crazy conclusion? The more heavily postmodern corners of academia might be a good place to look for that. In <a href=\"http://leiterreports.typepad.com/blog/2008/10/the-myth-of-the.html\">\"The Myth of the Postmodern University,\"</a> Brian Leiter argues that the influence of postmodernism on academia has been greatly exaggerated, but it has had significant impact on English, literature, and history. Leiter also quotes a reader e-mail suggesting that communication, media studies, cultural studies, and journalism may also belong on the of fields heavily influenced by postmodernism. So it may be worth looking at those.</p>\n<p>But my guess is that there may be less there than meets the eye. I don't doubt you can find many people with crazy pomo ideas in those fields, but I suspect you'd <em>also </em>find lots of people in those same fields who generally reject the crazy pomo ideas. At worst, I'd expect to find 50-odd percent support for the crazy view with a significant number of objectors. But I may be wrong about this, and it would be helpful to find good data.</p>\n<p>Another issue: it may seem surprising that I haven't been able to uncover clear evidence of a field corrupted by political ideology, the way philosophy of religion and Biblical scholarship are significantly corrupted by religious agendas. This may be because religions like Christianity have a fairly definite body of doctrine (left-wing theology aside), making bias easy to detect. Political categories like \"liberal\" or \"conservative,\" on the other hand, are squishier, which plausibly makes bias harder to detect.</p>\n<p>Indeed, though the effects may be less obvious, there does seem to be evidence for a general <a href=\"http://www.aaup.org/article/rethinking-plight-conservatives-higher-education#.Ul5kXxZUH-Y\">self-selection</a> <a href=\"http://www.insidehighered.com/news/2011/03/21/new_studies_back_theory_that_the_professoriate_is_liberal_because_of_self_selection\">effect</a>: liberal college students are more likely to pursue careers in academia. That certainly suggests you shouldn't vote for Democrats just because most college professors do, but it's unclear how it affects specific academic debates. The fact that even liberal economists tend to have many views that don't fit the liberal stereotype suggests these effects tend to get swamped by the effects of having relevant expertise, when someone's dealing with their own field. But perhaps there are other, less encouraging examples out there that I'm unaware of.</p>\n<p>Finally, left-right political bias may be much less important than nationalistic bias. I once heard it said (by Orwell? Russell? Google isn't helping me find the quote) that if you're from a small country that hasn't played an important role in world affairs, you'll likely be able to find an objective history of your country written by a foreign historian, but otherwise you're probably out of luck for an objective history of your country. Among US historians, there's an obvious counter-point that people like Howard Zinn and <a href=\"http://sundown.afro.illinois.edu\">James W. Loewen</a> don't seem at all unusual. Which is not to say they're necessary very objective, just that the biases among US historians may run in enough different directions to prevent the entire history profession from converging on nonsense.</p>\n<p>On the other hand, even among left-wing historians in the US, how many would go as far as gwern does and say <a href=\"http://www.gwern.net/Mistakes#the-american-revolution\">the American Revolution was a mistake</a>? Yet gwern seems to to have a strong case: it's definitely a bad thing when a war kills tens of thousands of people, and the US and Canada didn't turn out all that different. So, contrary to what many would expect, it may be that even left-wing US historians are <em>too positive</em> about America. This suggests that, on issues where nationalistic bias could be a problem, you should be cautious about accepting the majority opinion of historians unless you have international data.</p>\n<h1><strong>Postcript</strong></h1>\n<p>For some extra context for this post, see my discussion post on <a href=\"/lw/j09/academic_cliques/\">academic cliques</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x3zyEPFaJANB2BHmP": 1, "rWzGNdjuep56W5u2d": 1, "FkzScn5byCs9PxGsA": 1, "EvPPocx6FHcoDfygQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R8YpYTq8LoD3k948L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 41, "extendedScore": null, "score": 0.000113, "legacy": true, "legacyId": "24408", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I recently read Bryan Caplan's <em>The Myth Of The Rational Voter. </em>It's an excellent book in a lot of ways, and one of those ways is how it got me thinking about the issue of expert consensus. It's pushed me more towards thinking that hard data on what the experts in a given field believe about their area of expertise is <em>incredibly </em>useful. Specifically, based on examples from a variety of fields (listed below the fold), I'll conclude:</p>\n<ul>\n<li>When the data show an overwhelming consensus in favor of one view (say, if the number of dissenters is less than the <a href=\"http://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/\">Lizardman's Constant</a>), this almost always ought to swamp any other evidence a non-expert might think they have regarding the issue.</li>\n<li>When a strong but not overwhelming majority of experts favor one view, non-experts should take this as strong evidence in favor of that view, but there's a greater chance that evidence could be overcome by other evidence (even from a non-expert's point of view).&nbsp;</li>\n<li>When there is only barely a majority view among experts, or no agreement at all, this is much less informative than the previous two conditions. It may indicate agnosticism is the appropriate attitude, but in many cases non-experts needn't hesitate before having their own opinion.</li>\n<li>Expert opinion should be discounted when their opinions could be predicted solely from information not relevant to the truth of the claims. This may be the only reliable, easy heuristic a non-expert can use to figure out a particular group of experts should not be trusted.</li>\n</ul>\n<div>Notable incidental conclusions relating to specific fields include:</div>\n<div>\n<ul>\n<li>Economics doesn't have the kind of overwhelming consensus you find on some issues in the natural sciences, but there still seem to be a lot of things most economists agree on.</li>\n<li>Atheists shouldn't be timid about citing the majority opinion of philosophers as evidence for atheism.</li>\n<li>Non-experts should default to thinking probably there was a historical Jesus.</li>\n<li>You should really seriously accept the mainstream scientific consensus on global warming.</li>\n</ul>\n</div>\n<div><a id=\"more\"></a>\n<h1 id=\"Evolution\">Evolution</h1>\n</div>\n<div>The efforts of the anti-evolution movement have had the unintended consequence of making many people, especially in the atheist/rationalist/skeptic community, especially well aware of just how strong the evidence for evolution is. Unusually good explanations of the evidence for evolution is available in <a href=\"http://www.amazon.com/Greatest-Show-Earth-Evidence-Evolution/dp/1416594795\">books</a> and <a href=\"http://talkorigins.org/\">websites</a>. And the hard data on support for evolution among scientists is pretty much what you'd expect.&nbsp;Wikipedia has a <a href=\"http://en.wikipedia.org/wiki/Level_of_support_for_evolution#Scientific_support\">good summary</a>&nbsp;of this data.&nbsp;</div>\n<div><br></div>\n<div>Particularly noteworthy is a <a href=\"http://www.people-press.org/2009/07/09/section-5-evolution-climate-change-and-other-issues/\">Pew poll</a>&nbsp;that found that 97% of American scientists (as opposed to 61% of the general public) accept that humans and other living things have evolved over time. 87% of scientists (as opposed to a mere 32% of the general public) agree that evolution occurred due to natural processes, while only 8% of scientists claim evolution was guided by a supreme being.</div>\n<div><br></div>\n<div>It is a little hard to know what to make of this 8% number, especially given that <a href=\"http://www.usnews.com/news/blogs/god-and-country/2009/07/16/pew-survey-a-huge-god-gap-between-scientists-and-other-americans\">a majority of scientists say they believe in God or a higher power</a>. The 8% may include so-called \"theistic evolutionists,\" who claim God worked through natural mechanisms of evolution to create life, but it may also include Michael Behe-style views that evolution <em>could not </em>have occurred through purely natural processes. The Pew poll doesn't tell us how many of each type of view is included in the 8%.&nbsp;However, other evidence suggests even Behe-style views are extremely rare among scientists, especially those with training relevant to the evolution controversy. Compare, for example, the Discovery Institute's <a href=\"http://www.dissentfromdarwin.org\">\"Dissent from Darwin\"</a> list to the National Center for Science Education's <a href=\"http://ncse.com/taking-action/project-steve\">Project Steve</a>.</div>\n<div><br></div>\n<div>In total, based both having spent a considerable amount of time following the creation-evolution controversy, and the overwhelming support that evolution enjoys among scientists, I think those who accept the scientific consensus on evolution are safe saying something like: \"Given the overwhelming scientific consensus on evolution, it's extraordinarily unlikely that you know something all those scientists don't, or that there's a conspiracy to suppress the evidence for creationism. If you're not willing to just trust the scientists, here are some books and websites for you to read, but I'm not going to argue with you about it.\"</div>\n<h1 id=\"Economics\">Economics</h1>\n<p>Expert opinion plays a central role in Caplan's <em>Myth of the Rational Voter. </em>Caplan argues that voters aren't just ill-informed about many topics, they suffer from biases that lead them to have views that <em>systematically </em>differ from those of the experts. And Caplan focuses on economics, both because it's his own area of expertise and because of it's obvious relevance for policy making.</p>\n<p>I've heard <em>The Myth of the Rational Voter </em>described as having a \"libertarian\" perspective, but while Caplan is himself a libertarian, that didn't strike me as coming across particularly strongly in the book. I say this as someone who considers himself a fan of Paul Krugman, and in fact Caplan frequently cites Krugman in support of his points.</p>\n<p>Furthermore, Caplan has hard data to back his claims up: one study found that, in 2000, 72.5% of economists mainly agreed with the claim that \"tariffs and import quotas usually reduce the general welfare of society.\" A further 20.1% said they \"agreed with provisos,\" while only 6% \"generally disagreed.\" In another example Caplan cites, nearly three-quarters of economists generally disagreed with the statement \"wage-price controls should be used to control inflation.\"</p>\n<p>However, these numbers are not as overwhelming as the numbers on scientific support for evolution. And Caplan does not mind sometimes rejecting the majority view among his colleagues. He writes:</p>\n<blockquote>\n<p>If my premise is that the economic consensus is reliable, how can I reach a conclusion that the economic consensus rejects?...</p>\n<p>This complaint would be airtight if my premise were that the economic consensus is <em>infallible. </em>But my actual premise is merely that economists, like other experts, deserve the benefit of the doubt\u2014and that the burden of proof rests on those who would question the expert consensus. Since the rational voter assumption is part of that consensus, my responsibility as a naysayer is to refute it\u2014which is precisely why I needed to wrote this book.</p>\n</blockquote>\n<p>I take it there's a connection between the fact that consensus in economics tends to be weaker, and Caplan's willingness to contradict the consensus on some issues. As a majority gets smaller, the chances that a scholar with the minority position might have good reasons to reject the minority view gets larger. Furthermore, when the majority view only has around 70%-75% of experts in its favor, it's also much more plausible that an informed layman could have good reasons to reject the majority view. Even though I'm not an economist, I don't hesitate to think Caplan is probably right to reject the rational voter assumption. Still, non-experts who have who know little about a debate should generally be willing to accept the conclusions of a 70+% majority.</p>\n<p>Also relevant: Krugman himself has <a href=\"http://krugman.blogs.nytimes.com/2013/01/05/ideology-and-economics/\">written</a> about the issue of consensus in economics.&nbsp;His conclusion is that \"most of what economists do is indeed fairly objective and non-ideological,\" and can produce considerable agreement among economists. However, there are some important issues in business-cycle macroeconomics where there are big, ideologically-driven disagreements. Even so, he notes that in one panel of 42 economists from top schools, 80% agreed with the statement that the 2009 American Reinvestment and Recovery Act \"significantly boosted output and employment.\"</p>\n<p>Something I almost forgot to mention, because it seems obvious to me, but probably doesn't seem obvious to other people: the views of economists probably can't be explained away as driven by political ideology. Anecdotally, people like Krugman are a problem for attempts to dismiss economists as libertarian ideologues (though I guess some people would dismiss Krugman as a neo-liberal sellout).</p>\n<p>Furthermore, Caplan argues that data from the Survey of Americans and Economists on the Economy suggests the opinions of economists mostly cannot be explained as the result of ideology or socioeconomic class. He notes that, in spite of occasional accusations of \"free-market fundamentalism\" hurled at economists, the reality is that economists are acutely aware of the weaknesses of markets.</p>\n<h1 id=\"Philosophy\">Philosophy</h1>\n<p>This is my own field, as much as someone who's left academia can claim a field as his own. And in philosophy, we have unusually good data about what professional philosophers believe about the major disputes in their field, thanks to a <a href=\"http://philpapers.org/surveys/results.pl?affil=Target+faculty&amp;areas0=0&amp;areas_max=1&amp;grain=fine\">2009 survey</a> organized by philosophers&nbsp;David Bourget and David Chalmers through the website PhilPapers.org.</p>\n<p>Robin Hanson <a href=\"http://www.overcomingbias.com/2009/12/majoritarian-philosophy.html\">commented</a> when the survey data was released, and seemed happy to learn he agreed with the modal expert opinion on 25 out of the 30 questions surveyed. But this seemed to me, and still seems to me, the wrong reaction, given that on most questions there was <em>no </em>majority view. On the questions where there was a majority view, on about half of <em>those</em> there was still a large amount of disagreement, with less than 60% of philosophers accepting the majority view.</p>\n<p>Under such circumstances, I'm inclined to think the opinion of the experts tells us very little about what the right view is. My reaction was partly colored by experience within philosophy, which has led me to think most philosophers don't have very good reasons for their philosophical opinions and agnosticism towards particular questions is more often the right view than most philosophers would admit.</p>\n<p>It's interesting, though, to look at look at the three questions where there was <em>most </em>agreement. 81.6% of philosophers endorse non-skeptical realism about the external world, 75.1% endorse scientific realism, and 72.8% endorse atheism. Note that on all questions, there was a significant number of \"other\" answers, for example, on God, only 14.6% of philosophers endorsed theism, 5.5% were agnostic, while others endorsed various idiosyncratic views.</p>\n<p>My initial reaction to these numbers were that they were the exceptions that proved the rule. \"Non-skeptical realism about the external world,\" is basically just the claim that there's stuff (tables and chairs and trees and rocks and so on, or perhaps subatomic particles) that exists outside our minds and we can know about them. The fact that <em>only </em>about 80% of philosophers are able to agree on this seemingly commonsensical proposition just underlines how incapable of agreeing about anything philosophers are.</p>\n<p>Scientific realism, similarly, is just the view that science describes the real world, and the less-than-total agreement of philosophers on this idea is similarly disappointing. The question of God's existence, meanwhile, strikes me as having more in common with the question of whether fairies exist than most other questions on the survey.</p>\n<p>Yet as I've thought about the question more, particularly philosopher's opinions on God's existence, I've shifted towards thinking there are some questions where a strong majority of philosophers agreeing is significant. For one thing, it's striking how few theistic philosophers are interested in defending traditional arguments for the existence of God. With a few exceptions (most notably Richard Swinburne and William Lane Craig), they stick to arguing that belief in God is <em>reasonable. </em>That certainly seems worth mentioning if you're an atheist trying to persuade theists that there are no good arguments for the existence of God.</p>\n<p>Reading <em>The Myth of the Rational Voter </em>caused my views to shift further, because it made me realize the level of agreement you'll find among philosophers on the external world, science, and God is about as high as you'll find among economists on major questions of economics. This makes me think that probably atheists should <em>not </em>be shy citing strong support for atheism among philosophers as a reason to be an atheist. I wouldn't claim it's conclusive all by itself, but it does create a presumption that atheism is the most rational view, which should place a strong burden of proof on the theist to overcome.</p>\n<p>Arguably, there's also a lesson here for philosophy professors teaching undergrads (particularly in 101 classes). The standard approach to teaching philosophy is to make a show of being very even-handed in presenting any given debate, at most subtly prodding students towards the professor's preferred view. But it might be better for professors to be unapologetic about explaining to students that most philosophers are atheists and why.</p>\n<p>The value of taking the same approach to knowledge of the external world and science may be less obvious, but I suspect it may be even more important. Too many undergraduates come out of their philosophy 101 classes thinking the lesson of philosophy is that we don't really know anything, or that science is a matter of faith too. Philosophy professors should consider trying to actively fight that tendency.</p>\n<p>(What about the other issues that had relatively high levels agreement among philosophers? Professors might make a point of defending them too, though the next four most agreed-upon points from the PhilPapers survey aren't as exciting: the a priori, the analytic/synthetic distinction, the trolley problem, and cognitivism about moral judgement. At least, they're not as exciting until you connect them to other, more controversial, issues.)</p>\n<p>The broader lesson I take away from this is that you don't need to worry about looking for the kind of symptoms that Luke has cited in <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">diagnosing philosophy as a \"diseased discipline.\"</a>&nbsp;The example of philosophy suggests that when a group of experts is deeply confused about the subject-matter of their own discipline, they mostly fail to converge on any kind of strong majority agreement. That means all a non-expert needs to do is look at the data on expert opinion and if there's no strong majority agreement, they should feel pretty comfortable ignoring the experts when they feel it's necessary to do so.</p>\n<p>And yes, though I'm inclined towards agnosticism on a lot of philosophical questions, that does mean it's often OK for non-philosophers to have strong opinions on philosophical questions that philosophers themselves can't agree on. For example, I think Eliezer should feel pretty comfortable with his advocacy of one-boxing on <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb's problem</a>, even though a plurality of philosophers support two-boxing.</p>\n<h2 id=\"Philosophy_vs__philosophy_of_religion\">Philosophy vs. philosophy of religion</h2>\n<p>There's an important side-note to make here regarding the opinions of specialists in philosophy of religion. They, unlike professional philosophy as a whole, tend to be theists; 72.3% theists vs. 19.1% atheists. Some theistic philosophy-bloggers took this as vindication of theism, arguing philosophy of religion specialists know the most about the issues so they're who we should trust. On the other hand, it looks suspicious that they haven't managed to convince their colleagues in other specialties. What should we make of this?</p>\n<p>The answer becomes obvious once you know a little more about the professional dynamics surrounding philosophy of religion. Philosophy of religion is not taken very seriously in mainstream philosophy. That is not a controversial point; philosophers of religion know it and complain about it. When I was at Notre Dame (the department rated number one for philosophy of religion in the world), one of the department's philosophers of religion specifically had the job of taking the religious grad students aside and telling them they had to specialize in something other than philosophy of religion or else they would not get a job.</p>\n<p><em></em>This means atheist philosophers have basically no incentive to work in philosophy of religion. If you read the biographies of most philosophers of religion, overwhelmingly they were religious before they started studying philosophy and got into philosophy of religion because they really, really wanted to use philosophy to defend and/or elaborate on their religious beliefs. Even philosophers of religion are <a href=\"http://exapologist.blogspot.com/2013/09/recent-trend-on-exploring-bias-and.html\">beginning to notice</a> that philosophy of religion often resembles religious apologetics more than objective inquiry.</p>\n<p>I'll save the technical account of why this matters for the section on Biblical scholarship, but here I'll note that it's hard to explain away mainstream support for atheism in a parallel manner. Academics do tend to be less religious than the general population, true, but philosophers are an outlier even among other academics. A <a href=\"http://epiphenom.fieldofscience.com/2009/05/psychologists-are-least-religious-of.html\">2009 study</a> that looked at religious belief within the 20 largest academic disciplines found that only about 10 percent of academics are atheists, with psychologists being the most atheistic at 50 percent. 50 percent is extremely high compared to the general US population, but still not up to philosopher levels.</p>\n<h1 id=\"Biblical_scholarship\">Biblical scholarship</h1>\n<p>I unfortunately don't have good data on the opinions of Biblical scholars, but I'm going to talk about it because it's an area I know a lot about (particularly historical Jesus research), and there's an important point to be made here about how non-experts can decide whether the opinions of experts are trustworthy.</p>\n<p>The point is a rather obvious one: Christian apologists like William Lane Craig often cite the (alleged) consensus of Biblical scholarship in support of their arguments, but even if the basic claims were true, it wouldn't be to find that Biblical scholars tend to have views favorable to Christianity (and Judaism) because Biblical scholars are overwhelmingly Christians and Jews. Biblical scholars tend to get their degrees from seminaries rather than secular academic departments. Atheist Biblical scholar Jacques Berlinerblau once <a href=\"/Berlinerblau once estimated you'd \">estimated</a> that at a typical meeting of a thousand members of the Society for Biblical Literature, you'd find only a couple dozen atheists (I'm not sure if he meant to include people like Bart Ehrman, who identifies as an agnostic).</p>\n<p>Non-believing Biblical scholars tend to be former believers who deconverted in part due to their studies; this is true of Bart Ehrman, Gerd L\u00fcdemann, Michael Goulder, and Robert M. Price. Jacques Berlinerblau is an exception to the \"former believers\" rule. Hector Avalos is also a partial exception (his interest in the subject stems from his time a teen evangelist, but he deconverted before starting his graduate studies). I e-mailed Berlinerblau about this once, and he could think of one other scholar in the field who he didn't think had ever been a believer, but confirmed that such people are extraordinarily rare.</p>\n<p>This <em>doesn't </em>mean that outside the few dozen atheists and agnostics, Biblical scholars are all going to agree with William Lane Craig, because there are also plenty of liberal Christians like John Dominic Crossan and Marcus Borg in Biblical scholarship. It does mean that if it did turn out that most Biblical scholars had views favorable to (traditional) Christianity, it wouldn't mean much because it would be no more surprising that finding out that Muslim scholars tend to have views favorable to Islam. It's the Crossans and the Borgs that are surprising (regardless of the exact liberal Christian / conservative Christian breakdown).</p>\n<p>This common-sense argument in terms of what is and isn't surprising is reinforced by a Bayesian analysis: if the opinions of a group of experts can be predicted using information that isn't relevant to whether or not their opinions are true, knowing what their opinions are gives you no information on the claims in question. It's important to distinguish this heuristic from <em>post hoc </em>explaining away of expert opinions you don't like: conservative Christians often propose unflattering explanations for the opinions of the Crossans and the Ehrmans, but it's unlikely that they could have <em>predicted </em>their opinions using information not relevant to whether their opinions are right.</p>\n<p>So far, I probably haven't said anything that would surprise the average member of LessWrong: <em>of course </em>Jesus didn't perform any miracles or rise from the dead. So let's tackle a slightly harder question: was there a historical Jesus at all? Or, to put it somewhat more precisely: can Christianity be traced back to the followers of a single Jewish preacher who was executed by the Roman authorities in Palestine in the first half of the first century A.D.?</p>\n<p>That Christian scholars think Jesus existed isn't surprising. Even Crossan has his own version of the historical Jesus, as a wise teacher who's palatable to liberal churchgoers, that he wants to sell. Knowing the opinions of Crossan <em>et al.&nbsp;</em>may be <em>slightly </em>informative, since since many liberal Christian and Jewish scholars seem to have reconciled themselves to there being no historical Moses, so the fact that they haven't gone that route with Jesus may give us <em>some </em>information. But looking at the opinions of non-believing scholars specifically will be more informative.</p>\n<p>Though we don't have survey data because non-believing Biblical scholars are so rare, and scholars who doubt Jesus' historicity even rarer, we can make an estimate. Richard Carrier (a historical Jesus skeptic with a PhD in history from Columbia University) <a href=\"http://www.strangenotions.com/questioning-the-historicity-of-jesus/\">recently listed</a> Arthur Droge, Kurt Noll, and Thomas Thompson as people who \"agree historicity agnosticism is warranted,\" in addition to listing Thomas Brodie, Robert M. Price, and himself as being \"even more certain historicity is doubtful.\" Because historicity skeptics are so rare, and because Carrier knows the debate so well, this list is probably exhaustive at least as far as <em>publicly expressed </em>views are concerned.</p>\n<p>There are a number of difficulties here. First, it's unclear whether Berlinerblau's \"two dozen\" estimate includes agnostics. Second, categorizing Carrier's list of skeptics gets tricky in a few cases. Robert M. Price is a non-believer who's officially agnostic on whether Jesus existed, but he's written a lot about the issue and is quite confident agnosticism is the right view, justifying his inclusion on the \"more certain\" list. Brodie has <a href=\"http://freethoughtblogs.com/carrier/archives/2795\">apparently</a> tried to argue he can be a good Catholic while rejecting the idea of a historical Jesus (which, as I've already noted, is a position some people have already taken on Moses). Noll <a href=\"http://chronicle.com/article/The-Ethics-of-Being-a/47442/\">describes himself</a> as \"theistic off the job and professionally agnostic.\"</p>\n<p>Finally, Brodie has apparently been convinced there was no historical Jesus since the 1970s, but until recently was keeping quiet about it (for fear of the professional repercussions?) That raises the question of whether there might be other skeptics out there still in the closet. Still, I'd venture that probably at least 75% (Carrier's 6 divided by Berlinerblau's 24) of non-religious scholars with relevant expertise are at least fairly confident Jesus existed. Much of the remainder would be agnostic rather than full-blown mythicists. That would be strong, but not overwhelming, support for historicity.</p>\n<p>Having read both defenses of historicity by people like Ehrman, and critiques by Robert M. Price and Earl Doherty (who's an amateur, but has Carrier's endorsement), my impression is that even ignoring head counts the historicists have the better of the argument. But Carrier has said he doesn't expect most scholars to agree with him without having seen his research and arguments. He'll be presenting those in a book which is slated for release&nbsp;<a href=\"http://freethoughtblogs.com/carrier/archives/4090\">early next year</a>. I think the book <em>probably </em>won't persuade many people, but I don't think Carrier's approach is necessarily foolish. It's possible Carrier has some decisive evidence that other people just don't have access to, yet.</p>\n<h1 id=\"Climate_science\">Climate science</h1>\n<p>Reference to global warming on LessWrong sometimes elicit negative reactions (often with references to <a href=\"/lw/9l4/politics_is_the_mindkiller_is_the_mindkiller/\">a misinterpreted version of</a>&nbsp;<a href=\"/lw/gw/\">Politics is the Mind-Killer</a>), yet on the merits global warming seems to be in the same situation as evolution. Wikipedia, again, has a <a href=\"http://en.wikipedia.org/wiki/Scientific_opinion_on_climate_change\">good summary</a> of the data on expert opinion, even better than the one for evolution.&nbsp;Highlights include:</p>\n<ul>\n<li>The <a href=\"http://www.ipcc.ch/publications_and_data/ar4/syr/en/main.html\">2007 IPCC report</a> concluded that,&nbsp;\"Most of the observed increase in global average temperatures since the mid-20th century is very likely due to the observed increase in anthropogenic GHG concentrations,\" with \"very likely\" defined as &gt;90% certainty.</li>\n<li>A survey of members of the American Meteorological Society and American Geophysical Union found that 97% agreed that global temperatures had increased during the past 100 years and 84% agreed that human-caused warming was occurring.</li>\n<li><a href=\"http://tigger.uic.edu/~pdoran/012009_Doran_final.pdf\">Other</a> <a href=\"http://www.pnas.org/content/early/2010/06/04/1003187107.full.pdf+html\">studies</a> suggest that, among the climatologists who are most active in publishing on the issue, support for the mainstream consensus on the causes of global warming is even higher: 97-98%.</li>\n</ul>\n<div>I haven't spent as much time studying the global warming debate as I have studying the creation-evolution debate, but to the extent I have studied it, the evidence seems uniformly supportive of the view that human activity has caused significant warming. Reconstructions of past temperatures seem to show that current temperatures are the warmest we've had in 2000 years and possibly much longer. Importantly, while this may or may not be the first time global temperatures have been this high, the <em>rate </em>of warming since the mid-19th century seems to be truly unprecedented.</div>\n<div><br></div>\n<div>Michael Mann et al's so-called <a href=\"http://en.wikipedia.org/wiki/Hockey_stick_controversy\">\"hockey stick\"</a>&nbsp;graph has come under a lot of fire from skeptics, but (a) many other reconstructions have reached the same conclusion and (b) a panel formed by the National Research Council concluded that, while there were some problems with Mann et al's statistical analysis, these problems did not affect the conclusion. Furthermore, even if we didn't have the pre-1800 reconstructions, I understand that given what we know about CO2's heat-trapping properties, and given the increase in atmospheric CO2 levels due to burning fossil fuels, it would be surprising if humans <em>hadn't </em>caused significant warming.</div>\n<div><br></div>\n<div>Sometimes I regret not knowing the climate controversy as well as I know the evolution controversy, but what I've seen so far makes me think it's extremely likely that if I did study it in greater depth, doing so would just confirm the evolution-climate change parallel. Given that, it's probably not worth my time. So lately, when people have tried to argue with me about climate change, I've refused to argue with them just as I'd refuse to argue with a creationist. While I don't know of anything that's quite the equivalent of TalkOrigins for climate change, Wikipedia has an entire set of excellent (even by Wikipedia standards) articles on climate change and I'd recommend people who want to know more start there.</div>\n<div><br></div>\n<div>This is a good place to note how my analysis here differs from that in Vladimir_M's post <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">\"Some Heuristics for Evaluating the Soundness of the Academic Mainstream in Unfamiliar Fields.\"</a>&nbsp;Vladmir suggests \"ideological interest\" as one of two signs that a discipline may be in bad shape, and suggests climate science as an example of this. Vladmir may for all I know be right about there being a fair amount of bad research out there on the likely <em>effects </em>of climate change, but in general I think merely noticing the <em>potential </em>for ideological bias in field shouldn't greatly weaken your confidence in the opinion of a strong majority of experts.</div>\n<div><br></div>\n<div>As discussed in the section on Biblical scholarship, the test you <em>should </em>be applying&nbsp;is whether you could predict the expert opinions on the basis of ideology, without needing to know anything about the relevant evidence. Dismissing a field merely based on the <em>potential </em>for ideological bias is an invitation to ignore any conclusion you don't like. After all, creationists have no trouble coming up with ways to explain away the scientific consensus about evolution in terms of ideology.</div>\n<h1 id=\"Other_examples_\">Other examples?</h1>\n<p>The above examples largely exhaust the cases I know about where a strong majority of experts agree on an issue, yet the issue is controversial among people outside the field. The age of the earth, the age of the universe, and the Big Bang are also technically examples because young earth creationism, but you already knew that.</p>\n<p>However, I can think of some minor examples that seem to support the principle that when there's little to no agreement among the experts, non-experts should focus on other evidence if they're going to have an opinion. For example, <a href=\"http://www.preposterousuniverse.com/blog/2013/01/17/the-most-embarrassing-graph-in-modern-physics/\">there's currently no majority view among physicists on how to interpret quantum mechanics</a>, and nothing strikes me as inherently foolish about Eliezer's <a href=\"http://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence\">advocacy of many-worlds</a>. Similarly, though I don't have data my impression is that evolutionary biologists and psychologists are divided on evolutionary psychology, but I feel no hesitation in thinking that the basic tenets of evolutionary psychology are correct.</p>\n<p>Can we find counter-examples to the heuristics suggested above? A case where, for non-obvious reasons, a strong majority of the experts in some field have converged on a crazy conclusion? The more heavily postmodern corners of academia might be a good place to look for that. In <a href=\"http://leiterreports.typepad.com/blog/2008/10/the-myth-of-the.html\">\"The Myth of the Postmodern University,\"</a> Brian Leiter argues that the influence of postmodernism on academia has been greatly exaggerated, but it has had significant impact on English, literature, and history. Leiter also quotes a reader e-mail suggesting that communication, media studies, cultural studies, and journalism may also belong on the of fields heavily influenced by postmodernism. So it may be worth looking at those.</p>\n<p>But my guess is that there may be less there than meets the eye. I don't doubt you can find many people with crazy pomo ideas in those fields, but I suspect you'd <em>also </em>find lots of people in those same fields who generally reject the crazy pomo ideas. At worst, I'd expect to find 50-odd percent support for the crazy view with a significant number of objectors. But I may be wrong about this, and it would be helpful to find good data.</p>\n<p>Another issue: it may seem surprising that I haven't been able to uncover clear evidence of a field corrupted by political ideology, the way philosophy of religion and Biblical scholarship are significantly corrupted by religious agendas. This may be because religions like Christianity have a fairly definite body of doctrine (left-wing theology aside), making bias easy to detect. Political categories like \"liberal\" or \"conservative,\" on the other hand, are squishier, which plausibly makes bias harder to detect.</p>\n<p>Indeed, though the effects may be less obvious, there does seem to be evidence for a general <a href=\"http://www.aaup.org/article/rethinking-plight-conservatives-higher-education#.Ul5kXxZUH-Y\">self-selection</a> <a href=\"http://www.insidehighered.com/news/2011/03/21/new_studies_back_theory_that_the_professoriate_is_liberal_because_of_self_selection\">effect</a>: liberal college students are more likely to pursue careers in academia. That certainly suggests you shouldn't vote for Democrats just because most college professors do, but it's unclear how it affects specific academic debates. The fact that even liberal economists tend to have many views that don't fit the liberal stereotype suggests these effects tend to get swamped by the effects of having relevant expertise, when someone's dealing with their own field. But perhaps there are other, less encouraging examples out there that I'm unaware of.</p>\n<p>Finally, left-right political bias may be much less important than nationalistic bias. I once heard it said (by Orwell? Russell? Google isn't helping me find the quote) that if you're from a small country that hasn't played an important role in world affairs, you'll likely be able to find an objective history of your country written by a foreign historian, but otherwise you're probably out of luck for an objective history of your country. Among US historians, there's an obvious counter-point that people like Howard Zinn and <a href=\"http://sundown.afro.illinois.edu\">James W. Loewen</a> don't seem at all unusual. Which is not to say they're necessary very objective, just that the biases among US historians may run in enough different directions to prevent the entire history profession from converging on nonsense.</p>\n<p>On the other hand, even among left-wing historians in the US, how many would go as far as gwern does and say <a href=\"http://www.gwern.net/Mistakes#the-american-revolution\">the American Revolution was a mistake</a>? Yet gwern seems to to have a strong case: it's definitely a bad thing when a war kills tens of thousands of people, and the US and Canada didn't turn out all that different. So, contrary to what many would expect, it may be that even left-wing US historians are <em>too positive</em> about America. This suggests that, on issues where nationalistic bias could be a problem, you should be cautious about accepting the majority opinion of historians unless you have international data.</p>\n<h1 id=\"Postcript\"><strong>Postcript</strong></h1>\n<p>For some extra context for this post, see my discussion post on <a href=\"/lw/j09/academic_cliques/\">academic cliques</a>.</p>", "sections": [{"title": "Evolution", "anchor": "Evolution", "level": 1}, {"title": "Economics", "anchor": "Economics", "level": 1}, {"title": "Philosophy", "anchor": "Philosophy", "level": 1}, {"title": "Philosophy vs. philosophy of religion", "anchor": "Philosophy_vs__philosophy_of_religion", "level": 2}, {"title": "Biblical scholarship", "anchor": "Biblical_scholarship", "level": 1}, {"title": "Climate science", "anchor": "Climate_science", "level": 1}, {"title": "Other examples?", "anchor": "Other_examples_", "level": 1}, {"title": "Postcript", "anchor": "Postcript", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "81 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FwiPfF8Woe5JrzqEu", "uxsTyFLtSmxmniTzt", "9weLK2AJ9JEt2Tt8f", "fyZBtNB3Ki3fM4a6Y", "t9oXxwss8p6oXGg3W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T20:43:57.107Z", "modifiedAt": null, "url": null, "title": "[Link] Distance from Harvard", "slug": "link-distance-from-harvard", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.770Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5BYMZLxD8FdweRatS/link-distance-from-harvard", "pageUrlRelative": "/posts/5BYMZLxD8FdweRatS/link-distance-from-harvard", "linkUrl": "https://www.lesswrong.com/posts/5BYMZLxD8FdweRatS/link-distance-from-harvard", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Distance%20from%20Harvard&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Distance%20from%20Harvard%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BYMZLxD8FdweRatS%2Flink-distance-from-harvard%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Distance%20from%20Harvard%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BYMZLxD8FdweRatS%2Flink-distance-from-harvard", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BYMZLxD8FdweRatS%2Flink-distance-from-harvard", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 331, "htmlBody": "<p><strong>Related:</strong> <a href=\"/r/discussion/lw/85b/link_loss_of_local_knowledge_affecting/\">Loss of local knowledge affecting intellectual trends</a>, <a href=\"/r/discussion/lw/9j0/link_the_hyborian_age/\">The Hyborian Age</a></p>\n<p>This <a href=\"https://westhunt.wordpress.com/2013/10/15/distance-from-harvard/\">post</a> is from <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>'s excellent blog <a href=\"http://westhunt.wordpress.com/\">West Hunter</a>.</p>\n<blockquote>\n<p>Barry Marshall once said that if he had gone to Harvard, he would have <em>known </em>that stomach ulcers were caused by stress, and wouldn&rsquo;t even have considered the possibility that they might be caused by a bacterium.&nbsp; There are a number of other important innovators that sure look as if they benefited from living as far as possible from&nbsp; the sources of establishment opinion.&nbsp; Back when continental drift was officially nonsense,&nbsp; quite a few geologists in South Africa and Australia thought it must be correct &ndash; partly because there are local geological facts that are hard to explain any other way (like ancient glacial moraines in Australia whose rocks originated in South Africa) but also because physical distance translates into mental distance.</p>\n<p>Of course this does not always work &ndash; distance is useful, but not sufficient..&nbsp; Indonesia is pretty far from Harvard, but is a vast wasteland, intellectually.&nbsp; Ideally, you want a country full of people drawn from the&nbsp; populations that actually produce creative thinkers (Europeans, mostly) instead of the populations that ought to but don&rsquo;t.&nbsp; And it should be really, really far away.</p>\n<p>With the Internet and cell phones and all that,&nbsp; psychological isolation is harder to find. Once even California had some thoughts of its own, but that day is long past. <strong>If we want to keep progress from stalling out, we need people that don&rsquo;t get sucked into to the usual crap &ndash; because they <em>can&rsquo;t</em>.</strong></p>\n<p>The only real solution is interstellar colonization: the speed of light is your friend.&nbsp; A generation ship might do the job -&nbsp; even if it never arrived. It would be out there for hundreds of years, years in which the inhabitants could go their own way.&nbsp; <strong>Some of the ships would be boring, some of them would go crazy &ndash; but at least they&rsquo;d be <em>different</em>. </strong></p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5BYMZLxD8FdweRatS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 9, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "24420", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hf9ZP2wnPwZ5pcujv", "8zqy3XjQa5gk92wa2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T20:51:07.771Z", "modifiedAt": null, "url": null, "title": "[Link] Low-Hanging Poop", "slug": "link-low-hanging-poop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.671Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9CGAffRSj4TD7LkcH/link-low-hanging-poop", "pageUrlRelative": "/posts/9CGAffRSj4TD7LkcH/link-low-hanging-poop", "linkUrl": "https://www.lesswrong.com/posts/9CGAffRSj4TD7LkcH/link-low-hanging-poop", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Low-Hanging%20Poop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Low-Hanging%20Poop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9CGAffRSj4TD7LkcH%2Flink-low-hanging-poop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Low-Hanging%20Poop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9CGAffRSj4TD7LkcH%2Flink-low-hanging-poop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9CGAffRSj4TD7LkcH%2Flink-low-hanging-poop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 462, "htmlBody": "<p><strong>Related:</strong> <a href=\"/r/discussion/lw/h4u/link_son_of_lowhanging_fruit/\">Son of Low Hanging Fruit</a></p>\n<p><a href=\"https://westhunt.wordpress.com/2013/10/03/low-hanging-poop/\">Another post</a> on finding low hanging fruit from <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>'s blog <a href=\"http://westhunt.wordpress.com/\">West Hunter</a>.</p>\n<blockquote>\n<p><em>Clostridium difficile&nbsp;</em>causes a potentially serious kind of diarrhea triggered by antibiotic treatments. When the normal bacterial flora of the colon are hammered by a broad-spectrum antibiotic, <em>C. difficile</em> often takes over and causes real trouble.&nbsp; Mild cases are treated by discontinuing antibiotic therapy, which often works: if not, the doctors try oral metronidazole (Flagyl), then vancomycin , then intravenous metronidazole.&nbsp; This doesn&rsquo;t always work, and <em>C. difficile</em> infections kill about 14,000 people a year in the US.</p>\n<p>One recent <a href=\"http://www.nejm.org/doi/full/10.1056/NEJMoa1205037\">trial </a>shows that fecal bacteriotherapy, more commonly called a stool transplant, works like gangbusters, curing ~94% of patients. The trial was halted because the treatment worked so well that refusing to poopify the control group was clearly unethical.&nbsp; I read about this, but thought I&rsquo;d heard about such stool transplants some time ago.&nbsp; I had.&nbsp; It was mentioned in <em>The Making of a Surgeon</em>, by William Nolen, published in 1970. Some crazy intern &ndash; let us call him Hogan &ndash; tried a stool transplant on a woman with a <em>C. difficile infection. </em>He mixed some normal stool with chocolate milk and fed it to the lady.&nbsp; It made his boss so mad that he was dropped from the program at the end of the year.&nbsp; It also worked. It was inspired by a article in <em>Annals of Surgery,&nbsp;</em>so this certainly wasn&rsquo;t the first try.&nbsp; According to Wiki,&nbsp; there are more than 150 published reports on stool transplant, going back to 1958.</p>\n<p>So what took so damn long?&nbsp; Here we have a simple, cheap, highly effective treatment for <em>C. difficile</em> infection that has only become officially valid this year. Judging from the <em>H. pylori &nbsp;</em>story<em>,</em> it may still take years before it is in general use.</p>\n<p>Obviously, sheer disgust made it hard for doctors to embrace this treatment.&nbsp; There&rsquo;s a lesson here: in the search for low-hanging fruit,&nbsp; reconsider approaches that are embarrassing, or offensive, or downright disgusting.</p>\n<p><strong>Investigate methods were abandoned because people hated them, rather because of solid evidence showing that they didn&rsquo;t work.</strong></p>\n<p>Along those lines, no modern educational reformer utters a single syllable about corporal punishment: doesn&rsquo;t that make you suspect it&rsquo;s effective?&nbsp; I mean, why we aren&rsquo;t we caning kids anymore?&nbsp; The Egyptians said that a boy&rsquo;s ears are in his back: if you do not beat him he will not listen. Maybe they knew a thing or three.</p>\n<p>Sometimes, we hate the idea&rsquo;s <em>authors</em>: the more we hate them, the more likely we are to miss out on their correct insights. Even famous assholes had to be competent in <em>some</em> areas, or they wouldn&rsquo;t have been able to cause serious trouble.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9CGAffRSj4TD7LkcH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 46, "extendedScore": null, "score": 0.000222, "legacy": true, "legacyId": "24421", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cw6CpcBfyTyPWH3Ny"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T20:54:25.164Z", "modifiedAt": null, "url": null, "title": "Does Goal Setting Work?", "slug": "does-goal-setting-work", "viewCount": null, "lastCommentedAt": "2021-07-30T19:26:20.164Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BCKgFy2uq24JoJAjb/does-goal-setting-work", "pageUrlRelative": "/posts/BCKgFy2uq24JoJAjb/does-goal-setting-work", "linkUrl": "https://www.lesswrong.com/posts/BCKgFy2uq24JoJAjb/does-goal-setting-work", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20Goal%20Setting%20Work%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20Goal%20Setting%20Work%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCKgFy2uq24JoJAjb%2Fdoes-goal-setting-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20Goal%20Setting%20Work%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCKgFy2uq24JoJAjb%2Fdoes-goal-setting-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCKgFy2uq24JoJAjb%2Fdoes-goal-setting-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3326, "htmlBody": "<p><strong>tl;dr</strong>&nbsp;There's some disagreement over whether setting goals is a good idea. Anecdotally, enjoyment in setting goals and success at accomplishing them varies between people, for various possible reasons. Publicly setting goals may reduce motivation by providing a status gain before the goal is actually accomplished. Creative work may be better accomplished without setting goals about it. 'Process goals', 'systems' or 'habits' are probably better for motivation than 'outcome' goals. Specific goals are probably easier on motivation than unspecified goals. Having explicit set goals can cause problems in organizations, and maybe for individuals.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Introduction&nbsp;</h2>\n<blockquote>\n<p>I experimented by letting go of goals for a while and just going with the flow, but that produced even worse results. I know some people are fans of that style, but it hasn&rsquo;t worked well for me. I make much better progress &mdash; and I&rsquo;m generally happier and more fulfilled &mdash; when I wield greater conscious control over the direction of my life.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.stevepavlina.com/blog/2013/07/do-your-goals-conflict-with-your-personality\">Steve Pavlina</a>&nbsp;</span></p>\n<blockquote>\n<p class=\"MsoNormal\">The inherent problem with goal setting is related to how the brain works. Recent neuroscience research shows the brain works in a protective way, resistant to change. Therefore, any goals that require substantial behavioural change or thinking-pattern change will automatically be resisted. The brain is wired to seek rewards and avoid pain or discomfort, including fear. When fear of failure creeps into the mind of the goal setter it commences a de-motivator with a desire to return to known, comfortable behaviour and thought patterns.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.psychologytoday.com/blog/wired-success/201104/why-goal-setting-doesnt-work\">Ray Williams&nbsp;</a></span></p>\n<p class=\"MsoNormal\">I can&rsquo;t read these two quotes side by side and not be confused.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">There&rsquo;s been quite a bit of discussion within Less Wrong and CFAR about goals and goal setting. On the whole, CFAR seems to go with it being a good idea. There are some posts that recognize the possible dangers: see <a href=\"/lw/z8/image_vs_impact_can_public_commitment_be/\">patrissimo&rsquo;s post</a> on the problems with receiving status by publicly committing to goals. Basically, if you can achieve the status boost of actually accomplishing a goal by just talking about it in public, why do the hard work?&nbsp;</span>This discussion came up fairly recently with the Ottawa Less Wrong group; specifically, whether introducing group goal setting was a good idea.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I&rsquo;ve always set goals&ndash;by &lsquo;always&rsquo; I mean &lsquo;as far back as I can identify myself as some vaguely continuous version of my current self.&rsquo; At age twelve, some of my goals were concrete and immediate&ndash;&ldquo;get a time under 1 minute 12 seconds for a hundred freestyle and make the regional swim meet cut.&rdquo; Some were ambitious and unlikely&ndash;&ldquo;go to the Olympics for swimming,&rdquo; and &ldquo;be the youngest person to swim across Lake Ontario.&rdquo; Some were vague, like &ldquo;be beautiful&rdquo; or &ldquo;be a famous novelist.&rdquo; Some were chosen for bad reasons, like &ldquo;lose 10 pounds.&rdquo; My 12-year-old self wanted plenty of things that were unrealistic, or unhealthy, or incoherent, but I wanted them, and it seemed to make perfect sense to do something about getting them. I took the bus to swim practice at six am. I skipped breakfast and threw out the lunch my mom packed. Et cetera. I didn't write these goals down in a list format, but I certainly kept track of them, in diary entries among other things. I sympathize with the first quote, and the second quote confuses and kind of irritates me&ndash;seriously, Ray Williams, you have that little faith in people's abilities to change?</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">For me personally, I'm not sure what the alternative to having goals would be. Do things at random? Do whatever you have an immediate urge to do? Actually, I do know people like this. I know people whose stated desires aren&rsquo;t a good predictor of their actions at all, and I&rsquo;ve had a friend say to me &ldquo;wow, you really do plan everything. I just realized I don&rsquo;t plan anything at all.&rdquo; Some of these people get a lot of interesting stuff done. So this may just be an individual variation thing; my comfort with goal setting, and discomfort with making life up as I go, might be a result of my slightly-Aspergers need for control. It certainly comes at a cost&ndash;the cost of basing self-worth on an external criterion, and the resulting anxiety and feelings of inadequacy. I have an enormous amount of difficulty with the Buddhist virtue of &lsquo;non-striving.&rsquo; </span></p>\n<p class=\"MsoNormal\"><strong>Why the individual variation?</strong></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The concepts of the <a href=\"/lw/3w3/how_to_beat_procrastination/ \">motivation</a> <a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">equation</a> and <a href=\"http://blog.beeminder.com/nick/\">success spirals</a> give another hint at why goal-driven behaviour might vary between people. Nick Winter talks about this in his book <a href=\"http://www.nickwinter.net/the-motivation-hacker\">The Motivation Hacker</a>; he shows the difference between his past self, who had very low expectancy of success and set few goals, and his present self, with high expectancy of success and with goal-directed behaviour filling most of his time. </span></p>\n<p class=\"MsoNormal\">I actually remember a shift like this in my own life, although it was back in seventh grade and I&rsquo;ve probably editorialized the memories to make a good narrative. My sixth grade self didn&rsquo;t really have a concept of wanting something and thus doing something about it. At some point, over a period of a year or two, I experienced some minor successes. I was swimming faster, and for the first time ever, a coach made comments about my &lsquo;natural talent.&rsquo; My friends wanted to get on the honour roll with an 80% average, and in first semester, both of them did and I didn&rsquo;t; I was upset and decided to work harder, a concept I&rsquo;d never applied to school, and saw results the next semester when my average was on par with theirs. It only took a few events like that, inconsequential in themselves, before my self-image was of someone who could reliably accomplish things through hard work. My parents helpfully reinforced this self-stereotype by making proud comments about my willpower and determination.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">In hindsight I'm not sure whether this was a defining year; whether it actually made the difference, in the long run, or whether it was inevitable that some cluster of minor successes would have set off the same cascade later. It may be that some innate personality trait distinguishes the people who take those types of experiences and interpret them as success spirals from those who remained disengaged. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span lang=\"EN-GB\">The More Important Question</span></h2>\n<p class=\"MsoNormal\">Apart from the question of personal individual variation, though, there&rsquo;s a more relevant question. Given that you&rsquo;re already at a particular place on the continuum from planning-everything to doing-everything-as-you-feel-like-it, how much should you want to set goals, versus following urges? More importantly, what actions are helped versus harmed by explicit goal-setting.</p>\n<h2><strong>Creative Goals</strong></h2>\n<p class=\"MsoNormal\">As <a href=\"http://www.paulgraham.com/hs.html\">Paul Graham</a> points out, a lot of the cool things that have been accomplished in the past weren&rsquo;t done through self-discipline:</p>\n<blockquote>\n<p class=\"MsoNormal\">One of the most dangerous illusions you get from school is the idea that doing great things requires a lot of discipline. Most subjects are taught in such a boring way that it's only by discipline that you can flog yourself through them. So I was surprised when, early in college, I read a quote by Wittgenstein saying that he had no self-discipline and had never been able to deny himself anything, not even a cup of coffee.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Now I know a number of people who do great work, and it's the same with all of them. They have little discipline. They're all terrible procrastinators and find it almost impossible to make themselves do anything they're not interested in. One still hasn't sent out his half of the thank-you notes from his wedding, four years ago. Another has 26,000 emails in her inbox.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I'm not saying you can get away with zero self-discipline. You probably need about the amount you need to go running. I'm often reluctant to go running, but once I do, I enjoy it. And if I don't run for several days, I feel ill. It's the same with people who do great things. They know they'll feel bad if they don't work, and they have enough discipline to get themselves to their desks to start working. But once they get started, interest takes over, and discipline is no longer necessary.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Do you think Shakespeare was gritting his teeth and diligently trying to write Great Literature? Of course not. He was having fun. That's why he's so good.</span></p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This seems to imply that creative goals aren&rsquo;t a good place to apply goal setting. But I&rsquo;m not sure how much this is a fundamental truth. I recently made a Beeminder goal for writing fiction, and I&rsquo;ve written fifty pages since then. I actually don&rsquo;t have the writer&rsquo;s virtue of <a href=\"http://writerswritedaily.wordpress.com/\">just sitting down and writing</a>; in the past, I&rsquo;ve written most of my fiction by staying up late in a flow state. I can&rsquo;t turn this on and off, though, and more importantly, I have a life to schedule my writing around, and if the only way I can get a novel done is to stay up all night before a 12-hour shift at the hospital, I probably won&rsquo;t write that novel. I rarely want to do the hard work of writing; it&rsquo;s a lot easier to lie in bed thinking about that one awesome scene five chapters down the road and lamenting that I don&rsquo;t have time to write tonight because work in the morning.</span></p>\n<p class=\"MsoNormal\">Even if Shakespeare didn&rsquo;t write using discipline, I bet that he used <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">habits</a>. That he sat down every day with a pen and parchment and fully expected himself to write. That he had some kind of sacred writing time, not to be interrupted by urgent-but-unimportant demands. That he&rsquo;d built up some kind of success spiral around his ability to write plays that people would enjoy.&nbsp;</p>\n<h2><span lang=\"EN-GB\">Outcome versus process goals</span></h2>\n<blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Goal setting sets up an either-or polarity of success. The only true measure can either be 100% attainment or perfection, or 99% and less, which is failure. We can then excessively focus on the missing or incomplete part of our efforts, ignoring the successful parts. Fourthly, goal setting doesn't take into account random forces of chance. You can't control all the environmental variables to guarantee 100% success.</span></p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.psychologytoday.com/blog/wired-success/201104/why-goal-setting-doesnt-work\">Ray Williams</a></span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This quote talks about a type of goal that I don't actually set very often. Most of the &lsquo;bad&rsquo; goals that I had as a 12-year-old were unrealistic outcome goals, and I failed to accomplish plenty of them; I didn&rsquo;t go to the Olympics, I didn&rsquo;t swim across Lake Ontario, and I never got down to 110 pounds. But I still have the self-concept of someone who&rsquo;s good at accomplishing goals, and this is because I accomplished almost all of my more implicit &lsquo;process&rsquo; goals. I made it to swim practice seven times a week, waking up at four-thirty am year after year. This didn&rsquo;t automatically lead to Olympic success, obviously, but it was hard, and it impressed people. And yeah, I missed a few mornings, but in my mind 99% success or even 90% success at a goal is still pretty awesome.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">In fact, I can&rsquo;t think of any examples of outcome goals that I&rsquo;ve set recently. Even &ldquo;become a really awesome nurse&rdquo; feels like more of a process goal, because it's something I'll keep doing on a day-to-day basis, requiring a constant input of effort.&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://online.wsj.com/article/SB10001424052702304626104579121813075903866.html\">Scott Adams</a>, of Dilbert fame, refers to this dichotomy as &lsquo;systems&rsquo; versus &lsquo;goals&rsquo;:</span></p>\n<blockquote>\n<p class=\"MsoNormal\">Just after college, I took my first airplane trip, destination California, in search of a job. I was seated next to a businessman who was probably in his early 60s. I suppose I looked like an odd duck with my serious demeanor, bad haircut and cheap suit, clearly out of my element. I asked what he did for a living, and he told me he was the CEO of a company that made screws. He offered me some career advice. He said that every time he got a new job, he immediately started looking for a better one. For him, job seeking was not something one did when necessary. It was a continuing process... This was my first exposure to the idea that one should have a system instead of a goal. The system was to continually look for better options.</p>\n<p class=\"MsoNormal\">Throughout my career I've had my antennae up, looking for examples of people who use systems as opposed to goals. In most cases, as far as I can tell, the people who use systems do better. The systems-driven people have found a way to look at the familiar in new and more useful ways.</p>\n<p class=\"MsoNormal\">...To put it bluntly, goals are for losers. That's literally true most of the time. For example, if your goal is to lose 10 pounds, you will spend every moment until you reach the goal&mdash;if you reach it at all&mdash;feeling as if you were short of your goal. In other words, goal-oriented people exist in a state of nearly continuous failure that they hope will be temporary.</p>\n<p class=\"MsoNormal\">If you achieve your goal, you celebrate and feel terrific, but only until you realize that you just lost the thing that gave you purpose and direction. Your options are to feel empty and useless, perhaps enjoying the spoils of your success until they bore you, or to set new goals and re-enter the cycle of permanent presuccess failure.</p>\n</blockquote>\n<p class=\"MsoNormal\">I guess I agree with him&ndash;if you feel miserable when you've lost 9 pounds because you haven't accomplished your goal yet, and empty after you've lost 10 pounds because you no longer have a goal, then whatever you're calling 'goal setting' is a terrible idea. But that's not what 'goal setting' feels like to me. I feel increasingly awesome as I get closer towards a goal, and once it's done, I keep feeling awesome when I think about how I did it. Not awesome enough to never set another goal again, but awesome enough that I want to set lots more goals to get that feeling again. &nbsp;</p>\n<h2>SMART goals</h2>\n<blockquote>\n<p class=\"MsoNormal\">When I work with people as their coach and mentor, they often tell me they've set goals such as \"I want to be wealthy,\" or \"I want to be more beautiful/popular,\" \"I want a better relationship/ideal partner.\" They don't realize they've just described the symptoms or outcomes of the problems in their life. The cause of the problem, that many resist facing, is themselves. They don't realize that for a change to occur, if one is desirable, they must change themselves. Once they make the personal changes, everything around them can alter, which may make the goal irrelevant.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Ray Williams</span></p>\n<p class=\"MsoNormal\">And? Someone has to change themselves to fix the underlying problem? Are they going to do that more successfully by going with the flow?&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I think the more important dichotomy here is between vague goals and specific goals. I was exposed to the concept of SMART goals (specific, measurable, attainable, relevant, time-bound), at an early age, and though the concept has a lot of problems, the ability to Be Specific seems quite important. You can break down &ldquo;I want to be beautiful&rdquo; into subgoals like &ldquo;I&rsquo;ll learn to apply makeup properly&rdquo;, &ldquo;I&rsquo;ll eat healthy and exercise&rdquo;, &ldquo;I&rsquo;ll go clothing shopping with a friend who knows about fashion,&rdquo; etc. All of these feel more attainable than the original goal, and it&rsquo;s clear when they&rsquo;re accomplished. </span></p>\n<p class=\"MsoNormal\">That being said, I have a hard time setting any goal that isn&rsquo;t specific, attainable, and small. I&rsquo;ve <a href=\"/lw/hvw/how_i_became_more_ambitious/\">become more ambitious</a>&nbsp;since meeting lots of LW and CFAR people, but I still don&rsquo;t like large, long-term goals unless I can easily break them down into intermediate parts. This makes the idea of working on an unsolved problem, or in a startup where the events of the next year aren&rsquo;t clear, deeply frightening. And these are obviously important problems that someone needs to motivate themselves to work on.</p>\n<h2><span lang=\"EN-GB\">Problematic Goal-Driven Behaviour</span></h2>\n<blockquote>\n<p class=\"MsoNormal\">We argue that the beneficial effects of goal setting have been overstated and that systematic harm caused by goal setting has been largely ignored. We identify specific side effects associated with goal setting, including a narrow focus that neglects non-goal areas, a rise in unethical behaviour, distorted risk preferences, corrosion of organizational culture, and reduced intrinsic motivation. Rather than dispensing goal setting as a benign, over-the-counter treatment for motivation, managers and scholars need to conceptualize goal setting as a prescription-strength medication that requires careful dosing, consideration of harmful side effects, and close supervision.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.hbs.edu/faculty/Publication%20Files/09-083.pdf\">Goals Gone Wild</a>&nbsp;</span></p>\n<p class=\"MsoNormal\">This is a fairly compelling argument against goal-setting; that by setting an explicit goal and then optimizing towards that goal, you may be losing out on elements that were being accomplished better before, and maybe even rewarding actual negative behaviour. Members of an organization presumably already have assigned tasks and responsibilities, and aren&rsquo;t just doing whatever they feel like doing, but they might have done better with more freedom to prioritize their own work&ndash;the best environment is one with some structure and goals, but not too many. The phenomenon of &ldquo;teaching to the test&rdquo; for standardized testing is another example.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Given that humans <a href=\"/lw/6mj/trivers_on_selfdeception/\">aren&rsquo;t best described as unitary selves</a>, this metaphor extends to individuals. If one aspect of myself sets a personal goal to write two pages per day, another aspect of myself might respond by writing two pages on the easiest project I can think of, like a journal entry that no one will ever see. This violates the spirit of the goal it technically accomplishes.</span></p>\n<p class=\"MsoNormal\">A more problematic consideration is the relationship between intrinsic and extrinsic motivation. <a href=\"http://www.jwalkonline.org/docs/Grad%20Classes/Fall%2007/Org%20Psy/Cases/motivation%20articles/PERUSED/metaanalysis%20of%20extrinsic%20rewards.pdf\">Studies</a>&nbsp;show that rewarding or punishing children for tasks results in less intrinsic motivation, as measured by stated interest or by freely choosing to engage in the task. I&rsquo;ve noticed this tendency in myself; faced with a nursing instructor who was constantly quizzing me on the pathophysiology of my patients&rsquo; conditions, I responded by refusing to be curious about any of it or look up the answers to questions in any more detail than what she demanded, even though my previous self loved to spend hours on Google making sense of confusing diseases. If this is a problem that affects individuals setting goals for themselves&ndash;i.e. if setting a daily writing goal makes writing less fun&ndash;then I can easily see how goal-setting could be damaging.</p>\n<p class=\"MsoNormal\">I also notice that I&rsquo;m confused about the relationship between Beeminder&rsquo;s extrinsic motivation, in the form of punishment for derailing, and its effects on intrinsic motivation. Maybe the power of success spirals to increase intrinsic motivation offsets the negative effect of outside reward/punishment; or maybe the fact that users deliberately choose to use Beeminder means that it doesn&rsquo;t count as &ldquo;extrinsic.&rdquo; I&rsquo;m not sure.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span lang=\"EN-GB\">Conclusion</span></h2>\n<p class=\"MsoNormal\">There seems to be variation between individuals, in terms of both generally purposeful behaviour, and comfort level with calling it &lsquo;setting goals&rsquo;. This might be related to success spirals in the past, or it might be a factor of personality and general comfort with order versus chaos. I&rsquo;m not sure if it&rsquo;s been studied.</p>\n<p class=\"MsoNormal\">In the past, a lot of creative behaviour wasn&rsquo;t the result of deliberate goals. This may be a fundamental fact about creativity, or it may be a result of people&rsquo;s beliefs about creativity (&agrave; la <a href=\"http://www.apa.org/helpcenter/willpower-limited-resource.pdf\">ego depletion only happens if you belief in ego depletion</a>) or it may be a historical coincidence that isn&rsquo;t fundamental at all. In any case, if you aren&rsquo;t currently getting creative work done, and want to do more, I&rsquo;m not sure what the alternative is to purposefully trying to do more. Manipulating the environment to make flow easier to attain, maybe. (For example, if I quit my day job and moved to a writers' commune, I might write more without needing to try on a day-to-day basis).&nbsp;</p>\n<p class=\"MsoNormal\">Process goals, or systems, are probably better than outcome goals. Specific and realistic goals are probably better than vague and ambitious ones. A lot of this may be because it&rsquo;s easier to form habits and/or success spirals around well-specified behaviours that you can just do every day.</p>\n<p><!--EndFragment--></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Setting goals within an organization has a lot of potential problems, because workers can game the system and accomplish the letter of the goal in the easiest possible way. This likely happens within individuals too. Research shows that extrinsic motivation reduces intrinsic motivation, which is important to consider, but I'm not sure how it relates to individuals setting goals, as opposed to organizations.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 2, "udPbn9RthmgTtHMiG": 2, "fR7QfYx4JA3BnptT9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BCKgFy2uq24JoJAjb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 53, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "24422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>tl;dr</strong>&nbsp;There's some disagreement over whether setting goals is a good idea. Anecdotally, enjoyment in setting goals and success at accomplishing them varies between people, for various possible reasons. Publicly setting goals may reduce motivation by providing a status gain before the goal is actually accomplished. Creative work may be better accomplished without setting goals about it. 'Process goals', 'systems' or 'habits' are probably better for motivation than 'outcome' goals. Specific goals are probably easier on motivation than unspecified goals. Having explicit set goals can cause problems in organizations, and maybe for individuals.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Introduction_\">Introduction&nbsp;</h2>\n<blockquote>\n<p>I experimented by letting go of goals for a while and just going with the flow, but that produced even worse results. I know some people are fans of that style, but it hasn\u2019t worked well for me. I make much better progress \u2014 and I\u2019m generally happier and more fulfilled \u2014 when I wield greater conscious control over the direction of my life.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.stevepavlina.com/blog/2013/07/do-your-goals-conflict-with-your-personality\">Steve Pavlina</a>&nbsp;</span></p>\n<blockquote>\n<p class=\"MsoNormal\">The inherent problem with goal setting is related to how the brain works. Recent neuroscience research shows the brain works in a protective way, resistant to change. Therefore, any goals that require substantial behavioural change or thinking-pattern change will automatically be resisted. The brain is wired to seek rewards and avoid pain or discomfort, including fear. When fear of failure creeps into the mind of the goal setter it commences a de-motivator with a desire to return to known, comfortable behaviour and thought patterns.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.psychologytoday.com/blog/wired-success/201104/why-goal-setting-doesnt-work\">Ray Williams&nbsp;</a></span></p>\n<p class=\"MsoNormal\">I can\u2019t read these two quotes side by side and not be confused.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">There\u2019s been quite a bit of discussion within Less Wrong and CFAR about goals and goal setting. On the whole, CFAR seems to go with it being a good idea. There are some posts that recognize the possible dangers: see <a href=\"/lw/z8/image_vs_impact_can_public_commitment_be/\">patrissimo\u2019s post</a> on the problems with receiving status by publicly committing to goals. Basically, if you can achieve the status boost of actually accomplishing a goal by just talking about it in public, why do the hard work?&nbsp;</span>This discussion came up fairly recently with the Ottawa Less Wrong group; specifically, whether introducing group goal setting was a good idea.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I\u2019ve always set goals\u2013by \u2018always\u2019 I mean \u2018as far back as I can identify myself as some vaguely continuous version of my current self.\u2019 At age twelve, some of my goals were concrete and immediate\u2013\u201cget a time under 1 minute 12 seconds for a hundred freestyle and make the regional swim meet cut.\u201d Some were ambitious and unlikely\u2013\u201cgo to the Olympics for swimming,\u201d and \u201cbe the youngest person to swim across Lake Ontario.\u201d Some were vague, like \u201cbe beautiful\u201d or \u201cbe a famous novelist.\u201d Some were chosen for bad reasons, like \u201close 10 pounds.\u201d My 12-year-old self wanted plenty of things that were unrealistic, or unhealthy, or incoherent, but I wanted them, and it seemed to make perfect sense to do something about getting them. I took the bus to swim practice at six am. I skipped breakfast and threw out the lunch my mom packed. Et cetera. I didn't write these goals down in a list format, but I certainly kept track of them, in diary entries among other things. I sympathize with the first quote, and the second quote confuses and kind of irritates me\u2013seriously, Ray Williams, you have that little faith in people's abilities to change?</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">For me personally, I'm not sure what the alternative to having goals would be. Do things at random? Do whatever you have an immediate urge to do? Actually, I do know people like this. I know people whose stated desires aren\u2019t a good predictor of their actions at all, and I\u2019ve had a friend say to me \u201cwow, you really do plan everything. I just realized I don\u2019t plan anything at all.\u201d Some of these people get a lot of interesting stuff done. So this may just be an individual variation thing; my comfort with goal setting, and discomfort with making life up as I go, might be a result of my slightly-Aspergers need for control. It certainly comes at a cost\u2013the cost of basing self-worth on an external criterion, and the resulting anxiety and feelings of inadequacy. I have an enormous amount of difficulty with the Buddhist virtue of \u2018non-striving.\u2019 </span></p>\n<p class=\"MsoNormal\"><strong id=\"Why_the_individual_variation_\">Why the individual variation?</strong></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The concepts of the <a href=\"/lw/3w3/how_to_beat_procrastination/ \">motivation</a> <a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">equation</a> and <a href=\"http://blog.beeminder.com/nick/\">success spirals</a> give another hint at why goal-driven behaviour might vary between people. Nick Winter talks about this in his book <a href=\"http://www.nickwinter.net/the-motivation-hacker\">The Motivation Hacker</a>; he shows the difference between his past self, who had very low expectancy of success and set few goals, and his present self, with high expectancy of success and with goal-directed behaviour filling most of his time. </span></p>\n<p class=\"MsoNormal\">I actually remember a shift like this in my own life, although it was back in seventh grade and I\u2019ve probably editorialized the memories to make a good narrative. My sixth grade self didn\u2019t really have a concept of wanting something and thus doing something about it. At some point, over a period of a year or two, I experienced some minor successes. I was swimming faster, and for the first time ever, a coach made comments about my \u2018natural talent.\u2019 My friends wanted to get on the honour roll with an 80% average, and in first semester, both of them did and I didn\u2019t; I was upset and decided to work harder, a concept I\u2019d never applied to school, and saw results the next semester when my average was on par with theirs. It only took a few events like that, inconsequential in themselves, before my self-image was of someone who could reliably accomplish things through hard work. My parents helpfully reinforced this self-stereotype by making proud comments about my willpower and determination.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">In hindsight I'm not sure whether this was a defining year; whether it actually made the difference, in the long run, or whether it was inevitable that some cluster of minor successes would have set off the same cascade later. It may be that some innate personality trait distinguishes the people who take those types of experiences and interpret them as success spirals from those who remained disengaged. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"The_More_Important_Question\"><span lang=\"EN-GB\">The More Important Question</span></h2>\n<p class=\"MsoNormal\">Apart from the question of personal individual variation, though, there\u2019s a more relevant question. Given that you\u2019re already at a particular place on the continuum from planning-everything to doing-everything-as-you-feel-like-it, how much should you want to set goals, versus following urges? More importantly, what actions are helped versus harmed by explicit goal-setting.</p>\n<h2 id=\"Creative_Goals\"><strong>Creative Goals</strong></h2>\n<p class=\"MsoNormal\">As <a href=\"http://www.paulgraham.com/hs.html\">Paul Graham</a> points out, a lot of the cool things that have been accomplished in the past weren\u2019t done through self-discipline:</p>\n<blockquote>\n<p class=\"MsoNormal\">One of the most dangerous illusions you get from school is the idea that doing great things requires a lot of discipline. Most subjects are taught in such a boring way that it's only by discipline that you can flog yourself through them. So I was surprised when, early in college, I read a quote by Wittgenstein saying that he had no self-discipline and had never been able to deny himself anything, not even a cup of coffee.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Now I know a number of people who do great work, and it's the same with all of them. They have little discipline. They're all terrible procrastinators and find it almost impossible to make themselves do anything they're not interested in. One still hasn't sent out his half of the thank-you notes from his wedding, four years ago. Another has 26,000 emails in her inbox.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I'm not saying you can get away with zero self-discipline. You probably need about the amount you need to go running. I'm often reluctant to go running, but once I do, I enjoy it. And if I don't run for several days, I feel ill. It's the same with people who do great things. They know they'll feel bad if they don't work, and they have enough discipline to get themselves to their desks to start working. But once they get started, interest takes over, and discipline is no longer necessary.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Do you think Shakespeare was gritting his teeth and diligently trying to write Great Literature? Of course not. He was having fun. That's why he's so good.</span></p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This seems to imply that creative goals aren\u2019t a good place to apply goal setting. But I\u2019m not sure how much this is a fundamental truth. I recently made a Beeminder goal for writing fiction, and I\u2019ve written fifty pages since then. I actually don\u2019t have the writer\u2019s virtue of <a href=\"http://writerswritedaily.wordpress.com/\">just sitting down and writing</a>; in the past, I\u2019ve written most of my fiction by staying up late in a flow state. I can\u2019t turn this on and off, though, and more importantly, I have a life to schedule my writing around, and if the only way I can get a novel done is to stay up all night before a 12-hour shift at the hospital, I probably won\u2019t write that novel. I rarely want to do the hard work of writing; it\u2019s a lot easier to lie in bed thinking about that one awesome scene five chapters down the road and lamenting that I don\u2019t have time to write tonight because work in the morning.</span></p>\n<p class=\"MsoNormal\">Even if Shakespeare didn\u2019t write using discipline, I bet that he used <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">habits</a>. That he sat down every day with a pen and parchment and fully expected himself to write. That he had some kind of sacred writing time, not to be interrupted by urgent-but-unimportant demands. That he\u2019d built up some kind of success spiral around his ability to write plays that people would enjoy.&nbsp;</p>\n<h2 id=\"Outcome_versus_process_goals\"><span lang=\"EN-GB\">Outcome versus process goals</span></h2>\n<blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Goal setting sets up an either-or polarity of success. The only true measure can either be 100% attainment or perfection, or 99% and less, which is failure. We can then excessively focus on the missing or incomplete part of our efforts, ignoring the successful parts. Fourthly, goal setting doesn't take into account random forces of chance. You can't control all the environmental variables to guarantee 100% success.</span></p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.psychologytoday.com/blog/wired-success/201104/why-goal-setting-doesnt-work\">Ray Williams</a></span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This quote talks about a type of goal that I don't actually set very often. Most of the \u2018bad\u2019 goals that I had as a 12-year-old were unrealistic outcome goals, and I failed to accomplish plenty of them; I didn\u2019t go to the Olympics, I didn\u2019t swim across Lake Ontario, and I never got down to 110 pounds. But I still have the self-concept of someone who\u2019s good at accomplishing goals, and this is because I accomplished almost all of my more implicit \u2018process\u2019 goals. I made it to swim practice seven times a week, waking up at four-thirty am year after year. This didn\u2019t automatically lead to Olympic success, obviously, but it was hard, and it impressed people. And yeah, I missed a few mornings, but in my mind 99% success or even 90% success at a goal is still pretty awesome.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">In fact, I can\u2019t think of any examples of outcome goals that I\u2019ve set recently. Even \u201cbecome a really awesome nurse\u201d feels like more of a process goal, because it's something I'll keep doing on a day-to-day basis, requiring a constant input of effort.&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://online.wsj.com/article/SB10001424052702304626104579121813075903866.html\">Scott Adams</a>, of Dilbert fame, refers to this dichotomy as \u2018systems\u2019 versus \u2018goals\u2019:</span></p>\n<blockquote>\n<p class=\"MsoNormal\">Just after college, I took my first airplane trip, destination California, in search of a job. I was seated next to a businessman who was probably in his early 60s. I suppose I looked like an odd duck with my serious demeanor, bad haircut and cheap suit, clearly out of my element. I asked what he did for a living, and he told me he was the CEO of a company that made screws. He offered me some career advice. He said that every time he got a new job, he immediately started looking for a better one. For him, job seeking was not something one did when necessary. It was a continuing process... This was my first exposure to the idea that one should have a system instead of a goal. The system was to continually look for better options.</p>\n<p class=\"MsoNormal\">Throughout my career I've had my antennae up, looking for examples of people who use systems as opposed to goals. In most cases, as far as I can tell, the people who use systems do better. The systems-driven people have found a way to look at the familiar in new and more useful ways.</p>\n<p class=\"MsoNormal\">...To put it bluntly, goals are for losers. That's literally true most of the time. For example, if your goal is to lose 10 pounds, you will spend every moment until you reach the goal\u2014if you reach it at all\u2014feeling as if you were short of your goal. In other words, goal-oriented people exist in a state of nearly continuous failure that they hope will be temporary.</p>\n<p class=\"MsoNormal\">If you achieve your goal, you celebrate and feel terrific, but only until you realize that you just lost the thing that gave you purpose and direction. Your options are to feel empty and useless, perhaps enjoying the spoils of your success until they bore you, or to set new goals and re-enter the cycle of permanent presuccess failure.</p>\n</blockquote>\n<p class=\"MsoNormal\">I guess I agree with him\u2013if you feel miserable when you've lost 9 pounds because you haven't accomplished your goal yet, and empty after you've lost 10 pounds because you no longer have a goal, then whatever you're calling 'goal setting' is a terrible idea. But that's not what 'goal setting' feels like to me. I feel increasingly awesome as I get closer towards a goal, and once it's done, I keep feeling awesome when I think about how I did it. Not awesome enough to never set another goal again, but awesome enough that I want to set lots more goals to get that feeling again. &nbsp;</p>\n<h2 id=\"SMART_goals\">SMART goals</h2>\n<blockquote>\n<p class=\"MsoNormal\">When I work with people as their coach and mentor, they often tell me they've set goals such as \"I want to be wealthy,\" or \"I want to be more beautiful/popular,\" \"I want a better relationship/ideal partner.\" They don't realize they've just described the symptoms or outcomes of the problems in their life. The cause of the problem, that many resist facing, is themselves. They don't realize that for a change to occur, if one is desirable, they must change themselves. Once they make the personal changes, everything around them can alter, which may make the goal irrelevant.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Ray Williams</span></p>\n<p class=\"MsoNormal\">And? Someone has to change themselves to fix the underlying problem? Are they going to do that more successfully by going with the flow?&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I think the more important dichotomy here is between vague goals and specific goals. I was exposed to the concept of SMART goals (specific, measurable, attainable, relevant, time-bound), at an early age, and though the concept has a lot of problems, the ability to Be Specific seems quite important. You can break down \u201cI want to be beautiful\u201d into subgoals like \u201cI\u2019ll learn to apply makeup properly\u201d, \u201cI\u2019ll eat healthy and exercise\u201d, \u201cI\u2019ll go clothing shopping with a friend who knows about fashion,\u201d etc. All of these feel more attainable than the original goal, and it\u2019s clear when they\u2019re accomplished. </span></p>\n<p class=\"MsoNormal\">That being said, I have a hard time setting any goal that isn\u2019t specific, attainable, and small. I\u2019ve <a href=\"/lw/hvw/how_i_became_more_ambitious/\">become more ambitious</a>&nbsp;since meeting lots of LW and CFAR people, but I still don\u2019t like large, long-term goals unless I can easily break them down into intermediate parts. This makes the idea of working on an unsolved problem, or in a startup where the events of the next year aren\u2019t clear, deeply frightening. And these are obviously important problems that someone needs to motivate themselves to work on.</p>\n<h2 id=\"Problematic_Goal_Driven_Behaviour\"><span lang=\"EN-GB\">Problematic Goal-Driven Behaviour</span></h2>\n<blockquote>\n<p class=\"MsoNormal\">We argue that the beneficial effects of goal setting have been overstated and that systematic harm caused by goal setting has been largely ignored. We identify specific side effects associated with goal setting, including a narrow focus that neglects non-goal areas, a rise in unethical behaviour, distorted risk preferences, corrosion of organizational culture, and reduced intrinsic motivation. Rather than dispensing goal setting as a benign, over-the-counter treatment for motivation, managers and scholars need to conceptualize goal setting as a prescription-strength medication that requires careful dosing, consideration of harmful side effects, and close supervision.</p>\n</blockquote>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.hbs.edu/faculty/Publication%20Files/09-083.pdf\">Goals Gone Wild</a>&nbsp;</span></p>\n<p class=\"MsoNormal\">This is a fairly compelling argument against goal-setting; that by setting an explicit goal and then optimizing towards that goal, you may be losing out on elements that were being accomplished better before, and maybe even rewarding actual negative behaviour. Members of an organization presumably already have assigned tasks and responsibilities, and aren\u2019t just doing whatever they feel like doing, but they might have done better with more freedom to prioritize their own work\u2013the best environment is one with some structure and goals, but not too many. The phenomenon of \u201cteaching to the test\u201d for standardized testing is another example.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Given that humans <a href=\"/lw/6mj/trivers_on_selfdeception/\">aren\u2019t best described as unitary selves</a>, this metaphor extends to individuals. If one aspect of myself sets a personal goal to write two pages per day, another aspect of myself might respond by writing two pages on the easiest project I can think of, like a journal entry that no one will ever see. This violates the spirit of the goal it technically accomplishes.</span></p>\n<p class=\"MsoNormal\">A more problematic consideration is the relationship between intrinsic and extrinsic motivation. <a href=\"http://www.jwalkonline.org/docs/Grad%20Classes/Fall%2007/Org%20Psy/Cases/motivation%20articles/PERUSED/metaanalysis%20of%20extrinsic%20rewards.pdf\">Studies</a>&nbsp;show that rewarding or punishing children for tasks results in less intrinsic motivation, as measured by stated interest or by freely choosing to engage in the task. I\u2019ve noticed this tendency in myself; faced with a nursing instructor who was constantly quizzing me on the pathophysiology of my patients\u2019 conditions, I responded by refusing to be curious about any of it or look up the answers to questions in any more detail than what she demanded, even though my previous self loved to spend hours on Google making sense of confusing diseases. If this is a problem that affects individuals setting goals for themselves\u2013i.e. if setting a daily writing goal makes writing less fun\u2013then I can easily see how goal-setting could be damaging.</p>\n<p class=\"MsoNormal\">I also notice that I\u2019m confused about the relationship between Beeminder\u2019s extrinsic motivation, in the form of punishment for derailing, and its effects on intrinsic motivation. Maybe the power of success spirals to increase intrinsic motivation offsets the negative effect of outside reward/punishment; or maybe the fact that users deliberately choose to use Beeminder means that it doesn\u2019t count as \u201cextrinsic.\u201d I\u2019m not sure.</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"Conclusion\"><span lang=\"EN-GB\">Conclusion</span></h2>\n<p class=\"MsoNormal\">There seems to be variation between individuals, in terms of both generally purposeful behaviour, and comfort level with calling it \u2018setting goals\u2019. This might be related to success spirals in the past, or it might be a factor of personality and general comfort with order versus chaos. I\u2019m not sure if it\u2019s been studied.</p>\n<p class=\"MsoNormal\">In the past, a lot of creative behaviour wasn\u2019t the result of deliberate goals. This may be a fundamental fact about creativity, or it may be a result of people\u2019s beliefs about creativity (\u00e0 la <a href=\"http://www.apa.org/helpcenter/willpower-limited-resource.pdf\">ego depletion only happens if you belief in ego depletion</a>) or it may be a historical coincidence that isn\u2019t fundamental at all. In any case, if you aren\u2019t currently getting creative work done, and want to do more, I\u2019m not sure what the alternative is to purposefully trying to do more. Manipulating the environment to make flow easier to attain, maybe. (For example, if I quit my day job and moved to a writers' commune, I might write more without needing to try on a day-to-day basis).&nbsp;</p>\n<p class=\"MsoNormal\">Process goals, or systems, are probably better than outcome goals. Specific and realistic goals are probably better than vague and ambitious ones. A lot of this may be because it\u2019s easier to form habits and/or success spirals around well-specified behaviours that you can just do every day.</p>\n<p><!--EndFragment--></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Setting goals within an organization has a lot of potential problems, because workers can game the system and accomplish the letter of the goal in the easiest possible way. This likely happens within individuals too. Research shows that extrinsic motivation reduces intrinsic motivation, which is important to consider, but I'm not sure how it relates to individuals setting goals, as opposed to organizations.</span></p>", "sections": [{"title": "Introduction\u00a0", "anchor": "Introduction_", "level": 1}, {"title": "Why the individual variation?", "anchor": "Why_the_individual_variation_", "level": 2}, {"title": "The More Important Question", "anchor": "The_More_Important_Question", "level": 1}, {"title": "Creative Goals", "anchor": "Creative_Goals", "level": 1}, {"title": "Outcome versus process goals", "anchor": "Outcome_versus_process_goals", "level": 1}, {"title": "SMART goals", "anchor": "SMART_goals", "level": 1}, {"title": "Problematic Goal-Driven Behaviour", "anchor": "Problematic_Goal_Driven_Behaviour", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ocujnKZL38thXn62", "RWo4LwFzpHNQCTcYt", "Ty2tjPwv8uyPK9vrz", "5wMTZLZZmZEbXdoMD", "KJbQyFbXiiYDDWbaS", "DSnamjnW7Ad8vEEKd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T23:17:38.238Z", "modifiedAt": null, "url": null, "title": "How to Beat Procrastination (to some degree) (if you're identical to me)", "slug": "how-to-beat-procrastination-to-some-degree-if-you-re", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:08.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JbdJr6CkCpg6ixqy5/how-to-beat-procrastination-to-some-degree-if-you-re", "pageUrlRelative": "/posts/JbdJr6CkCpg6ixqy5/how-to-beat-procrastination-to-some-degree-if-you-re", "linkUrl": "https://www.lesswrong.com/posts/JbdJr6CkCpg6ixqy5/how-to-beat-procrastination-to-some-degree-if-you-re", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Beat%20Procrastination%20(to%20some%20degree)%20(if%20you're%20identical%20to%20me)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Beat%20Procrastination%20(to%20some%20degree)%20(if%20you're%20identical%20to%20me)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJbdJr6CkCpg6ixqy5%2Fhow-to-beat-procrastination-to-some-degree-if-you-re%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Beat%20Procrastination%20(to%20some%20degree)%20(if%20you're%20identical%20to%20me)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJbdJr6CkCpg6ixqy5%2Fhow-to-beat-procrastination-to-some-degree-if-you-re", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJbdJr6CkCpg6ixqy5%2Fhow-to-beat-procrastination-to-some-degree-if-you-re", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 894, "htmlBody": "<p>So, you procrastinate. A lot. And it's a really big problem in your life, and so you really want to, y'know, stop procrastinating. But for some reason, it's just not that easy.</p>\n<p>So, why do you procrastinate?</p>\n<p>It's probably a really hard question to answer. Is it \"because I'm lazy\"? That's not a useful answer, because \"stop being lazy\" is just as hard to do as \"stop procrastinating\". How about \"because of akrasia\"? That's not a real answer; that's just a restatement of the question in more vague terms. Maybe \"because of hyperbolic discounting\"? Even if that's true, that's not really a useful answer, because there's no way to turn hyperbolic discounting off. (Or is there? If you know of a way to turn hyperbolic discounting off, please tell us.)</p>\n<p>Maybe your procrastination has three parts. First, once you start doing something interesting, it's very hard for you to stop; second, having stopped, you don't usually feel like starting to do something useful; and third, having started, you often find yourself losing focus and wanting to do something else.</p>\n<p>So let's look at each of these parts in turn.</p>\n<h3>Once you start doing something interesting, it's hard for you to stop.</h3>\n<p>For you, this one's a pain in the ass. (I know this because for me, it's a pain in the ass, and since you're reading this article, you must be identical to me.) You've told yourself that once you find yourself doing something interesting, you're just going to stop immediately. But that doesn't work at all. You've tried setting a timer, and telling yourself that you'll definitely, absolutely stop when the timer goes off. But that doesn't work, either; you just ignore the timer. What if you set a timer to repeatedly and annoyingly beep at you until you tell it that you've started working? You repeatedly ignore the timer and quickly become annoyed.</p>\n<p>For you, once this problem has started, there just doesn't seem to be a way to stop it. So the solution is to just not start in the first place. The ideal situation is that you're not doing any interesting and fun activities whatsoever until you're done working for the day (unless, of course, one of those activities is part of the work you're supposed to get done).</p>\n<p>You should still take breaks, of course; don't expect to work for four hours solid without stopping. Just don't do anything interesting during your breaks. Listen to music, or stare out the window or something.</p>\n<p>And, of course, this raises the question: how do I avoid doing these interesting activities? It turns out that, compared to the rest of your procrastination, this one is really easy to deal with. Hopping on Facebook or whatever when you're not sure what to do is a breakable habit. So break it. And how do you do that?</p>\n<p>One technique is to neuter the worst culprits. Go into your computer's configuration and tell it that reddit doesn't exist. Then if you accidentally try to access reddit, you'll just get an error message. Stop making status updates on Facebook, don't accept friend requests, and block everyone from showing up in your news feed. Disable your IRC bouncer and only access IRC through the server's crappy web interface. Avoiding temptation is easier when you set yourself to be disappointed every time.</p>\n<p>Still, there's a bit of residual temptation left over. How do you avoid this? Just use plain force of will. Tell yourself, \"I need to avoid doing this right now.\" That ought to work. Hopefully.</p>\n<p>So now that you've got that fixed (kind of), you've got another problem on your hands.</p>\n<h3>You're not currently doing anything addictive, but you just don't feel like working, either.</h3>\n<p>The easy answer to this question: just do it anyway. You may feel kinda crappy, but this doesn't actually have any negative effects.</p>\n<p>Or maybe you really, really don't feel like working. All right. Why not? Is it because there's a fuckton of stuff you have to do, and getting it all done is going to suck royally? Well, you can only do one thing at a time, so figure out what the one thing you should do next is, and completely ignore every obligation except for that one. (Figuring out which task is the one you should do next should be easy. If it's not easy, make a to-do list and use it properly.) If that little piece still seems too arduous, figure out the next little piece of that little piece that you need to do, and ignore the rest of it for the time being. Repeat.</p>\n<p>All right, so now you're working (hopefully). But it's not going very well.</p>\n<h3>You're working, but you're not focused on your work at all; you're just thinking about other unrelated stuff, and about how much you'd like to do something other than working.</h3>\n<p>Part of the problem here is that you have ADD. (Since you're reading this article, I'm assuming you're identical to me, and so you have every disorder I have.) Consider medication and talk to your psychiatrist. Therapy's probably a good idea, too, and it's easier to get seen by a psychologist or therapist than by a psychiatrist.</p>\n<p>Remember to eliminate distractions, too. Close unnecessary browser tabs and applications. Set your IM status to \"do not disturb\". Maybe try writing some of your thoughts down.</p>\n<p>And once you've done that... hell, I have no idea. Good luck.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JbdJr6CkCpg6ixqy5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 1.3830229241265601e-06, "legacy": true, "legacyId": "24423", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>So, you procrastinate. A lot. And it's a really big problem in your life, and so you really want to, y'know, stop procrastinating. But for some reason, it's just not that easy.</p>\n<p>So, why do you procrastinate?</p>\n<p>It's probably a really hard question to answer. Is it \"because I'm lazy\"? That's not a useful answer, because \"stop being lazy\" is just as hard to do as \"stop procrastinating\". How about \"because of akrasia\"? That's not a real answer; that's just a restatement of the question in more vague terms. Maybe \"because of hyperbolic discounting\"? Even if that's true, that's not really a useful answer, because there's no way to turn hyperbolic discounting off. (Or is there? If you know of a way to turn hyperbolic discounting off, please tell us.)</p>\n<p>Maybe your procrastination has three parts. First, once you start doing something interesting, it's very hard for you to stop; second, having stopped, you don't usually feel like starting to do something useful; and third, having started, you often find yourself losing focus and wanting to do something else.</p>\n<p>So let's look at each of these parts in turn.</p>\n<h3 id=\"Once_you_start_doing_something_interesting__it_s_hard_for_you_to_stop_\">Once you start doing something interesting, it's hard for you to stop.</h3>\n<p>For you, this one's a pain in the ass. (I know this because for me, it's a pain in the ass, and since you're reading this article, you must be identical to me.) You've told yourself that once you find yourself doing something interesting, you're just going to stop immediately. But that doesn't work at all. You've tried setting a timer, and telling yourself that you'll definitely, absolutely stop when the timer goes off. But that doesn't work, either; you just ignore the timer. What if you set a timer to repeatedly and annoyingly beep at you until you tell it that you've started working? You repeatedly ignore the timer and quickly become annoyed.</p>\n<p>For you, once this problem has started, there just doesn't seem to be a way to stop it. So the solution is to just not start in the first place. The ideal situation is that you're not doing any interesting and fun activities whatsoever until you're done working for the day (unless, of course, one of those activities is part of the work you're supposed to get done).</p>\n<p>You should still take breaks, of course; don't expect to work for four hours solid without stopping. Just don't do anything interesting during your breaks. Listen to music, or stare out the window or something.</p>\n<p>And, of course, this raises the question: how do I avoid doing these interesting activities? It turns out that, compared to the rest of your procrastination, this one is really easy to deal with. Hopping on Facebook or whatever when you're not sure what to do is a breakable habit. So break it. And how do you do that?</p>\n<p>One technique is to neuter the worst culprits. Go into your computer's configuration and tell it that reddit doesn't exist. Then if you accidentally try to access reddit, you'll just get an error message. Stop making status updates on Facebook, don't accept friend requests, and block everyone from showing up in your news feed. Disable your IRC bouncer and only access IRC through the server's crappy web interface. Avoiding temptation is easier when you set yourself to be disappointed every time.</p>\n<p>Still, there's a bit of residual temptation left over. How do you avoid this? Just use plain force of will. Tell yourself, \"I need to avoid doing this right now.\" That ought to work. Hopefully.</p>\n<p>So now that you've got that fixed (kind of), you've got another problem on your hands.</p>\n<h3 id=\"You_re_not_currently_doing_anything_addictive__but_you_just_don_t_feel_like_working__either_\">You're not currently doing anything addictive, but you just don't feel like working, either.</h3>\n<p>The easy answer to this question: just do it anyway. You may feel kinda crappy, but this doesn't actually have any negative effects.</p>\n<p>Or maybe you really, really don't feel like working. All right. Why not? Is it because there's a fuckton of stuff you have to do, and getting it all done is going to suck royally? Well, you can only do one thing at a time, so figure out what the one thing you should do next is, and completely ignore every obligation except for that one. (Figuring out which task is the one you should do next should be easy. If it's not easy, make a to-do list and use it properly.) If that little piece still seems too arduous, figure out the next little piece of that little piece that you need to do, and ignore the rest of it for the time being. Repeat.</p>\n<p>All right, so now you're working (hopefully). But it's not going very well.</p>\n<h3 id=\"You_re_working__but_you_re_not_focused_on_your_work_at_all__you_re_just_thinking_about_other_unrelated_stuff__and_about_how_much_you_d_like_to_do_something_other_than_working_\">You're working, but you're not focused on your work at all; you're just thinking about other unrelated stuff, and about how much you'd like to do something other than working.</h3>\n<p>Part of the problem here is that you have ADD. (Since you're reading this article, I'm assuming you're identical to me, and so you have every disorder I have.) Consider medication and talk to your psychiatrist. Therapy's probably a good idea, too, and it's easier to get seen by a psychologist or therapist than by a psychiatrist.</p>\n<p>Remember to eliminate distractions, too. Close unnecessary browser tabs and applications. Set your IM status to \"do not disturb\". Maybe try writing some of your thoughts down.</p>\n<p>And once you've done that... hell, I have no idea. Good luck.</p>", "sections": [{"title": "Once you start doing something interesting, it's hard for you to stop.", "anchor": "Once_you_start_doing_something_interesting__it_s_hard_for_you_to_stop_", "level": 1}, {"title": "You're not currently doing anything addictive, but you just don't feel like working, either.", "anchor": "You_re_not_currently_doing_anything_addictive__but_you_just_don_t_feel_like_working__either_", "level": 1}, {"title": "You're working, but you're not focused on your work at all; you're just thinking about other unrelated stuff, and about how much you'd like to do something other than working.", "anchor": "You_re_working__but_you_re_not_focused_on_your_work_at_all__you_re_just_thinking_about_other_unrelated_stuff__and_about_how_much_you_d_like_to_do_something_other_than_working_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-16T23:57:31.920Z", "modifiedAt": null, "url": null, "title": "Time-logging programs and/or spreadsheets ", "slug": "time-logging-programs-and-or-spreadsheets", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Torello", "createdAt": "2013-07-01T17:38:37.441Z", "isAdmin": false, "displayName": "Torello"}, "userId": "xoRpeFN7K5MgDRcvM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wSH5TiztdLH5cATod/time-logging-programs-and-or-spreadsheets", "pageUrlRelative": "/posts/wSH5TiztdLH5cATod/time-logging-programs-and-or-spreadsheets", "linkUrl": "https://www.lesswrong.com/posts/wSH5TiztdLH5cATod/time-logging-programs-and-or-spreadsheets", "postedAtFormatted": "Wednesday, October 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Time-logging%20programs%20and%2For%20spreadsheets%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATime-logging%20programs%20and%2For%20spreadsheets%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSH5TiztdLH5cATod%2Ftime-logging-programs-and-or-spreadsheets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Time-logging%20programs%20and%2For%20spreadsheets%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSH5TiztdLH5cATod%2Ftime-logging-programs-and-or-spreadsheets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSH5TiztdLH5cATod%2Ftime-logging-programs-and-or-spreadsheets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>I currently log the total number of hours I work each day in an OpenOffice Spreadsheet.&nbsp; I input the start time, lunch/break time, and end time, and it calculates the total hours worked.&nbsp; I'm not savvy enough to create this type of spreadsheet myself, so I looked through a large number of templates online before finding one that works as I've described above.&nbsp; I'm still not crazy about the way that this spreadsheet is laid out. &nbsp;</p>\n<p>If you can link to a spreadsheet available for download similar to the one described above, please do so in the comments. &nbsp;</p>\n<p>If you use time-logging for various distinct projects throughout the day, please describe this process and link to the software you use (if possible).&nbsp;</p>\n<p>More of a meta-discussion: how time-logging this enhanced your performance or time management?, for what types of projects/activities is it best to time-log?, general comments about the idea</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wSH5TiztdLH5cATod", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.3830601198072768e-06, "legacy": true, "legacyId": "24424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-17T04:23:05.204Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, October 16-31", "slug": "group-rationality-diary-october-16-31", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:03.317Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nrMwsYBqxZvJj4LPG/group-rationality-diary-october-16-31", "pageUrlRelative": "/posts/nrMwsYBqxZvJj4LPG/group-rationality-diary-october-16-31", "linkUrl": "https://www.lesswrong.com/posts/nrMwsYBqxZvJj4LPG/group-rationality-diary-october-16-31", "postedAtFormatted": "Thursday, October 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20October%2016-31&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20October%2016-31%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrMwsYBqxZvJj4LPG%2Fgroup-rationality-diary-october-16-31%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20October%2016-31%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrMwsYBqxZvJj4LPG%2Fgroup-rationality-diary-october-16-31", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrMwsYBqxZvJj4LPG%2Fgroup-rationality-diary-october-16-31", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<div id=\"entry_t3_ir8\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>This is the public group instrumental rationality diary for October 16-31.</p>\n<blockquote style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">The poll earlier this month seems to be sufficiently in favor of maintaining the current schedule that extra votes are unlikely to change things much, but if you'd really like to register your opinion, you are welcome to do so <a href=\"/lw/ir8/group_rationality_diary_october_115_plus/9tol\">here</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Immediate past diary:&nbsp; <a href=\"/lw/ir8/group_rationality_diary_october_115_plus/\">October 1-15</a>&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Next diary: &nbsp;<a href=\"/r/discussion/lw/iy9/group_rationality_diary_november_115/\">November 1-15</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nrMwsYBqxZvJj4LPG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "24427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RRueaJkaFcxe6fGAT", "AmfDFXFQguDF42wfm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-17T17:51:45.330Z", "modifiedAt": null, "url": null, "title": "Meetup : Phoenix (Tempe) Less Wrong Meetup", "slug": "meetup-phoenix-tempe-less-wrong-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Danny_Hintze", "createdAt": "2010-12-04T23:01:40.826Z", "isAdmin": false, "displayName": "Danny_Hintze"}, "userId": "2fHm6t2WFDMPShg5b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T828CbrBSoioBonpd/meetup-phoenix-tempe-less-wrong-meetup", "pageUrlRelative": "/posts/T828CbrBSoioBonpd/meetup-phoenix-tempe-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/T828CbrBSoioBonpd/meetup-phoenix-tempe-less-wrong-meetup", "postedAtFormatted": "Thursday, October 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Phoenix%20(Tempe)%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Phoenix%20(Tempe)%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT828CbrBSoioBonpd%2Fmeetup-phoenix-tempe-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Phoenix%20(Tempe)%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT828CbrBSoioBonpd%2Fmeetup-phoenix-tempe-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT828CbrBSoioBonpd%2Fmeetup-phoenix-tempe-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sh'>Phoenix (Tempe) Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 October 2013 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting up in Hayden Library at ASU, probably to casually (causally? :P) discuss decision theory.</p>\n\n<p>Text me at 602-501-9420 if there are any issues.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sh'>Phoenix (Tempe) Less Wrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T828CbrBSoioBonpd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3840623344244278e-06, "legacy": true, "legacyId": "24428", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Phoenix__Tempe__Less_Wrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/sh\">Phoenix (Tempe) Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 October 2013 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting up in Hayden Library at ASU, probably to casually (causally? :P) discuss decision theory.</p>\n\n<p>Text me at 602-501-9420 if there are any issues.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Phoenix__Tempe__Less_Wrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/sh\">Phoenix (Tempe) Less Wrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Phoenix (Tempe) Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Phoenix__Tempe__Less_Wrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Phoenix (Tempe) Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Phoenix__Tempe__Less_Wrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-17T20:36:53.567Z", "modifiedAt": null, "url": null, "title": "Looking for opinions of people like Nick Bostrom or Anders Sandberg on current cryo techniques", "slug": "looking-for-opinions-of-people-like-nick-bostrom-or-anders", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A8oGJC3kwvYJLkqWN/looking-for-opinions-of-people-like-nick-bostrom-or-anders", "pageUrlRelative": "/posts/A8oGJC3kwvYJLkqWN/looking-for-opinions-of-people-like-nick-bostrom-or-anders", "linkUrl": "https://www.lesswrong.com/posts/A8oGJC3kwvYJLkqWN/looking-for-opinions-of-people-like-nick-bostrom-or-anders", "postedAtFormatted": "Thursday, October 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20opinions%20of%20people%20like%20Nick%20Bostrom%20or%20Anders%20Sandberg%20on%20current%20cryo%20techniques&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20opinions%20of%20people%20like%20Nick%20Bostrom%20or%20Anders%20Sandberg%20on%20current%20cryo%20techniques%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA8oGJC3kwvYJLkqWN%2Flooking-for-opinions-of-people-like-nick-bostrom-or-anders%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20opinions%20of%20people%20like%20Nick%20Bostrom%20or%20Anders%20Sandberg%20on%20current%20cryo%20techniques%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA8oGJC3kwvYJLkqWN%2Flooking-for-opinions-of-people-like-nick-bostrom-or-anders", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA8oGJC3kwvYJLkqWN%2Flooking-for-opinions-of-people-like-nick-bostrom-or-anders", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 571, "htmlBody": "<p>In June 2012, Robin Hanson wrote a post <a href=\"http://www.overcomingbias.com/2012/06/plastination-is-near.html\">promoting plastination</a> as a superior to cryopreservation as an approach to preserving people for later uploading. His post included a paragraph which said:</p>\n<blockquote>\n<p>We don&rsquo;t actually know that frozen brains preserve enough brain info. Until recently, ice formation in the freezing process ripped out huge brain chunks everywhere and shoved them to distant locations. Recent use of a special anti-freeze has reduced that, but we don&rsquo;t actually know if the anti-freeze gets to enough places. Or even if enough info is saved where it does go.</p>\n</blockquote>\n<p>This left me with the impression that the chances of the average cryopreserved person today of being later revived aren't great, even when you conditionalize on no existential catastrophe. More recently, I did a systematic read-through of the sequences for the first time (about a month 1/2 ago), and Eliezer's post <a href=\"/lw/wq/you_only_live_twice/\">You Only Live Twice</a> convinced me to finally sign up for cryonics for three reasons:</p>\n<ul>\n<li>It's cheaper than I realized</li>\n<li>Eliezer recommended Rudi Hoffman to help with the paperwork</li>\n<li>Eliezer's hard drive analogy convinced me the chances of revival (at least conditionalizing on no existential catastrophe) are good</li>\n</ul>\n<div>But then Paul Crowley pointed out to me that what I posted yesterday about <a href=\"/lw/iu0/trusting_expert_consensus/\">trusting experts</a>&nbsp;seems to imply I should take cryonics less seriously than I do. I thought about it awhile, and realizes that I suspect that even among academics who would be predisposed to take cryonics seriously, say attendees at the workshop that produced the <a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">Whole Brain Emulation roadmap</a>, Robin's more cautious perspective is probably more typical. Or is it? I've tried to find information online, and the one thing I've found is the <a href=\"http://www.imminst.org/cryonics_letter/\">Scientists' Open Letter on Cryonics</a>, which carries this disclaimer:</div>\n<blockquote>\n<div>Note: Signing of this letter does not imply endorsement of any particular cryonics organization or its practices. &nbsp;Opinions on how much cerebral ischemic injury (delay after clinical death) and preservation injury may be reversible in the future vary widely among signatories.</div>\n</blockquote>\n<p>I don't find that terribly encouraging. So now I'm back to being pessimistic about current cryopreservation techniques (though I'm still signing up for cryonics because the cost is low enough even given my current estimate of my chances). But I'd very much be curious to know if anyone knows what, say, Nick Bostrom or Anders Sandberg think about the issue. Anyone?</p>\n<p><strong>Edit:</strong> I'm aware of estimates given by LessWrong folks in the census of the chances of revival, but I don't know<em>&nbsp;</em>how much of that is people taking things like existential risk into account. There are lots of different ways you could arrive at a ~10% chance of revival overall:</p>\n<ul>\n<li>(50% chance of no existential catastrophe) * (30% chance current cryopreservation techniques are adequate) * (70% chance my fellow humans will come through for me beyond avoiding existential catastrophe) = 10.5%</li>\n</ul>\n<p>is one way. But:</p>\n<ul>\n<li>(15% chance no existential catastrophe) * (99% chance current cryopreservation techniques are adequate) *&nbsp;(70% chance my fellow humans will come through for me beyond avoiding existential catastrophe) = ~10.4%</li>\n</ul>\n<p>is a very similar conclusion from very different premises. Gwern&nbsp;has more on this sort of reasoning in <a href=\"http://www.gwern.net/plastination\">Plastination versus cryonics</a>, but I don't know who most of the people he links to are so I'm not sure whether to trust them. He does link to a <a href=\"http://www.overcomingbias.com/2009/03/break-cryonics-down.html\">breakdown of probabilities</a> by Robin, but I don't fully understand the way Robin is breaking the issue down.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A8oGJC3kwvYJLkqWN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "24429", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 184, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yKXKcyoBzWtECzXrE", "R8YpYTq8LoD3k948L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-17T22:34:07.587Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Decision Theory", "slug": "meetup-urbana-champaign-decision-theory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kY8yxgZxiA7HbTnRd/meetup-urbana-champaign-decision-theory", "pageUrlRelative": "/posts/kY8yxgZxiA7HbTnRd/meetup-urbana-champaign-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/kY8yxgZxiA7HbTnRd/meetup-urbana-champaign-decision-theory", "postedAtFormatted": "Thursday, October 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkY8yxgZxiA7HbTnRd%2Fmeetup-urbana-champaign-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkY8yxgZxiA7HbTnRd%2Fmeetup-urbana-champaign-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkY8yxgZxiA7HbTnRd%2Fmeetup-urbana-champaign-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/si'>Urbana-Champaign: Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.111220,-88.227363 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup will be held <a href=\"https://maps.google.com/maps?hl=en&amp;q=40.111220,+-88.227363\" rel=\"nofollow\">here</a> at 2pm Sunday October 20.</p>\n\n<p>Topics we might discuss include (but are not limited to):</p>\n\n<p>Pascal's mugging and pascal's wager</p>\n\n<p>Newcomb's problem\nThe ultimate Newcomb's problem</p>\n\n<p>acausal trade</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/nlptuXLBGZU\">Cross-posted</a> on the mailing list.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/si'>Urbana-Champaign: Decision Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kY8yxgZxiA7HbTnRd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.384325990341207e-06, "legacy": true, "legacyId": "24430", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Decision_Theory\">Discussion article for the meetup : <a href=\"/meetups/si\">Urbana-Champaign: Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.111220,-88.227363 </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup will be held <a href=\"https://maps.google.com/maps?hl=en&amp;q=40.111220,+-88.227363\" rel=\"nofollow\">here</a> at 2pm Sunday October 20.</p>\n\n<p>Topics we might discuss include (but are not limited to):</p>\n\n<p>Pascal's mugging and pascal's wager</p>\n\n<p>Newcomb's problem\nThe ultimate Newcomb's problem</p>\n\n<p>acausal trade</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/nlptuXLBGZU\">Cross-posted</a> on the mailing list.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Decision_Theory1\">Discussion article for the meetup : <a href=\"/meetups/si\">Urbana-Champaign: Decision Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Decision Theory", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Decision_Theory", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Decision Theory", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Decision_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T02:25:39.820Z", "modifiedAt": null, "url": null, "title": "story idea...", "slug": "story-idea", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:01.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vvf7bvnTRoi4gMQAe/story-idea", "pageUrlRelative": "/posts/vvf7bvnTRoi4gMQAe/story-idea", "linkUrl": "https://www.lesswrong.com/posts/vvf7bvnTRoi4gMQAe/story-idea", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20story%20idea...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Astory%20idea...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvvf7bvnTRoi4gMQAe%2Fstory-idea%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=story%20idea...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvvf7bvnTRoi4gMQAe%2Fstory-idea", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvvf7bvnTRoi4gMQAe%2Fstory-idea", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 372, "htmlBody": "<p>I have an idea for a setting for a writing project (or, possibly a comic) and was wondering what people thought of it.</p>\n<p>The setting is in the far future, but the rise of superintelligent ai, friendliness uncertain, has resulted in everyone living within a gameified version of the world, with technology strictly bounded by an ever-present nanotechnological fog that dissassembles anything beyond a certain level of apparent technical development - we're strictly limited from developing further AI- for purposes unknown to the protagonist. On death, people are \"resurrected\" (possessions are lost, but a fresh body is printed, and monitoring nanites at the cellular level constantly update a buffered version of \"you\" that is then printed when your vital signs are lost) at locations that look like gates. There are 'monsters', various \"magic\" items, 'npcs' / autonomous agents of the AI, quests and puzzles to solve. It is unknown to the protagonist whether the world is some kind of a test or not, although very rarely, some people are not resurrected. In other respects, though, it is simply a somewhat-in-the-future version of our Earth. Cities were, prior to the 'event', largely 3d printed, so there are city-scapes resembling different eras. Their ability to self repair autonomously has been left on, although some of them have grown 'cancerous', sprouting bits of mixed architecture in places. They have also been integrated into a reengineered ecosystem with the spawning of a bunch of synthetic organisms that can tear apart and consume cities or break down waste emissions.</p>\n<p>Sometimes, on 'resurrection', people get minor details adjusted - such as having abilities or skills they never had before, or subtly altered memories.</p>\n<p>The main character is a transgender woman who was cryogenically frozen, who wakes up to find herself in a more ideal body, in the future, with super powers. Kind of a Mary Sue for me I guess, although I'm planning to give her flaws to balance her out, although I haven't decided on which ones.</p>\n<p>I'm not a terribly experienced writer and can't promise it won't suck, but I like playing with these ideas and thinking about them... thoughts? Suggestions? Similar stuff I should read? I've read Diaspora and The Rapture of the Nerds already, and really enjoyed those...</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vvf7bvnTRoi4gMQAe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "24431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T02:43:11.632Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Jobs/project show and tell ", "slug": "meetup-washington-dc-jobs-project-show-and-tell", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CLZxbbJ67cAaoANmd/meetup-washington-dc-jobs-project-show-and-tell", "pageUrlRelative": "/posts/CLZxbbJ67cAaoANmd/meetup-washington-dc-jobs-project-show-and-tell", "linkUrl": "https://www.lesswrong.com/posts/CLZxbbJ67cAaoANmd/meetup-washington-dc-jobs-project-show-and-tell", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Jobs%2Fproject%20show%20and%20tell%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Jobs%2Fproject%20show%20and%20tell%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCLZxbbJ67cAaoANmd%2Fmeetup-washington-dc-jobs-project-show-and-tell%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Jobs%2Fproject%20show%20and%20tell%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCLZxbbJ67cAaoANmd%2Fmeetup-washington-dc-jobs-project-show-and-tell", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCLZxbbJ67cAaoANmd%2Fmeetup-washington-dc-jobs-project-show-and-tell", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sj'>Washington DC: Jobs/project show and tell </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to tell each other about the things we're working on. (I think this would segue well into doing another quick goals discussion).</p>\n\n<p>This is suspiciously similar to last meetup because it is last meetup: largely due to the Columbus trip, not many people were at the last meetup.</p>\n\n<p>The government is open again: We're meeting at the normal place again!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sj'>Washington DC: Jobs/project show and tell </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CLZxbbJ67cAaoANmd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3845586238144067e-06, "legacy": true, "legacyId": "24432", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell_\">Discussion article for the meetup : <a href=\"/meetups/sj\">Washington DC: Jobs/project show and tell </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to tell each other about the things we're working on. (I think this would segue well into doing another quick goals discussion).</p>\n\n<p>This is suspiciously similar to last meetup because it is last meetup: largely due to the Columbus trip, not many people were at the last meetup.</p>\n\n<p>The government is open again: We're meeting at the normal place again!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell_1\">Discussion article for the meetup : <a href=\"/meetups/sj\">Washington DC: Jobs/project show and tell </a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Jobs/project show and tell ", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell_", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Jobs/project show and tell ", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T04:06:54.419Z", "modifiedAt": null, "url": null, "title": "Subjective Altruism", "slug": "subjective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:06.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nZiQZBC43ys7vYeeG/subjective-altruism", "pageUrlRelative": "/posts/nZiQZBC43ys7vYeeG/subjective-altruism", "linkUrl": "https://www.lesswrong.com/posts/nZiQZBC43ys7vYeeG/subjective-altruism", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Subjective%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASubjective%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZiQZBC43ys7vYeeG%2Fsubjective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Subjective%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZiQZBC43ys7vYeeG%2Fsubjective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZiQZBC43ys7vYeeG%2Fsubjective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 859, "htmlBody": "<p>Let us assume for the purpose of this argument that Bayesian probabilities are subjective. Specifically, I am thinking in terms of the model of probability expressed in model 4 <a href=\"/lw/1iy/what_are_probabilities_anyway/\">here</a>. That is to say that the meaning of P(A)=2/3 is \"I as a decision agent care twice as much about possible world in which A is true relative to the possible world in which A is false.\" Let us also assume that it is possible to have preferences about things that we will never observe.</p>\n<p>Consider the following thought experiment:</p>\n<p>Alice and Bob are agents who disagree about the fairness of a coin. Alice believes that the coin will come up heads with probability 2/3 and Bob believes the coin will come up tails with probability 2/3. They discuss their reasons for a long time and realize that their disagreement comes from different initial prior assumptions, and they agree that both people have rational probabilities given their respective priors. Alice is given the opportunity to gamble on behalf of Bob. Alice must call heads or tails, then the coin will be flipped once. If Alice calls the coin correctly, then Bob will be given a dollar. If she calls the coin incorrectly, then nothing happens. Either way, nobody sees the result of the coin flip, and Alice and Bob never interact again. Should Alice call heads or tails?</p>\n<p>The meat of this question is this: When trying to being altruistic towards a person, should you maximize their expected utility under their priors or your own. I will present an argument here, but feel free to stop reading here and think about it on your own and post the results.</p>\n<p>First of all, notice there there are actually 3 options:</p>\n<p>1) Maximize your own expectation of Bob's utility</p>\n<p>2) Maximize Bob's expectation of his utility</p>\n<p>3) Maximize what Bob's expectation of his utility would be if he were to update on the evidence of everything that you have observed.</p>\n<p>At first it may have looked like the main options were 1 and 2, but I claim that 2 is actually a very bad option and the only question is between options 1 and 3. Option 2 is stupid because for example it would cause Alice to call tails even if she has already seen the coin flip and it came up heads. There is no reason for Alice not to update on all of the information she has. The only question is whose prior should she update from. In this specific thought experiment, we are assuming that 2 and 3 are the same, since Alice has already convinced herself that her observations could not change Bob's mind, but I think that as a general options 1 and 3 are somewhat reasonable answers, while 2 is not.</p>\n<p>Option 3 has the nice property that it does not have to observe Bob's utility function. It only has to observe Bob's expected utility of different choices. This is nice because in many ways \"expected utility\" seems like a more fundamental and possibly more well defined concept than \"utility.\" We are trying to be altruistic towards Bob. It seems natural to give Bob the most utility in the possible worlds that he \"cares about\" the most.</p>\n<p>On the other hand, we want the possible worlds that we care about most to be as good as possible. We may not ever be able to observe whether of not Bob gets the dollar, but it is not just Bob who wants to get the dollar. We also want Bob to get the dollar. We want Bob to get the dollar in the most important possible worlds, the worlds we assign a high probability to. What we want is for Bob to be happy in the worlds that are important. We may have subjectively assigned those possible worlds to be the most important ones, but from the standpoint of us as a decision agent, the worlds we assign high probability to really are more important than the other ones.</p>\n<p>Option 1 is also simpler than option 3. We just have a variable for Bob's utility in our utility function, and we do what maximizes our expected utility. If we took option 3, we would be maximize something that is not just a product of our utilities with our probabilities.&nbsp;</p>\n<p>Option 3 has some unfortunate consequences. For example, it might cause us to pray for a religious person even if we are very strongly atheist.&nbsp;</p>\n<p>I prefer option 1. I care about the worlds that are simple and therefore are given high probability. I want everyone to be happy in those worlds. I would not sacrifice the happiness in someone in a simple/probable/important world just because someone else thinks another world is important. Probability may be subjective, but relative to the probabilities that I use to make all my decisions, Bob's probabilities are just wrong.</p>\n<p>Option 3 is nice in situations where Alice and Bob will continue interacting, possibly even interacting through mutual simulation. If Alice and Bob were given a symmetric scenario, then this would become a prisoner dilemma, where Alice choosing heads corresponds to defecting, while Alice choosing tails corresponds to cooperating. However, I believe this is a separate issue.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nZiQZBC43ys7vYeeG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 1.384636828884751e-06, "legacy": true, "legacyId": "24433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["J7Gkz8aDxxSEQKXTN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T10:50:36.207Z", "modifiedAt": null, "url": null, "title": "Suggestion : make it easier to work out which tags to put on your article", "slug": "suggestion-make-it-easier-to-work-out-which-tags-to-put-on", "viewCount": null, "lastCommentedAt": "2013-10-24T03:15:47.810Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7kZoW9y8W5pGnaYuu/suggestion-make-it-easier-to-work-out-which-tags-to-put-on", "pageUrlRelative": "/posts/7kZoW9y8W5pGnaYuu/suggestion-make-it-easier-to-work-out-which-tags-to-put-on", "linkUrl": "https://www.lesswrong.com/posts/7kZoW9y8W5pGnaYuu/suggestion-make-it-easier-to-work-out-which-tags-to-put-on", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestion%20%3A%20make%20it%20easier%20to%20work%20out%20which%20tags%20to%20put%20on%20your%20article&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestion%20%3A%20make%20it%20easier%20to%20work%20out%20which%20tags%20to%20put%20on%20your%20article%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kZoW9y8W5pGnaYuu%2Fsuggestion-make-it-easier-to-work-out-which-tags-to-put-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestion%20%3A%20make%20it%20easier%20to%20work%20out%20which%20tags%20to%20put%20on%20your%20article%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kZoW9y8W5pGnaYuu%2Fsuggestion-make-it-easier-to-work-out-which-tags-to-put-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7kZoW9y8W5pGnaYuu%2Fsuggestion-make-it-easier-to-work-out-which-tags-to-put-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>It would improve the usefulness of article navigation, if people tended to use the same tag for the same thing.</p>\n<p>Currently, if you want to decide whether to tag your article \"fai\" or \"friendly_ai\", your best bet is to manually try:</p>\n<p><a href=\"/tag/fai/\">http://lesswrong.com/tag/fai/</a></p>\n<p><a href=\"/tag/friendly_ai/\">http://lesswrong.com/tag/friendly_ai/</a></p>\n<p>And count how many articles use which variant.&nbsp; But, even then, there might be other similar variants you didn't think to check.</p>\n<p>&nbsp;</p>\n<p>What would be nice is a tag cloud, listing how many articles there are (possibly weighted by ranking) that use each variant.&nbsp; The <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/Tags\">list of tags on the wiki</a> isn't dynamically generated, and is very incomplete.</p>\n<p>It wouldn't need to be something fancy, like:</p>\n<p><img src=\"http://tracker.in-portal.org/file_download.php?file_id=476&amp;type=bug\" alt=\"\" width=\"549\" height=\"422\" /></p>\n<p>Just an alphabetical list, with a number by each entry, would be an improvement over the current situation.</p>\n<p>\n<hr />\n<span style=\"color: #00ffff;\">If you are downvoting this article, and would like to provide constructive feedback, here's a place to provide it: <a href=\"/lw/iuq/suggestion_make_it_easier_to_work_out_which_tags/9wxm\">LINK</a></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7kZoW9y8W5pGnaYuu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "24434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T12:56:47.729Z", "modifiedAt": null, "url": null, "title": "Of all the SIA-doomsdays in the all the worlds...", "slug": "of-all-the-sia-doomsdays-in-the-all-the-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rrtGEhFCqYZD4v8cc/of-all-the-sia-doomsdays-in-the-all-the-worlds", "pageUrlRelative": "/posts/rrtGEhFCqYZD4v8cc/of-all-the-sia-doomsdays-in-the-all-the-worlds", "linkUrl": "https://www.lesswrong.com/posts/rrtGEhFCqYZD4v8cc/of-all-the-sia-doomsdays-in-the-all-the-worlds", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Of%20all%20the%20SIA-doomsdays%20in%20the%20all%20the%20worlds...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOf%20all%20the%20SIA-doomsdays%20in%20the%20all%20the%20worlds...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrrtGEhFCqYZD4v8cc%2Fof-all-the-sia-doomsdays-in-the-all-the-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Of%20all%20the%20SIA-doomsdays%20in%20the%20all%20the%20worlds...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrrtGEhFCqYZD4v8cc%2Fof-all-the-sia-doomsdays-in-the-all-the-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrrtGEhFCqYZD4v8cc%2Fof-all-the-sia-doomsdays-in-the-all-the-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 585, "htmlBody": "<p><em>Ideas developed with Paul Almond, who kept on flogging a dead horse until it started showing signs of life again.</em></p>\n<h2>Doomsday, SSA <em>and</em> SIA</h2>\n<p>Imagine there's a giant box filled with people, and clearly labelled (inside and out) \"(year of some people's lord) 2013\". There's another giant box somewhere else in space-time, labelled \"2014\". You happen to be currently in the 2013 box.</p>\n<p>Then the <a href=\"http://en.wikipedia.org/wiki/Self-Sampling_Assumption\">self-sampling assumption</a>&nbsp;(SSA) produces the <a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">doomsday argument</a>. It works approximately like this: SSA has a preference for universe with smaller numbers of observers (since it's more likely that you're one-in-a-hundred than one-in-a-billion). Therefore we expect that the number of observers in 2014 is smaller than we would otherwise \"objectively\" believe: the likelihood of doomsday is higher than we thought.</p>\n<p>What about the <a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">self-indication assumption</a> (SIA) - that makes the doomsday argument go away, right? Not at all! SIA has no effect on the number of observers expected in the 2014, but increases the expected number of observers in 2013. Thus we still expect that the number of observers in 2014 to be lower than we otherwise thought. There's an SIA doomsday too!</p>\n<h2>Enter causality</h2>\n<p>What's going on? SIA was supposed to defeat the doomsday argument! What happens is that I've implicitly cheated - by naming the boxes \"2013\" and \"2014\", I've heavily&nbsp;<em>implied</em> that these \"boxes\" figuratively correspond two subsequent years. But then I've treated them as independent for SIA, like two literal distinct boxes.<a id=\"more\"></a></p>\n<p>In reality, of course, the contents of two years are not independent. Causality connects the two: it's much more likely that there are many observers in 2014, if there were many in 2013. Indeed, most of the observers will exist in both years (and there will be some subtle SSA issues of \"<a href=\"http://philsci-archive.pitt.edu/8706/\">observer moments</a>\" that we're eliding here - does a future you count as the same observer or a different one). So causality removes the independence assumption, and though there may be some interesting SIA effects on&nbsp;<a href=\"/r/discussion/lw/ef3/the_real_sia_doomsday/\">changes in growth rates</a>, we won't see a real SIA doomsday.</p>\n<h2>Exit causality</h2>\n<p>But is causality itself a justified assumption? To some extent it is: some people who think they live in 2013/2014 will certainly be in a causal relationship with people who think they live in 2014/2013.</p>\n<p>But many will not! What of <a href=\"http://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brains</a>? What of Boltzmann worlds - brief worlds that last less than a year?</p>\n<p>What about deluded worlds: worlds where the background data coincidentally (or conspiratorially) imply that we exist on the planet Earth around the Sun, in 2013 - but where we really exist <em>inside</em> a star circling a space-station a few seconds after the seventh toroidal big bang, or something, and will soon wake up to this fact. What about simply deluded people - may we be alien nutjobs, dreaming we're humans? And of great relevance to arguments often presented here, what if we are short-term simulations run by some advanced species?</p>\n<p>All these are possible, with non-zero probability (the probability of simulations may be very high indeed, under <a href=\"http://www.simulation-argument.com/simulation.html\">some assumptions</a>). All of these break the causal link to 2014 or other future events. And hence all of them allow a genuine SIA doomsday argument to flourish: we should expect that seeing 2014 is less likely than is objectively implied, given that we think we are in the year 2013.</p>\n<p>Is <a href=\"/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">anthropic decision theory</a> (ADT) subject to the same doomsday argument? In that form, no. In any situation where causality breaks down, your decisions cease to have consequences, and ADT tosses them aside. But more complicated preferences or setups could bring doomsday into ADT as well.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rrtGEhFCqYZD4v8cc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 1.3851320339774664e-06, "legacy": true, "legacyId": "24402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Ideas developed with Paul Almond, who kept on flogging a dead horse until it started showing signs of life again.</em></p>\n<h2 id=\"Doomsday__SSA_and_SIA\">Doomsday, SSA <em>and</em> SIA</h2>\n<p>Imagine there's a giant box filled with people, and clearly labelled (inside and out) \"(year of some people's lord) 2013\". There's another giant box somewhere else in space-time, labelled \"2014\". You happen to be currently in the 2013 box.</p>\n<p>Then the <a href=\"http://en.wikipedia.org/wiki/Self-Sampling_Assumption\">self-sampling assumption</a>&nbsp;(SSA) produces the <a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">doomsday argument</a>. It works approximately like this: SSA has a preference for universe with smaller numbers of observers (since it's more likely that you're one-in-a-hundred than one-in-a-billion). Therefore we expect that the number of observers in 2014 is smaller than we would otherwise \"objectively\" believe: the likelihood of doomsday is higher than we thought.</p>\n<p>What about the <a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">self-indication assumption</a> (SIA) - that makes the doomsday argument go away, right? Not at all! SIA has no effect on the number of observers expected in the 2014, but increases the expected number of observers in 2013. Thus we still expect that the number of observers in 2014 to be lower than we otherwise thought. There's an SIA doomsday too!</p>\n<h2 id=\"Enter_causality\">Enter causality</h2>\n<p>What's going on? SIA was supposed to defeat the doomsday argument! What happens is that I've implicitly cheated - by naming the boxes \"2013\" and \"2014\", I've heavily&nbsp;<em>implied</em> that these \"boxes\" figuratively correspond two subsequent years. But then I've treated them as independent for SIA, like two literal distinct boxes.<a id=\"more\"></a></p>\n<p>In reality, of course, the contents of two years are not independent. Causality connects the two: it's much more likely that there are many observers in 2014, if there were many in 2013. Indeed, most of the observers will exist in both years (and there will be some subtle SSA issues of \"<a href=\"http://philsci-archive.pitt.edu/8706/\">observer moments</a>\" that we're eliding here - does a future you count as the same observer or a different one). So causality removes the independence assumption, and though there may be some interesting SIA effects on&nbsp;<a href=\"/r/discussion/lw/ef3/the_real_sia_doomsday/\">changes in growth rates</a>, we won't see a real SIA doomsday.</p>\n<h2 id=\"Exit_causality\">Exit causality</h2>\n<p>But is causality itself a justified assumption? To some extent it is: some people who think they live in 2013/2014 will certainly be in a causal relationship with people who think they live in 2014/2013.</p>\n<p>But many will not! What of <a href=\"http://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brains</a>? What of Boltzmann worlds - brief worlds that last less than a year?</p>\n<p>What about deluded worlds: worlds where the background data coincidentally (or conspiratorially) imply that we exist on the planet Earth around the Sun, in 2013 - but where we really exist <em>inside</em> a star circling a space-station a few seconds after the seventh toroidal big bang, or something, and will soon wake up to this fact. What about simply deluded people - may we be alien nutjobs, dreaming we're humans? And of great relevance to arguments often presented here, what if we are short-term simulations run by some advanced species?</p>\n<p>All these are possible, with non-zero probability (the probability of simulations may be very high indeed, under <a href=\"http://www.simulation-argument.com/simulation.html\">some assumptions</a>). All of these break the causal link to 2014 or other future events. And hence all of them allow a genuine SIA doomsday argument to flourish: we should expect that seeing 2014 is less likely than is objectively implied, given that we think we are in the year 2013.</p>\n<p>Is <a href=\"/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">anthropic decision theory</a> (ADT) subject to the same doomsday argument? In that form, no. In any situation where causality breaks down, your decisions cease to have consequences, and ADT tosses them aside. But more complicated preferences or setups could bring doomsday into ADT as well.</p>", "sections": [{"title": "Doomsday, SSA and SIA", "anchor": "Doomsday__SSA_and_SIA", "level": 1}, {"title": "Enter causality", "anchor": "Enter_causality", "level": 1}, {"title": "Exit causality", "anchor": "Exit_causality", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "49 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DNhFQJb5gkWoZrBBD", "svhbnSdxW3XmFXXTK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T13:39:29.612Z", "modifiedAt": null, "url": null, "title": "Creating an Optimal Future", "slug": "creating-an-optimal-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "QWaYLBKNsQRCrLFpw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WEqiLCREd55vNGp3B/creating-an-optimal-future", "pageUrlRelative": "/posts/WEqiLCREd55vNGp3B/creating-an-optimal-future", "linkUrl": "https://www.lesswrong.com/posts/WEqiLCREd55vNGp3B/creating-an-optimal-future", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Creating%20an%20Optimal%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACreating%20an%20Optimal%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWEqiLCREd55vNGp3B%2Fcreating-an-optimal-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Creating%20an%20Optimal%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWEqiLCREd55vNGp3B%2Fcreating-an-optimal-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWEqiLCREd55vNGp3B%2Fcreating-an-optimal-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 613, "htmlBody": "<p>Creating an Optimal Future. It sounds very arrogant when I type it out. A more reasonable claim would be that it is possible to create a Less Wrong Future, but for reasons that will shortly become apparent that felt like stepping too hard on other people&rsquo;s shoes. I suppose Working Towards an Optimal Future would be the best title for what I have in mind.</p>\n<p>Let me backtrack and start at the beginning. I am not a rationalist. Well, I am not a rationalist as the term applies in this community. Not completely anyway. I have only read some of the Sequences and, although I&rsquo;ve devoured HPMOR, I do understand and agree with a number of the criticisms that have been leveled toward it.</p>\n<p>But I am here because of that Optimal Future I have mentioned. The way I see it, we are not currently on a trajectory that will lead to an optimal future and I am fairly confident that you agree with me on that. From what I have seen and heard from various online communities over the years, quite a few people do agree with me on that.</p>\n<p>But the problem is, a few thousand people visit Less Wrong regularly, generating and evolving a unique memescape. And a few miles down the information highway, another few thousand people post to Humanity+ mailing lists; building up a different memescape. There is some overlap, naturally, but not nearly enough. And in another corner of the internet, environmentalist factions sit in their own forums and discuss a different set of problems affecting (trans)humanity&rsquo;s future. In yet another corner, socialists imagine utopias built on free access to nanofabricators (while anarchists imagine a similar utopia sans the government).</p>\n<p>All in all, there may be near to a million people looking at future problems and solutions. But as long as they do so in small fringe groups, the solutions they can think up are limited. Worse, &ldquo;junk&rdquo; memes start sweeping into the community, harming recruitment and giving the underlying philosophies a bad name. To push the metaphor about as far as it can go: these communities tend to get a bit inbred over time.</p>\n<p>And a million voices fail to affect policies in any way, because for all the hopes and fears they share they fail to coordinate and collaborate. Meanwhile, the world continues to move along a sub-optimal trajectory.</p>\n<p>Which, finally, leads us back to Optimal Future. In discussing the problems above with friends, we hit upon an obvious solution: build a place where all futurists and people who care about the future (but do not self identify as futurist) can discuss the relevant topics and hopefully find novel solutions through combining memes that one wouldn&rsquo;t normally think to combine.</p>\n<p>Which is why I am here now. The site has been built, but then that was always going to be the easiest part. The hard part is building a diverse and active community. That&rsquo;s where you come in. LessWrong is one of the most active future thinking communities on the web, and also a fairly controversial one. Having you as part of the community could make a lot of difference to us. In exchange we can offer you a wider audience and some new perspectives.</p>\n<p>So if you are curious as to how a Friendly AGI designed by anarchist would be different to one designed by Greens, feel like scaring communists with what horrors a corporate paperclip maximizer could commit, want to see how wide the spectrum of transhumanists really is, want to learn about cryptography or sousveillance, or feel like debating the pros and cons of open sourced AIs, come on down to optimalfuture.org and take a look at the bigger picture.</p>\n<p><a href=\"http://optimalfuture.org\" target=\"_blank\">http://optimalfuture.org/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WEqiLCREd55vNGp3B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -6, "extendedScore": null, "score": 1.3851719508922785e-06, "legacy": true, "legacyId": "24435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T13:48:15.154Z", "modifiedAt": null, "url": null, "title": "Blind Spot: Malthusian Crunch", "slug": "blind-spot-malthusian-crunch", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:24.137Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bokov", "createdAt": "2010-01-11T01:11:23.480Z", "isAdmin": false, "displayName": "bokov"}, "userId": "4sgsBYAsjDHNvB7Q6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JNYchZou2xxxLsQiF/blind-spot-malthusian-crunch", "pageUrlRelative": "/posts/JNYchZou2xxxLsQiF/blind-spot-malthusian-crunch", "linkUrl": "https://www.lesswrong.com/posts/JNYchZou2xxxLsQiF/blind-spot-malthusian-crunch", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Blind%20Spot%3A%20Malthusian%20Crunch&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABlind%20Spot%3A%20Malthusian%20Crunch%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJNYchZou2xxxLsQiF%2Fblind-spot-malthusian-crunch%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Blind%20Spot%3A%20Malthusian%20Crunch%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJNYchZou2xxxLsQiF%2Fblind-spot-malthusian-crunch", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJNYchZou2xxxLsQiF%2Fblind-spot-malthusian-crunch", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 982, "htmlBody": "<p>In an unrelated thread, one thing led to another and we got onto the subject of overpopulation and carrying capacity. I think this topic needs a post of its own.</p>\n<p>TLDR mathy version:</p>\n<p>let <strong>f(m,t)</strong> be the population that can be supported using the fraction of Earth's theoretical resource limit <strong>m</strong> we can exploit at technology level <strong>t</strong> &nbsp;</p>\n<p>let <strong>t =</strong> <strong>k(x)</strong> be the technology level at year <strong>x&nbsp; <br /></strong></p>\n<p>let <strong>p(x)</strong> be population at year <strong>x &nbsp;</strong></p>\n<p>What conditions must constant <strong>m</strong> and functions <strong> f(m,k(x))</strong>, <strong>k(x)</strong>, and <strong>p(x)</strong> satisfy in order to insure that <strong>p(x) - f(m,t) &gt; 0 </strong>for all <strong>x &gt; today()</strong>? What empirical data are relevant to estimating the probability that these conditions are all satisfied?</p>\n<p>Long version:</p>\n<p>Here I would like to explore the evidence for and against the possibility that the following assertions are true:</p>\n<ol>\n<li>Without human intervention, the carrying capacity of our environment (broadly defined<sup>1</sup>) is finite while there are no *intrinsic* limits on population growth.</li>\n<li>Therefore, if the carrying capacity of our environment is not extended at a sufficient rate to outpace population growth and/or population growth does not slow to a sufficient level that carrying capacity can keep up, carrying capacity will eventually become the limit on population growth. </li>\n<li>Abundant data from zoology show that the mechanisms by which carrying capacity limits population growth include starvation, epidemics, and violent competition for resources. If the momentum of population growth carries it past the carrying capacity an <a href=\"http://en.wikipedia.org/wiki/Overshoot_%28population%29\">overshoot</a> occurs, meaning that the population size doesn't just remain at a sustainable level but rather plummets drastically, sometimes to the point of extinction.</li>\n<li>The above three assertions imply that human intervention (by expanding the carrying capacity of our environment in various ways and by limiting our birth-rates in various ways) are what have to rely on to prevent the above scenario, let's call it the Malthusian Crunch.</li>\n<li>Just as the Nazis have discredited eugenics, mainstream environmentalists have discredited (at least among rationalists) the concept of finite carrying capacity by giving it a cultish stigma. Moreover, solutions that rely on sweeping, heavy-handed regulation have recieved so much attention (perhaps because the chain of causality is easier to understand) that to many people they seem like the *only* solutions. Finding these solutions unpalatable, they instead reject the problem itself. And by they, I mean us. </li>\n<li>The alternative most environmentalists either ignore or outright oppose is deliberately trying to accelerate the rate of technological advancement to increase the \"safety zone\" between expansion of carrying capacity and population growth. Moreover, we are close to a level of technology that would allow us to start colonizing the rest of the solar system. Obviously any given niche within the solar system will have its own finite carrying capacity, but it will be many orders of magnitude higher than that of Earth alone. Expanding into those niches won't prevent die-offs on Earth, but will at least be a partial hedge against total extinction and a necessary step toward eventual expansion to other star systems. </li>\n</ol>\n<p>Please note: I'm not proposing that the above assertions <em>must</em> be true, only that they have a high enough probability of being correct that they should be taken as seriously as, for example, grey goo:</p>\n<p>Predictions about the dangers of nanotech made in the 1980's shown no signs of coming true. Yet, there is no known logical or physical reason why they <em>can't</em> come true, so we don't ignore it. We calibrate how much effort should be put into mitigating the risks of nanotechnology by asking what observations should make us update the likelihood we assign to a grey-goo scenario. We approach mitigation strategies from an engineering mindset rather than a political one.</p>\n<p>Shouldn't we hold ourselves to the same standard when discussing population growth and overshoot? Substitute in some other existential risks you take seriously. Which of them have an expectation<sup>2</sup> of occuring <em>before</em> a Malthusian Crunch? Which of them have an expectation of occuring <em>after</em>?</p>\n<p>&nbsp;</p>\n<p>Footnotes:</p>\n<p>1: By carrying capacity, I mean finite resources such as easily extractable ores, water, air, EM spectrum, and land area. Certain very slowly replenishing resources such as fossil fuels and biodiversity also behave like finite resources on a human timescale. I also include non-finite resources that expand or replenish at a finite rate such as useful plants and animals, potable water, arable land, and breathable air. Technology expands carrying capacity by allowing us to exploit all resource more efficiently (paperless offices, telecommuting, fuel efficiency), open up reserves that were previously not economically feasible to exploit (shale oil, methane clathrates, high-rise buildings, seasteading), and accelerate the renewal of non-finite resources (agriculture, land reclamation projects, toxic waste remediation, desalinization plants).</p>\n<p>2: This is a hard question. I'm not asking which catastrophe is the mostly likely to happen <em>ever</em> while holding everything else constant (the possible ones will be tied for 1 and the impossible ones will be tied for 0). I'm asking you to mentally (or physically) draw a set of <a href=\"http://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator\">survival curves</a>, one for each catastrophe, with the x-axis representing time and the y-axis representing fraction of Everett branches where that catastrophe has not yet occured. Now, which curves are the upper bound on the curve representing Malthusian Crunch, and which curves are the lower bound? This is how, in my opinioon (as an aging researcher and biostatistician for whatever that's worth) you think about hazard functions, including those for existential hazards. Keep in mind that some hazard functions change over time because they are conditioned on other events or because they are cyclic in nature. This means that the thing most likely to wipe us out in the next 50 years is not necessarily the same as the thing most likely to wipe us out in the 50 years after that. I don't have a formal answer for how to transform that into optimal allocation of resources between mitigation efforts but that would be the next step.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JNYchZou2xxxLsQiF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 7, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "24436", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 185, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T16:09:36.876Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Saint Petersburg", "slug": "new-lw-meetup-saint-petersburg", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YwxWvNWEYjwicNQNP/new-lw-meetup-saint-petersburg", "pageUrlRelative": "/posts/YwxWvNWEYjwicNQNP/new-lw-meetup-saint-petersburg", "linkUrl": "https://www.lesswrong.com/posts/YwxWvNWEYjwicNQNP/new-lw-meetup-saint-petersburg", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Saint%20Petersburg&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Saint%20Petersburg%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYwxWvNWEYjwicNQNP%2Fnew-lw-meetup-saint-petersburg%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Saint%20Petersburg%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYwxWvNWEYjwicNQNP%2Fnew-lw-meetup-saint-petersburg", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYwxWvNWEYjwicNQNP%2Fnew-lw-meetup-saint-petersburg", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 557, "htmlBody": "<p><strong>This summary was posted to LW main on October 11th. The following week's summary is <a href=\"/lw/iut/new_lw_meetup_cologne_k%C3%B6ln/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/s0\">Saint Petersburg, Russia:&nbsp;<span class=\"date\">27 October 2013 04:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/qs\">Berlin: Fermi paradox discussion:&nbsp;<span class=\"date\">18 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/rw\">Brussels monthly meetup: games!:&nbsp;<span class=\"date\">12 October 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/rq\">Frankfurt (including effective altruism presentation):&nbsp;<span class=\"date\">27 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/s3\">Helsinki Meetup:&nbsp;<span class=\"date\">20 October 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/rz\">Israel Meetup (Tel Aviv): Dealing with Emotional Vampires:&nbsp;<span class=\"date\">17 October 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/s1\">Moscow, Beliefs 2:&nbsp;<span class=\"date\">13 October 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/ru\">Tucson Meetup:&nbsp;<span class=\"date\">12 October 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">12 October 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14:&nbsp;<span class=\"date\">12 October 2013 02:33AM</span></a></li>\n<li><a href=\"/meetups/ry\">London social:&nbsp;<span class=\"date\">13 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/s4\">[West LA] Complex problems, limited information, and rationality; How should we make decisions in real life?:&nbsp;<span class=\"date\">16 October 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YwxWvNWEYjwicNQNP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.3853123099974484e-06, "legacy": true, "legacyId": "24377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["j5cMu8zHQCwZwYhXS", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-18T20:48:37.869Z", "modifiedAt": null, "url": null, "title": "Better Rationality Through Lucid Dreaming", "slug": "better-rationality-through-lucid-dreaming", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:38.179Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6MNjzszLa2w8teDxs/better-rationality-through-lucid-dreaming", "pageUrlRelative": "/posts/6MNjzszLa2w8teDxs/better-rationality-through-lucid-dreaming", "linkUrl": "https://www.lesswrong.com/posts/6MNjzszLa2w8teDxs/better-rationality-through-lucid-dreaming", "postedAtFormatted": "Friday, October 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Better%20Rationality%20Through%20Lucid%20Dreaming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABetter%20Rationality%20Through%20Lucid%20Dreaming%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6MNjzszLa2w8teDxs%2Fbetter-rationality-through-lucid-dreaming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Better%20Rationality%20Through%20Lucid%20Dreaming%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6MNjzszLa2w8teDxs%2Fbetter-rationality-through-lucid-dreaming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6MNjzszLa2w8teDxs%2Fbetter-rationality-through-lucid-dreaming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 550, "htmlBody": "<p><em>Note: this post is no longer endorsed by the author, for reasons partially described <a href=\"/lw/kxy/simulate_and_defer_to_more_rational_selves/bbb2\">here</a>.</em></p>\n<p><em><strong>In the spirit of</strong> <a href=\"/lw/bd/my_way/\">radioing back to describe a path</a>:</em></p>\n<p>The truly absurd thing about dreams lies not with their content, but with the fact that we believe them. Perfectly outrageous and impossible things can occur in dreams without the slightest hesitance to accept them on the part of the dreamer. I have often dreamed myself into bizarre situations that come complete with constructed memories explaining how they secretly make sense!</p>\n<p>However, sometimes we break free from these illusions and become aware of the fact that we are dreaming. This is known as <a href=\"http://en.wikipedia.org/wiki/Lucid_dream\">lucid dreaming</a> and can be an extremely pleasant experience. Unfortunately, relatively few people experience lucid dreams \"naturally;\" fortunately, lucid dreaming is also a skill, and like any other skill it can be trained.</p>\n<p>While this is all very interesting, you may be wondering what it has to do with rationality. Simply put, I have found lucid dreaming perhaps the best training currently available when it comes to increasing general rationality skills. It is one thing to <a href=\"/lw/if/your_strength_as_a_rationalist/\">notice when you are confused</a> by ordinary misunderstandings or tricks; it is another to notice while your own brain is actively constructing memories and environments to fool you!</p>\n<p>I've been involved in lucid dreaming for about eight years now and teaching lucid dreaming for two, so I'm pretty familiar with it on a non-surface level. I've also been explicitly looking into the prospect of using lucid dreaming for rationality training purposes <a href=\"/lw/2yx/call_for_volunteers_rationalists_with/2v1y\">since 2010</a>, and I'm fairly confident that it will prove useful for at least some people here.</p>\n<p>If you can get yourself to the point where you can consistently induce lucid dreaming by noticing the inconsistencies and absurdities of your dream state,<sup>[1]</sup> I predict that you will become a much stronger rationalist in the process. If my prediction is correct, lucid dreaming allows you to hone rationality skills while also having fun, and best of all permits you to do this in your sleep!</p>\n<p>If this sounds appealing to you, perhaps the most concise and efficient resource for learning lucid dreaming is the book <em>Lucid Dreaming</em>, by Dr. Stephen LaBerge. However, this is a book and costs money. If you're not into that, a somewhat less efficient but much more comprehensive view of lucid dreaming can be found on the website <a href=\"http://www.dreamviews.com/\">dreamviews.com</a>. I further recommend that anyone interested in this check out the Facebook group <a href=\"https://www.facebook.com/groups/366172753508500/\">Rational Dreamers</a>. Recently founded by LW user BrienneStrohl, this group provides an opportunity to discuss lucid dreaming and related matters in an environment free from some of the mysticism and confusion that otherwise surrounds this issue.</p>\n<p>All in all, it seems that lucid dreaming may offer a method of training your rationality in a way that is fun,<sup>[2]</sup> interesting, and takes essentially none of your waking hours. Thus, if you are interested in increasing your general rationality, I strongly recommend investigating lucid dreaming. To be frank, my main concern about lucid dreaming as a rationality practice is simply that it seems too good to be true.</p>\n<p>&nbsp;</p>\n<p>[1] Note that this is only one of many ways of inducing lucid dreaming. However, most other techniques that I have tried are not necessarily useful forms of rationality practice, effective as they might be.</p>\n<p>[2] And, to be honest, \"fun\" is an understatement.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6MNjzszLa2w8teDxs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 14, "extendedScore": null, "score": 1.3855732497778117e-06, "legacy": true, "legacyId": "23656", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FBgozHEv7J72NCEPB", "5JDkW4MYXit2CquLs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-19T15:49:53.069Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, BeliefBusters", "slug": "meetup-moscow-beliefbusters", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vioRA7RXby5xFFXz9/meetup-moscow-beliefbusters", "pageUrlRelative": "/posts/vioRA7RXby5xFFXz9/meetup-moscow-beliefbusters", "linkUrl": "https://www.lesswrong.com/posts/vioRA7RXby5xFFXz9/meetup-moscow-beliefbusters", "postedAtFormatted": "Saturday, October 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20BeliefBusters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20BeliefBusters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvioRA7RXby5xFFXz9%2Fmeetup-moscow-beliefbusters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20BeliefBusters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvioRA7RXby5xFFXz9%2Fmeetup-moscow-beliefbusters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvioRA7RXby5xFFXz9%2Fmeetup-moscow-beliefbusters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sk'>Moscow, BeliefBusters</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 October 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we will <strong>create new beliefs</strong> from the old ones.</p>\n\n<p>We will also have short presentation about <strong>buying behaviour and attitudes toward money.</strong></p>\n\n<p>Please take your notes from the last gathering if you have it. If you did not come please think about beliefs you have, write some of them down and bring this notes to the meet up.</p>\n\n<p>We still have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131027_meet_up&amp;utm_content=20131027_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and we will continue <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131027_meet_up&amp;utm_content=20131027_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p>If you are going for the first time:</p>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sk'>Moscow, BeliefBusters</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vioRA7RXby5xFFXz9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3866414701555498e-06, "legacy": true, "legacyId": "24439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__BeliefBusters\">Discussion article for the meetup : <a href=\"/meetups/sk\">Moscow, BeliefBusters</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 October 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time we will <strong>create new beliefs</strong> from the old ones.</p>\n\n<p>We will also have short presentation about <strong>buying behaviour and attitudes toward money.</strong></p>\n\n<p>Please take your notes from the last gathering if you have it. If you did not come please think about beliefs you have, write some of them down and bring this notes to the meet up.</p>\n\n<p>We still have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131027_meet_up&amp;utm_content=20131027_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and we will continue <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131027_meet_up&amp;utm_content=20131027_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p>If you are going for the first time:</p>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__BeliefBusters1\">Discussion article for the meetup : <a href=\"/meetups/sk\">Moscow, BeliefBusters</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, BeliefBusters", "anchor": "Discussion_article_for_the_meetup___Moscow__BeliefBusters", "level": 1}, {"title": "Discussion article for the meetup : Moscow, BeliefBusters", "anchor": "Discussion_article_for_the_meetup___Moscow__BeliefBusters1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-19T16:46:12.051Z", "modifiedAt": null, "url": null, "title": "Creating a Text Shorthand for Uncertainty", "slug": "creating-a-text-shorthand-for-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.611Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ozziegooen", "createdAt": "2013-05-25T09:22:13.574Z", "isAdmin": false, "displayName": "ozziegooen"}, "userId": "efKySALtaLcvtp3jW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jsfSXH8mGrLy9pPqr/creating-a-text-shorthand-for-uncertainty", "pageUrlRelative": "/posts/jsfSXH8mGrLy9pPqr/creating-a-text-shorthand-for-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/jsfSXH8mGrLy9pPqr/creating-a-text-shorthand-for-uncertainty", "postedAtFormatted": "Saturday, October 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Creating%20a%20Text%20Shorthand%20for%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACreating%20a%20Text%20Shorthand%20for%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjsfSXH8mGrLy9pPqr%2Fcreating-a-text-shorthand-for-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Creating%20a%20Text%20Shorthand%20for%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjsfSXH8mGrLy9pPqr%2Fcreating-a-text-shorthand-for-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjsfSXH8mGrLy9pPqr%2Fcreating-a-text-shorthand-for-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 807, "htmlBody": "<div class=\"entry-content\">\n<p>Most things I find I discuss are highly uncertain, but it can be really confusing and wordy to state that uncertainty in writing.  In this last sentence for example I felt the need to write &ldquo;I find&rdquo; to point out uncertainty, for example.</p>\n<p>First, people are really bad at agreeing on probabilities.  So if I say something is &ldquo;very certain&rdquo;, that could mean 80% chance  to me and 95% chance to you.  This is rigorously explained in the <a href=\"http://www.amazon.com/Failure-Risk-Management-Broken-ebook/dp/B0026LTMAU/ref=la_B001JSJHIS_1_2?s=books&amp;ie=UTF8&amp;qid=1380144451&amp;sr=1-2\">Failure of Risk Management</a> (by the same author from How to Measure Anything), where it is explained further to say that this is especially true of risk managers.</p>\n<p>Second, there aren&rsquo;t too many words to use to indicate uncertainty.  I find that I need to repeat the same ones over and over again.  And when they are used, these words can be quite wordy and confusing.</p>\n<ul>\n<li><em>I think</em> that</li>\n<li><em>In my opinion</em></li>\n<li>It <em>makes sense</em> that</li>\n<li>There aren&rsquo;t <em>too</em> many things</li>\n<li><em>Perhaps,</em></li>\n</ul>\n<p>Several years ago some people made the language <a href=\"http://en.wikipedia.org/wiki/E-Prime\">E-Prime</a> in large part to make this uncertainty crystal clear.</p>\n<blockquote>\n<p>E-Prime (short for English-Prime, sometimes denoted &Eacute; or E&prime;) is a prescriptive version of the English language that excludes all forms of the verb to be. E-Prime does not allow the conjugations of to be&mdash;be, am, is, are, was, were, been, being&mdash; the archaic forms of to be (e.g. art, wast, wert), or the contractions of to be&mdash;&rsquo;s, &lsquo;m, &lsquo;re (e.g. I&rsquo;m, he&rsquo;s, she&rsquo;s, they&rsquo;re).<br /> Some scholars advocate using E-Prime as a device to clarify thinking and strengthen writing.[1] For example, the sentence &ldquo;the film was good&rdquo; could not be expressed under the rules of E-Prime, and the speaker might instead say &ldquo;I liked the film&rdquo; or &ldquo;the film made me laugh&rdquo;. The E-Prime versions communicate the speaker&rsquo;s experience rather than judgment, making it harder for the writer or reader to confuse opinion with fact.</p>\n</blockquote>\n<p>While I do intend to look more into E-prime, it seems like a bit much to use on a routine basis.</p>\n<h2 id=\"a-possible-written-solution\">A Possible (Written) Solution</h2>\n<p>I propose that we instead use a symbol at the end of our sentences or propositions to indicate uncertainty.</p>\n<h3 id=\"choosing-the-levels\">Choosing the Levels</h3>\n<p>A scale would have to be created of course in order to indicate what these levels are.  My guess is that the optimal (for usefulness, popularity, and accuracy) amount of levels would be around 5-10, especially because we aren&rsquo;t very good at accessing probability.</p>\n<p>Here&rsquo;s one example that makes sense to me:<br /> 0. ~50% 1. ~65% 2. ~80% 3. ~90% 4. ~95% 5. ~99.9%</p>\n<p>In cases where something is <em>unlikely</em>, this would just work the opposite way (50% to 0.01%).</p>\n<h3 id=\"choosing-a-symbol\">Choosing a Symbol</h3>\n<p>I think that any representation of certainty would have to be achievable with ASCII characters, if not the English keyboard.  Here are some possibilities.  Each is shown to be representative for a level of 4/5, according to a scale similar to what is shown above.</p>\n<h4 id=\"non-numberic-forms\">Non-Numeric forms</h4>\n<ul>\n<li>The universe is expanding.&rsquo;&rsquo;&rsquo;&rsquo;</li>\n<li>The universe is expanding.&ldquo;&ldquo;</li>\n<li>The universe is expanding.`</li>\n<li>The universe is expanding. &middot;&middot;&middot;&middot;</li>\n</ul>\n<h4 id=\"numeric-forms\">Numeric Forms</h4>\n<ul>\n<li>The universe is expanding. `4</li>\n<li>The universe is expanding.4*</li>\n<li>The universe is expanding (~4).</li>\n<li>The universe is expanding (c~4).</li>\n<li>The universe is expanding (c4).</li>\n<li>The universe is expanding ~c4.</li>\n<li>The universe is expanding (?4).</li>\n</ul>\n<p>My personal favorite at this point is to have a number with the tilda sign &ldquo;~&rdquo;, with a symbol for indication (like the &ldquo;c&rdquo; or &ldquo;?&rdquo;).  The dashes are be difficult to read and more confusing to newcomers (c3).</p>\n<h2 id=\"different-kinds-of-uncertainty\">Different Kinds of Uncertainty</h2>\n<p>So far we&rsquo;ve assumed that the definition of &lsquo;uncertainty&rsquo; is relatively clear, but sometimes there are different definitions of uncertainty.</p>\n<p>For instance, there&rsquo;s the certainty of &ldquo;the existing scientific literature strongly agrees that evolution is true&rdquo;, and the certainty of &ldquo;I personally am very certain that the Paleo diet is good, even though others might disagree.&rdquo;</p>\n<p>These could be indicated by different symbols.  This would require a small dictionary of symbols/standards, but this may not be very unreasonable.</p>\n<p>Say we use &lsquo;c&rsquo; to indicate &lsquo;consensus&rsquo; and &lsquo;i&rsquo; to indicate &lsquo;personal intuition&rsquo;, and &lsquo;r&rsquo; to indicate &lsquo;personal research/rationality&rsquo;.  Not all of these would need to be used in every instance, only the ones that are instantially relevant.</p>\n<p>Some statements could be as follows:</p>\n<ul>\n<li>The universe is expanding ~c5. </li>\n<li>I&rsquo;m not likely to do well in finance ~i4c1r2. </li>\n<li>Polyphasic sleep has a lot of potential ~r4c1.</li>\n<li>I was a poor math student ~i4r2 in high school, but have learned a lot ~i3r2 since then.</li>\n</ul>\n<p>Of course, we&rsquo;d need a definition for this, which is effectively a standard.  For now I&rsquo;ll call it &ldquo;Uncertainty Notation V0.1&rdquo;  I&rsquo;ll try it out in future posts as an experiment. <a href=\"http://www.ascii.cl/htmlcodes.html\">HTML Codes Reference</a></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jsfSXH8mGrLy9pPqr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 1.386694220155251e-06, "legacy": true, "legacyId": "24444", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div class=\"entry-content\">\n<p>Most things I find I discuss are highly uncertain, but it can be really confusing and wordy to state that uncertainty in writing.  In this last sentence for example I felt the need to write \u201cI find\u201d to point out uncertainty, for example.</p>\n<p>First, people are really bad at agreeing on probabilities.  So if I say something is \u201cvery certain\u201d, that could mean 80% chance  to me and 95% chance to you.  This is rigorously explained in the <a href=\"http://www.amazon.com/Failure-Risk-Management-Broken-ebook/dp/B0026LTMAU/ref=la_B001JSJHIS_1_2?s=books&amp;ie=UTF8&amp;qid=1380144451&amp;sr=1-2\">Failure of Risk Management</a> (by the same author from How to Measure Anything), where it is explained further to say that this is especially true of risk managers.</p>\n<p>Second, there aren\u2019t too many words to use to indicate uncertainty.  I find that I need to repeat the same ones over and over again.  And when they are used, these words can be quite wordy and confusing.</p>\n<ul>\n<li><em>I think</em> that</li>\n<li><em>In my opinion</em></li>\n<li>It <em>makes sense</em> that</li>\n<li>There aren\u2019t <em>too</em> many things</li>\n<li><em>Perhaps,</em></li>\n</ul>\n<p>Several years ago some people made the language <a href=\"http://en.wikipedia.org/wiki/E-Prime\">E-Prime</a> in large part to make this uncertainty crystal clear.</p>\n<blockquote>\n<p>E-Prime (short for English-Prime, sometimes denoted \u00c9 or E\u2032) is a prescriptive version of the English language that excludes all forms of the verb to be. E-Prime does not allow the conjugations of to be\u2014be, am, is, are, was, were, been, being\u2014 the archaic forms of to be (e.g. art, wast, wert), or the contractions of to be\u2014\u2019s, \u2018m, \u2018re (e.g. I\u2019m, he\u2019s, she\u2019s, they\u2019re).<br> Some scholars advocate using E-Prime as a device to clarify thinking and strengthen writing.[1] For example, the sentence \u201cthe film was good\u201d could not be expressed under the rules of E-Prime, and the speaker might instead say \u201cI liked the film\u201d or \u201cthe film made me laugh\u201d. The E-Prime versions communicate the speaker\u2019s experience rather than judgment, making it harder for the writer or reader to confuse opinion with fact.</p>\n</blockquote>\n<p>While I do intend to look more into E-prime, it seems like a bit much to use on a routine basis.</p>\n<h2 id=\"A_Possible__Written__Solution\">A Possible (Written) Solution</h2>\n<p>I propose that we instead use a symbol at the end of our sentences or propositions to indicate uncertainty.</p>\n<h3 id=\"Choosing_the_Levels\">Choosing the Levels</h3>\n<p>A scale would have to be created of course in order to indicate what these levels are.  My guess is that the optimal (for usefulness, popularity, and accuracy) amount of levels would be around 5-10, especially because we aren\u2019t very good at accessing probability.</p>\n<p>Here\u2019s one example that makes sense to me:<br> 0. ~50% 1. ~65% 2. ~80% 3. ~90% 4. ~95% 5. ~99.9%</p>\n<p>In cases where something is <em>unlikely</em>, this would just work the opposite way (50% to 0.01%).</p>\n<h3 id=\"Choosing_a_Symbol\">Choosing a Symbol</h3>\n<p>I think that any representation of certainty would have to be achievable with ASCII characters, if not the English keyboard.  Here are some possibilities.  Each is shown to be representative for a level of 4/5, according to a scale similar to what is shown above.</p>\n<h4 id=\"Non_Numeric_forms\">Non-Numeric forms</h4>\n<ul>\n<li>The universe is expanding.\u2019\u2019\u2019\u2019</li>\n<li>The universe is expanding.\u201c\u201c</li>\n<li>The universe is expanding.`</li>\n<li>The universe is expanding. \u00b7\u00b7\u00b7\u00b7</li>\n</ul>\n<h4 id=\"Numeric_Forms\">Numeric Forms</h4>\n<ul>\n<li>The universe is expanding. `4</li>\n<li>The universe is expanding.4*</li>\n<li>The universe is expanding (~4).</li>\n<li>The universe is expanding (c~4).</li>\n<li>The universe is expanding (c4).</li>\n<li>The universe is expanding ~c4.</li>\n<li>The universe is expanding (?4).</li>\n</ul>\n<p>My personal favorite at this point is to have a number with the tilda sign \u201c~\u201d, with a symbol for indication (like the \u201cc\u201d or \u201c?\u201d).  The dashes are be difficult to read and more confusing to newcomers (c3).</p>\n<h2 id=\"Different_Kinds_of_Uncertainty\">Different Kinds of Uncertainty</h2>\n<p>So far we\u2019ve assumed that the definition of \u2018uncertainty\u2019 is relatively clear, but sometimes there are different definitions of uncertainty.</p>\n<p>For instance, there\u2019s the certainty of \u201cthe existing scientific literature strongly agrees that evolution is true\u201d, and the certainty of \u201cI personally am very certain that the Paleo diet is good, even though others might disagree.\u201d</p>\n<p>These could be indicated by different symbols.  This would require a small dictionary of symbols/standards, but this may not be very unreasonable.</p>\n<p>Say we use \u2018c\u2019 to indicate \u2018consensus\u2019 and \u2018i\u2019 to indicate \u2018personal intuition\u2019, and \u2018r\u2019 to indicate \u2018personal research/rationality\u2019.  Not all of these would need to be used in every instance, only the ones that are instantially relevant.</p>\n<p>Some statements could be as follows:</p>\n<ul>\n<li>The universe is expanding ~c5. </li>\n<li>I\u2019m not likely to do well in finance ~i4c1r2. </li>\n<li>Polyphasic sleep has a lot of potential ~r4c1.</li>\n<li>I was a poor math student ~i4r2 in high school, but have learned a lot ~i3r2 since then.</li>\n</ul>\n<p>Of course, we\u2019d need a definition for this, which is effectively a standard.  For now I\u2019ll call it \u201cUncertainty Notation V0.1\u201d  I\u2019ll try it out in future posts as an experiment. <a href=\"http://www.ascii.cl/htmlcodes.html\">HTML Codes Reference</a></p>\n</div>", "sections": [{"title": "A Possible (Written) Solution", "anchor": "A_Possible__Written__Solution", "level": 1}, {"title": "Choosing the Levels", "anchor": "Choosing_the_Levels", "level": 2}, {"title": "Choosing a Symbol", "anchor": "Choosing_a_Symbol", "level": 2}, {"title": "Non-Numeric forms", "anchor": "Non_Numeric_forms", "level": 3}, {"title": "Numeric Forms", "anchor": "Numeric_Forms", "level": 3}, {"title": "Different Kinds of Uncertainty", "anchor": "Different_Kinds_of_Uncertainty", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-20T00:05:08.582Z", "modifiedAt": null, "url": null, "title": "[Link] You and Your Research", "slug": "link-you-and-your-research", "viewCount": null, "lastCommentedAt": "2013-10-20T00:05:08.582Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nLgzRwcShJMwxEYsJ/link-you-and-your-research", "pageUrlRelative": "/posts/nLgzRwcShJMwxEYsJ/link-you-and-your-research", "linkUrl": "https://www.lesswrong.com/posts/nLgzRwcShJMwxEYsJ/link-you-and-your-research", "postedAtFormatted": "Sunday, October 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20You%20and%20Your%20Research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20You%20and%20Your%20Research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnLgzRwcShJMwxEYsJ%2Flink-you-and-your-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20You%20and%20Your%20Research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnLgzRwcShJMwxEYsJ%2Flink-you-and-your-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnLgzRwcShJMwxEYsJ%2Flink-you-and-your-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 361, "htmlBody": "<p>I've seen <a href=\"http://en.wikipedia.org/wiki/Richard_Hamming\">Richard Hamming's</a> classic talk You And Your Research referenced several times on LessWrong and figured I would post the full version. The introduction is reproduced below:</p>\n<blockquote>\n<p>The title of my talk is, ``You and Your Research.'' It is not about managing research, it is about how you individually do your research. I could give a talk on the other subject - but it's not, it's about you. I'm not talking about ordinary run-of-the-mill research; I'm talking about great research. And for the sake of describing great research I'll occasionally say Nobel-Prize type of work. It doesn't have to gain the Nobel Prize, but I mean those kinds of things which we perceive are significant things. Relativity, if you want, Shannon's information theory, any number of outstanding theories - that's the kind of thing I'm talking about.</p>\n<p>Now, how did I come to do this study? At Los Alamos I was brought in to run the computing machines which other people had got going, so those scientists and physicists could get back to business. I saw I was a stooge. I saw that although physically I was the same, they were different. And to put the thing bluntly, I was envious. I wanted to know why they were so different from me. I saw Feynman up close. I saw Fermi and Teller. I saw Oppenheimer. I saw Hans Bethe: he was my boss. I saw quite a few very capable people. I became very interested in the difference between those who do and those who might have done.</p>\n<p>When I came to Bell Labs, I came into a very productive department. Bode was the department head at the time; Shannon was there, and there were other people. I continued examining the questions, ``Why?'' and ``What is the difference?'' I continued subsequently by reading biographies, autobiographies, asking people questions such as: ``How did you come to do this?'' I tried to find out what are the differences. And that's what this talk is about.</p>\n</blockquote>\n<p>I consider this talk good and useful not only for those interested in research, but for those interested in achieving much of anything. <a href=\"http://www.cs.virginia.edu/~robins/YouAndYourResearch.html\">Check it out!</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nLgzRwcShJMwxEYsJ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 1.3871054885455617e-06, "legacy": true, "legacyId": "24445", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-10-20T00:05:08.582Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-20T00:25:25.437Z", "modifiedAt": null, "url": null, "title": "Existential Risk II", "slug": "existential-risk-ii", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.511Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fowlertm", "createdAt": "2012-01-07T20:35:26.490Z", "isAdmin": false, "displayName": "fowlertm"}, "userId": "gDAt4FezxH8dDr5EY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/52L8bioRghCChzfNy/existential-risk-ii", "pageUrlRelative": "/posts/52L8bioRghCChzfNy/existential-risk-ii", "linkUrl": "https://www.lesswrong.com/posts/52L8bioRghCChzfNy/existential-risk-ii", "postedAtFormatted": "Sunday, October 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Existential%20Risk%20II&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExistential%20Risk%20II%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F52L8bioRghCChzfNy%2Fexistential-risk-ii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Existential%20Risk%20II%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F52L8bioRghCChzfNy%2Fexistential-risk-ii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F52L8bioRghCChzfNy%2Fexistential-risk-ii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2793, "htmlBody": "<p><strong>Meta</strong></p>\n<p>-This is not a duplicate of the&nbsp;<a href=\"/lw/8f0/existential_risk/\">original</a>&nbsp;less wrong x-risk primer. &nbsp;I like lukeprog's article just fine, but it works mostly as a punch in the gut for anyone who needs a wake up call. &nbsp;Very little of the actual research on x-risk is discussed in that article, so the gap that was there before it was published was largely there after. &nbsp;My article and his would work well being read together. &nbsp;</p>\n<p>-This was originally written to accompany a presentation I gave, hence the random inclusion of both hyperlinks and citations. &nbsp;It also lives, with minor differences, <a href=\"http://rulerstothesky.wordpress.com/2013/10/03/existential-risk-a-primer/\">here</a>.</p>\n<p>-Summary: For various reasons the future is scarier than a lot of people realize. &nbsp;All sorts of things could lead to the destruction of the human species, ranging from asteroid impacts to runaway AIs, and these things are united by the fact that any one of them could destroy the value of the future from a human perspective. &nbsp;The dangers can be separated into bangs (very sudden extinction), crunches (not fatal but crippling), shrieks (mostly curse with a little blessing), and whimpers (a long, slow fading), though there is nothing sacred about these categories. &nbsp;Some humans have are trying to prevent this, though their methods are still in their infancy. &nbsp;Much more should be done to support them. &nbsp;</p>\n<p><strong>In the beginning</strong></p>\n<p><span style=\"line-height: 1.5;\">I want to start this off with a </span><a style=\"line-height: 1.5;\" href=\"http://www.moreright.net/dont-trust-god/\">quote</a><span style=\"line-height: 1.5;\">, which nicely captures both how I use to feel about the idea of human extinction and how I feel about it now:</span></p>\n<blockquote>I think many atheists still trust in God. They&nbsp;say&nbsp;there is no God, but &hellip;[a]sk them how they think the future will go, especially with regards to Moral Progress, Human Evolution, Technological Progress, etc. There are a few different answers you will get: Some people just don&rsquo;t know or don&rsquo;t care. Some people will tell you stories of glorious progress&hellip; The ones who tell stories are the ones who haven&rsquo;t quite internalized that there is no god. The people who don&rsquo;t care aren&rsquo;t paying attention. The correct answer is not nervous excitement, or world-weary cynicism, it is&nbsp;fear. -Nyan Sandwich</blockquote>\n<p>Back when I was a Christian I gave some thought to the rapture, which is not entirely unlike extinction as far as most ten-year-olds can tell. &nbsp;Sometime during this period&nbsp;I found a slim little book of fiction which portrayed a damned soul's experience of burning in hell forever, and that did scare me. &nbsp;Such torment, as luck would have it, is easy enough to avoid if you just call god the right name and ask forgiveness often enough.</p>\n<p>When I was old enough to contemplate possible secular origins of the apocalypse, I was both an atheist and one of the people who tell glorious stories about the future. &nbsp;The potential fruits of technological development, from the end of aging to the creation of a benevolent super-human AI, excited me, and still excite me now. &nbsp;No doubt I would've admitted the possibility of human extinction, I don't really remember. &nbsp;But there wasn't the kind of internal siren that should go off when you start thinking seriously about one of the Worst Possible Outcomes. &nbsp;<em>That&nbsp;</em>I would remember.</p>\n<p>But as I've gotten older I've come to appreciate that most of us<span style=\"line-height: 1.5;\">&nbsp;are not afraid enough of the future. Those who are afraid, are often afraid for the wrong reasons.</span></p>\n<p><strong>What is an Existential Risk?</strong></p>\n<p>An existential risk or x-risk (to use a common abbreviation) is \"...one that threatens to annihilate Earth-originating intelligent life or permanently and drastically to curtail its potential\" (<a href=\"http://www.nickbostrom.com/papers/globalagenda.pdf\">Bostrom 2006</a>). The definition contains some subtlety, as not all x-risks involve the outright death of every human. Some could take potentially eons to complete, and some are even survivable. Positioning x-risks within the broader landscape of risks <a href=\"http://www.existential-risk.org/concept.html\">yields</a>&nbsp;something like this chart: &nbsp;<img src=\"http://geopolicraticus.files.wordpress.com/2013/02/qualitative-categories-of-risk.png?w=460&amp;h=345\" alt=\"\" width=\"460\" height=\"345\" />&nbsp;&nbsp;</p>\n<p>At the top right extreme is where Cthulu sleeps. &nbsp;They are risks that carry the potential to drastically and negatively affect this and every subsequent human generation. So as not to keep everyone in suspense, let's use this chart to put a face on the shadows.</p>\n<p><strong>Four Types of Existential Risks</strong></p>\n<p>Philosopher <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a> has outlined four broad categories of x-risk. &nbsp;In more recent papers he hasn't used the terminology that I'm using here, so maybe he thinks the names are obsolete. &nbsp;I find them evocative and useful, however, so I'll stick with them until I have a reason to change.</p>\n<p><em>Bangs</em> are probably the easiest&nbsp;risks to conceptualize. &nbsp;Any event which causes the sudden and complete extinction of humanity would count as a Bang. &nbsp;Think asteroid impacts, supervolcanic eruptions, or intentionally misused nanoweapons.</p>\n<p><em>Crunches</em> are risks which humans survive but which leaves us permanently unable to navigate to a more valuable future. &nbsp;An example might be depleting our planetary resources before we manage to build the infrastructure needed to mine asteroids or colonize other planets. &nbsp;After all the die-offs and fighting, some remnant of humanity could probably survive indefinitely, but it wouldn't be a world you'd want to wake up in.</p>\n<p><em>Shrieks</em> occur when a post-human civilization develops but only manages to realize a small amount of its potential. &nbsp;Shrieks are very difficult to effectively categorize, and I'm going to leave examples until the discussion below.</p>\n<p><em>Whimpers</em> are really long-term existential risks. &nbsp;The most straight forward is the heat death of the universe; within our current understanding of physics, no matter how advanced we get we will eventually be unable to escape the ravages of entropy. Another could be if we encounter a hostile alien civilization that decides to conquer us after we've already colonized the galaxy. Such a process could take a long time, and thus would count as a whimper.</p>\n<p>Just because whimpers are so much less immediate than other categories of risk and x-risk doesn't automatically mean we can just ignore them; <a href=\"https://docs.google.com/a/bigthink.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE\">it has been argued</a> that affecting the far future is one of the most important projects facing humanity, and thus we should take the time to do it right.</p>\n<p>Sharp readers will no doubt have noticed that there is quite a bit of fuzziness to these classifications. &nbsp;Where, for example, should we put all-out nuclear war, the establishment of an oppressive global dictatorship, or the development of a dangerous and uncontrollable superintelligent AI? If everyone dies in the war it counts as a bang, but if it makes a nightmare of the biosphere while leaving a good fraction of humanity intact it would be a crunch. &nbsp;A global dictatorship wouldn't be an x-risk unless it used some (probably technological) means to achieve near-total control and long-term stability, in which case it would be a crunch. &nbsp;But it isn't hard to imagine such a situation in which&nbsp;<em>some</em> parts of life did get better, like if a violently oppressive government continued to develop advanced medicines so that citizens were universally healthier and longer-lived than people today. &nbsp;If that happened, it would be a Shriek. &nbsp;A similar analysis applies to the AI, with the possible outcomes being Bang, Crunch, and Shriek depending on just how badly we misprogrammed it.</p>\n<p><strong>What Ties These Threads Together?</strong></p>\n<p><strong></strong>Even if you think existential threats deserve more attention, the rationale for treating them as a diverse but unified phenomenon may not be obvious. &nbsp;In addition to the crucial but (relatively) straightforward work of, say, tracking Near-Earth Objects (NEOs), existential risk researchers also think seriously about alien invasions and rogue AIs. With such a range of speculativeness, why group x-risks together at all?</p>\n<p>It turns out that they share a cluster of features which does give them some cohesion and make them worth studying under a single label, not all of which I discuss here. &nbsp;First and most obvious is that should any of them occur the consequences would be truly vast relative to any other kind of risk. &nbsp;To see why, think about the difference between a catastrophe that kills 99% of humanity and one that kills 100%. &nbsp;As big a tragedy as the former would be, there's a chance humans could recover and build a post-human civilization. &nbsp;But if every person dies, then the entire value of our future is lost (<a href=\"http://www.existential-risk.org/concept.html\">Bostrom 2013</a>).</p>\n<p>Second, these are not risks which admit of a trial and error approach. &nbsp;Pretty much by definition a collision with an x-risk will spell doom for humanity, and so we must be more proactive in our strategies for reducing them. Related to this, we as a species have neither the cultural nor biological instincts needed to prepare us for the possibility of extinction. &nbsp;A group of people might live through several droughts and thus develop strong collective norms towards planning ahead and keeping generous food reserves. &nbsp;But they cannot have gone extinct multiple times, and thus they can't rely on their shared experience and cultural memory to guide them in the future. &nbsp;I certainly hope we can develop a set of norms and institutions which makes us all safer, but we can't wait to learn from history. &nbsp;We're going to have to start well in advance, or we won't survive.</p>\n<p>A final commonality I'll mention is that the solutions to quite a number of x-risks are themselves x-risks. &nbsp;A powerful enough government could effectively halt research into dangerous pathogens or nano-replicators. &nbsp;But given how States have generally comported themselves in the past, one would do well to be cautious before investing them with that kind of power. &nbsp;Ditto for a superhuman AI, which could set up an infrastructure to protect us from asteroids, nuclear war, or even other less Friendly AI. Get the coding just a little wrong, though, <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">and it might reuse your carbon to make paperclips</a>.</p>\n<p>It is indeed a knife edge along which we creep towards the future.</p>\n<p><strong>Measuring the Monsters</strong></p>\n<p>A first step is getting straight about how likely survival is. &nbsp;The reader may have encountered predictions of the \"we have only a 50% chance of surviving the next hundred years\" variety. &nbsp;Examining the validity of such estimates is worth doing, but I won't be taking up that challenge here; I tend to agree that these figures involves a lot of subjective judgement, but that even if the chances were very very small it would still be worth taking seriously (<a href=\"/Bostrom 2006\">Bostrom 2006</a>). &nbsp; At any rate, it seems to me that trying to calculate an overall likelihood of human extinction is going to be premature before we've nailed down probabilities for some of the different possible extinction scenarios. &nbsp;It is to the techniques which x-risk researchers rely on to try and do this that I now turn.</p>\n<p>X-risk-assessments rely on both&nbsp;<em>direct</em> and&nbsp;<em>indirect</em> methods (<a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom 2002</a>). &nbsp;Using a direct method involves building a detailed causal model of the phenomenon and using that to generate a risk probability, while indirect methods include&nbsp;arguments, thought experiments, and information that we use to constrain and refine our guesses.</p>\n<p>As far as I know for some x-risks we could use direct methods if we just had a way to gather the relevant information. &nbsp;If we knew where all the NEOs were we could use settled physics to predict whether any of them posed a threat and then prioritize accordingly. But&nbsp;we don't where they all are, so we might instead examine the frequency of impacts throughout the history of the Earth and then reason about whether or not we think an impact will happen soon.&nbsp;&nbsp; It would be nice to exclusively use direct methods, but we supplement with indirect methods when we can't, and of course for x-risks like AI we are in an even more uncertain position than we are for NEOs.</p>\n<p><strong>The Fermi Paradox</strong></p>\n<p>Applying indirect methods can lead to some strange and counter-intuitive territory, an example of which is the mysteries surrounding the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">Fermi Paradox</a>. &nbsp;The central question is: in a universe with so many potential hotbeds of life, why is it that when we listen for stirring in the void all we hear is silence? &nbsp;Many feel that the universe must be teeming with life, some of it intelligent, so why haven't we see any sign of it yet?</p>\n<p>Musing about possible <a href=\"http://io9.com/11-of-the-weirdest-solutions-to-the-fermi-paradox-456850746\">solutions </a>to the Fermi Paradox can be a lot of fun, and it's worth pointing out that we haven't been looking that long or that hard for signals yet. Nevertheless I think the argument has some meat to it.</p>\n<p>Observing this state of affairs, some have postulated the existence of at least one&nbsp;<a href=\"http://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a>, a step in the chain of development from the first organisms to space-faring civilizations that must be&nbsp;<em>extremely</em> hard to achieve. &nbsp;<strong>&nbsp;</strong></p>\n<p>This is cause for concern because the Great Filter could be in front of us or behind us. &nbsp;Let me explain: imagine a continuum with the simplest self-replicating molecules on one side and the Star Trek Enterprise on the other. &nbsp;From our position on the continuum we want to know whether or not we have already passed one of the hardest steps, but we have only our own planet to look at. &nbsp;So imagine that we send out probes to thousands of different worlds in the hopes that we will learn something.</p>\n<p>If we find lots of simple eukaryotes that means that the Great Filter is probably not before the development of membrane-bound organelles. The list of possible places on the continuum the Great Filter could be shrinks just a little bit. &nbsp;If instead we find lots of mammals and reptiles (or creatures that are very different but about as advanced), that means the Great Filter is probably not before the rise of complex organisms, so the places the Great Filter might be hiding shrinks again. &nbsp;Worst of all would be if we find the dead ruins of many different advanced civilizations. &nbsp;This would imply that the real killer is yet to come, and we will almost certainly not survive it.</p>\n<p>As happy as many people would be to discover evidence of life in the universe, a <a href=\"http://www.nickbostrom.com/extraterrestrial.pdf\">case</a>&nbsp;has been made that we should hope to find only barren rocks waiting for us in the final frontier. If not even simple bacteria evolve on most worlds, then there is still a chance that the Great Filter is behind us, and we can worry only about the new challenges ahead, which may or not be Filters as great as the ones in the past.</p>\n<p>If all this seems really abstract out there, that's because it is. &nbsp;But I hope it is clear how this sort of thinking can help us interpret new data, make better guesses, form new hypotheses, etc. &nbsp;When dealing with stakes this high and information this limited, one must do the best they can with what's available.</p>\n<p><strong style=\"line-height: 1.5;\">Mitigation</strong></p>\n<p>What priority should we place on reducing existential risk and how can we do that? I don't know of anyone who thinks&nbsp;<em>all</em> our effort should go towards mitigating x-risks; there are lots of pressing issues which are not x-risks that are worth our attention, like abject poverty or geopolitical instability. &nbsp;But I feel comfortable saying we aren't doing nearly as much as we should be. Given the stakes and the fact that there probably won't be a second chance we are going to have to meet x-risks head on and be aggressively proactive in mitigating them.</p>\n<p>Suppose we taboo 'aggressively proactive', what's left? &nbsp;Well the first step, as it so often is, will be just to get the right people to be aware of the problem (<a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom 2002</a>). &nbsp;Thankfully this is starting to be the case as more funding and brain power go into existential risk reduction. We have to get to a point where we are spending at least as much time, energy, and effort making new technology safe as we do making it more powerful. &nbsp;More international cooperation on these matters will be necessary, and there should be some sort of mechanism by which efforts to develop existentially-threatening technologies like super-virulent pathogens can be stopped. &nbsp;I don't like recommending this at all, but almost anything is preferable to extinction.</p>\n<p>In the meantime both research that directly reduces x-risk (like <a href=\"http://neo.jpl.nasa.gov/index.html\">NEO detection</a>), as well as research that will help elucidate deep and foundational issues in x-risk (<a href=\"https://www.google.co.kr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCgQFjAA&amp;url=http%3A%2F%2Fwww.fhi.ox.ac.uk%2F&amp;ei=wBpjUsiaOaW5iQeT8oD4Cg&amp;usg=AFQjCNGJPMbDHCz5yjqBjGcRTQvsdUBXAw&amp;bvm=bv.54934254,d.aGc\">FHI</a>&nbsp;and <a href=\"https://www.google.co.kr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CD0QFjAC&amp;url=http%3A%2F%2Fintelligence.org%2F&amp;ei=5BpjUuG2J8uSiQfJ94Bg&amp;usg=AFQjCNHTe9xWSVEOAHRidIEtZfcA9ufViA&amp;bvm=bv.54934254,d.aGc\">MIRI</a>) should be encouraged. &nbsp;It's a stereotype that research papers always end with a call for more research, but as was pointed out by lukeprog in a talk he gave, there's more research done on lipstick than on friendly AI. &nbsp;This generalizes to x-risk more broadly, and represents the truly worrying state of our priorities. &nbsp;</p>\n<p><strong>Conclusion</strong></p>\n<p>Though I maintain we should be more fearful of what's to come, that should not obscure the fact that the human potential is vast and truly exciting. &nbsp;If the right steps are taken, we and our descendants will have a future better than most can even dream of. &nbsp;Life spans measured in eons could be spent learning and loving in ways our terrestrial languages don't even have words for yet. &nbsp;The vision of a post-human civilization flinging it's trillions of descendants into the universe to light up the dark is tremendously inspiring. &nbsp;It's worth fighting for.</p>\n<p>But we have much work ahead of us.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "52L8bioRghCChzfNy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 1.3871244960218432e-06, "legacy": true, "legacyId": "24372", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Meta\">Meta</strong></p>\n<p>-This is not a duplicate of the&nbsp;<a href=\"/lw/8f0/existential_risk/\">original</a>&nbsp;less wrong x-risk primer. &nbsp;I like lukeprog's article just fine, but it works mostly as a punch in the gut for anyone who needs a wake up call. &nbsp;Very little of the actual research on x-risk is discussed in that article, so the gap that was there before it was published was largely there after. &nbsp;My article and his would work well being read together. &nbsp;</p>\n<p>-This was originally written to accompany a presentation I gave, hence the random inclusion of both hyperlinks and citations. &nbsp;It also lives, with minor differences, <a href=\"http://rulerstothesky.wordpress.com/2013/10/03/existential-risk-a-primer/\">here</a>.</p>\n<p>-Summary: For various reasons the future is scarier than a lot of people realize. &nbsp;All sorts of things could lead to the destruction of the human species, ranging from asteroid impacts to runaway AIs, and these things are united by the fact that any one of them could destroy the value of the future from a human perspective. &nbsp;The dangers can be separated into bangs (very sudden extinction), crunches (not fatal but crippling), shrieks (mostly curse with a little blessing), and whimpers (a long, slow fading), though there is nothing sacred about these categories. &nbsp;Some humans have are trying to prevent this, though their methods are still in their infancy. &nbsp;Much more should be done to support them. &nbsp;</p>\n<p><strong id=\"In_the_beginning\">In the beginning</strong></p>\n<p><span style=\"line-height: 1.5;\">I want to start this off with a </span><a style=\"line-height: 1.5;\" href=\"http://www.moreright.net/dont-trust-god/\">quote</a><span style=\"line-height: 1.5;\">, which nicely captures both how I use to feel about the idea of human extinction and how I feel about it now:</span></p>\n<blockquote>I think many atheists still trust in God. They&nbsp;say&nbsp;there is no God, but \u2026[a]sk them how they think the future will go, especially with regards to Moral Progress, Human Evolution, Technological Progress, etc. There are a few different answers you will get: Some people just don\u2019t know or don\u2019t care. Some people will tell you stories of glorious progress\u2026 The ones who tell stories are the ones who haven\u2019t quite internalized that there is no god. The people who don\u2019t care aren\u2019t paying attention. The correct answer is not nervous excitement, or world-weary cynicism, it is&nbsp;fear. -Nyan Sandwich</blockquote>\n<p>Back when I was a Christian I gave some thought to the rapture, which is not entirely unlike extinction as far as most ten-year-olds can tell. &nbsp;Sometime during this period&nbsp;I found a slim little book of fiction which portrayed a damned soul's experience of burning in hell forever, and that did scare me. &nbsp;Such torment, as luck would have it, is easy enough to avoid if you just call god the right name and ask forgiveness often enough.</p>\n<p>When I was old enough to contemplate possible secular origins of the apocalypse, I was both an atheist and one of the people who tell glorious stories about the future. &nbsp;The potential fruits of technological development, from the end of aging to the creation of a benevolent super-human AI, excited me, and still excite me now. &nbsp;No doubt I would've admitted the possibility of human extinction, I don't really remember. &nbsp;But there wasn't the kind of internal siren that should go off when you start thinking seriously about one of the Worst Possible Outcomes. &nbsp;<em>That&nbsp;</em>I would remember.</p>\n<p>But as I've gotten older I've come to appreciate that most of us<span style=\"line-height: 1.5;\">&nbsp;are not afraid enough of the future. Those who are afraid, are often afraid for the wrong reasons.</span></p>\n<p><strong id=\"What_is_an_Existential_Risk_\">What is an Existential Risk?</strong></p>\n<p>An existential risk or x-risk (to use a common abbreviation) is \"...one that threatens to annihilate Earth-originating intelligent life or permanently and drastically to curtail its potential\" (<a href=\"http://www.nickbostrom.com/papers/globalagenda.pdf\">Bostrom 2006</a>). The definition contains some subtlety, as not all x-risks involve the outright death of every human. Some could take potentially eons to complete, and some are even survivable. Positioning x-risks within the broader landscape of risks <a href=\"http://www.existential-risk.org/concept.html\">yields</a>&nbsp;something like this chart: &nbsp;<img src=\"http://geopolicraticus.files.wordpress.com/2013/02/qualitative-categories-of-risk.png?w=460&amp;h=345\" alt=\"\" width=\"460\" height=\"345\">&nbsp;&nbsp;</p>\n<p>At the top right extreme is where Cthulu sleeps. &nbsp;They are risks that carry the potential to drastically and negatively affect this and every subsequent human generation. So as not to keep everyone in suspense, let's use this chart to put a face on the shadows.</p>\n<p><strong id=\"Four_Types_of_Existential_Risks\">Four Types of Existential Risks</strong></p>\n<p>Philosopher <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a> has outlined four broad categories of x-risk. &nbsp;In more recent papers he hasn't used the terminology that I'm using here, so maybe he thinks the names are obsolete. &nbsp;I find them evocative and useful, however, so I'll stick with them until I have a reason to change.</p>\n<p><em>Bangs</em> are probably the easiest&nbsp;risks to conceptualize. &nbsp;Any event which causes the sudden and complete extinction of humanity would count as a Bang. &nbsp;Think asteroid impacts, supervolcanic eruptions, or intentionally misused nanoweapons.</p>\n<p><em>Crunches</em> are risks which humans survive but which leaves us permanently unable to navigate to a more valuable future. &nbsp;An example might be depleting our planetary resources before we manage to build the infrastructure needed to mine asteroids or colonize other planets. &nbsp;After all the die-offs and fighting, some remnant of humanity could probably survive indefinitely, but it wouldn't be a world you'd want to wake up in.</p>\n<p><em>Shrieks</em> occur when a post-human civilization develops but only manages to realize a small amount of its potential. &nbsp;Shrieks are very difficult to effectively categorize, and I'm going to leave examples until the discussion below.</p>\n<p><em>Whimpers</em> are really long-term existential risks. &nbsp;The most straight forward is the heat death of the universe; within our current understanding of physics, no matter how advanced we get we will eventually be unable to escape the ravages of entropy. Another could be if we encounter a hostile alien civilization that decides to conquer us after we've already colonized the galaxy. Such a process could take a long time, and thus would count as a whimper.</p>\n<p>Just because whimpers are so much less immediate than other categories of risk and x-risk doesn't automatically mean we can just ignore them; <a href=\"https://docs.google.com/a/bigthink.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE\">it has been argued</a> that affecting the far future is one of the most important projects facing humanity, and thus we should take the time to do it right.</p>\n<p>Sharp readers will no doubt have noticed that there is quite a bit of fuzziness to these classifications. &nbsp;Where, for example, should we put all-out nuclear war, the establishment of an oppressive global dictatorship, or the development of a dangerous and uncontrollable superintelligent AI? If everyone dies in the war it counts as a bang, but if it makes a nightmare of the biosphere while leaving a good fraction of humanity intact it would be a crunch. &nbsp;A global dictatorship wouldn't be an x-risk unless it used some (probably technological) means to achieve near-total control and long-term stability, in which case it would be a crunch. &nbsp;But it isn't hard to imagine such a situation in which&nbsp;<em>some</em> parts of life did get better, like if a violently oppressive government continued to develop advanced medicines so that citizens were universally healthier and longer-lived than people today. &nbsp;If that happened, it would be a Shriek. &nbsp;A similar analysis applies to the AI, with the possible outcomes being Bang, Crunch, and Shriek depending on just how badly we misprogrammed it.</p>\n<p><strong id=\"What_Ties_These_Threads_Together_\">What Ties These Threads Together?</strong></p>\n<p><strong></strong>Even if you think existential threats deserve more attention, the rationale for treating them as a diverse but unified phenomenon may not be obvious. &nbsp;In addition to the crucial but (relatively) straightforward work of, say, tracking Near-Earth Objects (NEOs), existential risk researchers also think seriously about alien invasions and rogue AIs. With such a range of speculativeness, why group x-risks together at all?</p>\n<p>It turns out that they share a cluster of features which does give them some cohesion and make them worth studying under a single label, not all of which I discuss here. &nbsp;First and most obvious is that should any of them occur the consequences would be truly vast relative to any other kind of risk. &nbsp;To see why, think about the difference between a catastrophe that kills 99% of humanity and one that kills 100%. &nbsp;As big a tragedy as the former would be, there's a chance humans could recover and build a post-human civilization. &nbsp;But if every person dies, then the entire value of our future is lost (<a href=\"http://www.existential-risk.org/concept.html\">Bostrom 2013</a>).</p>\n<p>Second, these are not risks which admit of a trial and error approach. &nbsp;Pretty much by definition a collision with an x-risk will spell doom for humanity, and so we must be more proactive in our strategies for reducing them. Related to this, we as a species have neither the cultural nor biological instincts needed to prepare us for the possibility of extinction. &nbsp;A group of people might live through several droughts and thus develop strong collective norms towards planning ahead and keeping generous food reserves. &nbsp;But they cannot have gone extinct multiple times, and thus they can't rely on their shared experience and cultural memory to guide them in the future. &nbsp;I certainly hope we can develop a set of norms and institutions which makes us all safer, but we can't wait to learn from history. &nbsp;We're going to have to start well in advance, or we won't survive.</p>\n<p>A final commonality I'll mention is that the solutions to quite a number of x-risks are themselves x-risks. &nbsp;A powerful enough government could effectively halt research into dangerous pathogens or nano-replicators. &nbsp;But given how States have generally comported themselves in the past, one would do well to be cautious before investing them with that kind of power. &nbsp;Ditto for a superhuman AI, which could set up an infrastructure to protect us from asteroids, nuclear war, or even other less Friendly AI. Get the coding just a little wrong, though, <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">and it might reuse your carbon to make paperclips</a>.</p>\n<p>It is indeed a knife edge along which we creep towards the future.</p>\n<p><strong id=\"Measuring_the_Monsters\">Measuring the Monsters</strong></p>\n<p>A first step is getting straight about how likely survival is. &nbsp;The reader may have encountered predictions of the \"we have only a 50% chance of surviving the next hundred years\" variety. &nbsp;Examining the validity of such estimates is worth doing, but I won't be taking up that challenge here; I tend to agree that these figures involves a lot of subjective judgement, but that even if the chances were very very small it would still be worth taking seriously (<a href=\"/Bostrom 2006\">Bostrom 2006</a>). &nbsp; At any rate, it seems to me that trying to calculate an overall likelihood of human extinction is going to be premature before we've nailed down probabilities for some of the different possible extinction scenarios. &nbsp;It is to the techniques which x-risk researchers rely on to try and do this that I now turn.</p>\n<p>X-risk-assessments rely on both&nbsp;<em>direct</em> and&nbsp;<em>indirect</em> methods (<a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom 2002</a>). &nbsp;Using a direct method involves building a detailed causal model of the phenomenon and using that to generate a risk probability, while indirect methods include&nbsp;arguments, thought experiments, and information that we use to constrain and refine our guesses.</p>\n<p>As far as I know for some x-risks we could use direct methods if we just had a way to gather the relevant information. &nbsp;If we knew where all the NEOs were we could use settled physics to predict whether any of them posed a threat and then prioritize accordingly. But&nbsp;we don't where they all are, so we might instead examine the frequency of impacts throughout the history of the Earth and then reason about whether or not we think an impact will happen soon.&nbsp;&nbsp; It would be nice to exclusively use direct methods, but we supplement with indirect methods when we can't, and of course for x-risks like AI we are in an even more uncertain position than we are for NEOs.</p>\n<p><strong id=\"The_Fermi_Paradox\">The Fermi Paradox</strong></p>\n<p>Applying indirect methods can lead to some strange and counter-intuitive territory, an example of which is the mysteries surrounding the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">Fermi Paradox</a>. &nbsp;The central question is: in a universe with so many potential hotbeds of life, why is it that when we listen for stirring in the void all we hear is silence? &nbsp;Many feel that the universe must be teeming with life, some of it intelligent, so why haven't we see any sign of it yet?</p>\n<p>Musing about possible <a href=\"http://io9.com/11-of-the-weirdest-solutions-to-the-fermi-paradox-456850746\">solutions </a>to the Fermi Paradox can be a lot of fun, and it's worth pointing out that we haven't been looking that long or that hard for signals yet. Nevertheless I think the argument has some meat to it.</p>\n<p>Observing this state of affairs, some have postulated the existence of at least one&nbsp;<a href=\"http://en.wikipedia.org/wiki/Great_Filter\">Great Filter</a>, a step in the chain of development from the first organisms to space-faring civilizations that must be&nbsp;<em>extremely</em> hard to achieve. &nbsp;<strong>&nbsp;</strong></p>\n<p>This is cause for concern because the Great Filter could be in front of us or behind us. &nbsp;Let me explain: imagine a continuum with the simplest self-replicating molecules on one side and the Star Trek Enterprise on the other. &nbsp;From our position on the continuum we want to know whether or not we have already passed one of the hardest steps, but we have only our own planet to look at. &nbsp;So imagine that we send out probes to thousands of different worlds in the hopes that we will learn something.</p>\n<p>If we find lots of simple eukaryotes that means that the Great Filter is probably not before the development of membrane-bound organelles. The list of possible places on the continuum the Great Filter could be shrinks just a little bit. &nbsp;If instead we find lots of mammals and reptiles (or creatures that are very different but about as advanced), that means the Great Filter is probably not before the rise of complex organisms, so the places the Great Filter might be hiding shrinks again. &nbsp;Worst of all would be if we find the dead ruins of many different advanced civilizations. &nbsp;This would imply that the real killer is yet to come, and we will almost certainly not survive it.</p>\n<p>As happy as many people would be to discover evidence of life in the universe, a <a href=\"http://www.nickbostrom.com/extraterrestrial.pdf\">case</a>&nbsp;has been made that we should hope to find only barren rocks waiting for us in the final frontier. If not even simple bacteria evolve on most worlds, then there is still a chance that the Great Filter is behind us, and we can worry only about the new challenges ahead, which may or not be Filters as great as the ones in the past.</p>\n<p>If all this seems really abstract out there, that's because it is. &nbsp;But I hope it is clear how this sort of thinking can help us interpret new data, make better guesses, form new hypotheses, etc. &nbsp;When dealing with stakes this high and information this limited, one must do the best they can with what's available.</p>\n<p><strong style=\"line-height: 1.5;\" id=\"Mitigation\">Mitigation</strong></p>\n<p>What priority should we place on reducing existential risk and how can we do that? I don't know of anyone who thinks&nbsp;<em>all</em> our effort should go towards mitigating x-risks; there are lots of pressing issues which are not x-risks that are worth our attention, like abject poverty or geopolitical instability. &nbsp;But I feel comfortable saying we aren't doing nearly as much as we should be. Given the stakes and the fact that there probably won't be a second chance we are going to have to meet x-risks head on and be aggressively proactive in mitigating them.</p>\n<p>Suppose we taboo 'aggressively proactive', what's left? &nbsp;Well the first step, as it so often is, will be just to get the right people to be aware of the problem (<a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom 2002</a>). &nbsp;Thankfully this is starting to be the case as more funding and brain power go into existential risk reduction. We have to get to a point where we are spending at least as much time, energy, and effort making new technology safe as we do making it more powerful. &nbsp;More international cooperation on these matters will be necessary, and there should be some sort of mechanism by which efforts to develop existentially-threatening technologies like super-virulent pathogens can be stopped. &nbsp;I don't like recommending this at all, but almost anything is preferable to extinction.</p>\n<p>In the meantime both research that directly reduces x-risk (like <a href=\"http://neo.jpl.nasa.gov/index.html\">NEO detection</a>), as well as research that will help elucidate deep and foundational issues in x-risk (<a href=\"https://www.google.co.kr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCgQFjAA&amp;url=http%3A%2F%2Fwww.fhi.ox.ac.uk%2F&amp;ei=wBpjUsiaOaW5iQeT8oD4Cg&amp;usg=AFQjCNGJPMbDHCz5yjqBjGcRTQvsdUBXAw&amp;bvm=bv.54934254,d.aGc\">FHI</a>&nbsp;and <a href=\"https://www.google.co.kr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CD0QFjAC&amp;url=http%3A%2F%2Fintelligence.org%2F&amp;ei=5BpjUuG2J8uSiQfJ94Bg&amp;usg=AFQjCNHTe9xWSVEOAHRidIEtZfcA9ufViA&amp;bvm=bv.54934254,d.aGc\">MIRI</a>) should be encouraged. &nbsp;It's a stereotype that research papers always end with a call for more research, but as was pointed out by lukeprog in a talk he gave, there's more research done on lipstick than on friendly AI. &nbsp;This generalizes to x-risk more broadly, and represents the truly worrying state of our priorities. &nbsp;</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>Though I maintain we should be more fearful of what's to come, that should not obscure the fact that the human potential is vast and truly exciting. &nbsp;If the right steps are taken, we and our descendants will have a future better than most can even dream of. &nbsp;Life spans measured in eons could be spent learning and loving in ways our terrestrial languages don't even have words for yet. &nbsp;The vision of a post-human civilization flinging it's trillions of descendants into the universe to light up the dark is tremendously inspiring. &nbsp;It's worth fighting for.</p>\n<p>But we have much work ahead of us.</p>", "sections": [{"title": "Meta", "anchor": "Meta", "level": 1}, {"title": "In the beginning", "anchor": "In_the_beginning", "level": 1}, {"title": "What is an Existential Risk?", "anchor": "What_is_an_Existential_Risk_", "level": 1}, {"title": "Four Types of Existential Risks", "anchor": "Four_Types_of_Existential_Risks", "level": 1}, {"title": "What Ties These Threads Together?", "anchor": "What_Ties_These_Threads_Together_", "level": 1}, {"title": "Measuring the Monsters", "anchor": "Measuring_the_Monsters", "level": 1}, {"title": "The Fermi Paradox", "anchor": "The_Fermi_Paradox", "level": 1}, {"title": "Mitigation", "anchor": "Mitigation", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FGTgeweYNxmMBx4fz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-20T00:47:03.927Z", "modifiedAt": null, "url": null, "title": "Meditation Trains Metacognition", "slug": "meditation-trains-metacognition", "viewCount": null, "lastCommentedAt": "2020-06-11T11:53:02.853Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JMgffu9AzhYpTpHFJ/meditation-trains-metacognition", "pageUrlRelative": "/posts/JMgffu9AzhYpTpHFJ/meditation-trains-metacognition", "linkUrl": "https://www.lesswrong.com/posts/JMgffu9AzhYpTpHFJ/meditation-trains-metacognition", "postedAtFormatted": "Sunday, October 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meditation%20Trains%20Metacognition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeditation%20Trains%20Metacognition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJMgffu9AzhYpTpHFJ%2Fmeditation-trains-metacognition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meditation%20Trains%20Metacognition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJMgffu9AzhYpTpHFJ%2Fmeditation-trains-metacognition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJMgffu9AzhYpTpHFJ%2Fmeditation-trains-metacognition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2471, "htmlBody": "<p dir=\"ltr\"><em>Summary: Some forms of meditation may train key skills of metacognition, serving as powerful tools for applied rationality. I expect aspiring rationalists to advance more quickly with a regular practice of mindfulness meditation.</em></p>\n<p dir=\"ltr\"><a id=\"more\"></a></p>\n<p dir=\"ltr\">The state of scientific research on meditation isn't great. Although there's evidence that it does something good--probably something involving down-regulation of negative affect--there are many basic questions<sup>1</sup> that either haven't been studied at all or haven't been studied well enough to let me update much. According to <a href=\"http://www.ashanamind.com/wp-content/uploads/2013/03/physiological-effects_Sedlmeier_12.pdf\">a meta-analysis</a> by Sedlmeier et al., one problem with evaluating the research is that it's hard to pin down what meditation is, let alone what it does or why it does it. In their words,</p>\n<blockquote>\n<p dir=\"ltr\"><span>...two of our main findings are that (a) meditation has a substantial impact on psychological variables, indicated by a medium-sized (e.g., <a href=\"http://www.lrdc.pitt.edu/schneider/P2465/Readings/Cohen,%201988%20(Statistical%20Power,%20273-406).pdf\">Cohen, 1988</a>) global effect, and (b) its effects might be somewhat stronger for negative emotional than for cognitive variables. Due to the lack of a comprehensive theoretical approach (and results from studies derived therefrom), it is still unclear how meditation works... Moreover, a closer look at the studies included in the meta-analysis revealed that they differed in many respects that might have affected the results.<sup>2</sup></span></p>\n</blockquote>\n<p dir=\"ltr\">So I just want to be clear that I don't mean in this post to wholeheartedly recommend daily meditation as the best possible use of 1/24th of your time.</p>\n<p dir=\"ltr\">Nevertheless, my own experience and reports from several of my friends suggest a specific cognitive result from a certain flavor of meditation that will be very good news for rationality if we can reliably reproduce it.<sup>3</sup> In <a href=\"/lw/ipm/a_map_of_bay_area_memespace/\">a recent post</a>, Julia Galef pointed out exactly what I consider to be far and away the greatest benefit I've reaped from my meditative practices over the years. She wrote,</p>\n<blockquote>\n<p dir=\"ltr\"><span>Meditation seems to train you to stop automatically identifying with all of your thoughts, so that, for example, when the thought \"John&rsquo;s a jerk\" pops into your head, you don&rsquo;t assume that John necessarily is a jerk. You take the thought as something your brain produced, which may or may not be true, and may or may not be useful -- and this ability to take a step back from your thoughts and reflect on them is arguably one of the building blocks of rationality.</span></p>\n</blockquote>\n<p dir=\"ltr\">I'd like to delve more deeply into how and why this could work. There seem to be multiple paths to establishing the central rationality skills comprising metacognition--several highly advanced rationalist I know have no background in meditation--so meditation is by no means a necessary condition for successfully applied rationality. I think it may, however, have the highest signal to noise ratio among methods for developing foundational metacognitive abilities. At a minimum, I expect that regular practice of certain kinds of meditation would help aspiring rationalists to advance more quickly.</p>\n<h2 dir=\"ltr\">What kind of metacognitive skills am I talking about?</h2>\n<p dir=\"ltr\">How about an example. When you read these words, you're probably hearing a little voice inside your head that's reading them to you aloud, so to speak. Your relationship to this imaginary voice (aka \"subvocalization\" or \"inner speech\") may be quite a bit more intimate than you realize. It's likely with you not only when you read, but when you ride the bus home and think, \"Maybe I'll have steak for dinner\"; it's with you when you've had an awkward interaction with someone you admire and you think, \"God, I must have looked like such an idiot\"; it's with you most of the time, in fact, during your waking hours and maybe even when you dream.<sup>5</sup></p>\n<p dir=\"ltr\"><strong>Exercise One</strong></p>\n<p dir=\"ltr\">This fact may be more salient for you if you try to turn it off for a while. <a href=\"http://www.online-stopwatch.com/countdown-timer/\">Set a timer</a> for one minute, and force yourself not to verbally narrate your experience. When the minute is up, jot down a brief note about how it felt. Three two one go.</p>\n<p dir=\"ltr\">No, really, don't read the next paragraph 'til you've done the exercise.<sup>6</sup></p>\n<p dir=\"ltr\">Even if you managed to go the entire minute without subvocalizing, it probably didn't feel like the natural way of things. It probably took effort, and possibly a great deal of effort. But I predict that most of you didn't go through the whole minute in total subjective silence. (If you succeeded and it did feel like the natural way of things, I'd very much like to know.)</p>\n<p dir=\"ltr\"><strong>Exercise Two</strong></p>\n<p dir=\"ltr\">Now set the timer for a minute again, but this time <em>don't</em> force yourself not to subvocalize. Simply notice when words arise in consciousness. Don't bother doing anything with them. Just be aware of them.</p>\n<p dir=\"ltr\">Again, when the minute is up, make a brief note about how it felt. In particular, how did it differ from the first exercise, and how did it differ from your usual experience?</p>\n<p dir=\"ltr\"><strong>Exercise Three</strong></p>\n<p dir=\"ltr\">Finally, notice that your present increased awareness of subvocalizations lets you change things about them that you couldn't change if you weren't aware of them. For example, you're now reading this in the voice of Morgan Freeman. (You're welcome.)</p>\n<p dir=\"ltr\">Now pick some other aspect of subvocalization to change--perhaps the accent, or the speed, or the pitch--and read the next sentence in that way. Set a timer for one minute, and experiment with things you can change about your experience of inner speech.</p>\n<h2 dir=\"ltr\">What does this have to do with rationality?</h2>\n<p dir=\"ltr\">In general terms, what have you done in the above exercises? You've become aware of a mental process that usually runs in the background whether you like it or not. You've gained and exercised some degree of control over it. You've come up with and tested, in real time, alternative ways of running your own cognitive software.</p>\n<p dir=\"ltr\">Now, this has merely been a simple illustration. My point is not that swapping Morgan Freeman for yourself as official narrator would itself improve your daily life. (Although that may well be.) Rather, my point is that these skills are central to rationality and are cultivated by meditation. Those of you with a strong background in meditation probably did not learn anything important from these exercises, and wouldn't have regardless of your rationality training. Stepping back from your experiences in a way that lets you examine them and modify them is so old hat, if you meditate a lot, that you may even have forgotten what it's like not to have that action available as primitive.</p>\n<p dir=\"ltr\">This is extraordinarily valuable! There are three abilities that together form the bridge between knowledge of rationality and the application thereof. They are</p>\n<ol>\n<li>the ability to introspect and promote a sought cognitive process into consciousness</li>\n<li>the ability to not identify with any particular cognitive process you become aware of</li>\n<li>the ability to make changes to cognitive processes you&rsquo;re aware of in media res</li>\n</ol>\n<p>For example, even if you understand how important it is to <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">make beliefs pay rent in anticipated experiences</a>, actually doing that can be really hard. Why is it so hard? Possibly for a few different reasons, but prominent among them is the following. If you've thought something lots of times without ever explicitly identifying it as something you're thinking, without putting much distance between yourself and the thought, your sense of self gets tangled up in it. It's not nice to let go of something that close to you, even if it's useless or harmful. It feels sort of like trying to kick out your own child when you know you can no longer afford to take care of her--and it feels distinctly unlike taking a broken blender to the dump, which is closer to what should really be going on.</p>\n<p dir=\"ltr\">Other directly related examples include <a href=\"/lw/if/your_strength_as_a_rationalist/\">noticing and tending to confusion</a>, <a href=\"/lw/gq/the_proper_use_of_humility/\">actually behaving as though you might be wrong</a> when you think you might be wrong, and <a href=\"/lw/gw/politics_is_the_mindkiller/\">thinking about politics</a> without your head exploding.</p>\n<p dir=\"ltr\">Note that merely willing problems solved is not a reliable way of solving them. Resolving to not identify with your thoughts isn't the same as <em>causing</em> yourself to not identify with your thoughts. There's a reason you identify with your thoughts in the first place, and it's not because you decided to. If you don't alter any of the mechanisms that actually give rise to the problem, nothing will change--which is why, I think, it's possible to possess oodles of declarative knowledge about rationality without making a single significant improvement to your life.</p>\n<p dir=\"ltr\">One way or another, you have to get some distance between yourself and your thoughts and feelings if you want to let go of them or change them. That's exactly what meditation teaches you to do.</p>\n<h2 dir=\"ltr\">What does this have to do with meditation?<br /></h2>\n<p>There are many kinds of meditation. Some involve intense concentration on very specific sensations, like visualizations of <a href=\"http://en.wikipedia.org/wiki/Mandala\">geometric patterns</a>, repeated phrases called <a href=\"http://en.wikipedia.org/wiki/Mantra\">mantra</a>, the <a href=\"http://www.ese-an.org/s/1061-susoku-kan.html\">breath</a>, or <a href=\"http://www.wildmind.org/walking/overview\">movements</a> (not all meditation is done seated and motionless). There are interpersonal types of meditation that can involve <a href=\"http://integrationtraining.co.uk/blog/2008/07/eye-gazing-meditation.html\">maintaining eye contact</a> with someone for extended periods, imagining someone hurting and <a href=\"http://en.wikipedia.org/wiki/Mett%C4%81\">nurturing the desire to help them</a>, or <a href=\"http://en.wikipedia.org/wiki/Maithuna\">sex</a>. The kind I'm most familiar with is a form of Japanese Buddhist meditation called <a href=\"http://en.wikipedia.org/wiki/Shikantaza\">shikantaza</a>, which translates roughly to \"just sitting\". Although it comes with basically no instructions, as the name suggests, in practice it's nearly identical to the most general form of \"mindfulness meditation\".</p>\n<p dir=\"ltr\"><a href=\"http://en.wikipedia.org/wiki/Mindfulness\">Mindfulness</a> is one of the most popular meditative practices in the West, and of the types I know about, it's the one I expect to be most relevant to applied rationality. Though all of the above, in one way or another, teach the backward step<sup>7</sup> that allows you to stop identifying with thoughts, mindfulness is <em>only</em> that. Exercise two above is a limited form of mindfulness meditation. Although there's a whole family of practices that fall under the heading of \"mindfulness\", what they have in common is the cultivation of awareness.</p>\n<p dir=\"ltr\">All I mean by \"cultivation of awareness\" is the power to broaden/focus attention to encompass more things, or more specific things. I've often heard practitioners describe it as \"openness to the world\". Ordinarily, we experience a lot of things on which we don't bother to turn our subjective spotlights of attention, sometimes because they're just not important, and sometimes because we actively avoid stimuli we perceive to be aversive.</p>\n<p dir=\"ltr\">Subvocalization is an example. Other examples are the sounds in your external environment, what you know about how those around you are feeling, the sound of your own breath and heartbeat, the sensation of flinching away from a painful thought, the temperature in the room, the colors and shapes that appear behind closed eyelids, and the sensation of confusion. I find it difficult to describe the most general form of this, because without analogy to more specific forms, all I've got is that it's experiencing... what you experience. Which really just sounds like the default mode of living, doesn't it? But in practice it can feel very different.</p>\n<p dir=\"ltr\">When you're well practiced at noticing these things, at welcoming them into your attention, you're acutely aware of not being them. And when you don't feel as though you are your thoughts and feelings, it becomes emotionally easier to let go of them or to modify them. Changing your mind feels less like losing a part of yourself.</p>\n<h2 dir=\"ltr\">Further Resources<br /></h2>\n<ul>\n<li>Sam Harris recently posted <a href=\"http://www.samharris.org/blog/item/mindfulness-meditation\">an excellent introduction to mindfulness meditation</a> in the form of two audio tracks (one nine minutes, the other twenty-six). I recommend them pretty highly. They each guide you through a meditation session without any annoying religious or new-agey distractions.</li>\n<li>I find <a href=\"http://www.researchgate.net/publication/7307092_Self-referential_processing_in_our_brain--a_meta-analysis_of_imaging_studies_on_the_self/file/d912f50eb278e1a473.pdf\">this meta-analysis</a> by Northoff et al. of neuroimaging studies of self-referential cognitive processing to be fascinating for all sorts of reasons. Chief among them is the light it sheds on how and why including clear-cut self/other distinctions in models of human minds doesn't always work so well. (Link is to a PDF.)</li>\n<li>If you want to take a soaring leap across the bridge between knowledge of rationality and the application thereof, you simply must try a <a href=\"http://rationality.org/workshops/\">CFAR workshop</a>. I did one of these back in April, and it was every bit as fun as it was effective (which was very).</li>\n</ul>\n<hr />\n<h2 dir=\"ltr\">Notes</h2>\n<p dir=\"ltr\">1. Off the top of my head: What aspects of particular forms of meditation cause the various purported benefits? If we pinpoint those aspects, can we harness their corresponding benefits individually without committing to meditation as a whole? Can we improve upon them? Does meditation have different effects when practiced in a religious context? What is the relationship between meditation and hypnosis? How do the effects differ among different age groups? Does learning to meditate while young have any effect on adult meditation?</p>\n<p dir=\"ltr\">2. Sedlmeier et al. (2012). <a href=\"http://www.ashanamind.com/wp-content/uploads/2013/03/physiological-effects_Sedlmeier_12.pdf\">The Psychological Effects of Meditation: A Meta-Analysis</a>. Psychological Bulletin, 138(6) 1139-1171.</p>\n<p dir=\"ltr\">3. I don't consider lack of supporting double-blind studies much evidence against my thesis, largely because the result in question would only show up in tests of metacognitive techniques I expect not to occur to the vast majority of researchers just yet.</p>\n<p dir=\"ltr\">4. In case you're wondering about my relevant background: I did Vinyasa yoga throughout high school, taught it during college, trained at a residential Soto Zen temple for a summer, have maintained a fairly regular practice of Zen meditation for about five years now, practiced Tai Chi (very casually) off and on for most of my life, and have a degree in religious studies with a focus on East Asian Buddhism.</p>\n<p dir=\"ltr\">5. <a href=\"http://www.sciencedaily.com/releases/2013/07/130716080028.htm\">Fun fact</a>: You internally simulate your voice in parallel with actual talking.</p>\n<p dir=\"ltr\">6. Yes, you're doing it right. If you're trying to do it at all, you're doing it right. The idea is to find out what it feels like to make the effort, not to beat the game. There is no game. Some of the comments below have me concerned that I may be contributing to the \"meditation means being brain dead\" misconception. This exercise isn't meant to teach you the One True Way of Meditation. It's just to point at certain kinds of movements your mind makes. Monks who have been meditating for multiple hours a day for decades don't have completely featureless minds when they meditate. That isn't even close to what meditation means to them. Beginners are given exercises along these lines because it's an easier entry point, like training wheels. Eventually, counting breaths simply becomes irrelevant.</p>\n<p dir=\"ltr\">7. The Japanese &ldquo;Su sube[karaku] mochi[iyo] eko-hensho no taiho o mochi-iyo,&rdquo; from <a href=\"http://the-middle-way.org/subpage8.html\">Dogen Zengi&rsquo;s instructions for meditation</a> (1227) literally translates to English (character-by-character) as &ldquo;Remember/employ of backward step turning light/consciousness reflecting/illuminating.&rdquo; (Dogen loved wordplay, and the double meanings are intentional.) Though <a href=\"http://terebess.hu/zen/Fukanzazengi-6.pdf\">most translators</a> render this something like, &ldquo;Take the backward step that turns your light inwardly to illuminate the self,&rdquo; the characters for &ldquo;self&rdquo; and &ldquo;inward&rdquo; do not in fact appear in this part of the text. Thus, his central instruction for meditation is to step consciousness backward so it can be generally reflective. This is why I say that in practice shikantaza and mindfulness amount to the same thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AiNyf5iwbpc7mehiX": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JMgffu9AzhYpTpHFJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 56, "extendedScore": null, "score": 0.000131, "legacy": true, "legacyId": "24425", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p dir=\"ltr\"><em>Summary: Some forms of meditation may train key skills of metacognition, serving as powerful tools for applied rationality. I expect aspiring rationalists to advance more quickly with a regular practice of mindfulness meditation.</em></p>\n<p dir=\"ltr\"><a id=\"more\"></a></p>\n<p dir=\"ltr\">The state of scientific research on meditation isn't great. Although there's evidence that it does something good--probably something involving down-regulation of negative affect--there are many basic questions<sup>1</sup> that either haven't been studied at all or haven't been studied well enough to let me update much. According to <a href=\"http://www.ashanamind.com/wp-content/uploads/2013/03/physiological-effects_Sedlmeier_12.pdf\">a meta-analysis</a> by Sedlmeier et al., one problem with evaluating the research is that it's hard to pin down what meditation is, let alone what it does or why it does it. In their words,</p>\n<blockquote>\n<p dir=\"ltr\"><span>...two of our main findings are that (a) meditation has a substantial impact on psychological variables, indicated by a medium-sized (e.g., <a href=\"http://www.lrdc.pitt.edu/schneider/P2465/Readings/Cohen,%201988%20(Statistical%20Power,%20273-406).pdf\">Cohen, 1988</a>) global effect, and (b) its effects might be somewhat stronger for negative emotional than for cognitive variables. Due to the lack of a comprehensive theoretical approach (and results from studies derived therefrom), it is still unclear how meditation works... Moreover, a closer look at the studies included in the meta-analysis revealed that they differed in many respects that might have affected the results.<sup>2</sup></span></p>\n</blockquote>\n<p dir=\"ltr\">So I just want to be clear that I don't mean in this post to wholeheartedly recommend daily meditation as the best possible use of 1/24th of your time.</p>\n<p dir=\"ltr\">Nevertheless, my own experience and reports from several of my friends suggest a specific cognitive result from a certain flavor of meditation that will be very good news for rationality if we can reliably reproduce it.<sup>3</sup> In <a href=\"/lw/ipm/a_map_of_bay_area_memespace/\">a recent post</a>, Julia Galef pointed out exactly what I consider to be far and away the greatest benefit I've reaped from my meditative practices over the years. She wrote,</p>\n<blockquote>\n<p dir=\"ltr\"><span>Meditation seems to train you to stop automatically identifying with all of your thoughts, so that, for example, when the thought \"John\u2019s a jerk\" pops into your head, you don\u2019t assume that John necessarily is a jerk. You take the thought as something your brain produced, which may or may not be true, and may or may not be useful -- and this ability to take a step back from your thoughts and reflect on them is arguably one of the building blocks of rationality.</span></p>\n</blockquote>\n<p dir=\"ltr\">I'd like to delve more deeply into how and why this could work. There seem to be multiple paths to establishing the central rationality skills comprising metacognition--several highly advanced rationalist I know have no background in meditation--so meditation is by no means a necessary condition for successfully applied rationality. I think it may, however, have the highest signal to noise ratio among methods for developing foundational metacognitive abilities. At a minimum, I expect that regular practice of certain kinds of meditation would help aspiring rationalists to advance more quickly.</p>\n<h2 dir=\"ltr\" id=\"What_kind_of_metacognitive_skills_am_I_talking_about_\">What kind of metacognitive skills am I talking about?</h2>\n<p dir=\"ltr\">How about an example. When you read these words, you're probably hearing a little voice inside your head that's reading them to you aloud, so to speak. Your relationship to this imaginary voice (aka \"subvocalization\" or \"inner speech\") may be quite a bit more intimate than you realize. It's likely with you not only when you read, but when you ride the bus home and think, \"Maybe I'll have steak for dinner\"; it's with you when you've had an awkward interaction with someone you admire and you think, \"God, I must have looked like such an idiot\"; it's with you most of the time, in fact, during your waking hours and maybe even when you dream.<sup>5</sup></p>\n<p dir=\"ltr\"><strong id=\"Exercise_One\">Exercise One</strong></p>\n<p dir=\"ltr\">This fact may be more salient for you if you try to turn it off for a while. <a href=\"http://www.online-stopwatch.com/countdown-timer/\">Set a timer</a> for one minute, and force yourself not to verbally narrate your experience. When the minute is up, jot down a brief note about how it felt. Three two one go.</p>\n<p dir=\"ltr\">No, really, don't read the next paragraph 'til you've done the exercise.<sup>6</sup></p>\n<p dir=\"ltr\">Even if you managed to go the entire minute without subvocalizing, it probably didn't feel like the natural way of things. It probably took effort, and possibly a great deal of effort. But I predict that most of you didn't go through the whole minute in total subjective silence. (If you succeeded and it did feel like the natural way of things, I'd very much like to know.)</p>\n<p dir=\"ltr\"><strong id=\"Exercise_Two\">Exercise Two</strong></p>\n<p dir=\"ltr\">Now set the timer for a minute again, but this time <em>don't</em> force yourself not to subvocalize. Simply notice when words arise in consciousness. Don't bother doing anything with them. Just be aware of them.</p>\n<p dir=\"ltr\">Again, when the minute is up, make a brief note about how it felt. In particular, how did it differ from the first exercise, and how did it differ from your usual experience?</p>\n<p dir=\"ltr\"><strong id=\"Exercise_Three\">Exercise Three</strong></p>\n<p dir=\"ltr\">Finally, notice that your present increased awareness of subvocalizations lets you change things about them that you couldn't change if you weren't aware of them. For example, you're now reading this in the voice of Morgan Freeman. (You're welcome.)</p>\n<p dir=\"ltr\">Now pick some other aspect of subvocalization to change--perhaps the accent, or the speed, or the pitch--and read the next sentence in that way. Set a timer for one minute, and experiment with things you can change about your experience of inner speech.</p>\n<h2 dir=\"ltr\" id=\"What_does_this_have_to_do_with_rationality_\">What does this have to do with rationality?</h2>\n<p dir=\"ltr\">In general terms, what have you done in the above exercises? You've become aware of a mental process that usually runs in the background whether you like it or not. You've gained and exercised some degree of control over it. You've come up with and tested, in real time, alternative ways of running your own cognitive software.</p>\n<p dir=\"ltr\">Now, this has merely been a simple illustration. My point is not that swapping Morgan Freeman for yourself as official narrator would itself improve your daily life. (Although that may well be.) Rather, my point is that these skills are central to rationality and are cultivated by meditation. Those of you with a strong background in meditation probably did not learn anything important from these exercises, and wouldn't have regardless of your rationality training. Stepping back from your experiences in a way that lets you examine them and modify them is so old hat, if you meditate a lot, that you may even have forgotten what it's like not to have that action available as primitive.</p>\n<p dir=\"ltr\">This is extraordinarily valuable! There are three abilities that together form the bridge between knowledge of rationality and the application thereof. They are</p>\n<ol>\n<li>the ability to introspect and promote a sought cognitive process into consciousness</li>\n<li>the ability to not identify with any particular cognitive process you become aware of</li>\n<li>the ability to make changes to cognitive processes you\u2019re aware of in media res</li>\n</ol>\n<p>For example, even if you understand how important it is to <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">make beliefs pay rent in anticipated experiences</a>, actually doing that can be really hard. Why is it so hard? Possibly for a few different reasons, but prominent among them is the following. If you've thought something lots of times without ever explicitly identifying it as something you're thinking, without putting much distance between yourself and the thought, your sense of self gets tangled up in it. It's not nice to let go of something that close to you, even if it's useless or harmful. It feels sort of like trying to kick out your own child when you know you can no longer afford to take care of her--and it feels distinctly unlike taking a broken blender to the dump, which is closer to what should really be going on.</p>\n<p dir=\"ltr\">Other directly related examples include <a href=\"/lw/if/your_strength_as_a_rationalist/\">noticing and tending to confusion</a>, <a href=\"/lw/gq/the_proper_use_of_humility/\">actually behaving as though you might be wrong</a> when you think you might be wrong, and <a href=\"/lw/gw/politics_is_the_mindkiller/\">thinking about politics</a> without your head exploding.</p>\n<p dir=\"ltr\">Note that merely willing problems solved is not a reliable way of solving them. Resolving to not identify with your thoughts isn't the same as <em>causing</em> yourself to not identify with your thoughts. There's a reason you identify with your thoughts in the first place, and it's not because you decided to. If you don't alter any of the mechanisms that actually give rise to the problem, nothing will change--which is why, I think, it's possible to possess oodles of declarative knowledge about rationality without making a single significant improvement to your life.</p>\n<p dir=\"ltr\">One way or another, you have to get some distance between yourself and your thoughts and feelings if you want to let go of them or change them. That's exactly what meditation teaches you to do.</p>\n<h2 dir=\"ltr\" id=\"What_does_this_have_to_do_with_meditation_\">What does this have to do with meditation?<br></h2>\n<p>There are many kinds of meditation. Some involve intense concentration on very specific sensations, like visualizations of <a href=\"http://en.wikipedia.org/wiki/Mandala\">geometric patterns</a>, repeated phrases called <a href=\"http://en.wikipedia.org/wiki/Mantra\">mantra</a>, the <a href=\"http://www.ese-an.org/s/1061-susoku-kan.html\">breath</a>, or <a href=\"http://www.wildmind.org/walking/overview\">movements</a> (not all meditation is done seated and motionless). There are interpersonal types of meditation that can involve <a href=\"http://integrationtraining.co.uk/blog/2008/07/eye-gazing-meditation.html\">maintaining eye contact</a> with someone for extended periods, imagining someone hurting and <a href=\"http://en.wikipedia.org/wiki/Mett%C4%81\">nurturing the desire to help them</a>, or <a href=\"http://en.wikipedia.org/wiki/Maithuna\">sex</a>. The kind I'm most familiar with is a form of Japanese Buddhist meditation called <a href=\"http://en.wikipedia.org/wiki/Shikantaza\">shikantaza</a>, which translates roughly to \"just sitting\". Although it comes with basically no instructions, as the name suggests, in practice it's nearly identical to the most general form of \"mindfulness meditation\".</p>\n<p dir=\"ltr\"><a href=\"http://en.wikipedia.org/wiki/Mindfulness\">Mindfulness</a> is one of the most popular meditative practices in the West, and of the types I know about, it's the one I expect to be most relevant to applied rationality. Though all of the above, in one way or another, teach the backward step<sup>7</sup> that allows you to stop identifying with thoughts, mindfulness is <em>only</em> that. Exercise two above is a limited form of mindfulness meditation. Although there's a whole family of practices that fall under the heading of \"mindfulness\", what they have in common is the cultivation of awareness.</p>\n<p dir=\"ltr\">All I mean by \"cultivation of awareness\" is the power to broaden/focus attention to encompass more things, or more specific things. I've often heard practitioners describe it as \"openness to the world\". Ordinarily, we experience a lot of things on which we don't bother to turn our subjective spotlights of attention, sometimes because they're just not important, and sometimes because we actively avoid stimuli we perceive to be aversive.</p>\n<p dir=\"ltr\">Subvocalization is an example. Other examples are the sounds in your external environment, what you know about how those around you are feeling, the sound of your own breath and heartbeat, the sensation of flinching away from a painful thought, the temperature in the room, the colors and shapes that appear behind closed eyelids, and the sensation of confusion. I find it difficult to describe the most general form of this, because without analogy to more specific forms, all I've got is that it's experiencing... what you experience. Which really just sounds like the default mode of living, doesn't it? But in practice it can feel very different.</p>\n<p dir=\"ltr\">When you're well practiced at noticing these things, at welcoming them into your attention, you're acutely aware of not being them. And when you don't feel as though you are your thoughts and feelings, it becomes emotionally easier to let go of them or to modify them. Changing your mind feels less like losing a part of yourself.</p>\n<h2 dir=\"ltr\" id=\"Further_Resources\">Further Resources<br></h2>\n<ul>\n<li>Sam Harris recently posted <a href=\"http://www.samharris.org/blog/item/mindfulness-meditation\">an excellent introduction to mindfulness meditation</a> in the form of two audio tracks (one nine minutes, the other twenty-six). I recommend them pretty highly. They each guide you through a meditation session without any annoying religious or new-agey distractions.</li>\n<li>I find <a href=\"http://www.researchgate.net/publication/7307092_Self-referential_processing_in_our_brain--a_meta-analysis_of_imaging_studies_on_the_self/file/d912f50eb278e1a473.pdf\">this meta-analysis</a> by Northoff et al. of neuroimaging studies of self-referential cognitive processing to be fascinating for all sorts of reasons. Chief among them is the light it sheds on how and why including clear-cut self/other distinctions in models of human minds doesn't always work so well. (Link is to a PDF.)</li>\n<li>If you want to take a soaring leap across the bridge between knowledge of rationality and the application thereof, you simply must try a <a href=\"http://rationality.org/workshops/\">CFAR workshop</a>. I did one of these back in April, and it was every bit as fun as it was effective (which was very).</li>\n</ul>\n<hr>\n<h2 dir=\"ltr\" id=\"Notes\">Notes</h2>\n<p dir=\"ltr\">1. Off the top of my head: What aspects of particular forms of meditation cause the various purported benefits? If we pinpoint those aspects, can we harness their corresponding benefits individually without committing to meditation as a whole? Can we improve upon them? Does meditation have different effects when practiced in a religious context? What is the relationship between meditation and hypnosis? How do the effects differ among different age groups? Does learning to meditate while young have any effect on adult meditation?</p>\n<p dir=\"ltr\">2. Sedlmeier et al. (2012). <a href=\"http://www.ashanamind.com/wp-content/uploads/2013/03/physiological-effects_Sedlmeier_12.pdf\">The Psychological Effects of Meditation: A Meta-Analysis</a>. Psychological Bulletin, 138(6) 1139-1171.</p>\n<p dir=\"ltr\">3. I don't consider lack of supporting double-blind studies much evidence against my thesis, largely because the result in question would only show up in tests of metacognitive techniques I expect not to occur to the vast majority of researchers just yet.</p>\n<p dir=\"ltr\">4. In case you're wondering about my relevant background: I did Vinyasa yoga throughout high school, taught it during college, trained at a residential Soto Zen temple for a summer, have maintained a fairly regular practice of Zen meditation for about five years now, practiced Tai Chi (very casually) off and on for most of my life, and have a degree in religious studies with a focus on East Asian Buddhism.</p>\n<p dir=\"ltr\">5. <a href=\"http://www.sciencedaily.com/releases/2013/07/130716080028.htm\">Fun fact</a>: You internally simulate your voice in parallel with actual talking.</p>\n<p dir=\"ltr\">6. Yes, you're doing it right. If you're trying to do it at all, you're doing it right. The idea is to find out what it feels like to make the effort, not to beat the game. There is no game. Some of the comments below have me concerned that I may be contributing to the \"meditation means being brain dead\" misconception. This exercise isn't meant to teach you the One True Way of Meditation. It's just to point at certain kinds of movements your mind makes. Monks who have been meditating for multiple hours a day for decades don't have completely featureless minds when they meditate. That isn't even close to what meditation means to them. Beginners are given exercises along these lines because it's an easier entry point, like training wheels. Eventually, counting breaths simply becomes irrelevant.</p>\n<p dir=\"ltr\">7. The Japanese \u201cSu sube[karaku] mochi[iyo] eko-hensho no taiho o mochi-iyo,\u201d from <a href=\"http://the-middle-way.org/subpage8.html\">Dogen Zengi\u2019s instructions for meditation</a> (1227) literally translates to English (character-by-character) as \u201cRemember/employ of backward step turning light/consciousness reflecting/illuminating.\u201d (Dogen loved wordplay, and the double meanings are intentional.) Though <a href=\"http://terebess.hu/zen/Fukanzazengi-6.pdf\">most translators</a> render this something like, \u201cTake the backward step that turns your light inwardly to illuminate the self,\u201d the characters for \u201cself\u201d and \u201cinward\u201d do not in fact appear in this part of the text. Thus, his central instruction for meditation is to step consciousness backward so it can be generally reflective. This is why I say that in practice shikantaza and mindfulness amount to the same thing.</p>", "sections": [{"title": "What kind of metacognitive skills am I talking about?", "anchor": "What_kind_of_metacognitive_skills_am_I_talking_about_", "level": 1}, {"title": "Exercise One", "anchor": "Exercise_One", "level": 2}, {"title": "Exercise Two", "anchor": "Exercise_Two", "level": 2}, {"title": "Exercise Three", "anchor": "Exercise_Three", "level": 2}, {"title": "What does this have to do with rationality?", "anchor": "What_does_this_have_to_do_with_rationality_", "level": 1}, {"title": "What does this have to do with meditation?", "anchor": "What_does_this_have_to_do_with_meditation_", "level": 1}, {"title": "Further Resources", "anchor": "Further_Resources", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "62 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WzPJRNYWhMXQTEj69", "a7n8GdKiAZRX86T5A", "5JDkW4MYXit2CquLs", "GrDqnMjhqoxiqpQPw", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-20T02:43:39.630Z", "modifiedAt": null, "url": null, "title": "An Introduction To Rationality", "slug": "an-introduction-to-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kerspoon", "createdAt": "2011-12-27T09:53:49.512Z", "isAdmin": false, "displayName": "kerspoon"}, "userId": "XvZ9yyyJNeDwWhECW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fpsxNCE6Jrcoeucta/an-introduction-to-rationality", "pageUrlRelative": "/posts/fpsxNCE6Jrcoeucta/an-introduction-to-rationality", "linkUrl": "https://www.lesswrong.com/posts/fpsxNCE6Jrcoeucta/an-introduction-to-rationality", "postedAtFormatted": "Sunday, October 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Introduction%20To%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Introduction%20To%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpsxNCE6Jrcoeucta%2Fan-introduction-to-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Introduction%20To%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpsxNCE6Jrcoeucta%2Fan-introduction-to-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpsxNCE6Jrcoeucta%2Fan-introduction-to-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2450, "htmlBody": "<p><em>This article is an attempt to <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">summarize basic material</a>, and thus probably won't have anything new for the hard core posting crowd. It'd be interesting to know whether you think there's anything essential I missed, though.</em></p>\n<h2>Summary</h2>\n<p>We have a mental model of the world that we call our beliefs. This world does not always reflect reality very well as our perceptions distort the information coming from our senses. The same is true of our desires such that we do not accurately know why we desire something. It may also be true for other parts of our subconscious. We, as conscious beings, can notice when there is a discrepancy and try to correct for it. This is known as Epistemic rationality. Seen this way science is a formalised version of epistemic rationality. Instrumental rationality is the act of doing what we value, whatever that may be.</p>\n<p>Both forms of rationality can be learnt and it is the aim of this document to convince you that it is both a good idea and worth your time.</p>\n<h2>Hill Walker's Analogy</h2>\n<p>Reality is the physical hills, streams, roads and landmarks (the territory) and our beliefs are the map we use to navigate them. We know that the map is not the same thing as the territory but we hope it is accurate enough to understand things about it all the same. Errors in the map can easily lead to us doing the wrong thing.</p>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/map.jpg\" alt=\"\" width=\"192\" height=\"201\" /> <img src=\"http://kerspoon.com/img/2013-rationality/earth.jpg\" alt=\"\" width=\"226\" height=\"226\" /></p>\n<h2>The Lens That Distorts</h2>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/reality_and_belief.jpg\" alt=\"\" /></p>\n<p>We see the world through a filter or lens called perception. This is necessary; there is too much information in the world to map fully but it is very important to quickly recognise certain things. For example it is useful to recognise that a lion is about to attack you.</p>\n<p>To go back to our hill walker's analogy: we are quicker navigating with a hill walker's map than an aerial photograph. This is because the map has been filtered to only show what is commonly needed by walkers. Too much information can slow our judgement or confuse us. On the other hand incorrect information or a lack of information is just as bad. If when out walking we are told to turn left at the second stream yet the hill has more streams than the map then we will get lost. The map was not adequate in this situation.</p>\n<p>In the same way our own maps can be wrong caused by errors in our perceptual filter. Optical illusions are a nice example of such a phenomenon where there is a distortion between map and territory.</p>\n<h2>An Example</h2>\n<p>Light from the sun bounces off our shoelaces, which are untied, and hits our eye (reality). These signals get perceived (the lens) as an untied shoelace and thus we have the belief (map) that our shoelaces are untied. In this case the territory and map reflect the same thing but the map contains a condensed version; the exact position of the laces were not deemed important and hence were filtered out by our perception.</p>\n<h2>Truth</h2>\n<p><cite> \"The sentence 'snow is white' is true if, and only if, snow is white.\" </cite>-- Alfred Tarski</p>\n<p>What is being said here is that if the reality is that 'snow is white' then we should believe that 'snow is white'. In other words, we should try to make our belief match reality. Unfortunately we cannot directly tell if in reality snow is white but given enough evidence we should believe it as true.</p>\n<p>By default, our subconscious believes what it sees, it has to. You have no time to question your belief if a lion is coming towards you. We could not have survived this long as a species if we questioned and tested everything. Yet there are some times where we are reproducibly, predictably, irrational. That is, we do not update our belief correctly based upon evidence.</p>\n<h2>Rationality</h2>\n<ul>\n<li>\n<p><strong>Epistemic rationality</strong>: believing, and updating on evidence, so as to systematically improve the correspondence between your map and the territory. The art of obtaining beliefs that correspond to reality as closely as possible. This correspondence is commonly termed \"truth\" or \"accuracy\".</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Instrumental rationality</strong>: achieving your values. Not necessarily \"your values\" in the sense of being selfish values or unshared values: \"your values\" means anything you care about. The art of choosing actions that steer the future toward outcomes ranked higher in your preferences. Sometimes referred to as \"winning\".</p>\n</li>\n</ul>\n<p>These definitions are the two kinds of rationality used here. Epistemic rationality is making the map match the territory. An example of an epistemic rationality error would be believing a cloud of steam is a ghost. Instrumental rationality is about doing what you should based upon what you value. An instrumental rationality error would be working rather than going to your friend's birthday when your values say you should have gone.</p>\n<h2>Why Should We Be Rational?</h2>\n<ul>\n<li>\n<p>Curiosity &ndash; an innate desire to know things. e.g. &ldquo;how does that work?&rdquo;</p>\n</li>\n<li>\n<p>Pragmatism &ndash; we can better achieve what we want in the world if we understand it better. e.g. &ldquo;I want to build an aeroplane; therefore I need to know about lift and aerodynamics&rdquo;, &ldquo;I want some milk; I need to know whether to go to the fridge or store&rdquo;. If we are irrational then our belief may cause a plane crash or walking to the store when there is milk in the fridge.</p>\n</li>\n<li>\n<p>Morality - the belief that truth seeking is important to society, and therefore it is a moral requirement to be rational.</p>\n</li>\n</ul>\n<h2>What Rationality Is And Is Not</h2>\n<p>Being rational does not mean thinking without emotion. Being rational and being logical are different things. It may be more rational for you to go along with something you believe to be incorrect if it fits with your values.</p>\n<p>For example, in an argument with a friend it may be logical to stick to what you know is true but rationally you may just concede a point of argument and help them even if you think it is not in their best interest. It all depends on your values and following your values is part of the definition of instrumental rationality.</p>\n<p>If you say &ldquo;the rational thing to do is X but the right thing is Y&rdquo; then you are using a different definition for rationality than is intended here. The right thing is always the rational thing by definition.</p>\n<p>Rationality also differs for different people. In other words the same action may be rational for one person and irrational for another. This could be due to:</p>\n<ul>\n<li>\n<p>Different realities. For example, living in a country with deadly spiders should give you more reason to be afraid of them. Hence the belief that spiders are scary is only rational in certain countries.</p>\n</li>\n<li>\n<p>Different values. For example two people may agree on how unlikely it is to win the lottery but one may still value the prize enough to enter. Hence playing the lottery can be either rational or irrational depending on your values.</p>\n</li>\n<li>\n<p>Incorrect beliefs. If you believe that a light-bulb will work without electricity and do not have sufficient evidence to support the claim then your belief is wrong. In fact if you believe anything without sufficient evidence then you are wrong but what constitutes sufficient evidence is down to your values and hence is another valid reason for beliefs to differ.</p>\n</li>\n</ul>\n<p>When confronting someone who has different beliefs this could be down to the points above, in which case it is worth trying to see the world through their lens to understand how their belief came about. You may still conclude that their belief is wrong. There is no problem with this. Not all beliefs are based in rationality.</p>\n<p>An important note in such discussions is to consider whether you are actually arguing over different points or using different definitions. For example, two people may argue about how many people live in New York because each is using a slightly different definition of New York. The same sort of thing happens when the old saying &ldquo;if a tree falls in the woods and no one is around to hear it does it make a sound?&rdquo;. People may have a different answer for this based upon their definition of &ldquo;makes a sound&rdquo; but there expected experiences are generally the same; that there are sound waves but not the perception of sound.</p>\n<h2>Science and Rationality</h2>\n<p>Science is a system of acquiring knowledge based on scientific method, and the organized body of knowledge gained through such research. In other words it is the act of testing theories and gaining information from those tests. A theory proposes an explanation for an event, that's all it is. This theory may or may not be useful; a useful theory is one that is:</p>\n<ol>\n<li>\n<p>Logically consistent,</p>\n</li>\n<li>\n<p>Can be used as a tool for prediction.</p>\n</li>\n</ol>\n<p>If this seems familiar to rationality then you are correct. Science attempts to obtain a true map of reality using a specific set of techniques. It is in essence a more formalised version of rationality.</p>\n<p>A theory is very strongly linked to a belief. Indeed they should both have the two traits listed above; to be logically consistent and to be a predictor of events. Just like a theory, a belief proposes an explanation for events.</p>\n<h2>The Lens That Sees Its Flaws</h2>\n<p>We, as conscious beings, have the ability to correct the distortion to our perceptual filter. We may not be able to see through an optical illusion (our lens will always be flawed) but we can choose to believe that it is an illusion based upon other evidence and our own thoughts.</p>\n<p>In the image on the left below (M&uuml;ller-Lyer illusion) we may see the lines as different lengths but through other evidence choose not to believe what our senses are telling us. The image to the right (Munker illusion) is an illusion that is even harder to believe: the red and purple looking colours on the top parts are actually the same; as is the green and turquoise looking ones on the bottom image. I hope that you check this for yourself. Even after you convince yourself of the illusion it will still be very hard to see them as the same colour. We may never be able to fix the flaws in our perception but by being aware of them we can reduce the mistakes we make because of it.</p>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/optical-illusion-1.jpg\" alt=\"\" width=\"290\" height=\"223\" /> <img src=\"http://kerspoon.com/img/2013-rationality/optical-illusion-2.jpg\" alt=\"\" width=\"312\" height=\"261\" /></p>\n<h2>Our Modular Brain</h2>\n<p>We don't know what many of our desires are, we don't know where they come from, and we can be wrong about our own motivations. For example, we may think the desire to give oral sex is for the pleasure of our partner. However our bodies create this desire as a test for health, fertility and infidelity. We consciously feel the desire and ascribe reason to it but it can be a different reason to the subconscious one.</p>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/reality_and_consciousness.jpg\" alt=\"\" /> <br /> Because of this we should treat the signals coming from our subconscious as distorted by yet another lens, one that hides much of what is behind it. We (speaking as the conscious) notice our subconscious desires and try to infer why they have occurred. Yet this understanding may be wrong. Again these flaws in our lens can be corrected. You probably already temper the amount of sugar and fat you eat even though you have a subconscious desire to eat more.</p>\n<p>Another example is that there are no seriously dangerous spiders in the UK so there is no rational reason to be afraid of them. Yet is seems to be a universal trait. On seeing a spider and feeling afraid, we can recognise that particular flaw in our subconscious reasoning and choose to act differently by not running away.</p>\n<h2>What should we do?</h2>\n<p>To be epistemicly rational we must look for systematic flaws in the perceptual filters (lenses) between reality and our brain as well as within different parts of the brain. We must then train ourselves to recognise and correct them when they occur. Simply talking or thinking about them is not enough, active training in recognising and dealing with such errors is needed.</p>\n<p>To be instrumentally rational we must define our values, updating them as needed, and learn how to achieve these values. Simply having well defined values is not enough, everything we do should work towards achieving them in some way. There are methods that can be learnt to help with achieving goals and it would make sense to learn these especially if you are prone to procrastination. The links below are a few examples of such things on procrastination, self help, and achieving goals.</p>\n<ul>\n<li>\n<p><a href=\"/lw/3w3/how_to_beat_procrastination/\">http://lesswrong.com/lw/3w3/how_to_beat_procrastination/</a></p>\n</li>\n<li>\n<p><a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">http://lesswrong.com/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/</a></p>\n</li>\n<li>\n<p><a href=\"/lw/2p5/humans_are_not_automatically_strategic\">http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic</a></p>\n</li>\n</ul>\n<p>One of the most important concepts to grasp is the best way to update our beliefs based upon what we experience (the evidence). Thomas Bayes formulated this mathematically such that if we were behaving rationally we would expect to follow Bayes' Theorem.</p>\n<p>Research shows that, in some cases at least, people do not generally follow this model. Hence one of our first goals should be to think about evidence in a Bayesian way. See <a href=\"http://yudkowsky.net/rational/bayes\">http://yudkowsky.net/rational/bayes</a> for an intuitive explanation of Bayes' Theorem.</p>\n<p>One important concept comes out of the theorem that I will briefly introduce here. Evidence must update our existing beliefs not replace them. If a test comes up positive for cancer the probability that you have it depends on the accuracy of the test AND the prevalence for cancer in the general population. This is likely to seem strange unless you think of it in a Bayesian way.</p>\n<h2>Summary</h2>\n<ul>\n<li>\n<p>The physical world is reality.</p>\n</li>\n<li>\n<p>Inside our brains we have beliefs.</p>\n</li>\n<li>\n<p>Our beliefs are meant to mirror reality.</p>\n</li>\n<li>\n<p>A good belief and a good scientific theory is:</p>\n<ul>\n<li>\n<p>Logically consistent &ndash; fits in with every other good belief.</p>\n</li>\n<li>\n<p>A predictor of events &ndash; helps you predict the future and explain past events.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Our perception of the world can distort the beliefs.</p>\n</li>\n<li>\n<p>We can change how we perceive things through conscious thought.</p>\n</li>\n<li>\n<p>This can reduce the error between reality and our beliefs.</p>\n</li>\n<li>\n<p>This is called rationality.</p>\n</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fpsxNCE6Jrcoeucta", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 17, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "24446", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This article is an attempt to <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">summarize basic material</a>, and thus probably won't have anything new for the hard core posting crowd. It'd be interesting to know whether you think there's anything essential I missed, though.</em></p>\n<h2 id=\"Summary\">Summary</h2>\n<p>We have a mental model of the world that we call our beliefs. This world does not always reflect reality very well as our perceptions distort the information coming from our senses. The same is true of our desires such that we do not accurately know why we desire something. It may also be true for other parts of our subconscious. We, as conscious beings, can notice when there is a discrepancy and try to correct for it. This is known as Epistemic rationality. Seen this way science is a formalised version of epistemic rationality. Instrumental rationality is the act of doing what we value, whatever that may be.</p>\n<p>Both forms of rationality can be learnt and it is the aim of this document to convince you that it is both a good idea and worth your time.</p>\n<h2 id=\"Hill_Walker_s_Analogy\">Hill Walker's Analogy</h2>\n<p>Reality is the physical hills, streams, roads and landmarks (the territory) and our beliefs are the map we use to navigate them. We know that the map is not the same thing as the territory but we hope it is accurate enough to understand things about it all the same. Errors in the map can easily lead to us doing the wrong thing.</p>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/map.jpg\" alt=\"\" width=\"192\" height=\"201\"> <img src=\"http://kerspoon.com/img/2013-rationality/earth.jpg\" alt=\"\" width=\"226\" height=\"226\"></p>\n<h2 id=\"The_Lens_That_Distorts\">The Lens That Distorts</h2>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/reality_and_belief.jpg\" alt=\"\"></p>\n<p>We see the world through a filter or lens called perception. This is necessary; there is too much information in the world to map fully but it is very important to quickly recognise certain things. For example it is useful to recognise that a lion is about to attack you.</p>\n<p>To go back to our hill walker's analogy: we are quicker navigating with a hill walker's map than an aerial photograph. This is because the map has been filtered to only show what is commonly needed by walkers. Too much information can slow our judgement or confuse us. On the other hand incorrect information or a lack of information is just as bad. If when out walking we are told to turn left at the second stream yet the hill has more streams than the map then we will get lost. The map was not adequate in this situation.</p>\n<p>In the same way our own maps can be wrong caused by errors in our perceptual filter. Optical illusions are a nice example of such a phenomenon where there is a distortion between map and territory.</p>\n<h2 id=\"An_Example\">An Example</h2>\n<p>Light from the sun bounces off our shoelaces, which are untied, and hits our eye (reality). These signals get perceived (the lens) as an untied shoelace and thus we have the belief (map) that our shoelaces are untied. In this case the territory and map reflect the same thing but the map contains a condensed version; the exact position of the laces were not deemed important and hence were filtered out by our perception.</p>\n<h2 id=\"Truth\">Truth</h2>\n<p><cite> \"The sentence 'snow is white' is true if, and only if, snow is white.\" </cite>-- Alfred Tarski</p>\n<p>What is being said here is that if the reality is that 'snow is white' then we should believe that 'snow is white'. In other words, we should try to make our belief match reality. Unfortunately we cannot directly tell if in reality snow is white but given enough evidence we should believe it as true.</p>\n<p>By default, our subconscious believes what it sees, it has to. You have no time to question your belief if a lion is coming towards you. We could not have survived this long as a species if we questioned and tested everything. Yet there are some times where we are reproducibly, predictably, irrational. That is, we do not update our belief correctly based upon evidence.</p>\n<h2 id=\"Rationality\">Rationality</h2>\n<ul>\n<li>\n<p><strong>Epistemic rationality</strong>: believing, and updating on evidence, so as to systematically improve the correspondence between your map and the territory. The art of obtaining beliefs that correspond to reality as closely as possible. This correspondence is commonly termed \"truth\" or \"accuracy\".</p>\n</li>\n</ul>\n<ul>\n<li>\n<p><strong>Instrumental rationality</strong>: achieving your values. Not necessarily \"your values\" in the sense of being selfish values or unshared values: \"your values\" means anything you care about. The art of choosing actions that steer the future toward outcomes ranked higher in your preferences. Sometimes referred to as \"winning\".</p>\n</li>\n</ul>\n<p>These definitions are the two kinds of rationality used here. Epistemic rationality is making the map match the territory. An example of an epistemic rationality error would be believing a cloud of steam is a ghost. Instrumental rationality is about doing what you should based upon what you value. An instrumental rationality error would be working rather than going to your friend's birthday when your values say you should have gone.</p>\n<h2 id=\"Why_Should_We_Be_Rational_\">Why Should We Be Rational?</h2>\n<ul>\n<li>\n<p>Curiosity \u2013 an innate desire to know things. e.g. \u201chow does that work?\u201d</p>\n</li>\n<li>\n<p>Pragmatism \u2013 we can better achieve what we want in the world if we understand it better. e.g. \u201cI want to build an aeroplane; therefore I need to know about lift and aerodynamics\u201d, \u201cI want some milk; I need to know whether to go to the fridge or store\u201d. If we are irrational then our belief may cause a plane crash or walking to the store when there is milk in the fridge.</p>\n</li>\n<li>\n<p>Morality - the belief that truth seeking is important to society, and therefore it is a moral requirement to be rational.</p>\n</li>\n</ul>\n<h2 id=\"What_Rationality_Is_And_Is_Not\">What Rationality Is And Is Not</h2>\n<p>Being rational does not mean thinking without emotion. Being rational and being logical are different things. It may be more rational for you to go along with something you believe to be incorrect if it fits with your values.</p>\n<p>For example, in an argument with a friend it may be logical to stick to what you know is true but rationally you may just concede a point of argument and help them even if you think it is not in their best interest. It all depends on your values and following your values is part of the definition of instrumental rationality.</p>\n<p>If you say \u201cthe rational thing to do is X but the right thing is Y\u201d then you are using a different definition for rationality than is intended here. The right thing is always the rational thing by definition.</p>\n<p>Rationality also differs for different people. In other words the same action may be rational for one person and irrational for another. This could be due to:</p>\n<ul>\n<li>\n<p>Different realities. For example, living in a country with deadly spiders should give you more reason to be afraid of them. Hence the belief that spiders are scary is only rational in certain countries.</p>\n</li>\n<li>\n<p>Different values. For example two people may agree on how unlikely it is to win the lottery but one may still value the prize enough to enter. Hence playing the lottery can be either rational or irrational depending on your values.</p>\n</li>\n<li>\n<p>Incorrect beliefs. If you believe that a light-bulb will work without electricity and do not have sufficient evidence to support the claim then your belief is wrong. In fact if you believe anything without sufficient evidence then you are wrong but what constitutes sufficient evidence is down to your values and hence is another valid reason for beliefs to differ.</p>\n</li>\n</ul>\n<p>When confronting someone who has different beliefs this could be down to the points above, in which case it is worth trying to see the world through their lens to understand how their belief came about. You may still conclude that their belief is wrong. There is no problem with this. Not all beliefs are based in rationality.</p>\n<p>An important note in such discussions is to consider whether you are actually arguing over different points or using different definitions. For example, two people may argue about how many people live in New York because each is using a slightly different definition of New York. The same sort of thing happens when the old saying \u201cif a tree falls in the woods and no one is around to hear it does it make a sound?\u201d. People may have a different answer for this based upon their definition of \u201cmakes a sound\u201d but there expected experiences are generally the same; that there are sound waves but not the perception of sound.</p>\n<h2 id=\"Science_and_Rationality\">Science and Rationality</h2>\n<p>Science is a system of acquiring knowledge based on scientific method, and the organized body of knowledge gained through such research. In other words it is the act of testing theories and gaining information from those tests. A theory proposes an explanation for an event, that's all it is. This theory may or may not be useful; a useful theory is one that is:</p>\n<ol>\n<li>\n<p>Logically consistent,</p>\n</li>\n<li>\n<p>Can be used as a tool for prediction.</p>\n</li>\n</ol>\n<p>If this seems familiar to rationality then you are correct. Science attempts to obtain a true map of reality using a specific set of techniques. It is in essence a more formalised version of rationality.</p>\n<p>A theory is very strongly linked to a belief. Indeed they should both have the two traits listed above; to be logically consistent and to be a predictor of events. Just like a theory, a belief proposes an explanation for events.</p>\n<h2 id=\"The_Lens_That_Sees_Its_Flaws\">The Lens That Sees Its Flaws</h2>\n<p>We, as conscious beings, have the ability to correct the distortion to our perceptual filter. We may not be able to see through an optical illusion (our lens will always be flawed) but we can choose to believe that it is an illusion based upon other evidence and our own thoughts.</p>\n<p>In the image on the left below (M\u00fcller-Lyer illusion) we may see the lines as different lengths but through other evidence choose not to believe what our senses are telling us. The image to the right (Munker illusion) is an illusion that is even harder to believe: the red and purple looking colours on the top parts are actually the same; as is the green and turquoise looking ones on the bottom image. I hope that you check this for yourself. Even after you convince yourself of the illusion it will still be very hard to see them as the same colour. We may never be able to fix the flaws in our perception but by being aware of them we can reduce the mistakes we make because of it.</p>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/optical-illusion-1.jpg\" alt=\"\" width=\"290\" height=\"223\"> <img src=\"http://kerspoon.com/img/2013-rationality/optical-illusion-2.jpg\" alt=\"\" width=\"312\" height=\"261\"></p>\n<h2 id=\"Our_Modular_Brain\">Our Modular Brain</h2>\n<p>We don't know what many of our desires are, we don't know where they come from, and we can be wrong about our own motivations. For example, we may think the desire to give oral sex is for the pleasure of our partner. However our bodies create this desire as a test for health, fertility and infidelity. We consciously feel the desire and ascribe reason to it but it can be a different reason to the subconscious one.</p>\n<p><img src=\"http://kerspoon.com/img/2013-rationality/reality_and_consciousness.jpg\" alt=\"\"> <br> Because of this we should treat the signals coming from our subconscious as distorted by yet another lens, one that hides much of what is behind it. We (speaking as the conscious) notice our subconscious desires and try to infer why they have occurred. Yet this understanding may be wrong. Again these flaws in our lens can be corrected. You probably already temper the amount of sugar and fat you eat even though you have a subconscious desire to eat more.</p>\n<p>Another example is that there are no seriously dangerous spiders in the UK so there is no rational reason to be afraid of them. Yet is seems to be a universal trait. On seeing a spider and feeling afraid, we can recognise that particular flaw in our subconscious reasoning and choose to act differently by not running away.</p>\n<h2 id=\"What_should_we_do_\">What should we do?</h2>\n<p>To be epistemicly rational we must look for systematic flaws in the perceptual filters (lenses) between reality and our brain as well as within different parts of the brain. We must then train ourselves to recognise and correct them when they occur. Simply talking or thinking about them is not enough, active training in recognising and dealing with such errors is needed.</p>\n<p>To be instrumentally rational we must define our values, updating them as needed, and learn how to achieve these values. Simply having well defined values is not enough, everything we do should work towards achieving them in some way. There are methods that can be learnt to help with achieving goals and it would make sense to learn these especially if you are prone to procrastination. The links below are a few examples of such things on procrastination, self help, and achieving goals.</p>\n<ul>\n<li>\n<p><a href=\"/lw/3w3/how_to_beat_procrastination/\">http://lesswrong.com/lw/3w3/how_to_beat_procrastination/</a></p>\n</li>\n<li>\n<p><a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">http://lesswrong.com/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/</a></p>\n</li>\n<li>\n<p><a href=\"/lw/2p5/humans_are_not_automatically_strategic\">http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic</a></p>\n</li>\n</ul>\n<p>One of the most important concepts to grasp is the best way to update our beliefs based upon what we experience (the evidence). Thomas Bayes formulated this mathematically such that if we were behaving rationally we would expect to follow Bayes' Theorem.</p>\n<p>Research shows that, in some cases at least, people do not generally follow this model. Hence one of our first goals should be to think about evidence in a Bayesian way. See <a href=\"http://yudkowsky.net/rational/bayes\">http://yudkowsky.net/rational/bayes</a> for an intuitive explanation of Bayes' Theorem.</p>\n<p>One important concept comes out of the theorem that I will briefly introduce here. Evidence must update our existing beliefs not replace them. If a test comes up positive for cancer the probability that you have it depends on the accuracy of the test AND the prevalence for cancer in the general population. This is likely to seem strange unless you think of it in a Bayesian way.</p>\n<h2 id=\"Summary1\">Summary</h2>\n<ul>\n<li>\n<p>The physical world is reality.</p>\n</li>\n<li>\n<p>Inside our brains we have beliefs.</p>\n</li>\n<li>\n<p>Our beliefs are meant to mirror reality.</p>\n</li>\n<li>\n<p>A good belief and a good scientific theory is:</p>\n<ul>\n<li>\n<p>Logically consistent \u2013 fits in with every other good belief.</p>\n</li>\n<li>\n<p>A predictor of events \u2013 helps you predict the future and explain past events.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Our perception of the world can distort the beliefs.</p>\n</li>\n<li>\n<p>We can change how we perceive things through conscious thought.</p>\n</li>\n<li>\n<p>This can reduce the error between reality and our beliefs.</p>\n</li>\n<li>\n<p>This is called rationality.</p>\n</li>\n</ul>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Hill Walker's Analogy", "anchor": "Hill_Walker_s_Analogy", "level": 1}, {"title": "The Lens That Distorts", "anchor": "The_Lens_That_Distorts", "level": 1}, {"title": "An Example", "anchor": "An_Example", "level": 1}, {"title": "Truth", "anchor": "Truth", "level": 1}, {"title": "Rationality", "anchor": "Rationality", "level": 1}, {"title": "Why Should We Be Rational?", "anchor": "Why_Should_We_Be_Rational_", "level": 1}, {"title": "What Rationality Is And Is Not", "anchor": "What_Rationality_Is_And_Is_Not", "level": 1}, {"title": "Science and Rationality", "anchor": "Science_and_Rationality", "level": 1}, {"title": "The Lens That Sees Its Flaws", "anchor": "The_Lens_That_Sees_Its_Flaws", "level": 1}, {"title": "Our Modular Brain", "anchor": "Our_Modular_Brain", "level": 1}, {"title": "What should we do?", "anchor": "What_should_we_do_", "level": 1}, {"title": "Summary", "anchor": "Summary1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 15}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2TPph4EGZ6trEbtku", "RWo4LwFzpHNQCTcYt", "33KewgYhNSxFpbpXg", "PBRWb2Em5SNeWYwwB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-21T03:11:18.401Z", "modifiedAt": null, "url": null, "title": "Open Thread, October 20 - 26, 2013", "slug": "open-thread-october-20-26-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.190Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oe9W6iuM6YN4nCMvR/open-thread-october-20-26-2013", "pageUrlRelative": "/posts/oe9W6iuM6YN4nCMvR/open-thread-october-20-26-2013", "linkUrl": "https://www.lesswrong.com/posts/oe9W6iuM6YN4nCMvR/open-thread-october-20-26-2013", "postedAtFormatted": "Monday, October 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20October%2020%20-%2026%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20October%2020%20-%2026%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foe9W6iuM6YN4nCMvR%2Fopen-thread-october-20-26-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20October%2020%20-%2026%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foe9W6iuM6YN4nCMvR%2Fopen-thread-october-20-26-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foe9W6iuM6YN4nCMvR%2Fopen-thread-october-20-26-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>\n<div id=\"entry_t3_ith\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">\n<div class=\"md\">\n<div>\n<p style=\"margin: 0px 0px 1em;\"><span style=\"line-height: 12.660714149475098px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>\n<div><span style=\"line-height: 12.660714149475098px;\"><br /></span></div>\n</div>\n</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oe9W6iuM6YN4nCMvR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.388631010530535e-06, "legacy": true, "legacyId": "24448", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 220, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-21T03:58:55.739Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Anthropics with Robin Hanson", "slug": "meetup-washington-dc-anthropics-with-robin-hanson", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LBKrNSyG3G287yr37/meetup-washington-dc-anthropics-with-robin-hanson", "pageUrlRelative": "/posts/LBKrNSyG3G287yr37/meetup-washington-dc-anthropics-with-robin-hanson", "linkUrl": "https://www.lesswrong.com/posts/LBKrNSyG3G287yr37/meetup-washington-dc-anthropics-with-robin-hanson", "postedAtFormatted": "Monday, October 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Anthropics%20with%20Robin%20Hanson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Anthropics%20with%20Robin%20Hanson%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBKrNSyG3G287yr37%2Fmeetup-washington-dc-anthropics-with-robin-hanson%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Anthropics%20with%20Robin%20Hanson%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBKrNSyG3G287yr37%2Fmeetup-washington-dc-anthropics-with-robin-hanson", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBKrNSyG3G287yr37%2Fmeetup-washington-dc-anthropics-with-robin-hanson", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sl'>Washington DC: Anthropics with Robin Hanson</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to discuss anthropics: things like the Doomsday Argument, self-sampling, and the Great Filter. Robin Hanson will be leading the discussion.</p>\n\n<p>We'll meet in the courtyard at the center of the National Portrait Gallery, as usual.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sl'>Washington DC: Anthropics with Robin Hanson</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LBKrNSyG3G287yr37", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.3886757301155234e-06, "legacy": true, "legacyId": "24449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Anthropics_with_Robin_Hanson\">Discussion article for the meetup : <a href=\"/meetups/sl\">Washington DC: Anthropics with Robin Hanson</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to discuss anthropics: things like the Doomsday Argument, self-sampling, and the Great Filter. Robin Hanson will be leading the discussion.</p>\n\n<p>We'll meet in the courtyard at the center of the National Portrait Gallery, as usual.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Anthropics_with_Robin_Hanson1\">Discussion article for the meetup : <a href=\"/meetups/sl\">Washington DC: Anthropics with Robin Hanson</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Anthropics with Robin Hanson", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Anthropics_with_Robin_Hanson", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Anthropics with Robin Hanson", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Anthropics_with_Robin_Hanson1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-21T08:51:31.457Z", "modifiedAt": null, "url": null, "title": "[Link] Trouble at the lab", "slug": "link-trouble-at-the-lab", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:07.772Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BCYwJSpQKR3dq9ENE/link-trouble-at-the-lab", "pageUrlRelative": "/posts/BCYwJSpQKR3dq9ENE/link-trouble-at-the-lab", "linkUrl": "https://www.lesswrong.com/posts/BCYwJSpQKR3dq9ENE/link-trouble-at-the-lab", "postedAtFormatted": "Monday, October 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Trouble%20at%20the%20lab&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Trouble%20at%20the%20lab%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCYwJSpQKR3dq9ENE%2Flink-trouble-at-the-lab%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Trouble%20at%20the%20lab%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCYwJSpQKR3dq9ENE%2Flink-trouble-at-the-lab", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCYwJSpQKR3dq9ENE%2Flink-trouble-at-the-lab", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 275, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/erk/link_the_real_end_of_science/\">The Real End of Science </a></p>\n<p><a href=\"http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble\">From</a> the Economist.</p>\n<blockquote>\n<p>&ldquo;I SEE a train wreck looming,&rdquo; warned Daniel Kahneman, an eminent psychologist, in an open letter last year. The premonition concerned research on a phenomenon known as &ldquo;priming&rdquo;. Priming studies suggest that decisions can be influenced by apparently irrelevant actions or events that took place just before the cusp of choice. They have been a boom area in psychology over the past decade, and some of their insights have already made it out of the lab and into the toolkits of policy wonks keen on &ldquo;nudging&rdquo; the populace.</p>\n<p>Dr Kahneman and a growing number of his colleagues fear that a lot of this priming research is poorly founded. Over the past few years various researchers have made systematic attempts to replicate some of the more widely cited priming experiments. Many of these replications have failed. In April, for instance, a paper in <em class=\"Italic\">PLoS ONE</em>, a journal, reported that nine separate experiments had not managed to reproduce the results of a famous study from 1998 purporting to show that thinking about a professor before taking an intelligence test leads to a higher score than imagining a football hooligan.</p>\n<p>&nbsp;</p>\n<p>The idea that the same experiments always get the same results, no matter who performs them, is one of the cornerstones of science&rsquo;s claim to objective truth. If a systematic campaign of replication does not lead to the same results, then either the original research is flawed (as the replicators claim) or the replications are (as many of the original researchers on priming contend). Either way, something is awry.</p>\n<p>...</p>\n</blockquote>\n<p><br />I recommend reading the whole thing.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BCYwJSpQKR3dq9ENE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "24451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBzd6thZHLvEdbzgt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-21T11:23:30.990Z", "modifiedAt": null, "url": null, "title": "Criticisms of the Metaethics", "slug": "criticisms-of-the-metaethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:27.893Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B5BWxBzxaAkwnuBgj/criticisms-of-the-metaethics", "pageUrlRelative": "/posts/B5BWxBzxaAkwnuBgj/criticisms-of-the-metaethics", "linkUrl": "https://www.lesswrong.com/posts/B5BWxBzxaAkwnuBgj/criticisms-of-the-metaethics", "postedAtFormatted": "Monday, October 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Criticisms%20of%20the%20Metaethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACriticisms%20of%20the%20Metaethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5BWxBzxaAkwnuBgj%2Fcriticisms-of-the-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Criticisms%20of%20the%20Metaethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5BWxBzxaAkwnuBgj%2Fcriticisms-of-the-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5BWxBzxaAkwnuBgj%2Fcriticisms-of-the-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1344, "htmlBody": "<p>I'll admit that I'm using the LessWrong Board to try and figure out flaws in my own philosophical ideas. I should also make a disclaimer that I <em>do not dispute the usefulness of Eliezer's ideas for the purposes of building a Friendy AI. </em></p>\r\n<p>My criticisms are designed for other purposes- namely, that contrary to what I am led to believe most of this site believes <strong>Eliezer's metaethics does not work for solving ethical dilemnas except as a set of arbitrary rules,&nbsp;and in no way the stand-out best choice compared to any other&nbsp;self-consistent deontological or consequentialist system.</strong></p>\r\n<p>I'll also admit that I have something of a bias, for those looking in- I find an interesting intellectual challenge to look through philosophies and find weak points in them, so I may have been over-eager to find a bias that doesn't exist. I have been attempting to find an appropriate flaw for some time as some of my posts may have foreshadowed.</p>\r\n<p>Finally, I will note that I am also attempting to dodge attacks on Elizier's ethics despite it's connections to Eliezer's epistemology.</p>\r\n<p>---------------------------------------</p>\r\n<p>1: My Basic Argument</p>\r\n<p>Typically, people ask two things out of ethics- a reason to be ethical in the first place, and a way to resolve ethical dilemnas. Eliezer gets around the former by, effectively, appealing to the fact that people want to be moral even if there is no universially compelling argument.</p>\r\n<p>The problem with Eliezer's metaethics are based around what I call the A-case after the character I invented to be in it the first time I thought up this idea. A has two options- Option 1 is the best choice from a Consequentialist perspective and A is smart enough to figure that out. However, following Option 1 would make A feel very guilty for some reason (which A cannot overcome merely by thinking about it), whereas Option 2 would feel morally&nbsp;right on an emotive level.</p>\r\n<p>This, of course, implies that A is not greatly influenced by consequentialism- but that's quite plausible. Perhaps you have to be irrational to an intelligent non-consequentialist, but an irrational non-consequentialist smart enough to perform a utility calculation as a theoretical exercise is plausible.</p>\r\n<p>How can we say that the <em>right </em>thing for A to do is Option 1, in such a way as be both rational and in any way convincing to A? From the premises, it is likely that any possible argument will be rejected by A in such a manner that you can't claim A is being irrational.</p>\r\n<p>This can also be used against any particular deontological code- in fact more effectively due to greater plausibility- by substituting it for Consequentialism and claiming that according to said code it is A's moral duty. You can define <em>should </em>all you like, but A is using a different definition of <em>should </em>(not part of the opening scenario, but a safe inference except for a few unusual philosophers). You are talking about two different things.</p>\r\n<p>-----------------------------------------------------------------</p>\r\n<p>2: Addressing Counterarguments</p>\r\n<p>i:</p>\r\n<p>It could be argued that A has a rightness function which, on reflection, will lead A to embrace consequentialism as best for humanity as a whole. This is, however, not necessarily correct- to use an extreme case, what if A is being asked to kill A's own innocent&nbsp;lover, or&nbsp;her own baby? (\"Her\" because it's likely a much stronger intution that way) Some people in A's posistion have said rightness functions- it is easily possible A does not.</p>\r\n<p>In addition, a follower of Lesswrong morality in it's standard form has a dilemna here. If you say that A is still morally obliged to kill her own baby, then Eleizer's own arguments can be turned against you- still pulling a child off the traintracks regardless of any 'objective' right. If you say she isn't, you've conceded the case.</p>\r\n<p>A deontological&nbsp;theory is either founded on intuitions or not. If not&nbsp;Hume's Is-Ought distinction refutes it. If it is, then it faces similiar dilemnas in scenarios like this. Intuitions, however, do not add up to a logically consistent philosophy- \"moral luck\"&nbsp;(the idea a person can be more or less morally responsible based on factors outside their control) feels like an oxymoron at first, but many intuitions depend on it.</p>\r\n<p>ii:</p>\r\n<p>One possible counteragument is that A wants to do things in the world, and merely following A's feelings turns A into a morality pump making actions which don't make sense. However, there are several problems with this.</p>\r\n<p>i- A's actions probably make sense from the perspective of \"Make A feel morally justified\". A can't self-modify (at least not directly), after all.</p>\r\n<p>ii- Depending on the strengths of the emotions, A does not necessarily care even if A is aware of the inconsistencies in their actions. There are plenty of possible cases- a person dealing with those with whom they have close emotional ties, biases related to race or physical attractiveness, condeming large numbers of innocents to death etc.</p>\r\n<p>iii:</p>\r\n<p>A final counterargument would be that the way to solve this is through a Coherentist style Reflective Equilibrium. Even if Coherentism is not epistemically true, by treating intuitions as if it were true and following the Coherentist philosophy the result could feel satisfying. The problem is- what if it doesn't? If a person's emotions are strong enough, no amount of Reflective Equilibrium is strong enough to contradict them.</p>\r\n<p>If you take an emotivist posistion, however, you have the problem Emotivism has no solution when feelings contradict each other.</p>\r\n<p>------------------------------------------------------------------</p>\r\n<p>3: Conclusions</p>\r\n<p>My contention here is that we have a serious problem. The concept of right and wrong is like the concept of personal identity- merely something to be abolished for a more accurate view of what exists. It can be replaced with \"Wants\" (for people who have a unified moral system but various feelings),&nbsp; \"Moralities\" (systematic moral codes which are internally coherent), and \"Pseudo-Moralities\"&nbsp;with no objective morality even in the Yudowskyite sense&nbsp;existing.</p>\r\n<p>A delusion exists of morality in most human minds, of course- just as a delusion exists of personal identity in most if not all human minds. \"Moralities\" can still exist in terms of groups of entities who all want similiar things or agree with basic moral rules, that can be taken to their logical conclusions.</p>\r\n<p>Why can that not lead to morality? It can, but if you accept a morality on that basis it implies that rational argument (as opposed to emotional argument, which is a different matter) is in many cases entirely impossible with humans with different moralities, just as it is with aliens.</p>\r\n<p>This leaves&nbsp;two types of rational argument possible about ethical questions:</p>\r\n<p>-Demonstrating that a person would want something different if they knew all the facts- whether facts such as \"God doesn't exist\", facts such as \"This action won't have the consequences you think it will\", or facts about the human psyche.</p>\r\n<p>-Showing a person's Morality has internal inconsistencies, which in most people will mean they discard it. (With mere moral Wants this is more debatable)</p>\r\n<p>Arguably it also leads a third- demonstrating to a person that they do not really want what they think they want. However, this is a philosophical can of worms which I don't want to open up (metaphorically speaking) because it is highly complicated (I can think of plenty of arguments against the possibility of such, even if I am not so convinced they are true as to assert it) and because solving it does not contribute much to the main issue.</p>\r\n<p>Eliezer's morality cannot even work out on that basis, however. In any scenario where an individual B:</p>\r\n<p>i- Acts against Eliezer's moral code</p>\r\n<p>ii- Feels morally right about doing so, and would have felt guilty for following Eliezer's ideas</p>\r\n<p>Then they can argue against somebody trying to use Eliezer's ideas against them by pointing out that regardless of any Objective Morality, Eliezer still has a good case for dragging children off train tracks.</p>\r\n<p>I will not delve into what proportion of humans can be said to make up a single Morality due to having basically similiar premises and intuitions. Although there are reasons to doubt it is as large as you'd think (take the A case), I'm not sure if it would work.</p>\r\n<p>In conclusion- there is no Universially Compelling argument amongst humans, or even amongst rational humans.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B5BWxBzxaAkwnuBgj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -6, "extendedScore": null, "score": 1.3890933431130434e-06, "legacy": true, "legacyId": "24452", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-21T22:02:19.113Z", "modifiedAt": null, "url": null, "title": "What Can We Learn About Human Psychology from Christian Apologetics?", "slug": "what-can-we-learn-about-human-psychology-from-christian", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.140Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wf8anBt9JzgrDYaqG/what-can-we-learn-about-human-psychology-from-christian", "pageUrlRelative": "/posts/Wf8anBt9JzgrDYaqG/what-can-we-learn-about-human-psychology-from-christian", "linkUrl": "https://www.lesswrong.com/posts/Wf8anBt9JzgrDYaqG/what-can-we-learn-about-human-psychology-from-christian", "postedAtFormatted": "Monday, October 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Can%20We%20Learn%20About%20Human%20Psychology%20from%20Christian%20Apologetics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Can%20We%20Learn%20About%20Human%20Psychology%20from%20Christian%20Apologetics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWf8anBt9JzgrDYaqG%2Fwhat-can-we-learn-about-human-psychology-from-christian%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Can%20We%20Learn%20About%20Human%20Psychology%20from%20Christian%20Apologetics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWf8anBt9JzgrDYaqG%2Fwhat-can-we-learn-about-human-psychology-from-christian", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWf8anBt9JzgrDYaqG%2Fwhat-can-we-learn-about-human-psychology-from-christian", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2301, "htmlBody": "<p>A couple months ago I set up a Skype meeting Robin Hanson to chat about the book he's working on. But the first thing he wanted to talk wasn't directly related to the book. He'd read some of <a href=\"http://www.patheos.com/blogs/hallq/\">my work</a> critiquing Christian apologetics, and said something to the effect of even though people who spend a lot of time arguing about religion are extreme cases, maybe they somehow shed light on the psychology of ordinary people. I didn't have a good response at the time; I had taken a shot at discussing the sociology of apologetics in my <a href=\"http://www.patheos.com/blogs/hallq/free-ebook/\">first book</a>, but I was never terribly satisfied with that chapter and hadn't thought about the subject much since writing it.</p>\n<p>Since then, I've thought about it more, and now have a better answer for Robin. The take-away is that to understand Christian apologetics, you need to see it as a giant exercise is violating Eliezer's advice in the <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\">Against Rationalization</a> subsequence, particularly <a href=\"/lw/js/the_bottom_line/\">The Bottom Line</a>. What's particularly noteworthy is the enormous amount of effort many Christians put into doing so, rather than just shrugging their shoulders and saying \"I believe on faith.\" (Note: everything I say here is probably applicable to some degree to other forms of apologetics, but I'll focus on Christian apologetics and in particular <em>Protestant </em>apologetics because it's what I'm most familiar with.)</p>\n<p><a id=\"more\"></a></p>\n<p>And I need to emphasize from the start that we are talking about a lot of Christians here. Big name professional apologists are rare, but then so by definition are \"big names\" in any field. <em>Consumers </em>of apologetics are not so rare: countless evangelicals have read C. S. Lewis' apologetic work&nbsp;<em>Mere Christianity </em>and it's number 3 on <em>Christianity Today's </em>list of <a href=\"http://www.christianitytoday.com/ct/2006/october/23.51.html\">\"The Top 50 Books That Have Shaped Evangelicals.\"</a>&nbsp;Immediately following <em>Mere Christianity </em>on the list is another apologetic work,&nbsp;Francis Schaeffer's <em>The God Who Is There.&nbsp;</em>Josh McDowell's <em>The Evidence That Demands A Verdict </em>is number 13. McDowell's <em>More Than A Carpenter </em>has reportedly sold 15 million copies, while Lee Strobel's <em>Case For... </em>books have reportedly sold 10 million copies all together.</p>\n<p>I think apologists are best-seen as a highly specialized kind of religious professional, in some ways analogous to priests and ministers. Indeed there's overlap: many prominent apologists have had less well-known careers as pastors, while many evangelical pastors brush up on their apologetic arguments to share them with their congregations.</p>\n<p>The second thing you need to understand, if you want to make sense of apologetics, is that apologists are in the business of pretending the purpose of apologetics is something other than what it actually is. This is not something you will learn <em>even from reading many atheist critiques of apologetics, </em>because many critics are willing to politely play along with the pretense that the purpose of apologetics is to open minded-skeptics and debates between skeptics and believers are serious intellectual engagements.</p>\n<p>Such politeness may actually be smart tactics, if you are addressing believers and your goal is to persuade them, but that's not what I'll be doing here. Instead, I'll be addressing the mostly-atheist readership of LessWrong, and my goal will be to see what we can learn from apologetics about human psychology in general.</p>\n<p>One of the best discussions I've read of the false pretenses of apologetics is a relatively brief section in Robert J. Miller's commentary on a debate between evangelical apologist William Lane Craig and liberal Christian scholar John Dominic Crossan (published alongside other commentaries and a transcript of the debate as <em><a href=\"http://www.amazon.com/Will-Real-Jesus-Please-Stand/dp/0801021758\">Will the Real Jesus Please Stand Up?</a></em>). Miller writes:</p>\n<blockquote>\n<p>Why is it that few, if any, outsiders will be persuaded by Craig's apology? From the way he presents it, we get the impression that he thinks nobody who is informed, rational, and sincere could disagree with it...</p>\n<p>I used to think this way myself when I was a fervent believer in the power of apologetics. I was a philosophy major at a Catholic college. I was utterly convinced not only that Christianity was the one true religion that God intended for all humanity, but also that the Catholic Church was the one true church that Christ intended for all Christians. From my study of Thomas Aquinas and modern Christian apologetics, I clearly saw that the central truths of Christianity (and of Catholicism) could be grasped by reason if only one was sincerely seeking God's truth, was humble enough to accept it, and took the time to inform oneself and follow the arguments.</p>\n<p>All of this made perfect sense to me, and none of my teachers or fellow students (all of whom were Catholics) gave me any reason to question it. I tried out various apologetic arguments on my like-minded friends, who found them quite convincing. Occasionally they suggested improvements in my arguments, but none of us doubted the effectiveness of apologetics. The only real puzzle in my mind was this: since the truths of Christianity and Catholicism are so evident, why are they not more universally recognized? I concluded that those outside my religion or my church just did not know or did not understand these apologetic arguments, or that they were not completely sincere about seeking the truth...</p>\n<p>This mind-set held together until I went to graduate school at secular universities and got to know people who had different religions. For the first time in my life, I got to know people who took other religions as seriously as I took mine. I knew these people were well educated and highly rational, and I could tell from our conversations that they were sincere. A few were people of great goodness and spiritual depth. Yet none of them was persuaded by my apologetics.</p>\n</blockquote>\n<p>This means that if the purpose of apologetics is taken at face-value, \"apologies are almost always abject failures.\" However, he writes:</p>\n<blockquote>\n<p>The is another, more promising way to evaluate the apologetic genre. We can determine its audience, not by whom it seems to be aimed at, but by who actually reads it. And we can determine its purpose, not by what the author seems to intended, but by how it actually functions. if we proceed like this, we reach two important findings: (1) the audience for an apology is insiders; (2) its function is to support what the audience already believes.</p>\n<p>This is nothing new to apologists, who know full well that their audiences are insiders. (Why else would Craig speak at Moody Memorial Church or write for Baker Book House?) So why do apologists write as if they were addressing outsiders? They do that, not because they are mistaken about their audience, but because that is the convention of the apologetic genre. An apt comparison is the genre of the open letter. An open letter may begin, \"To the President of the United States,\" but both author and readers understand that the real audience is the general public. Readers don't think they are reading the president's mail... Authors of fables write about talking animals because that is how fables go, not because anyone thinks that animals really talk.</p>\n</blockquote>\n<p>While Miller makes good points, he is too kind to treat the pretense of persuading outsiders as a mere genre convention and imply nobody believes it. He certainly seems to have believed his arguments would persuade outsiders when he was a Catholic college student.</p>\n<p>Furthermore, both Josh McDowell and Lee Strobel make their self-presentation as former skeptics persuaded by overwhelming evidence a central part of their marketing. Their fans seem to mostly believe the marketing, and would therefore conclude Miller is wrong about the purpose of apologetics. But scratch the surface, and you start to see marketing is all it is. In recent editions of his books, McDowell claims that in college he traveled Europe researching the evidence for Christianity, but I've been unable to find any record of this claim prior to the 1999 edition of <em>The Evidence that Demands a Verdict </em>(the first edition was published in 1972).&nbsp;</p>\n<p>Lee Strobel's <em>Case for... </em>books go even further playing up the \"former skeptic\" angle. They consist of a series of interviews with Christian apologists, presented in narrative form with Strobel feigning skepticism and objectivity while pitching the apologists softball questions. In my experience, many of Strobel's fans believe their reading an account of Strobel's conversion. More attentive readers will notice Strobel only claims to be \"retracing\" his conversion. Strobel's earlier book, <em>Inside the Mind of Unchurched Harry and Mary, </em>gives the real story: Strobel started going to church because of his wife, found it emotionally moving, and then started reading up on apologetics to assure himself it was all true.</p>\n<p>Apologetics is marketed this way because fans of apologetics want to believe it. And in his reply to Miller, Craig tries to keep up that image of apologetics, even while conceding some of Miller's points. Craig says he publishes at with evangelical publishing houses because \"it is extraordinarily difficult to interest nonevangelical presses in publishing a defense of the historical resurrection of Jesus.\" Somehow, Craig doesn't consider that this might be because the audience for such material is composed almost entirely of evangelicals.</p>\n<p>Craig concedes that few outsiders will be persuaded by his arguments, but then says there are exceptions to this rule. He has a couple stories of how, after one of his appearances on a college campus, a staff member from a campus Christian org (presumably the one that organized the event) told him he'd made some converts.</p>\n<p>He also tells tells a story about meeting an investment banker who says he had \"wanted to believe in Jesus,\" but had trouble buying the resurrection story. So he joined a small group at a local church and spent some time talking to one of the ministers there, who \"laid out for him the evidence for Jesus' miraculous resurrection. After reading a book of evangelical responses to the liberal Jesus Seminar, the man says that \"I asked Jesus into my life.\"</p>\n<p>But Craig concedes the people in his anecdotes are unusual, so before I say anything about them, let's talk about the majority of apologetics consumers who are already believers. For many, I suspect, apologetics gives them a few extra good feels about their faith, but that's the extent of what it does for them. Miller certainly doesn't make it sound like his college-age self would have faced a major crisis of faith without apologetics.</p>\n<p>For other Christians, however, consuming apologetics is part of a desperate attempt to hold on to their beliefs in the face of doubts. The ranks of the atheist movement are full of ex-Christians who went through an apologetics-reading phase for this reason. My impression, furthermore, is that there are Christians who have succeeded where many current atheists have failed. For example, Christian apologist Mike Licona (who made headlines when he was forced to resign from his position at Southern Evangelical Seminary for his ever-so-slight deviations from the inerrantist party line) <a href=\"http://www.reclaimingthemind.org/blog/2011/10/mike-licona-on-christian-doubt/\">credits</a> his mentor in apologetics, Gary Habermas, with saving his faith.</p>\n<p>In fact, when I read Eliezer say that, in the Orthodox Judaism of his childhood, <a href=\"/lw/jy/avoiding_your_beliefs_real_weak_points/\">\"You're allowed to doubt. &nbsp;You're just not allowed to successfully doubt,\"</a> this struck me as a pretty good expression of an attitude that's common in evangelical Protestant apologetics. They may not take it as far as it's taken in Eliezer's account of Judaism&mdash;they don't raise doubts just to have a competition over who can come up with the most complicated explanation&mdash;but there's a resigned recognition that doubt is inevitable. So they talk about struggling with doubt, dealing with doubt, overcoming doubt, living with doubt. The message is that doubt can be embraced or at least tolerated, as long as you don't, as Eliezer would put it, doubt successfully.</p>\n<p>Apologetics, though, seems to serve another, stranger purpose. Once, in college, I attended an apologetics talk put on by the local Campus Crusade chapter, and after the talk ran into an acquaintance who I got to talking with. He explained friends of his had told him about how Christianity had saved their lives, which made him want to convert, but he wasn't sure he could really believe it, hence going to the talk.</p>\n<p>This seems to be part of a pattern with other stories I've heard, like Lee Strobel's story (the relatively unvarnished version from <em>Inside the Mind...</em>) and Craig's story of the investment banker: people decide they want to convert for emotional reasons, but some can't believe it at first, so they use apologetics as a tool to get themselves to believe what they've decided they want to believe.</p>\n<p>In <a href=\"/lw/js/the_bottom_line/\">\"The Bottom Line,\"</a> Eliezer imagines the owner of a box paying a clever arguer to argue that there's a diamond inside. This is, in effect, the role of apologists, to make a living as clever arguers serving people who've decided they want to believe certain religious doctrines are true. As someone who's had rationalist instincts since before I knew anything about rationalism (as an intellectual tradition or movement), part of me is surprised that this would <em>ever </em>work. Shouldn't it be obvious to people that they're fooling themselves?</p>\n<p>On the other hand, it says something about people's need to feel rational that they would go to the trouble, rather than just satisfying themselves with believing on faith, as many religious believers seem to do. In fact, this need may be more widespread than most people realize. In <em>Why People Believe Weird Things, </em>Michael Shermer reports on a study that found that while even most religious believers tend to assume other people believe for non-rational reasons, when you ask religious people about their own reasons for their religious beliefs, they're more likely to cite the argument from design than faith.</p>\n<p>(What does all this mean for domains outside religion? I'm not actually sure, though there's some rather obvious connections you could draw with people's information-consuming habits in other areas. But that's a problem for another day...)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 3, "ZzxvopS4BwLuQy42n": 1, "LDTSbmXtokYAsEq8e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wf8anBt9JzgrDYaqG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 65, "extendedScore": null, "score": 0.00019, "legacy": true, "legacyId": "24376", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34XxbRFe54FycoCDw", "dHQkDNMhj692ayx78"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-22T02:10:25.174Z", "modifiedAt": null, "url": null, "title": "Meetup : Princeton NJ Meetup", "slug": "meetup-princeton-nj-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:28.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RMzRPozL7eEu6ScXp/meetup-princeton-nj-meetup", "pageUrlRelative": "/posts/RMzRPozL7eEu6ScXp/meetup-princeton-nj-meetup", "linkUrl": "https://www.lesswrong.com/posts/RMzRPozL7eEu6ScXp/meetup-princeton-nj-meetup", "postedAtFormatted": "Tuesday, October 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Princeton%20NJ%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Princeton%20NJ%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMzRPozL7eEu6ScXp%2Fmeetup-princeton-nj-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Princeton%20NJ%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMzRPozL7eEu6ScXp%2Fmeetup-princeton-nj-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMzRPozL7eEu6ScXp%2Fmeetup-princeton-nj-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sm'>Princeton NJ Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 November 2013 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Small World Coffee, 14 Witherspoon St.  Princeton, NJ 08540</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>My girlfriend and I have just moved to Princeton, and would like to meet any LessWrong readers who go to the university or live in the area. I used to co-run the Oxford meetups a long time ago, and it'd be good to do something similar again. All welcome!\nWe don't have any particular topics in mind, but we're A) willing to discuss anything LW-ey, as well as just socialise, and B) capable of coming up with conversation topics, so don't feel you have to have anything clever to say to be welcome!\nAs such, we hereby precommit to being at Small World Coffe* from 1pm to 3pm. By the sign of the paperclip so shall ye know us. If you prefer a different time/place, please let me know, either by PM or in the comments.\n*unless people recommend a better option before October 31st. At that point the time and date are fixed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sm'>Princeton NJ Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RMzRPozL7eEu6ScXp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3899270991023834e-06, "legacy": true, "legacyId": "24455", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Princeton_NJ_Meetup\">Discussion article for the meetup : <a href=\"/meetups/sm\">Princeton NJ Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 November 2013 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Small World Coffee, 14 Witherspoon St.  Princeton, NJ 08540</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>My girlfriend and I have just moved to Princeton, and would like to meet any LessWrong readers who go to the university or live in the area. I used to co-run the Oxford meetups a long time ago, and it'd be good to do something similar again. All welcome!\nWe don't have any particular topics in mind, but we're A) willing to discuss anything LW-ey, as well as just socialise, and B) capable of coming up with conversation topics, so don't feel you have to have anything clever to say to be welcome!\nAs such, we hereby precommit to being at Small World Coffe* from 1pm to 3pm. By the sign of the paperclip so shall ye know us. If you prefer a different time/place, please let me know, either by PM or in the comments.\n*unless people recommend a better option before October 31st. At that point the time and date are fixed.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Princeton_NJ_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/sm\">Princeton NJ Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Princeton NJ Meetup", "anchor": "Discussion_article_for_the_meetup___Princeton_NJ_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Princeton NJ Meetup", "anchor": "Discussion_article_for_the_meetup___Princeton_NJ_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-22T07:59:30.855Z", "modifiedAt": null, "url": null, "title": "LINK: \"This novel epigenetic clock can be used to address a host of questions in developmental biology, cancer and aging research.\"", "slug": "link-this-novel-epigenetic-clock-can-be-used-to-address-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.205Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cd4eiqiYvJfxbBZ6S/link-this-novel-epigenetic-clock-can-be-used-to-address-a", "pageUrlRelative": "/posts/cd4eiqiYvJfxbBZ6S/link-this-novel-epigenetic-clock-can-be-used-to-address-a", "linkUrl": "https://www.lesswrong.com/posts/cd4eiqiYvJfxbBZ6S/link-this-novel-epigenetic-clock-can-be-used-to-address-a", "postedAtFormatted": "Tuesday, October 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20%22This%20novel%20epigenetic%20clock%20can%20be%20used%20to%20address%20a%20host%20of%20questions%20in%20developmental%20biology%2C%20cancer%20and%20aging%20research.%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20%22This%20novel%20epigenetic%20clock%20can%20be%20used%20to%20address%20a%20host%20of%20questions%20in%20developmental%20biology%2C%20cancer%20and%20aging%20research.%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcd4eiqiYvJfxbBZ6S%2Flink-this-novel-epigenetic-clock-can-be-used-to-address-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20%22This%20novel%20epigenetic%20clock%20can%20be%20used%20to%20address%20a%20host%20of%20questions%20in%20developmental%20biology%2C%20cancer%20and%20aging%20research.%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcd4eiqiYvJfxbBZ6S%2Flink-this-novel-epigenetic-clock-can-be-used-to-address-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcd4eiqiYvJfxbBZ6S%2Flink-this-novel-epigenetic-clock-can-be-used-to-address-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>The paper is called <a href=\"http://genomebiology.com/2013/14/10/R115\">DNA methylation age and human tissues and cell types</a>&nbsp;and it's from Genome Biology. Here is a <a href=\"http://www.nature.com/news/chemical-clock-tracks-ageing-more-precisely-than-ever-before-1.13981\"><em>Nature</em>&nbsp;article</a> based on the paper.</p>\n<p>I have submitted this to LW because of its relevance to the measurement of aging and, hence, to life extension. Here is a bit from the&nbsp;<em>Nature</em>&nbsp;piece:</p>\n<blockquote>\n<p>\"Ageing is a major health problem, and interestingly there are really no objective measures of aging, other than a verified birth date,\" says Darryl Shibata, a pathologist at the University of Southern California in Los Angeles. \"Studies like this one provide important new efforts to increase the rigour of human aging studies.\"</p>\n</blockquote>\n<p>Note: The discrepancy in spelling (\"ageing\" vs. \"aging\") is in the original.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cd4eiqiYvJfxbBZ6S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 1.3902555173895256e-06, "legacy": true, "legacyId": "24457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-22T12:13:26.610Z", "modifiedAt": null, "url": null, "title": "Is it immoral to have children?", "slug": "is-it-immoral-to-have-children", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:35.613Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qrP9Rawdq87RpwSBQ/is-it-immoral-to-have-children", "pageUrlRelative": "/posts/qrP9Rawdq87RpwSBQ/is-it-immoral-to-have-children", "linkUrl": "https://www.lesswrong.com/posts/qrP9Rawdq87RpwSBQ/is-it-immoral-to-have-children", "postedAtFormatted": "Tuesday, October 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20it%20immoral%20to%20have%20children%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20it%20immoral%20to%20have%20children%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrP9Rawdq87RpwSBQ%2Fis-it-immoral-to-have-children%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20it%20immoral%20to%20have%20children%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrP9Rawdq87RpwSBQ%2Fis-it-immoral-to-have-children", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrP9Rawdq87RpwSBQ%2Fis-it-immoral-to-have-children", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 776, "htmlBody": "<p>In \"The Immorality of Having Children\" (2013, <a href=\"http://www.jefftk.com/rachels-2013-immorality-having-children.pdf\">pdf</a>) Rachels presents the \"Famine Relief Argument against Having Children\":</p>\n<blockquote>Conceiving and raising a child costs hundreds of thousands of dollars; that money would be far better spent on famine relief; therefore, conceiving and raising children is immoral.</blockquote>\n<p>They present this as a special case of Peter Singer's argument from <a href=\"http://www.utilitarianism.net/singer/by/1972----.htm\">Famine, Affluence, and Morality</a> (1972), which is why they haven't called it something more reasonable like the \"Opportunity Cost Argument\".</p>\n<p>[Note: the use of \"Famine Relief\" here is in reference to Peter Singer's 1972 example, but famine relief is not where your money does the most good. &nbsp;Treat the argument as \"that money would be far better spent on GiveWell's top charities\" or whatever organization you think is most effective.]</p>\n<p>It's true that having and raising a child is very expensive. They use an estimate of $227k for the direct expenditure through age 18 while noting that college [1] and time costs could make this much higher. Let's use a higher estimate of $500k to account for these. Considered over twenty years, that's $25k/year or $2k/month. This puts it at the top of the range of expenses, next to housing. It's also true that this money can do a lot of good when spent on effective charities. At <a href=\"http://www.givewell.org/\">GiveWell</a>'s current best estimate of $2.3k this is enough money to save nearly one life per month. [2]</p>\n<p>But perhaps we shouldn't be thinking of this money as an expense at all, and instead more as an investment? Could having kids be a contender for the most effective charity? That is, could having and raising kids be one of the most effective things you could do with your time and money?</p>\n<p>For example you could convince your kid to be unusually generous, donating far more than they cost to raise. Except that it's much cheaper to convince other people's kids to be generous, and our influence on the adult behavior of our children is not that big. Alternatively, if you're unusually smart, by having kids you could help make there be more smart people in the future. But how many more generations will pass before we learn enough about the genetics of intelligence to make this aspect of parental genetics irrelevant? Rachels considers the idea that your having children might greatly benefit the world, and rightly finds it insufficient. While your child may do a lot of good, for the expense there are much better options. Having kids is not a contender for the most effective charity, or even very close.</p>\n<p>Having kids is a special case of spending your time and money in ways that make you happy. A moral system for human beings needs to allow some amount of this. It's like working for $56k at a job you enjoy instead of getting $72k at a job you like less. [3] Or spending your free time reading instead of working extra hours building up a consulting business. Keeping in mind both the cost and that on average people <a href=\"/lw/erj\">don't seem</a> to be happier parenting, if having kids is what would make you most happy for the expense in time and money then it seems justified.</p>\n<p>(This is how Julia and I thought of it when deciding whether we should have kids.)</p>\n<p>&nbsp;</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/p/ok-to-have-kids\">on my blog</a>.</em></small></p>\n<p><br /> [1] College is currently in a huge state of flux. Advertised costs are rising far faster than inflation as colleges realize they can get away with near perfect price discrimination in the form of \"either pay the extremely high sticker price or give us all your financial data so we can determine exactly how much you can afford.\" At the same time online courses and mixed models are getting to where they can provide much of the value of traditional lecture courses, and in some ways do better. I have very little idea what to budget for college for a kid born now; likely costs range from \"free\" to \"all you have\".</p>\n<p>[2] Rachels uses a much lower number:</p>\n<blockquote>Givewell.org, which assesses charities, estimates that a life is saved for every $205 spent on expanding immunization coverage for children in Africa Sub-Saharan&mdash;apparently one of the most cost-effective projects. See L. Brenzel et al. 2006, p. 401</blockquote>\n<p>Their Brenzel citation is to the <a href=\"http://www.dcp2.org/pubs/DCP/20/FullText\">Vaccine-Preventable Diseases section</a> of the <a href=\"https://en.wikipedia.org/wiki/Disease_Control_Priorities_Project\">DCP2</a>. The $205 number is \"Estimated cost per death averted for the Traditional Immunization Program in Sub-Saharan Africa and South Asia\" in table 20.5.</p>\n<p>[3] This is a $16k difference, which comes from taking $500k over 20 years and dividing by two for the two parents, and then adding some for taxes. &nbsp;Though the earnings difference is likely to last more like 40 years.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b7ZSAGimsbzrLR5CR": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qrP9Rawdq87RpwSBQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 25, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "24458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 320, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["siB3rGkGmn46GFKQt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-22T15:31:55.951Z", "modifiedAt": null, "url": null, "title": "[LINK] What Can Internet Ads Teach Us About Persuasion?", "slug": "link-what-can-internet-ads-teach-us-about-persuasion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WAMkbNQ9FtTBo3jkP/link-what-can-internet-ads-teach-us-about-persuasion", "pageUrlRelative": "/posts/WAMkbNQ9FtTBo3jkP/link-what-can-internet-ads-teach-us-about-persuasion", "linkUrl": "https://www.lesswrong.com/posts/WAMkbNQ9FtTBo3jkP/link-what-can-internet-ads-teach-us-about-persuasion", "postedAtFormatted": "Tuesday, October 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20What%20Can%20Internet%20Ads%20Teach%20Us%20About%20Persuasion%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20What%20Can%20Internet%20Ads%20Teach%20Us%20About%20Persuasion%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAMkbNQ9FtTBo3jkP%2Flink-what-can-internet-ads-teach-us-about-persuasion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20What%20Can%20Internet%20Ads%20Teach%20Us%20About%20Persuasion%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAMkbNQ9FtTBo3jkP%2Flink-what-can-internet-ads-teach-us-about-persuasion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAMkbNQ9FtTBo3jkP%2Flink-what-can-internet-ads-teach-us-about-persuasion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 351, "htmlBody": "<p><a href=\"http://www.slate.com/articles/business/moneybox/2013/07/how_one_weird_trick_conquered_the_internet_what_happens_when_you_click_on.single.html\">How \"one weird trick\" conquered the Internet</a>. Some excerpt I found interesting:</p>\n<blockquote>Research on persuasion shows the more arguments you list in favor of something, regardless of the quality of those arguments, the more that people tend to believe it,&rdquo; Norton says. &ldquo;Mainstream ads sometimes use long lists of bullet points&mdash;people don&rsquo;t read them, but it&rsquo;s persuasive to know there are so many reasons to buy.&rdquo; OK, but if more is better, then why only one trick? &ldquo;People want a simple solution that has a ton of support.&rdquo;</blockquote>\n<p>I actually see this technique used in a lot of religious apologetics. There's even a name for one of them: The <a href=\"http://en.wikipedia.org/wiki/Gish_gallop#Debates\">Gish Gallop.</a> Would it be fair to say that this technique is taking advantage of a naive or intuitive understanding of Bayesian updates?</p>\n<blockquote>What about all the weirdness? &ldquo;A word like &lsquo;weird&rsquo; is not so negative, and kind of intriguing,&rdquo; says Oleg Urminsky of the University of Chicago Booth School of Business. &ldquo;There&rsquo;s this foot-in-the-door model. If you lead with a strong, unbelievable claim it may turn people off. But if you start with &lsquo;isn&rsquo;t this kind of weird?&rsquo; it lowers the stakes.&rdquo; The model also explains why some ads ask you to click on your age first. &ldquo;Giving your age is low-stakes but it begins the dialogue. The hard sell comes later.&rdquo;</blockquote>\n<p>The \"click on your age\" first gambit seems a bit like <a href=\"/lw/4e/cached_selves/\">Cached Selves</a>.</p>\n<blockquote>&ldquo;People tend to think something is important if it&rsquo;s secret,&rdquo; says Michael Norton, a marketing professor at Harvard Business School. &ldquo;Studies find that we give greater credence to information if we&rsquo;ve been told it was once &lsquo;classified.&rsquo; Ads like this often purport to be the work of one man, telling you something &lsquo;they&rsquo; don&rsquo;t want you to know.&rdquo; The knocks on Big Pharma not only offered a tempting needle-free fantasy; they also had a whiff of secret knowledge, bolstering the ad&rsquo;s credibility</blockquote>\n<p>Humanity's love affair with secrecy and its importance seems to go back quite a bit. The world's largest religion seems to have <a href=\"http://deusdiapente.wordpress.com/2012/09/11/christianity-was-basically-inevitable/\">started out as one of many mystery religions in the Greco-Roman world at the time</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WAMkbNQ9FtTBo3jkP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.3906813438486763e-06, "legacy": true, "legacyId": "24459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHYBdijDcAKQ6e45Z"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-22T16:01:55.279Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social Meetup (and AskMeAnything about the CFAR workshop)", "slug": "meetup-london-social-meetup-and-askmeanything-about-the-cfar", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:06.401Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kerspoon", "createdAt": "2011-12-27T09:53:49.512Z", "isAdmin": false, "displayName": "kerspoon"}, "userId": "XvZ9yyyJNeDwWhECW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pa8pnhSrG7foHGvgz/meetup-london-social-meetup-and-askmeanything-about-the-cfar", "pageUrlRelative": "/posts/Pa8pnhSrG7foHGvgz/meetup-london-social-meetup-and-askmeanything-about-the-cfar", "linkUrl": "https://www.lesswrong.com/posts/Pa8pnhSrG7foHGvgz/meetup-london-social-meetup-and-askmeanything-about-the-cfar", "postedAtFormatted": "Tuesday, October 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20Meetup%20(and%20AskMeAnything%20about%20the%20CFAR%20workshop)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20Meetup%20(and%20AskMeAnything%20about%20the%20CFAR%20workshop)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPa8pnhSrG7foHGvgz%2Fmeetup-london-social-meetup-and-askmeanything-about-the-cfar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20Meetup%20(and%20AskMeAnything%20about%20the%20CFAR%20workshop)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPa8pnhSrG7foHGvgz%2Fmeetup-london-social-meetup-and-askmeanything-about-the-cfar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPa8pnhSrG7foHGvgz%2Fmeetup-london-social-meetup-and-askmeanything-about-the-cfar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sn'>London Social Meetup (and AskMeAnything about the CFAR workshop)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 October 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespears's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London are having another awesome meetup and we would love for you to come!\nThis one will mostly be a social chat but I have just returned from a very interesting week in San Francisco where I went to a CFAR workshop (rationalist training camp) and stayed at Leverage Research (a load of people living together and trying to improve the world). Because of that I thought people might want to know all the exciting things they have told me. The plan is to meet at The Shakespeare Inn, 200m from Holborn Underground at 2pm on Sunday 27th . We will officially finish at 4pm but honestly people tend to enjoy it so much they want to stay much longer, and regularly do. We will have a sign with the LessWrong logo on it so you can find us easily.\nIf you have any questions, or are thinking of coming, feel free to email me (james) at kerspoon+lw@gmail.com. Otherwise, just turn up!\nHope to see you there,\nJames\nP.S err on the side of turning-up, we're friendly, and it's fun :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sn'>London Social Meetup (and AskMeAnything about the CFAR workshop)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pa8pnhSrG7foHGvgz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.3907095781222812e-06, "legacy": true, "legacyId": "24460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__and_AskMeAnything_about_the_CFAR_workshop_\">Discussion article for the meetup : <a href=\"/meetups/sn\">London Social Meetup (and AskMeAnything about the CFAR workshop)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 October 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespears's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London are having another awesome meetup and we would love for you to come!\nThis one will mostly be a social chat but I have just returned from a very interesting week in San Francisco where I went to a CFAR workshop (rationalist training camp) and stayed at Leverage Research (a load of people living together and trying to improve the world). Because of that I thought people might want to know all the exciting things they have told me. The plan is to meet at The Shakespeare Inn, 200m from Holborn Underground at 2pm on Sunday 27th . We will officially finish at 4pm but honestly people tend to enjoy it so much they want to stay much longer, and regularly do. We will have a sign with the LessWrong logo on it so you can find us easily.\nIf you have any questions, or are thinking of coming, feel free to email me (james) at kerspoon+lw@gmail.com. Otherwise, just turn up!\nHope to see you there,\nJames\nP.S err on the side of turning-up, we're friendly, and it's fun :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__and_AskMeAnything_about_the_CFAR_workshop_1\">Discussion article for the meetup : <a href=\"/meetups/sn\">London Social Meetup (and AskMeAnything about the CFAR workshop)</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social Meetup (and AskMeAnything about the CFAR workshop)", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__and_AskMeAnything_about_the_CFAR_workshop_", "level": 1}, {"title": "Discussion article for the meetup : London Social Meetup (and AskMeAnything about the CFAR workshop)", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__and_AskMeAnything_about_the_CFAR_workshop_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-23T03:32:42.338Z", "modifiedAt": null, "url": null, "title": "The Center For Open Science makes $1.3mm available to validate 50 cancer studies. ", "slug": "the-center-for-open-science-makes-usd1-3mm-available-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SLBK8MKEJq2WBurz5/the-center-for-open-science-makes-usd1-3mm-available-to", "pageUrlRelative": "/posts/SLBK8MKEJq2WBurz5/the-center-for-open-science-makes-usd1-3mm-available-to", "linkUrl": "https://www.lesswrong.com/posts/SLBK8MKEJq2WBurz5/the-center-for-open-science-makes-usd1-3mm-available-to", "postedAtFormatted": "Wednesday, October 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Center%20For%20Open%20Science%20makes%20%241.3mm%20available%20to%20validate%2050%20cancer%20studies.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Center%20For%20Open%20Science%20makes%20%241.3mm%20available%20to%20validate%2050%20cancer%20studies.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLBK8MKEJq2WBurz5%2Fthe-center-for-open-science-makes-usd1-3mm-available-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Center%20For%20Open%20Science%20makes%20%241.3mm%20available%20to%20validate%2050%20cancer%20studies.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLBK8MKEJq2WBurz5%2Fthe-center-for-open-science-makes-usd1-3mm-available-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSLBK8MKEJq2WBurz5%2Fthe-center-for-open-science-makes-usd1-3mm-available-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<blockquote>\n<p style=\"margin: 0px 0px 10px; color: #000000; font-family: Helvetica, Arial, sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 20px; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;\">Yesterday, they announced that<span class=\"Apple-converted-space\">&nbsp;</span><a style=\"color: #00709a; text-decoration: none;\" rel=\"nofollow\" href=\"http://centerforopenscience.org/\" target=\"_self\">The Center For Open Science</a><span class=\"Apple-converted-space\">&nbsp;</span>was making $1.3mm available, via ScienceExchange, to reproduce and validate 50 important cancer biology studies.</p>\n<p>I am excited about this for a bunch of reasons; 1) reproducing and validating research is critical, 2) The Center For Open Science is taking a marketplace model to funding this work, and 3) it points to the broader potential for Science Exchange to break down silos, open up research, and lead to better and faster scientific discovery.</p>\n</blockquote>\n<p><span style=\"color: #000000; font-family: Helvetica, Arial, sans-serif; font-size: 15px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 20px; orphans: auto; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;\"><br /></span>http://www.businessinsider.com/open-science-2013-10</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SLBK8MKEJq2WBurz5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 6, "extendedScore": null, "score": 1.3913602288996693e-06, "legacy": true, "legacyId": "24463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-23T11:43:49.300Z", "modifiedAt": null, "url": null, "title": "Review of studies says you can decrease motivated cognition through self-affirmation", "slug": "review-of-studies-says-you-can-decrease-motivated-cognition", "viewCount": null, "lastCommentedAt": "2014-06-13T06:06:03.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Beckstead", "createdAt": "2011-08-19T23:58:47.870Z", "isAdmin": false, "displayName": "Nick_Beckstead"}, "userId": "Sjm96fPXwa2x4eHHv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h2jeunTEz6CnMJmNB/review-of-studies-says-you-can-decrease-motivated-cognition", "pageUrlRelative": "/posts/h2jeunTEz6CnMJmNB/review-of-studies-says-you-can-decrease-motivated-cognition", "linkUrl": "https://www.lesswrong.com/posts/h2jeunTEz6CnMJmNB/review-of-studies-says-you-can-decrease-motivated-cognition", "postedAtFormatted": "Wednesday, October 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Review%20of%20studies%20says%20you%20can%20decrease%20motivated%20cognition%20through%20self-affirmation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReview%20of%20studies%20says%20you%20can%20decrease%20motivated%20cognition%20through%20self-affirmation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2jeunTEz6CnMJmNB%2Freview-of-studies-says-you-can-decrease-motivated-cognition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Review%20of%20studies%20says%20you%20can%20decrease%20motivated%20cognition%20through%20self-affirmation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2jeunTEz6CnMJmNB%2Freview-of-studies-says-you-can-decrease-motivated-cognition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2jeunTEz6CnMJmNB%2Freview-of-studies-says-you-can-decrease-motivated-cognition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 596, "htmlBody": "<p>I read this article today and thought LW might find it interesting. The key finding is that in a number of different experiments, simple \"self-affirmations\" (such as writing about relationships with your friends or something else that makes you feel good about yourself) make people more open to changing their mind in cases where changing their mind would be damaging to their self-image. The proposed explanation is that people need to maintain a certain level of self-worth, and one way they do that is by refusing to accept evidence that would damage their sense of self-worth. But if they have a high enough sense of self-worth, they are less likely to do this. I haven't reviewed any of these studies personally, but the idea makes some sense and sounds pretty easy to try. Hat tip to Dan Keys for putting me onto the idea. I searched LW for \"Sherman self-affirmation\" and didn't see this discussed anywhere on LW, but I didn't look very hard.</p>\n<p><br />Title:&nbsp;<a href=\"http://cdp.sagepub.com/content/11/4/119.short\">Accepting Threatening Information: Self&ndash;Affirmation and the Reduction of Defensive Biases</a></p>\n<p>Authors: David K. Sherman and Geoffrey L. Cohen</p>\n<p>Citation details: Current Directions in Psychological Science August 2002 vol. 11 no. 4 119-123</p>\n<p>Abstract:&nbsp;Why do people resist evidence that challenges the validity of long&ndash;held beliefs? And why do they persist in maladaptive behavior even when persuasive information or personal experience recommends change? We argue that such defensive tendencies are driven, in large part, by a fundamental motivation to protect the perceived worth and integrity of the self. Studies of social&ndash;political debate, health&ndash;risk assessment, and responses to team victory or defeat have shown that people respond to information in a less defensive and more open&ndash;minded manner when their self&ndash;worth is buttressed by an affirmation of an alternative source of identity. Self&ndash;affirmed individuals are more likely to accept information that they would otherwise view as threatening, and subsequently to change their beliefs and even their behavior in a desirable fashion. Defensive biases have an adaptive function for maintaining self&ndash;worth, but maladaptive consequences for promoting change and reducing social conflict.</p>\n<p>Key quotes: \"Pro-choice partisans and pro-life partisans were presented with a debate between two activists on opposite sides of the abortion dispute&hellip;.However, this confirmation bias was sharply attenuated among participants who affirmed a valued source of self-worth (by writing about a personally important value, such as their relations with friends)....although all participants left the debate feeling more confident in their beliefs about abortion than they had before, this polarization in attitude was significantly reduced among self-affirmed participants (cf. Lord et al., 1979).\" &nbsp;p. 120</p>\n<p>\"In one study (Cohen et al., 2000), devout opponents and proponents of capital punishment were presented with a persuasive scientific report that contradicted their beliefs about the death penalty&rsquo;s effectiveness as a deterrent for crime....the responses of participants who received an affirmation of a valued self-identity (by writing about a personally important value, or by being provided with positive feedback on an important skill) proved more favorable.Self affirmed participants were less critical of the reported research, they suspected less bias on the part of the authors, and they even changed their overall attitudes toward capital punishment in the direction of the report they read.\" p. 121</p>\n<p>\"In one study, athletes who had just completed an intramural volleyball game assessed the extent to which each of a series of factors contributed to their team&rsquo;s victory or defeat. As in past research (Lau &amp; Russell, 1980),winners made more internal attributions for their victories than losers did for their defeats. However, among athletes who had reflected on an important value irrelevant to athletics, this self-serving bias was attenuated.\" p. 122</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h2jeunTEz6CnMJmNB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 1.3918231394839874e-06, "legacy": true, "legacyId": "24465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-23T12:27:55.524Z", "modifiedAt": null, "url": null, "title": "Confusion about science and technology", "slug": "confusion-about-science-and-technology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:10.024Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RuQRoPBrbHCfdyTih/confusion-about-science-and-technology", "pageUrlRelative": "/posts/RuQRoPBrbHCfdyTih/confusion-about-science-and-technology", "linkUrl": "https://www.lesswrong.com/posts/RuQRoPBrbHCfdyTih/confusion-about-science-and-technology", "postedAtFormatted": "Wednesday, October 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confusion%20about%20science%20and%20technology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfusion%20about%20science%20and%20technology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRuQRoPBrbHCfdyTih%2Fconfusion-about-science-and-technology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confusion%20about%20science%20and%20technology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRuQRoPBrbHCfdyTih%2Fconfusion-about-science-and-technology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRuQRoPBrbHCfdyTih%2Fconfusion-about-science-and-technology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>Science is <a href=\"http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble\">not particularly reliable</a>.</p>\n<p>And yet, we have remarkable technology, and can do medical marvels.</p>\n<p>My tentative theory is that there's a lot of knowledge that's less formal than science in engineering, manufacturing, and the practice of medicine which makes it possible to get work done, and some fairly effective methods of filtering information that comes from science.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RuQRoPBrbHCfdyTih", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 1.3918647236003964e-06, "legacy": true, "legacyId": "24466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-23T18:15:05.790Z", "modifiedAt": null, "url": null, "title": "Meetup : Saskatoon - Rhetorical Fallacies!", "slug": "meetup-saskatoon-rhetorical-fallacies", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nicholas_Rutherford", "createdAt": "2013-08-01T02:29:11.736Z", "isAdmin": false, "displayName": "Nicholas_Rutherford"}, "userId": "nucgkHPJBwJuK8sY7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qJpdFEwK7HPyLQjcM/meetup-saskatoon-rhetorical-fallacies", "pageUrlRelative": "/posts/qJpdFEwK7HPyLQjcM/meetup-saskatoon-rhetorical-fallacies", "linkUrl": "https://www.lesswrong.com/posts/qJpdFEwK7HPyLQjcM/meetup-saskatoon-rhetorical-fallacies", "postedAtFormatted": "Wednesday, October 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saskatoon%20-%20Rhetorical%20Fallacies!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saskatoon%20-%20Rhetorical%20Fallacies!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJpdFEwK7HPyLQjcM%2Fmeetup-saskatoon-rhetorical-fallacies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saskatoon%20-%20Rhetorical%20Fallacies!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJpdFEwK7HPyLQjcM%2Fmeetup-saskatoon-rhetorical-fallacies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJpdFEwK7HPyLQjcM%2Fmeetup-saskatoon-rhetorical-fallacies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/so'>Saskatoon - Rhetorical Fallacies!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 October 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another Saskatoon meetup at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>We'll be learning about 7 common rhetorical fallacies, why they are wrong and how to identify them. This is going to be based off of the rhetorical fallacy lesson from clearerthinking.org. I'd suggest going through it. It is free,  takes about 45 minutes and you can see if you beat my score of 35 points!</p>\n\n<p>More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/so'>Saskatoon - Rhetorical Fallacies!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qJpdFEwK7HPyLQjcM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.39219213792356e-06, "legacy": true, "legacyId": "24468", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saskatoon___Rhetorical_Fallacies_\">Discussion article for the meetup : <a href=\"/meetups/so\">Saskatoon - Rhetorical Fallacies!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 October 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another Saskatoon meetup at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>We'll be learning about 7 common rhetorical fallacies, why they are wrong and how to identify them. This is going to be based off of the rhetorical fallacy lesson from clearerthinking.org. I'd suggest going through it. It is free,  takes about 45 minutes and you can see if you beat my score of 35 points!</p>\n\n<p>More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saskatoon___Rhetorical_Fallacies_1\">Discussion article for the meetup : <a href=\"/meetups/so\">Saskatoon - Rhetorical Fallacies!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saskatoon - Rhetorical Fallacies!", "anchor": "Discussion_article_for_the_meetup___Saskatoon___Rhetorical_Fallacies_", "level": 1}, {"title": "Discussion article for the meetup : Saskatoon - Rhetorical Fallacies!", "anchor": "Discussion_article_for_the_meetup___Saskatoon___Rhetorical_Fallacies_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-23T19:30:40.188Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area Meetup:  Goal Factoring", "slug": "meetup-durham-nc-triangle-area-meetup-goal-factoring", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TzayCXnq7fXWxcnBP/meetup-durham-nc-triangle-area-meetup-goal-factoring", "pageUrlRelative": "/posts/TzayCXnq7fXWxcnBP/meetup-durham-nc-triangle-area-meetup-goal-factoring", "linkUrl": "https://www.lesswrong.com/posts/TzayCXnq7fXWxcnBP/meetup-durham-nc-triangle-area-meetup-goal-factoring", "postedAtFormatted": "Wednesday, October 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%20Meetup%3A%20%20Goal%20Factoring&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%20Meetup%3A%20%20Goal%20Factoring%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTzayCXnq7fXWxcnBP%2Fmeetup-durham-nc-triangle-area-meetup-goal-factoring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%20Meetup%3A%20%20Goal%20Factoring%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTzayCXnq7fXWxcnBP%2Fmeetup-durham-nc-triangle-area-meetup-goal-factoring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTzayCXnq7fXWxcnBP%2Fmeetup-durham-nc-triangle-area-meetup-goal-factoring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sp'>Durham NC/Triangle Area Meetup:  Goal Factoring</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2411 N Roxboro St., Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Can we discover values, drives, and emotions we didn't know we possessed, by a process like Goal Factoring?  Let's find out, tomorrow!</p>\n\n<p>7:00 gather <br />\n7:30 discussion <br />\n9:30ish unstructured social time, either at the house or possibly at Fullsteam</p>\n\n<p>We suggest parking on Ellerbee, one block north of Club.  Our house is green with a red door, and will have its porch light on.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sp'>Durham NC/Triangle Area Meetup:  Goal Factoring</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TzayCXnq7fXWxcnBP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24469", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup___Goal_Factoring\">Discussion article for the meetup : <a href=\"/meetups/sp\">Durham NC/Triangle Area Meetup:  Goal Factoring</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2411 N Roxboro St., Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Can we discover values, drives, and emotions we didn't know we possessed, by a process like Goal Factoring?  Let's find out, tomorrow!</p>\n\n<p>7:00 gather <br>\n7:30 discussion <br>\n9:30ish unstructured social time, either at the house or possibly at Fullsteam</p>\n\n<p>We suggest parking on Ellerbee, one block north of Club.  Our house is green with a red door, and will have its porch light on.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup___Goal_Factoring1\">Discussion article for the meetup : <a href=\"/meetups/sp\">Durham NC/Triangle Area Meetup:  Goal Factoring</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area Meetup:  Goal Factoring", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup___Goal_Factoring", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area Meetup:  Goal Factoring", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup___Goal_Factoring1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-23T20:42:55.334Z", "modifiedAt": null, "url": null, "title": "[link] The World's Most Powerful MRI Takes Shape", "slug": "link-the-world-s-most-powerful-mri-takes-shape", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:15.794Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "moreati", "createdAt": "2011-03-31T22:21:28.991Z", "isAdmin": false, "displayName": "moreati"}, "userId": "rxrassLcfMeHxYm3S", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pw6vLJmARw363cgLa/link-the-world-s-most-powerful-mri-takes-shape", "pageUrlRelative": "/posts/pw6vLJmARw363cgLa/link-the-world-s-most-powerful-mri-takes-shape", "linkUrl": "https://www.lesswrong.com/posts/pw6vLJmARw363cgLa/link-the-world-s-most-powerful-mri-takes-shape", "postedAtFormatted": "Wednesday, October 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20The%20World's%20Most%20Powerful%20MRI%20Takes%20Shape&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20The%20World's%20Most%20Powerful%20MRI%20Takes%20Shape%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpw6vLJmARw363cgLa%2Flink-the-world-s-most-powerful-mri-takes-shape%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20The%20World's%20Most%20Powerful%20MRI%20Takes%20Shape%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpw6vLJmARw363cgLa%2Flink-the-world-s-most-powerful-mri-takes-shape", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpw6vLJmARw363cgLa%2Flink-the-world-s-most-powerful-mri-takes-shape", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p><a href=\"http://spectrum.ieee.org/biomedical/imaging/the-worlds-most-powerful-mri-takes-shape\">http://spectrum.ieee.org/biomedical/imaging/the-worlds-most-powerful-mri-takes-shape</a></p>\n<blockquote>Standard hospital scanners have a spatial resolution of about 1 millimeter, covering about 10 000 neurons, and a time resolution of about a second. The INUMAC will be able to image an area of about 0.1 mm, or 1000 neurons, and see changes occurring as fast as one-tenth of a second, according to Pierre V&eacute;drine, director of the project at the French Alternative Energies and Atomic Energy Commission, in Paris</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pw6vLJmARw363cgLa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 1.392331592515579e-06, "legacy": true, "legacyId": "24470", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-24T19:36:50.073Z", "modifiedAt": null, "url": null, "title": "Meetup : Tempe, AZ: How to Measure Anything I", "slug": "meetup-tempe-az-how-to-measure-anything-i", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:36.379Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oo4txM7PM438WgKyM/meetup-tempe-az-how-to-measure-anything-i", "pageUrlRelative": "/posts/oo4txM7PM438WgKyM/meetup-tempe-az-how-to-measure-anything-i", "linkUrl": "https://www.lesswrong.com/posts/oo4txM7PM438WgKyM/meetup-tempe-az-how-to-measure-anything-i", "postedAtFormatted": "Thursday, October 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tempe%2C%20AZ%3A%20How%20to%20Measure%20Anything%20I&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tempe%2C%20AZ%3A%20How%20to%20Measure%20Anything%20I%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foo4txM7PM438WgKyM%2Fmeetup-tempe-az-how-to-measure-anything-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tempe%2C%20AZ%3A%20How%20to%20Measure%20Anything%20I%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foo4txM7PM438WgKyM%2Fmeetup-tempe-az-how-to-measure-anything-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foo4txM7PM438WgKyM%2Fmeetup-tempe-az-how-to-measure-anything-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sq'>Tempe, AZ: How to Measure Anything I</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 October 2013 02:00:32PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual, we are meeting at the entrance to Hayden Library. This week, we will play a round of Zendo and discuss Section I of <a href=\"http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/0470539399/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1382643364&amp;sr=1-1&amp;keywords=how+to+measure+anything\" rel=\"nofollow\">How to Measure Anything</a> (Section I includes chapter 1-3). See <a href=\"http://lesswrong.com/lw/i8n/how_to_measure_anything/\">here</a> for a very \"strong\" review from <a href=\"http://lesswrong.com/user/lukeprog/overview/\">lukeprog</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sq'>Tempe, AZ: How to Measure Anything I</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oo4txM7PM438WgKyM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3936288860232765e-06, "legacy": true, "legacyId": "24473", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_I\">Discussion article for the meetup : <a href=\"/meetups/sq\">Tempe, AZ: How to Measure Anything I</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 October 2013 02:00:32PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As usual, we are meeting at the entrance to Hayden Library. This week, we will play a round of Zendo and discuss Section I of <a href=\"http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/0470539399/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1382643364&amp;sr=1-1&amp;keywords=how+to+measure+anything\" rel=\"nofollow\">How to Measure Anything</a> (Section I includes chapter 1-3). See <a href=\"http://lesswrong.com/lw/i8n/how_to_measure_anything/\">here</a> for a very \"strong\" review from <a href=\"http://lesswrong.com/user/lukeprog/overview/\">lukeprog</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_I1\">Discussion article for the meetup : <a href=\"/meetups/sq\">Tempe, AZ: How to Measure Anything I</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tempe, AZ: How to Measure Anything I", "anchor": "Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_I", "level": 1}, {"title": "Discussion article for the meetup : Tempe, AZ: How to Measure Anything I", "anchor": "Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_I1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ybYBCK9D7MZCcdArB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T00:24:49.147Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM", "slug": "meetup-urbana-champaign-dark-arts-meetup-sunday-october-27", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bFQotih8ht8k5cagt/meetup-urbana-champaign-dark-arts-meetup-sunday-october-27", "pageUrlRelative": "/posts/bFQotih8ht8k5cagt/meetup-urbana-champaign-dark-arts-meetup-sunday-october-27", "linkUrl": "https://www.lesswrong.com/posts/bFQotih8ht8k5cagt/meetup-urbana-champaign-dark-arts-meetup-sunday-october-27", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Dark%20Arts%20Meetup%20Sunday%20October%2027%2C%20Illini%20Union%20Courtyard%20Cafe%20(Ground%20Floor)%2C%202PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Dark%20Arts%20Meetup%20Sunday%20October%2027%2C%20Illini%20Union%20Courtyard%20Cafe%20(Ground%20Floor)%2C%202PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFQotih8ht8k5cagt%2Fmeetup-urbana-champaign-dark-arts-meetup-sunday-october-27%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Dark%20Arts%20Meetup%20Sunday%20October%2027%2C%20Illini%20Union%20Courtyard%20Cafe%20(Ground%20Floor)%2C%202PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFQotih8ht8k5cagt%2Fmeetup-urbana-champaign-dark-arts-meetup-sunday-october-27", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFQotih8ht8k5cagt%2Fmeetup-urbana-champaign-dark-arts-meetup-sunday-october-27", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sr'>Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 October 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> 40.109549, -88.227200</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What tricks do marketers, politicians, and others use to exploit human biases, and how can we counter them?\nThings we might talk about include (but are not limited to):\nanchoring\ncognitive ease\ndifferentiation and distinctiveness\nappearance heuristics\nsocial heuristics\nsystems 1 and 2</p>\n\n<p>Moving meetups inside because it's getting cold out.\nCoordinates are +40\u00b0 6' 34.38\", -88\u00b0 13' 37.92\"</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/Gd0VSa6jvus\">Cross posted on the mailing list</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sr'>Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bFQotih8ht8k5cagt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.393901081722878e-06, "legacy": true, "legacyId": "24474", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Dark_Arts_Meetup_Sunday_October_27__Illini_Union_Courtyard_Cafe__Ground_Floor___2PM\">Discussion article for the meetup : <a href=\"/meetups/sr\">Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 October 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> 40.109549, -88.227200</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>What tricks do marketers, politicians, and others use to exploit human biases, and how can we counter them?\nThings we might talk about include (but are not limited to):\nanchoring\ncognitive ease\ndifferentiation and distinctiveness\nappearance heuristics\nsocial heuristics\nsystems 1 and 2</p>\n\n<p>Moving meetups inside because it's getting cold out.\nCoordinates are +40\u00b0 6' 34.38\", -88\u00b0 13' 37.92\"</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/Gd0VSa6jvus\">Cross posted on the mailing list</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Dark_Arts_Meetup_Sunday_October_27__Illini_Union_Courtyard_Cafe__Ground_Floor___2PM1\">Discussion article for the meetup : <a href=\"/meetups/sr\">Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Dark_Arts_Meetup_Sunday_October_27__Illini_Union_Courtyard_Cafe__Ground_Floor___2PM", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Dark_Arts_Meetup_Sunday_October_27__Illini_Union_Courtyard_Cafe__Ground_Floor___2PM1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T02:28:50.885Z", "modifiedAt": null, "url": null, "title": "What should normal people do?", "slug": "what-should-normal-people-do", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:35.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "seez", "createdAt": "2013-07-31T22:30:37.599Z", "isAdmin": false, "displayName": "seez"}, "userId": "njhbHuqFGd9r6ivMv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YCjPpcy6yS3wttpoQ/what-should-normal-people-do", "pageUrlRelative": "/posts/YCjPpcy6yS3wttpoQ/what-should-normal-people-do", "linkUrl": "https://www.lesswrong.com/posts/YCjPpcy6yS3wttpoQ/what-should-normal-people-do", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20should%20normal%20people%20do%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20should%20normal%20people%20do%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCjPpcy6yS3wttpoQ%2Fwhat-should-normal-people-do%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20should%20normal%20people%20do%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCjPpcy6yS3wttpoQ%2Fwhat-should-normal-people-do", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCjPpcy6yS3wttpoQ%2Fwhat-should-normal-people-do", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 466, "htmlBody": "<p>What should a not-very-smart person do? &nbsp;Suppose you know a not-very-smart person (around or below average intelligence). &nbsp;S/he read about rationality, has utilitarian inclinations, and wants to make the world better. &nbsp;However, s/he isn't smart enough to discover new knowledge in most fields, or contribute very much to a conversation of more knowledgeable experts on a given topic. &nbsp;Let's assume s/he has no exceptional talents in any area.</p>\n<p>How do you think a person like that could best use his/her time and energy? &nbsp;What would you tell the person to do? &nbsp;This person may be, compared to average LW readership, less capable of noticing the irrationality in his/her actions even if s/he wants to be rid of it, and less easily able to notice the flaws in a bad argument. &nbsp;S/he may never be able to deeply understand why certain arguments are correct, certain scientific facts have to be the way they are, and telling him/her to be unsure or sure about anything seems dangerous if s/he doesn't really understand why. &nbsp;</p>\n<p>My practical advice might be:</p>\n<p>1) If you want to give to charity, follow GiveWell recommendations. &nbsp;</p>\n<p>2) Learn about the basic biases, and commit to resisting them in your own life.&nbsp;</p>\n<p>3) &nbsp;Follow advice that has been tested, that correctly predicts a positive outcome. &nbsp;If a hypothesis is untestable (there's an unsensible dragon in your garage) or doesn't predict anything (fire comes from phlogiston in combustable substances), or is tested and demonstrably false (god will smite you if you say it doesn't exist), don't waste time and energy on it. &nbsp;If you want to improve, look for tested methods that have significant positive results relevant to the area of interest. &nbsp;Similarly, if a person regularly gives you advice that does not lead to good outcomes, stop following it, and if someone gives advice that leads to good outcomes, start paying attention even if you like that person less. &nbsp;</p>\n<p>&nbsp;</p>\n<p>At a more general level, my thoughts are tentative, but might include basic LW tenets such as:</p>\n<p>1) Don't be afraid of the truth, because you're already enduring it.</p>\n<p>2) If all the experts in a field agree on something, they might be wrong, but you are extremely unlikely to be better at uncovering the truth, so follow their advice, which might appear to conflict with...</p>\n<p>3) Don't trust deep wisdom. &nbsp;Use Occam's razor, think about simple, basic reasons something might be true (this seems good for religion and moral issues, bad for scientific ideas and understanding)</p>\n<p>4) If you find yourself flinching away from an idea, notice that, and give it extra attention. &nbsp;</p>\n<p>Note: &nbsp;I mean this as a serious, and hopefully non-insulting question. &nbsp;Most people are intellectually near-average or below-average, and I have not seen extensive discussion on how to help them lead happier lives that make the world a better place.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YCjPpcy6yS3wttpoQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 33, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "24476", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T11:43:09.909Z", "modifiedAt": null, "url": null, "title": "[Link] Cognitive biases about violence as a negotiating tactic", "slug": "link-cognitive-biases-about-violence-as-a-negotiating-tactic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.483Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W48c4JhzhCPrzQhJM/link-cognitive-biases-about-violence-as-a-negotiating-tactic", "pageUrlRelative": "/posts/W48c4JhzhCPrzQhJM/link-cognitive-biases-about-violence-as-a-negotiating-tactic", "linkUrl": "https://www.lesswrong.com/posts/W48c4JhzhCPrzQhJM/link-cognitive-biases-about-violence-as-a-negotiating-tactic", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Cognitive%20biases%20about%20violence%20as%20a%20negotiating%20tactic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Cognitive%20biases%20about%20violence%20as%20a%20negotiating%20tactic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW48c4JhzhCPrzQhJM%2Flink-cognitive-biases-about-violence-as-a-negotiating-tactic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Cognitive%20biases%20about%20violence%20as%20a%20negotiating%20tactic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW48c4JhzhCPrzQhJM%2Flink-cognitive-biases-about-violence-as-a-negotiating-tactic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW48c4JhzhCPrzQhJM%2Flink-cognitive-biases-about-violence-as-a-negotiating-tactic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 231, "htmlBody": "<p>Max Abrahms, \"<a href=\"http://www.academia.edu/4770419/The_Credibility_Paradox_Violence_as_a_Double-Edged_Sword_in_International_Politics_International_Studies_Quarterly_December_2013_\">The Credibility Paradox: Violence as a Double-Edged Sword in International Politics</a>,\" <em>International Studies Quarterly</em> 2013.</p>\n<blockquote>\n<p><strong>Abstract</strong>: Implicit in the rationalist literature on bargaining over the last half-century is the political utility of violence. Given our anarchical international system populated with egoistic actors, violence is thought to promote concessions by lending credibility to their threats. From the vantage of bargaining theory, then, empirical research on terrorism poses a puzzle. For non-state actors, terrorism signals a credible threat in comparison to less extreme tactical alternatives. In recent years, however, a spate of studies across disciplines and methodologies has nonetheless found that neither escalating to terrorism nor with terrorism encourages government concessions. In fact, perpetrating terrorist acts reportedly lowers the likelihood of government compliance, particularly as the civilian casualties rise. The apparent tendency for this extreme form of violence to impede concessions challenges the external validity of bargaining theory, as traditionally understood. In this study, I propose and test an important psychological refinement to the standard rationalist narrative. Via an experiment on a national sample of adults, I find evidence of a newfound cognitive heuristic undermining the coercive logic of escalation enshrined in bargaining theory. Due to this oversight, mainstream bargaining theory overestimates the political utility of violence, particularly as an instrument of coercion.</p>\n</blockquote>\n<p>I found this via <a href=\"https://www.schneier.com/\">Bruce Schneier's blog</a>, which frequently features very valuable analysis clustered around societal and computer security.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W48c4JhzhCPrzQhJM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "24479", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T16:07:50.493Z", "modifiedAt": null, "url": null, "title": "Link dump: Future of Humanity institute technical reports", "slug": "link-dump-future-of-humanity-institute-technical-reports", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DTZRpgf8QabJ65K8z/link-dump-future-of-humanity-institute-technical-reports", "pageUrlRelative": "/posts/DTZRpgf8QabJ65K8z/link-dump-future-of-humanity-institute-technical-reports", "linkUrl": "https://www.lesswrong.com/posts/DTZRpgf8QabJ65K8z/link-dump-future-of-humanity-institute-technical-reports", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%20dump%3A%20Future%20of%20Humanity%20institute%20technical%20reports&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%20dump%3A%20Future%20of%20Humanity%20institute%20technical%20reports%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTZRpgf8QabJ65K8z%2Flink-dump-future-of-humanity-institute-technical-reports%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%20dump%3A%20Future%20of%20Humanity%20institute%20technical%20reports%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTZRpgf8QabJ65K8z%2Flink-dump-future-of-humanity-institute-technical-reports", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTZRpgf8QabJ65K8z%2Flink-dump-future-of-humanity-institute-technical-reports", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 742, "htmlBody": "<p>For those who may be interested in these things, here are the links to all the FHI's technical reports.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/gcr-survey.pdf\">Global Catastrophic Risks Survey</a>: At the Global Catastrophic Risk Conference in Oxford (17\u201020 July, 2008) an informal survey was circulated among participants, asking them to make their best guess at the chance that there will be disasters of different types before 2100. This report summarizes the main results.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/gcr-workshop-record.pdf\">Record of the Workshop on Policy Foresight and Global Catastrophic Risks</a>: On 21 July 2008, the Policy Foresight Programme, in conjunction with the Future of Humanity Institute, hosted a day-long workshop on &ldquo;Policy Foresight and Global Catastrophic Risks&rdquo; at the James Martin 21st Century School at the University of Oxford. This document provides a record of the day&rsquo;s discussion.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">Whole Brain Emulation: a Roadmap</a>: This report aims at providing a preliminary roadmap for Whole Brain Emulations (possible future one\u2010to\u2010one modelling of the function of the human brain), sketching out key technologies that would need to be developed or refined, and identifying key problems or uncertainties.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/utility-indifference.pdf\">Utility Indifference</a>: A utility-function-based method for making an Artificial Intelligence indifferent to certain facts or states of the world, which can be used to make certain security precautions more successful.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/machine-intelligence-survey-2011.pdf\">Machine Intelligence Survey</a>: At the FHI Winter Intelligence conference on machine intelligence 16/1 2011 an informal poll was conducted to elicit the views of the participants on various questions related to the emergence of machine intelligence. This report summarizes the results.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/indefinite-survival-backup.pdf\">Indefinite Survival through Backup Copies</a>: Continually copying yourself may help you preserve yourself from destruction. As long as the copies fate is independent, increasing the number of copies at a logarithmic rate is enough to ensure a non-zero probability of surviving for ever. The model is of more general use for many similar processes.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/anthropics-why-probability-isnt-enough.pdf\">Anthropics: why Probability isn&rsquo;t enough</a>: This report argues that the current treatment of anthropic and self-locating problems over-emphasises the importance of anthropic probabilities, and ignores other relevant and important factors, such as whether the various copies of the agents in question consider that they are acting in a linked fashion and whether they are mutually altruistic towards each other. These help to reinterpret the decisions, rather than probabilities, as the fundamental objects of interest in anthropic problems.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/nash-equilibrium-unilaterialists-curse.pdf\">Nash equilibrium of identical agents facing the Unilateralist's Curse</a>: This report is an addendum to the '<a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\">Unilateralist's Curse</a>' of Nick Bostrom, Thomas Douglas and Anders Sandberg. It demonstrates that if there are identical agents facing a situation where any one of them can implement a policy unilaterally, then the best strategies they can implement are also Nash equilibriums.</p>\n<p><a href=\"http://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf\">AI arms race</a>: a simple model of AI arms race (though it can be generalised). Some of the insights are obvious - that the competing teams are more likely to take safety precautions if there are not too many of them, if they agree with each other's values and if skill is more important than risk-taking in developing a functioning AI. But one result is surprising: that teams are most likely to take risks if they know the capabilities of their own team or their opponents'. In this case, the less you know, the safer you'll behave.</p>\n<p>Please cite these reports as:</p>\n<div>\n<ul>\n<li>Sandberg, A. &amp; Bostrom, N. (2008): &ldquo;Global Catastrophic Risks Survey&rdquo;, <em>Technical Report</em> #2008-1, Future of Humanity Institute, Oxford University: pp. 1-5.</li>\n<li>Tickell, C. et al. (2008): &ldquo;Record of the Workshop on Policy Foresight and Global Catastrophic Risks&rdquo;, <em>Technical Report</em> #2008-2, Future of Humanity Institute, Oxford University: pp. 1-19.</li>\n<li>Sandberg, A. &amp; Bostrom, N. (2008): &ldquo;Whole Brain Emulation: a Roadmap&rdquo;, <em>Technical Report</em> #2008-3, Future of Humanity Institute, Oxford University: pp. 1-130.</li>\n<li>Armstrong, S. (2010): &ldquo;Utility Indifference&rdquo;, <em>Technical Report</em> #2010-1, Future of Humanity Institute, Oxford University: pp. 1-5.</li>\n<li>Sandberg, A. &amp; Bostrom, N. (2011): &ldquo;Machine Intelligence Survey&rdquo;, <em>Technical Report</em> #2011-1, Future of Humanity Institute, Oxford University: pp. 1-12.</li>\n<li>Sandberg, A. &amp; Armstrong, S. (2012): &ldquo;Indefinite Survival through Backup Copies&rdquo;, <em>Technical Report</em> #2012-1, Future of Humanity Institute, Oxford University: pp. 1-5.</li>\n<li>Armstrong, S. (2012): &ldquo;Anthropics: why Probability isn&rsquo;t enough&rdquo;, <em>Technical Report</em> #2012-2, Future of Humanity Institute, Oxford University: pp. 1-10.</li>\n<li>Armstrong, S. (2012): &ldquo;Nash equilibrium of identical agents facing the Unilateralist's Curse&rdquo;, <em>Technical Report</em> #2012-3, Future of Humanity Institute, Oxford University: pp. 1-5.</li>\n<li>Armstrong, S. &amp; Bostrom, N. &amp; Shulman, C. (2013): &ldquo;Racing to the precipice: a model of artificial intelligence development&rdquo;, <em>Technical Report</em> #2013-1, Future of Humanity Institute, Oxford University: pp. 1-8.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DTZRpgf8QabJ65K8z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 1.394793061106122e-06, "legacy": true, "legacyId": "24481", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T16:10:18.306Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Cologne (K\u00f6ln)", "slug": "new-lw-meetup-cologne-koeln", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j5cMu8zHQCwZwYhXS/new-lw-meetup-cologne-koeln", "pageUrlRelative": "/posts/j5cMu8zHQCwZwYhXS/new-lw-meetup-cologne-koeln", "linkUrl": "https://www.lesswrong.com/posts/j5cMu8zHQCwZwYhXS/new-lw-meetup-cologne-koeln", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Cologne%20(K%C3%B6ln)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Cologne%20(K%C3%B6ln)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5cMu8zHQCwZwYhXS%2Fnew-lw-meetup-cologne-koeln%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Cologne%20(K%C3%B6ln)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5cMu8zHQCwZwYhXS%2Fnew-lw-meetup-cologne-koeln", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5cMu8zHQCwZwYhXS%2Fnew-lw-meetup-cologne-koeln", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 605, "htmlBody": "<p><strong>This summary was posted to LW main on October 18th. The following week's summary is <a href=\"/lw/iw2/new_lw_meetup_princeton_nj/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/s0\"></a><a href=\"/meetups/sa\">First Meetup in Cologne (K&ouml;ln):&nbsp;<span class=\"date\">27 October 2013 04:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/s8\">Atlanta October Meetup (Second of Two):&nbsp;<span class=\"date\">19 October 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/s9\">Atlanta: Rationalist Movie Night!:&nbsp;<span class=\"date\">26 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/qs\">Berlin: Fermi paradox discussion:&nbsp;<span class=\"date\">18 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/se\">Bristol meetup:&nbsp;<span class=\"date\">20 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/rq\">Frankfurt (including effective altruism presentation):&nbsp;<span class=\"date\">27 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/s3\">Helsinki Meetup:&nbsp;<span class=\"date\">20 October 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/sd\">Moscow, weekday gathering:&nbsp;<span class=\"date\">21 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/sh\">Phoenix (Tempe) Less Wrong Meetup:&nbsp;<span class=\"date\">18 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/s0\">Saint Petersburg, Russia:&nbsp;<span class=\"date\">27 October 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/si\">Urbana-Champaign: Decision Theory:&nbsp;<span class=\"date\">20 October 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">19 October 2019 01:30PM</span></a> </li>\n<li><a href=\"/meetups/sc\">[Cambridge MA] LW/Methods of Rationality meetup (Yudkowsky attending):&nbsp;<span class=\"date\">18 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/sg\">[Cambridge MA] LW meetup: Polyphasic sleep:&nbsp;<span class=\"date\">20 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/sf\">Durham/RTLW HPMoR discussion, ch. 90-93:&nbsp;<span class=\"date\">19 October 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/s7\">[Melbourne] Sunday Brunch Club - Sunday 20th October:&nbsp;<span class=\"date\">20 October 2013 07:08PM</span></a></li>\n<li><a href=\"/meetups/sj\">Washington DC: Jobs/project show and tell :&nbsp;<span class=\"date\">20 October 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j5cMu8zHQCwZwYhXS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.394795392618458e-06, "legacy": true, "legacyId": "24437", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2zQortoMjphzw9ox5", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T16:38:53.384Z", "modifiedAt": null, "url": null, "title": "Less Wrong\u2019s political bias ", "slug": "less-wrong-s-political-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:00.974Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sophronius", "createdAt": "2011-06-13T16:06:52.555Z", "isAdmin": false, "displayName": "Sophronius"}, "userId": "WswxjHqPo9W6K55NA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/czybHfMHvdjiEdQ86/less-wrong-s-political-bias", "pageUrlRelative": "/posts/czybHfMHvdjiEdQ86/less-wrong-s-political-bias", "linkUrl": "https://www.lesswrong.com/posts/czybHfMHvdjiEdQ86/less-wrong-s-political-bias", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%E2%80%99s%20political%20bias%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%E2%80%99s%20political%20bias%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FczybHfMHvdjiEdQ86%2Fless-wrong-s-political-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%E2%80%99s%20political%20bias%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FczybHfMHvdjiEdQ86%2Fless-wrong-s-political-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FczybHfMHvdjiEdQ86%2Fless-wrong-s-political-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1136, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:HyphenationZone>21</w:HyphenationZone> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-ansi-language:EN-US; mso-fareast-language:EN-US;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">(Disclaimer: This post refers to a certain political party as being somewhat crazy, which got some people upset, so sorry about that. That is not what this post is *about*, however. The article is instead about Less Wrong's social norms against pointing certain things out. I have edited it a bit to try and make it less provocative.)</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">A well-known post around these parts is Yudkowski&rsquo;s &ldquo;politics is the mind killer&rdquo;. This article proffers an important point: People tend to go funny in the head when discussing politics, as politics is largely about signalling tribal affiliation. The conclusion drawn from this by the Less Wrong crowd seems simple: Don&rsquo;t discuss political issues, or at least keep it as fair and balanced as possible when you do. However, I feel that there is a very real downside to treating political issues in this way, which I shall try to explain here. Since this post is (indirectly) about politics, I will try to bring this as gently as possible so as to avoid mind-kill. As a result this post is a bit lengthier than I would like it to be, so I apologize for that in advance.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">I find that a good way to examine the value of a policy is to ask in which of all possible worlds this policy would work, and in which worlds it would not. So let&rsquo;s start by imagining a perfectly convenient world: In a universe whose politics are entirely reasonable and fair, people start political parties to represent certain interests and preferences. For example, you might have the kitten party for people who like kittens, and the puppy party for people who favour puppies. In this world Less Wrong&rsquo;s unofficial policy is entirely reasonable: There is no sense in discussing politics, since politics is only about personal preferences, and any discussion of this can only lead to a &ldquo;Jay kittens, boo dogs!&rdquo; emotivism contest. At best you can do a poll now and again to see what people currently favour. </span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">Now let&rsquo;s imagine a less reasonable world, where things don&rsquo;t have to happen for good reasons and the universe doesn&rsquo;t give a crap about what&rsquo;s fair. In this unreasonable world, you can get a &ldquo;Thrives through Bribes&rdquo; party or an &ldquo;Appeal to emotions&rdquo; party or a &ldquo;Do stupid things for stupid reasons&rdquo; party as well as more reasonable parties that actually try to be about something. In this world it makes no sense to pretend that all parties are equal, because there is really no reason to believe that they are. </span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">As you might have guessed, I believe that we live in the second world. As a result, I do not believe that all parties are equally valid/crazy/corrupt, and as such I like to be able to identify which are the most crazy/corrupt/stupid. Now I happen to be fairly happy with the political system where I live. We have a good number of more-or-less reasonable parties here, and only one major crazy party that gives me the creeps. The advantage of this is that whenever I am in a room with intelligent people, I can safely say something like &ldquo;That crazy racist party sure is crazy and racist&rdquo;, and everyone will go &ldquo;Yup, they sure are, now do you want to talk about something of substance?&rdquo; This seems to me the only reasonable reply. </span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">The problem is that Less Wrong seems primarily US-based, and in the US&hellip; things do not go like this. In the US, it seems to me that there are only two significant parties, one of which is flawed and which I do not agree with on many points, while the other is, well&hellip; can I just say that some of the things they profess do not so much sound wrong as they sound crazy? And yet, it seems to me that everyone here is being very careful to <em style=\"mso-bidi-font-style: normal;\">not point this out</em>, because doing so would necessarily be favouring one party over the other, and why, that&rsquo;s <em style=\"mso-bidi-font-style: normal;\">politics</em>! That&rsquo;s not what we do here on Less Wrong!</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">And from what I can tell, based on the discussion I have seen so far and participated in on Less Wrong, this introduces a major bias. Pick any major issue of contention, and chances are that the two major parties will tend to have opposing views on the subject. And naturally, the saner party of the two tends to hold a more reasonable view, because they are less crazy. But you can&rsquo;t defend the more reasonable point of view now, because then you&rsquo;re defending the less-crazy party, and that&rsquo;s<em style=\"mso-bidi-font-style: normal;\"> politics</em>. Instead, you can get free karma just by saying something trite like &ldquo;well, both sides have important points on the matter&rdquo; or &ldquo;both parties have their own flaws&rdquo; or &ldquo;politics in general are messed up&rdquo;, because that just sounds so reasonable and fair who doesn&rsquo;t like things to be reasonable and fair? But I don&rsquo;t think we live in a reasonable and fair world.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">It&rsquo;s hard to prove the existence of such a bias and so this is mostly just an impression I have. But I can give a couple of points in support of this impression. Firstly there are the frequent accusations of group think towards Less Wrong, which I am increasingly though reluctantly prone to agree with. I can&rsquo;t help but notice that posts which remark on for example *retracted* being a thing tend to get quite a few downvotes while posts that take care to express the <em style=\"mso-bidi-font-style: normal;\">nuance</em> of the issue get massive upvotes<em> </em>regardless of whether really <em>are</em> two sides on the issue<em style=\"mso-bidi-font-style: normal;\"></em>. Then there are the community poll results, which show that for example 30% of Less Wrongers favour a particular political allegiance even though only 1% of voters vote for the most closely corresponding party. I sincerely doubt that this skewed representation is the result of honest and reasonable discussion on Less Wrong that has convinced members to follow what is otherwise a minority view, since <em style=\"mso-bidi-font-style: normal;\">I have never seen any such discussion</em>. So without necessarily criticizing the position itself, I have to wonder what causes this skewed representation. I fear that this &ldquo;let&rsquo;s not criticize political views&rdquo; stance is causing Less Wrong to shift towards holding more and more eccentric views, since a lack of criticism can be taken as tacit approval. What especially worries me is that giving the impression that all sides are equal automatically lends credibility to the craziest viewpoint, as proponents of that side can now say that sceptics take their views seriously which benefits them the most. This seems to me literally the worst possible outcome of any politics debate. </span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span style=\"mso-ansi-language: EN-GB;\" lang=\"EN-GB\">I find that the same rule holds for politics as for life in general: You can try to win or you can give up and lose by default, but you can&rsquo;t choose not to play.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "czybHfMHvdjiEdQ86", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": -11, "extendedScore": null, "score": -2.8e-05, "legacy": true, "legacyId": "24483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 354, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T19:38:27.559Z", "modifiedAt": null, "url": null, "title": "Why you really want a physicist to speak at your funeral", "slug": "why-you-really-want-a-physicist-to-speak-at-your-funeral", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DJNxovmK6ZSjXT2fz/why-you-really-want-a-physicist-to-speak-at-your-funeral", "pageUrlRelative": "/posts/DJNxovmK6ZSjXT2fz/why-you-really-want-a-physicist-to-speak-at-your-funeral", "linkUrl": "https://www.lesswrong.com/posts/DJNxovmK6ZSjXT2fz/why-you-really-want-a-physicist-to-speak-at-your-funeral", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20you%20really%20want%20a%20physicist%20to%20speak%20at%20your%20funeral&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20you%20really%20want%20a%20physicist%20to%20speak%20at%20your%20funeral%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDJNxovmK6ZSjXT2fz%2Fwhy-you-really-want-a-physicist-to-speak-at-your-funeral%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20you%20really%20want%20a%20physicist%20to%20speak%20at%20your%20funeral%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDJNxovmK6ZSjXT2fz%2Fwhy-you-really-want-a-physicist-to-speak-at-your-funeral", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDJNxovmK6ZSjXT2fz%2Fwhy-you-really-want-a-physicist-to-speak-at-your-funeral", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 253, "htmlBody": "<p>A response to Aaron Freeman's \"<a href=\"http://www.npr.org/templates/story/story.php?storyId=4675953\">You Want a Physicist to Speak at Your Funeral</a>.\"</p>\n<p>If I had a physicist speak at my funeral, I would hope that he would talk about a lot more than the conservation of energy. I don't particularly care about what happens to my energy.</p>\n<p>If I am lucky, he will speak about relativity. My family will probably have the mistaken intuition that only things in the present are truly real. Teach them about spacetime. They need to know that time and space are connected - that me being in the past is just like me being far away. The difference is that we will only have one way communication. Even if they will no longer be able talk to me, I will still talk to them through memories.</p>\n<p>If I am not so lucky, he will speak about quantum mechanics. If I die young, my family will be grieving over the potential future I have lost. Teach them about many worlds. They need to know that our world is constantly splitting - that just before I died, the world split off a different future in which I am still alive. There is another world, just as real as our own, in which I survive. This world will even interact with our own in very tiny ways.</p>\n<p>I want a physicist to speak at my funeral. I want everyone to understand that my continued existence is way more verifiable than a religious afterlife and way more substantial than a simple conservation of energy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DJNxovmK6ZSjXT2fz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -8, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "24484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T21:17:21.833Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014Inside the 5-Second Level", "slug": "meetup-west-la-inside-the-5-second-level", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9WxFpt8AkJATMDzNg/meetup-west-la-inside-the-5-second-level", "pageUrlRelative": "/posts/9WxFpt8AkJATMDzNg/meetup-west-la-inside-the-5-second-level", "linkUrl": "https://www.lesswrong.com/posts/9WxFpt8AkJATMDzNg/meetup-west-la-inside-the-5-second-level", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94Inside%20the%205-Second%20Level&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94Inside%20the%205-Second%20Level%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WxFpt8AkJATMDzNg%2Fmeetup-west-la-inside-the-5-second-level%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94Inside%20the%205-Second%20Level%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WxFpt8AkJATMDzNg%2Fmeetup-west-la-inside-the-5-second-level", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9WxFpt8AkJATMDzNg%2Fmeetup-west-la-inside-the-5-second-level", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ss'>West LA\u2014Inside the 5-Second Level</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 October 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you are good at improvising.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>ED-209: You have 5 seconds to comply. \u2014<a href=\"http://en.wikiquote.org/wiki/RoboCop\" rel=\"nofollow\">RoboCop</a></p>\n</blockquote>\n\n<p>Of all generalized rationality skills, this is the most important. If you cannot think the right thought in the first five seconds upon encountering anything, inertia will almost certainly carry you where you do not <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">want to be</a>. So the question for discussion is: what are the generalized principles for turning abstract principles into thoughts at the 5-second level? And how do we apply those principles to themselves, so that we use them on the 5-second level?</p>\n\n<p><strong>Recommended Reading</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/5kz/the_5second_level/\">The 5-Second Level</a></li>\n<li><a href=\"http://lesswrong.com/lw/1ai/problems_vs_tasks/\">Problems vs. Tasks</a></li>\n<li><a href=\"http://lesswrong.com/lw/8j4/5second_level_case_study_value_of_information/\">5-Second Level Case Study: Value of information</a></li>\n<li><a href=\"http://images.lesswrong.com/t3_5x8_0.png?v=768eb3d2d6c3e55d569bf94b7865090d\">The PNG of Rationality</a></li>\n<li><a href=\"http://lesswrong.com/lw/eox/from_first_principles/\">5-Second Skill: From First Principles</a></li>\n<li><a href=\"http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/\">How an Algorithm Feels from the Inside</a></li>\n</ul>\n\n<p>Prior exposure to Less Wrong is not required. However, if you show up without prior exposure to Less Wrong, you will be vaporized within five seconds.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ss'>West LA\u2014Inside the 5-Second Level</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9WxFpt8AkJATMDzNg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3950860500435443e-06, "legacy": true, "legacyId": "24485", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Inside_the_5_Second_Level\">Discussion article for the meetup : <a href=\"/meetups/ss\">West LA\u2014Inside the 5-Second Level</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 October 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you are good at improvising.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>ED-209: You have 5 seconds to comply. \u2014<a href=\"http://en.wikiquote.org/wiki/RoboCop\" rel=\"nofollow\">RoboCop</a></p>\n</blockquote>\n\n<p>Of all generalized rationality skills, this is the most important. If you cannot think the right thought in the first five seconds upon encountering anything, inertia will almost certainly carry you where you do not <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">want to be</a>. So the question for discussion is: what are the generalized principles for turning abstract principles into thoughts at the 5-second level? And how do we apply those principles to themselves, so that we use them on the 5-second level?</p>\n\n<p><strong id=\"Recommended_Reading\">Recommended Reading</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/5kz/the_5second_level/\">The 5-Second Level</a></li>\n<li><a href=\"http://lesswrong.com/lw/1ai/problems_vs_tasks/\">Problems vs. Tasks</a></li>\n<li><a href=\"http://lesswrong.com/lw/8j4/5second_level_case_study_value_of_information/\">5-Second Level Case Study: Value of information</a></li>\n<li><a href=\"http://images.lesswrong.com/t3_5x8_0.png?v=768eb3d2d6c3e55d569bf94b7865090d\">The PNG of Rationality</a></li>\n<li><a href=\"http://lesswrong.com/lw/eox/from_first_principles/\">5-Second Skill: From First Principles</a></li>\n<li><a href=\"http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/\">How an Algorithm Feels from the Inside</a></li>\n</ul>\n\n<p>Prior exposure to Less Wrong is not required. However, if you show up without prior exposure to Less Wrong, you will be vaporized within five seconds.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Inside_the_5_Second_Level1\">Discussion article for the meetup : <a href=\"/meetups/ss\">West LA\u2014Inside the 5-Second Level</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014Inside the 5-Second Level", "anchor": "Discussion_article_for_the_meetup___West_LA_Inside_the_5_Second_Level", "level": 1}, {"title": "Recommended Reading", "anchor": "Recommended_Reading", "level": 2}, {"title": "Discussion article for the meetup : West LA\u2014Inside the 5-Second Level", "anchor": "Discussion_article_for_the_meetup___West_LA_Inside_the_5_Second_Level1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JcpzFpPBSmzuksmWM", "Pmfk7ruhWaHj9diyv", "xDiqYyqeqPo92PojS", "W6oJz6eiQ3q5t84Z3", "yA4gF5KrboK2m2Xu7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-25T21:47:57.878Z", "modifiedAt": null, "url": null, "title": "Methods of Introspection: Brainstorming and Discussion", "slug": "methods-of-introspection-brainstorming-and-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:22.007Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9AfCxbAAwrsEA5MWp/methods-of-introspection-brainstorming-and-discussion", "pageUrlRelative": "/posts/9AfCxbAAwrsEA5MWp/methods-of-introspection-brainstorming-and-discussion", "linkUrl": "https://www.lesswrong.com/posts/9AfCxbAAwrsEA5MWp/methods-of-introspection-brainstorming-and-discussion", "postedAtFormatted": "Friday, October 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Methods%20of%20Introspection%3A%20Brainstorming%20and%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMethods%20of%20Introspection%3A%20Brainstorming%20and%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9AfCxbAAwrsEA5MWp%2Fmethods-of-introspection-brainstorming-and-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Methods%20of%20Introspection%3A%20Brainstorming%20and%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9AfCxbAAwrsEA5MWp%2Fmethods-of-introspection-brainstorming-and-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9AfCxbAAwrsEA5MWp%2Fmethods-of-introspection-brainstorming-and-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>At a recent meetup one of the topics of discussion was methods of introspection. This is an interesting topic to me because sometimes all it takes is becoming aware of a new method to clarify an area where you were making little progress. Different methods also appeal more to people with different cognitive styles. &nbsp;I think it would be awesome to be surprised by useful modes we haven't thought of before!</p>\n<p>Before you fill your brain up with what we came up with, I am asking you to spend 1-5 minutes writing down names or descriptions of the different methods you use when engaged in thought. &nbsp;This can be normal thought, metacognition, etc. If you can think of a label or description that communicates something useful about it, it's fair game. An example if you have no idea what I'm talking about: imagining counterfactuals is a method of introspection, meditation can also be one, etc. It is a lot harder to brainstorm once you see 20 different ideas.</p>\n<p>Rot13 notes from meeting, you don't have to rot13 your comments, but this means you should make your comment before reading others.</p>\n<p>Vaare Fvzhyngbe/Perngvat Aneengvirf</p>\n<p>Znxvat/Grfgvat cerqvpgvbaf</p>\n<p>&nbsp;</p>\n<p>Gheavat bss vaare prafbe/fgernz bs pbafpvbhfarff</p>\n<p>&nbsp;</p>\n<p>Tbny snpgbevat</p>\n<p>&nbsp;</p>\n<p>Frrvat ceboyrz sebz 3eq crefba/ WBBGF / qrivy'f nqibpngr</p>\n<p>&nbsp;</p>\n<p>Fgehpgherq Oenvafgbezvat</p>\n<p>&nbsp;</p>\n<p>Dhnagvslvat orunivbef/gubhtugf</p>\n<p>Npgvivgl Ybtf/Wbheanyvat</p>\n<p>&nbsp;</p>\n<p>Zrqvgngvba</p>\n<p>&nbsp;</p>\n<p>Pbyynobengvir vagebfcrpgvba, fgehpgherq be hafgehpgherq</p>\n<p>&nbsp;</p>\n<p>Pynffvslvat artngvir gubhtug cnggreaf:</p>\n<p>Vafreg ovnfrf naq snyynpvrf urer?</p>\n<p>&nbsp;</p>\n<p>Zrgubqf bs pbzcnevat fhotbnyf? (VSF genafnpgvbaf?)</p>\n<p>&nbsp;</p>\n<p>Pbtavgvir Bagbybtvrf:</p>\n<p>Flfgrz 1&amp;2</p>\n<p>Vagreany Snzvyl Flfgrzf</p>\n<p>Znfybj'f uvrenepul</p>\n<p>Crefbanyvgl glcrf/Vqragvgl</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9AfCxbAAwrsEA5MWp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.3951150221181017e-06, "legacy": true, "legacyId": "24486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-26T13:06:39.481Z", "modifiedAt": null, "url": null, "title": "Intellectual energy", "slug": "intellectual-energy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:27.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4PipnE3nwjXbJZCKW/intellectual-energy", "pageUrlRelative": "/posts/4PipnE3nwjXbJZCKW/intellectual-energy", "linkUrl": "https://www.lesswrong.com/posts/4PipnE3nwjXbJZCKW/intellectual-energy", "postedAtFormatted": "Saturday, October 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intellectual%20energy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntellectual%20energy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PipnE3nwjXbJZCKW%2Fintellectual-energy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intellectual%20energy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PipnE3nwjXbJZCKW%2Fintellectual-energy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PipnE3nwjXbJZCKW%2Fintellectual-energy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<p>One of the big variations I see between people is the amount of energy they habitually put into thinking, and I haven't seen this discussed anywhere.</p>\n<p>General advice about improving health and lowering intellectual friction would seem to help increase the ability to think, and ideas like \"take five minutes to consider the problem\" adds impetus, but I'm not sure what the general difference is between me and most people, or <a href=\"http://slatestarcodex.com/2013/10/25/list-of-fictional-drugs-banned-by-the-fda/\">Yvain</a> and me.</p>\n<p>Intellectual drive isn't an unalloyed good-- cranks have high drive combined with low self-editinig, and some types of depression include a compulsion to think about topics that cause misery and/or inertia. Part (all?) of the value of meditation is getting some time off from thinking. Still, increasing intellectual drive would probably be a good thing for a lot of people.&nbsp;</p>\n<p>Has anyone found that rationality training or anything else increases the default desire to think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4PipnE3nwjXbJZCKW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 31, "extendedScore": null, "score": 1.3959853163946072e-06, "legacy": true, "legacyId": "24488", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-26T13:59:59.544Z", "modifiedAt": null, "url": null, "title": "Only You Can Prevent Your Mind From Getting Killed By Politics", "slug": "only-you-can-prevent-your-mind-from-getting-killed-by", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6jghdNrzZD6aoNsw8/only-you-can-prevent-your-mind-from-getting-killed-by", "pageUrlRelative": "/posts/6jghdNrzZD6aoNsw8/only-you-can-prevent-your-mind-from-getting-killed-by", "linkUrl": "https://www.lesswrong.com/posts/6jghdNrzZD6aoNsw8/only-you-can-prevent-your-mind-from-getting-killed-by", "postedAtFormatted": "Saturday, October 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Only%20You%20Can%20Prevent%20Your%20Mind%20From%20Getting%20Killed%20By%20Politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnly%20You%20Can%20Prevent%20Your%20Mind%20From%20Getting%20Killed%20By%20Politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jghdNrzZD6aoNsw8%2Fonly-you-can-prevent-your-mind-from-getting-killed-by%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Only%20You%20Can%20Prevent%20Your%20Mind%20From%20Getting%20Killed%20By%20Politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jghdNrzZD6aoNsw8%2Fonly-you-can-prevent-your-mind-from-getting-killed-by", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jghdNrzZD6aoNsw8%2Fonly-you-can-prevent-your-mind-from-getting-killed-by", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1400, "htmlBody": "<p>Follow-up to: <a href=\"/lw/9l4/politics_is_the_mindkiller_is_the_mindkiller/\">\"Politics is the mind-killer\" is the mind-killer</a>,&nbsp;<a href=\"/lw/iu0/trusting_expert_consensus/\">Trusting Expert Consensus</a></p>\n<p><a href=\"/lw/gw/\">Gratuitous political digs</a>&nbsp;are to be avoided.&nbsp;Indeed, I edited&nbsp;<a href=\"/lw/isk/a_voting_puzzle_some_political_science_and_a_nerd/\">my post on voting</a>&nbsp;to keep it from sounding any more partisan than necessary. But the fact that writers shouldn't gratuitously mind-kill their readers doesn't mean that, when they do, the readers' reaction is rational. The rules for readers are different from the rules for writers. And it&nbsp;<em>especially&nbsp;</em>doesn't mean that when a writer talks about a \"political\" topic for a reason, readers can use \"politics!\" as an excuse for attacking a statement of fact that makes them uncomfortable.<a id=\"more\"></a></p>\n<p>Imagine an alternate history where <a href=\"http://wiki.lesswrong.com/wiki/Color_politics\">Blue and Green</a> remain important political identities into the early stages of the space age. Blues, for complicated ideological reasons, tend to support trying to put human beings on the moon, while Greens, for complicated ideological reasons, tend to oppose it. But in addition to the ideological reasons, it has become popular for Greens to oppose attempting a moonshot on the grounds that the moon is made of cheese, and any landing vehicle put on the moon would sink into the cheese.</p>\n<p>Suppose you're a Green, but you know perfectly well that the claim the moon is made of cheese is ridiculous. You tell yourself that you needn't be too embarrassed by your fellow Greens on this point. On the whole, the Green ideology is vastly superior to the Blue ideology, and furthermore some Blues have begun arguing we should go to the moon because the moon is made of gold and we could get rich mining the gold. That's just as ridiculous as the assertion that the moon is made of cheese.</p>\n<p>Now imagine that one day, you're talking with someone who you <em>strongly suspect </em>is a Blue, and they remark on how irrational it is for so many people to believe the moon is made of cheese. When you hear that, you may be inclined to get defensive. <a href=\"/lw/gw/\">Politics is the mind-killer</a>, arguments are soldiers, so the point about the irrationality of the cheese-mooners may suddenly sound like a soldier for the other side that must be defeated.</p>\n<p>Except... you <em>know </em>the claim that the moon is made of cheese is ridiculous. So let me suggest that, in that moment, it's your duty as a rationalist to <em>not </em>chastise them for making such a \"politically charged\" remark, and <em>not</em> demand they refrain from saying such things unless they make it <em>perfectly clear </em>they're not attacking all Greens or saying it's irrational to oppose a moon shot, or anything like that.</p>\n<p><a href=\"/lw/gz/policy_debates_should_not_appear_onesided/\">Quoth Eliezer</a>:</p>\n<blockquote>\n<p>Robin Hanson recently proposed stores where banned products could be sold. &nbsp;There are a number of excellent arguments for such a policy&mdash;an inherent right of individual liberty, the career incentive of bureaucrats to prohibit everything, legislators being just as biased as individuals. &nbsp;But even so (I replied), some poor, honest, not overwhelmingly educated mother of 5 children is going to go into these stores and buy a \"Dr. Snakeoil's Sulfuric Acid Drink\" for her arthritis and die, leaving her orphans to weep on national television.</p>\n<p>I was just making a simple factual observation. &nbsp;Why did some people think it was an argument in favor of regulation?</p>\n</blockquote>\n<p>Just as commenters shouldn't have assumed Eliezer's factual observation was an argument in favor of regulation, you shouldn't assume the suspected Blue's observation is a pro-moon shot or anti-Green argument.</p>\n<p>The above parable was inspired by some of the discussion of global warming I've seen on LessWrong. According to the <a href=\"/lw/fp5/2012_survey_results/\">2012 LessWrong readership survey</a>, the mean confidence of LessWrong readers in human-caused global warming is 79%, and the <em>median&nbsp;</em>confidence is 90%. That's more or less in line with the current scientific consensus.</p>\n<p>Yet references to anthropogenic global warming (AGW) in posts on LessWrong often elicit negative reactions. For example, last year Stuart Armstrong once wrote a post titled, <a href=\"/lw/aw6/global_warming_is_a_better_test_of_irrationality/\">\"Global warming is a better test of irrationality than theism.\"</a>&nbsp;His thesis was non-obvious, yet on reflection, I think, probably correct. AGW-denialism is a closer analog to creationism than theism. As bad as theism is, it isn't a rejection of a generally accepted (among scientists) scientific claim with a lot of evidence behind it just because the claim clashes with your ideological. Creationism and AGW-denialism do fall under that category, though.</p>\n<p>Stuart's post was massively down voted&mdash;currently at -2, but at one point I think it went as low as -7. Why? Judging from the comments, not because people were saying, \"yeah, global warming denialism is irrational, but it's not clear it's worse than theism.\" <a href=\"/lw/aw6/global_warming_is_a_better_test_of_irrationality/61er\">Here's the most-upvoted comment</a> (currently at +44), which was also cited as \"best reaction I've seen to discussion of global warming <em>anywhere</em>\" in the comment thread on my post&nbsp;<a href=\"/lw/iu0/trusting_expert_consensus/\">Trusting Expert Consensus</a>:</p>\n<blockquote>\n<p>Here's the main thing that bothers me about this debate. There's a set of many different questions involving the degree of past and current warming, the degree to which such warming should be attributed to humans, the degree to which future emissions would cause more warming, the degree to which future emissions will happen given different assumptions, what good and bad effects future warming can be expected to have at different times and given what assumptions (specifically, what probability we should assign to catastrophic and even existential-risk damage), what policies will mitigate the problem how much and at what cost, how important the problem is relative to other problems, what ethical theory to use when deciding whether a policy is good or bad, and how much trust we should put in different aspects of the process that produced the standard answers to these questions and alternatives to the standard answers. These are questions that empirical evidence, theory, and scientific authority bear on to different degrees, and a LessWronger ought to separate them out as a matter of habit, and yet even here some vague combination of all these questions tends to get mashed together into a vague question of whether to believe \"the global warming consensus\" or \"the pro-global warming side\", to the point where when Stuart says some class of people is more irrational than theists, I have no idea if he's talking about me. If the original post had said something like, \"everyone whose median estimate of climate sensitivity to doubled CO2 is lower than 2 degrees Celsius is more irrational than theists\", I might still complain about it falling afoul of anti-politics norms, but at least it would help create the impression that the debate was about ideas rather than tribes.</p>\n</blockquote>\n<p>If you read Stuart's original post, it's clear this comment is reading ambiguity into the post where none exists. You could argue that Stuart was a <em>little </em>careless in switching between talking about AGW and global warming simpliciter, but I think his meaning is clear: he thinks rejection of AGW is irrational, which entails that he thinks the stronger \"no warming for any reason\" claim is irrational. And there's <em>no justification whatsoever </em>for suggesting Stuart's post could be read as saying,&nbsp;\"if your estimate of future warming is only 50% of the estimate I prefer you're irrational\"&mdash;or as taking a position on ethical theories, for that matter.&nbsp;</p>\n<p>What's going on here? Well, the LessWrong readership is mostly on-board with the scientific view on global warming. But many identify as libertarians, and they're aware that in the US many <em>other</em> conservatives/libertarians reject that scientific consensus (<a href=\"http://www.people-press.org/2008/05/08/a-deeper-partisan-divide-over-global-warming/\">and no, that's not just a stereotype</a>). So hearing someone say AGW denialism is irrational is really uncomfortable for them, <em>even if they agree. </em>This leaves them wanting <em>some kind of excuse to </em>complain,<em> </em>one guy thinks of \"this is ambiguous and too political\" as that excuse, and a bunch of people upvote it.</p>\n<p>(If you still don't find any of this odd, think of the \"skeptic\" groups that freely mock ufologists or psychics or whatever, but which are reluctant to say anything bad about religion, even though in truth the group is dominated by atheists. Far from a perfect parallel, but it's still worth thinking about.)</p>\n<p>When the title for this post popped into my head, I had to stop and ask myself if it was actually true, or just a funny Smokey the Bear reference. But in an important sense it is: the broader society isn't going to stop spontaneously labeling various straightforward empirical questions as Blue or Green issues. If you want to stop your mind from getting killed by whatever issues other people have decided are political, the only way is to control how you react to that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "BtQRRKTPxagBH6KrG": 1, "HXA9WxPpzZCCEwXHT": 1, "DdgSyQoZXjj3KnF4N": 1, "MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6jghdNrzZD6aoNsw8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 59, "extendedScore": null, "score": 0.000164, "legacy": true, "legacyId": "24461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 144, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uxsTyFLtSmxmniTzt", "R8YpYTq8LoD3k948L", "9weLK2AJ9JEt2Tt8f", "CcC8MocynqKPmMPwL", "PeSzc9JTBxhaYRp9b", "x9FNKTEt68Rz6wQ6P", "Qvw8TFpSH2pfWWyfS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-26T15:47:03.222Z", "modifiedAt": null, "url": null, "title": "Replicating Douglas Lenat's Traveller TCS win with publicly-known techniques", "slug": "replicating-douglas-lenat-s-traveller-tcs-win-with-publicly", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.197Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Myr7PLikhzYgPFhuy/replicating-douglas-lenat-s-traveller-tcs-win-with-publicly", "pageUrlRelative": "/posts/Myr7PLikhzYgPFhuy/replicating-douglas-lenat-s-traveller-tcs-win-with-publicly", "linkUrl": "https://www.lesswrong.com/posts/Myr7PLikhzYgPFhuy/replicating-douglas-lenat-s-traveller-tcs-win-with-publicly", "postedAtFormatted": "Saturday, October 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Replicating%20Douglas%20Lenat's%20Traveller%20TCS%20win%20with%20publicly-known%20techniques&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReplicating%20Douglas%20Lenat's%20Traveller%20TCS%20win%20with%20publicly-known%20techniques%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyr7PLikhzYgPFhuy%2Freplicating-douglas-lenat-s-traveller-tcs-win-with-publicly%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Replicating%20Douglas%20Lenat's%20Traveller%20TCS%20win%20with%20publicly-known%20techniques%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyr7PLikhzYgPFhuy%2Freplicating-douglas-lenat-s-traveller-tcs-win-with-publicly", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyr7PLikhzYgPFhuy%2Freplicating-douglas-lenat-s-traveller-tcs-win-with-publicly", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 282, "htmlBody": "<p>Douglas Lenat's program <a href=\"http://en.wikipedia.org/wiki/Eurisko\">EURISKO</a> is legendary in the AI community for a distinct real-world achievement: allowing Lenat to win the the Traveller TCS roleplaying game tournament two years in a row (and then semi-voluntarily not competing subsequent years). Lenat never released EURISKO's source code, leaving how he managed to pull off this feat somewhat of a mystery. Yet Lenat's later work based on EURISKO does not seem to have yielded anything else in the way of practical benefits.&nbsp;</p>\n<p>Some time ago on LessWrong, someone proposed <a href=\"/lw/10g/lets_reimplement_eurisko/\">trying to figure out what Lenat did and reimplementing EURISKO</a>. But <a href=\"/lw/10g/lets_reimplement_eurisko/sxe\">Eliezer is worried this could be dangerous</a>. So I have another proposal: see if Lenat's accomplishment can be replicated using machine learning and genetic programming techniques that are already publicly known.</p>\n<p>My suspicion is that Lenat's TCS win tells us more about TCS than about EURISKO, that TCS is likely a game that's inherently vulnerable to the \"find winning strategies by simulating a lots of games on a computer\" meta-strategy. I've heard, for example, that battles are often tactically trivial, with the outcome of battles effectively determined by the composition of the two fleets (and fleet composition is what Lenat used EURISKO for). If that hypothesis is correct, though, it suggests it shouldn't be necessary to reimplement EURISKO specifically to get a program that's good at designing TCS fleets. If that turns out not to be the case, it would be evidence that there really is something special about EURISKO after all.</p>\n<p>Does anyone know if anyone has tried this? As a novice computer programmer, I think it might be a good project to hone my programming skills. Input on how to approach such a project would be appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Myr7PLikhzYgPFhuy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "24490", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["t47TeAbBYxYgqDGQT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-26T19:03:24.547Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta: November Meetup (First of Two)", "slug": "meetup-atlanta-november-meetup-first-of-two", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/46NG44WuvEWbo5FYS/meetup-atlanta-november-meetup-first-of-two", "pageUrlRelative": "/posts/46NG44WuvEWbo5FYS/meetup-atlanta-november-meetup-first-of-two", "linkUrl": "https://www.lesswrong.com/posts/46NG44WuvEWbo5FYS/meetup-atlanta-november-meetup-first-of-two", "postedAtFormatted": "Saturday, October 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%3A%20November%20Meetup%20(First%20of%20Two)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%3A%20November%20Meetup%20(First%20of%20Two)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46NG44WuvEWbo5FYS%2Fmeetup-atlanta-november-meetup-first-of-two%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%3A%20November%20Meetup%20(First%20of%20Two)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46NG44WuvEWbo5FYS%2Fmeetup-atlanta-november-meetup-first-of-two", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F46NG44WuvEWbo5FYS%2Fmeetup-atlanta-november-meetup-first-of-two", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/st'>Atlanta: November Meetup (First of Two)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 November 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> 1314 Hosea L Williams Dr. NE, Atlanta</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us! We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times! Please check back for the location. Please contact me if you have cat allergies, as our meeting space usually has cats. Incredibly cute cats. Check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/Atlanta.Lesswrong/ where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/st'>Atlanta: November Meetup (First of Two)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "46NG44WuvEWbo5FYS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3963235318047643e-06, "legacy": true, "legacyId": "24492", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta__November_Meetup__First_of_Two_\">Discussion article for the meetup : <a href=\"/meetups/st\">Atlanta: November Meetup (First of Two)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 November 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> 1314 Hosea L Williams Dr. NE, Atlanta</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us! We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times! Please check back for the location. Please contact me if you have cat allergies, as our meeting space usually has cats. Incredibly cute cats. Check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/Atlanta.Lesswrong/ where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta__November_Meetup__First_of_Two_1\">Discussion article for the meetup : <a href=\"/meetups/st\">Atlanta: November Meetup (First of Two)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta: November Meetup (First of Two)", "anchor": "Discussion_article_for_the_meetup___Atlanta__November_Meetup__First_of_Two_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta: November Meetup (First of Two)", "anchor": "Discussion_article_for_the_meetup___Atlanta__November_Meetup__First_of_Two_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-27T22:26:59.864Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Thinking Fast and Slow Discussion", "slug": "meetup-urbana-champaign-thinking-fast-and-slow-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WHYPjYsPhzYQkm3DT/meetup-urbana-champaign-thinking-fast-and-slow-discussion", "pageUrlRelative": "/posts/WHYPjYsPhzYQkm3DT/meetup-urbana-champaign-thinking-fast-and-slow-discussion", "linkUrl": "https://www.lesswrong.com/posts/WHYPjYsPhzYQkm3DT/meetup-urbana-champaign-thinking-fast-and-slow-discussion", "postedAtFormatted": "Sunday, October 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHYPjYsPhzYQkm3DT%2Fmeetup-urbana-champaign-thinking-fast-and-slow-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHYPjYsPhzYQkm3DT%2Fmeetup-urbana-champaign-thinking-fast-and-slow-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHYPjYsPhzYQkm3DT%2Fmeetup-urbana-champaign-thinking-fast-and-slow-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/su'>Urbana-Champaign: Thinking Fast and Slow Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coordinates are:\n40.109545,-88.227318\nMeetup will be held in the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Discussion will be about the first 70 pages of Thinking Fast and Slow, by Daniel Kahneman. Up to but not including \"Norms, Surprises, and Causes.\"</p>\n\n<p>If you don't have it, googling \"Thinking Fast and Slow PDF\" could help you find t' booty yer lookin' for, if ye be a swashbucklin' scallywag.</p>\n\n<p>There will also be cookies.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/unSBuY9VFwg\">Cross posted</a> on the mailing list.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/su'>Urbana-Champaign: Thinking Fast and Slow Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WHYPjYsPhzYQkm3DT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3978835993315817e-06, "legacy": true, "legacyId": "24494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion\">Discussion article for the meetup : <a href=\"/meetups/su\">Urbana-Champaign: Thinking Fast and Slow Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coordinates are:\n40.109545,-88.227318\nMeetup will be held in the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Discussion will be about the first 70 pages of Thinking Fast and Slow, by Daniel Kahneman. Up to but not including \"Norms, Surprises, and Causes.\"</p>\n\n<p>If you don't have it, googling \"Thinking Fast and Slow PDF\" could help you find t' booty yer lookin' for, if ye be a swashbucklin' scallywag.</p>\n\n<p>There will also be cookies.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/unSBuY9VFwg\">Cross posted</a> on the mailing list.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/su\">Urbana-Champaign: Thinking Fast and Slow Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Thinking Fast and Slow Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Thinking Fast and Slow Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-28T00:59:16.871Z", "modifiedAt": null, "url": null, "title": "Open Thread, October 27 - 31, 2013", "slug": "open-thread-october-27-31-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:07.971Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mare-of-night", "createdAt": "2013-04-06T13:26:03.532Z", "isAdmin": false, "displayName": "mare-of-night"}, "userId": "6thzLTpEnEpcZF8bf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LD9uQCqhho9RCXJig/open-thread-october-27-31-2013", "pageUrlRelative": "/posts/LD9uQCqhho9RCXJig/open-thread-october-27-31-2013", "linkUrl": "https://www.lesswrong.com/posts/LD9uQCqhho9RCXJig/open-thread-october-27-31-2013", "postedAtFormatted": "Monday, October 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20October%2027%20-%2031%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20October%2027%20-%2031%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLD9uQCqhho9RCXJig%2Fopen-thread-october-27-31-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20October%2027%20-%2031%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLD9uQCqhho9RCXJig%2Fopen-thread-october-27-31-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLD9uQCqhho9RCXJig%2Fopen-thread-october-27-31-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 12.65625px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LD9uQCqhho9RCXJig", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3980283003870523e-06, "legacy": true, "legacyId": "24495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 384, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-28T02:25:20.296Z", "modifiedAt": null, "url": null, "title": "Is it worth your time to read a lot of self help and how to books?", "slug": "is-it-worth-your-time-to-read-a-lot-of-self-help-and-how-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.830Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EDg2zgGaSPC6sThzb/is-it-worth-your-time-to-read-a-lot-of-self-help-and-how-to", "pageUrlRelative": "/posts/EDg2zgGaSPC6sThzb/is-it-worth-your-time-to-read-a-lot-of-self-help-and-how-to", "linkUrl": "https://www.lesswrong.com/posts/EDg2zgGaSPC6sThzb/is-it-worth-your-time-to-read-a-lot-of-self-help-and-how-to", "postedAtFormatted": "Monday, October 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20it%20worth%20your%20time%20to%20read%20a%20lot%20of%20self%20help%20and%20how%20to%20books%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20it%20worth%20your%20time%20to%20read%20a%20lot%20of%20self%20help%20and%20how%20to%20books%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEDg2zgGaSPC6sThzb%2Fis-it-worth-your-time-to-read-a-lot-of-self-help-and-how-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20it%20worth%20your%20time%20to%20read%20a%20lot%20of%20self%20help%20and%20how%20to%20books%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEDg2zgGaSPC6sThzb%2Fis-it-worth-your-time-to-read-a-lot-of-self-help-and-how-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEDg2zgGaSPC6sThzb%2Fis-it-worth-your-time-to-read-a-lot-of-self-help-and-how-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 427, "htmlBody": "<p>Some time ago, I noticed that lukeprog seems to have an exceptional knack for reading up on a subject and actually putting what he's learned to practical use. At first I thought he might be some weird genetic freak, literally - some people are good at math, others are good at languages, maybe Luke is just good at putting stuff he's read to practical use. That, or I hoped he had some secret to it that he could put into words and share with the rest of us.</p>\n<p>But then then it occurred to me that Luke, by his own account, has spent a ridiculous amount of time reading self-help books. In a LessWrong post, he says, <a href=\"/lw/7k6/my_favorite_popular_scientific_selfhelp_books/\">\"I've spent several years studying scientific self-help\"</a>; IIRC his personal site used to have a ridiculous number of reviews of self-help books, including ones with a less scientific approach. This makes me suspect that probably, spending all that time reading self-help books made him better at learning how to do things. Even reading crappy self-help books (which again IIRC Luke's old website said he did a lot of, at first) may have helped, insofar as it taught him to tell good advice from bad.</p>\n<p>He may be able to verbalize some of what he learned, like \"look for books that take a scientific approach,\" but I suspect he's developed considerable non-verbalizable yet learnable skill in this area. I see a parallel here for my own skill at doing library research factual questions: some of it I can verbalize (Google it, read the Wikipedia article, look rigorous academic work, <a href=\"/lw/iu0/trusting_expert_consensus/\">look for hard data on the opinions of experts</a>), but a lot of it is stuff I can't verbalize, which another person could only gain through spending as much time doing research as I have.</p>\n<p>I've previously had an aversion to reading much in the way of self help and how to books because of an expectation that mostly they'll suck, but now I think that maybe, when I get some spare time, I should buckle down, pick a topic (maybe writing), and read a bunch of how to books anyway, with a hope of actually getting better at the thing but mostly as an exercise in learning to learn how to do things. But I'm curious to know if other people think this is a good idea - if they see a flaw in my logic, or if anyone who's read a lot of self help and how to books can comment on whether they think it helped them tell good advice from bad.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EDg2zgGaSPC6sThzb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 25, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "24497", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8pP8xyJYEzvfdDZ2e", "R8YpYTq8LoD3k948L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-28T06:08:35.679Z", "modifiedAt": null, "url": null, "title": "Social incentives affecting beliefs", "slug": "social-incentives-affecting-beliefs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:27.895Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J9vjGKa82zZZLWpk4/social-incentives-affecting-beliefs", "pageUrlRelative": "/posts/J9vjGKa82zZZLWpk4/social-incentives-affecting-beliefs", "linkUrl": "https://www.lesswrong.com/posts/J9vjGKa82zZZLWpk4/social-incentives-affecting-beliefs", "postedAtFormatted": "Monday, October 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Social%20incentives%20affecting%20beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASocial%20incentives%20affecting%20beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9vjGKa82zZZLWpk4%2Fsocial-incentives-affecting-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Social%20incentives%20affecting%20beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9vjGKa82zZZLWpk4%2Fsocial-incentives-affecting-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ9vjGKa82zZZLWpk4%2Fsocial-incentives-affecting-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 769, "htmlBody": "<p>Having weird ideas relative to your friends and associates means paying social costs.&nbsp; If you share your weird ideas, you'll have more arguments, your associates will see you as weird and you'll experience some degree of rejection and decreased status.&nbsp; If you keep your weird ideas to yourself, you'll have to lead a double life of secret constructed knowledge on the one hand and public facade on the other.</p>\n<p>For people reading this site, the most vivid analogy here might be being forced to live in a town full of religious hicks in the south of the USA, with minimal contact with the outside world.&nbsp; (I've heard from reliable sources that the stereotypes about the South are accurate.)&nbsp; Not many of us would choose to do this voluntarily.</p>\n<p>The weirder your beliefs get relative to your peer group, the greater the social costs you'll have to pay.&nbsp; Imagine we plot the beliefs of your associates on a multidimensional plot and put a hook at the center of mass of this plot.&nbsp; Picture yourself attached with an elastic band to this hook.&nbsp; The farther you stray from the center of mass, the greater the force pulling you towards conventional beliefs.</p>\n<p>This theorizing has a few straightforward implications:</p>\n<ul>\n<li>If you notice yourself paying a high social or psychological cost for your current set of beliefs, and you have reasons to not abandon the beliefs (e.g. you think they're correct), consider trying to find a new set of associates where those psychosocial costs are lower (either people who agree with you more, people who are less judgmental, or some combination) so you can stop paying the costs.&nbsp; If you can't find any such associates, create some: convince a close friend or two of your beliefs, so you have a new center of mass to anchor yourself on.&nbsp; Also cultivate psychological health through improving your relationships, meditation, self-love and acceptance, etc.</li>\n<li>If you're trying to help a group have accurate beliefs on aggregate, stay nonjudgmental so that the forces pulling people towards conventional wisdom will be lower, and they'll be more influenced by the <em>evidence</em> they encounter as opposed to the <em>incentives</em> they encounter.&nbsp; You may say \"well, I'm only judgmental towards peoples' beliefs when they're <em>incorrect</em>.\"&nbsp; But even if you happen to be perfect at figuring out which beliefs are incorrect, this is still a bad idea.&nbsp; If I'm trying to figure out whether to officially adopt some belief as part of my thinking, I'll calculate my expected social cost of holding the belief using the probability that it's incorrect times the penalty in the case where it's incorrect.&nbsp; So even punishing only the incorrect beliefs will counterfactually decrease the rate of people holding unusual beliefs.</li>\n</ul>\n<p>Some more bizarre ideas:</p>\n<ul>\n<li>Deliberately habituate yourself to/adapt to the social costs associated with having weird ideas.&nbsp; Practice coming across as \"eccentric\" rather than \"kooky\" when explaining your weird ideas, and state them confidently as if they're naturally and obviously true, to decrease status loss effects.&nbsp; Consider adopting a posture of aloofness or mystery.&nbsp; Or for a completely alternative approach, deliberately adopt a few beliefs that you suspect are true but your social group rejects, and keep them secret to practice having your own model of the world independent of that of your social group.</li>\n<li>If you notice a weird idea of yours is either not getting adopted by you because of social costs, or is costing you \"rent\" in terms of social costs you are having to pay to maintain it, do a cost-benefit analysis and deliberately either maintain the belief and pay the upkeep costs or discard it from your everyday mental life (preferably making a note at the time you discard it).&nbsp; You have to pick your battles.</li>\n<li>Start being kinda proud of the weird things you think you've figured out, in order to cancel out the psychosocial punishment for weird ideas with a dose of psychosocial reward.&nbsp; Keep your pride to yourself to avoid being humiliated if your beliefs turn out to be proven wrong.&nbsp; The point is to be guided only by the evidence you have, even if that evidence is biased or incomplete, rather than solely the opinion of the herd.&nbsp; (Of course, the herd's opinion should be considered evidence.&nbsp; But if you're doing it right, you'll err on the side of agreeing with the herd too much and agreeing with the herd too little about the same amount... unfortunately, agreeing with the herd too little and being wrong generally hurts you much more than agreeing with the herd too much and being wrong.)</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J9vjGKa82zZZLWpk4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 3, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-28T11:52:13.319Z", "modifiedAt": null, "url": null, "title": "Supplementing memory with experience sampling", "slug": "supplementing-memory-with-experience-sampling", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.064Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d43SRwDu5wpBArdDo/supplementing-memory-with-experience-sampling", "pageUrlRelative": "/posts/d43SRwDu5wpBArdDo/supplementing-memory-with-experience-sampling", "linkUrl": "https://www.lesswrong.com/posts/d43SRwDu5wpBArdDo/supplementing-memory-with-experience-sampling", "postedAtFormatted": "Monday, October 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Supplementing%20memory%20with%20experience%20sampling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASupplementing%20memory%20with%20experience%20sampling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd43SRwDu5wpBArdDo%2Fsupplementing-memory-with-experience-sampling%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Supplementing%20memory%20with%20experience%20sampling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd43SRwDu5wpBArdDo%2Fsupplementing-memory-with-experience-sampling", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd43SRwDu5wpBArdDo%2Fsupplementing-memory-with-experience-sampling", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 452, "htmlBody": "<p>If you asked me how happy I've been, I'd think back over my recent life and synthesize my memories into a judgement.  Since I'm the one experiencing my life you would think this would be accurate, but our memories aren't fair.  For example, people who had their hand in 57&deg; water for 60 seconds rated the experience as less pleasant than people who had their hand in the same 57&deg; water for the same 60 seconds, followed by 30 seconds with the water slowly rising to 59&deg;. (Kahneman 1993, <a href=\"http://brainimaging.waisman.wisc.edu/~perlman/0903-EmoPaper/KahnemanFredricksonSchreiberRedelmeier_1993_WhenMorePainIsPreferredToLess.pdf\">pdf</a>) This is the <a href=\"https://en.wikipedia.org/wiki/Peak%E2%80%93end_rule\">peak-end rule</a> where when we look back at an experience we don't really consider the duration and instead evaluate it based on how it was at its peak and how it ended.</p>\n<p>This disagreement between emotion as it is experienced and emotion as it is remembered is called the memory-experience gap, and the peak-end rule is only one of the causes.  The problem is, generally we only have access to memories of our emotion, which means if you're given the ice-water choice you'll repeatedly choose the option with more suffering.  How can we get around this?</p>\n<p>When psychologists want to get at experiential emotion they give people little timers.  Every time the timer goes off the person writes down how happy/sad they are at that moment.  This is an external sampling method that lets us use any sort of aggregation we would like, and it's fair in a way our internal methods are not.  When I first read about this I thought \"neat\" and moved on, but recently I realized I that with a computer in my pocket I could do this myself.   After <a href=\"https://www.facebook.com/jefftk/posts/630685254572\">asking around</a> I ended up with the <a href=\"https://play.google.com/store/apps/details?id=bsoule.tagtime\">TagTime</a> Android app, which is the only way I've found to do this that (a) works without an internet connection and (b) has an equal probability of sampling at every moment.</p>\n<p>The response screen looks like:</p>\n<p><img style=\"margin:20px\" src=\"http://www.jefftk.com/tagtime-screenshot-sm.png\" border=\"1\" alt=\"\" /></p>\n<p>You tap tags to say which ones currently apply.  I have them sorted by frequency. To add new tags you turn the phone sideways and type text:</p>\n<p><img style=\"margin:20px\" src=\"http://www.jefftk.com/tagtime-addtags-screenshot.png\" border=\"1\" alt=\"\" /></p>\n<p>That's a little annoying, but most of the time I'm not entering a new tag.</p>\n<p>I have tags for happiness (numbers 0-9, added as I need them), for aspects of activities, and for people I'm with.  Every so often I email the data to myself and add it to my <a href=\"http://www.jefftk.com/tagtime.log\">full log</a> which backs a <a href=\"http://www.jefftk.com/happiness_graph\">graph</a>:</p>\n<p><a href=\"http://www.jefftk.com/happiness_graph\"><img style=\"margin:20px\" src=\"http://www.jefftk.com/happiness_graph_screenshot.png\" alt=\"\" /></a></p>\n<p>Retrospective happiness still matters; you want to be happy with your life looking back.  Because this is our memory, however, we're already aware of it and already optimize for it in our life.  Adding sampled data should allow us to adjust that optimization to fix the things that are important but hidden by our biased memories.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d43SRwDu5wpBArdDo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 1.3986490308763272e-06, "legacy": true, "legacyId": "24502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-28T12:17:16.479Z", "modifiedAt": null, "url": null, "title": "Goal Setting and Goal Achievement", "slug": "goal-setting-and-goal-achievement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kerspoon", "createdAt": "2011-12-27T09:53:49.512Z", "isAdmin": false, "displayName": "kerspoon"}, "userId": "XvZ9yyyJNeDwWhECW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SQAosQR38nEJMvaiY/goal-setting-and-goal-achievement", "pageUrlRelative": "/posts/SQAosQR38nEJMvaiY/goal-setting-and-goal-achievement", "linkUrl": "https://www.lesswrong.com/posts/SQAosQR38nEJMvaiY/goal-setting-and-goal-achievement", "postedAtFormatted": "Monday, October 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Goal%20Setting%20and%20Goal%20Achievement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoal%20Setting%20and%20Goal%20Achievement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQAosQR38nEJMvaiY%2Fgoal-setting-and-goal-achievement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Goal%20Setting%20and%20Goal%20Achievement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQAosQR38nEJMvaiY%2Fgoal-setting-and-goal-achievement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQAosQR38nEJMvaiY%2Fgoal-setting-and-goal-achievement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2543, "htmlBody": "<p>The reason people succeed is well-focused, regular, effort.</p>\n<p>The reason for failure is more often lack of effort rather than the direction of the effort. For example, more people put on weight because they couldn't stick to their diet than those that chose a diet that wouldn't have caused weight loss.</p>\n<h2>1. How the Brain Decides what Task you Will Perform</h2>\n<p>You subconscious decides on your motivation for a task, but you can consciously choose to use your self-control and not do the easiest task. For example, my subconscious is saying that watching TV requires the least motivation but I have chosen to use up some of my self-control and write this article instead.</p>\n<p>I'm going to discuss a few tactics to reduce the self-control required to complete a task (such as setting up a trigger) as well as ways of making your subconscious disfavour your distractions (for example by removing the plug from your TV).</p>\n<h3>1.1 Motivation</h3>\n<p>A good guideline for the motivation that your subconscious gives you for a task is given in the equation below.</p>\n<p><img src=\"http://kerspoon.com/img/2013-goals/procrastination-equation.png\" alt=\"\" width=\"548\" height=\"99\" /> <br /> The Expectancy and Value can be thought of in terms of gambling. The Value is how much you get paid if you win and the Expectancy is the probability of winning. The Value of a task changes constantly as it is how much you value completing the task at any given moment. Delay is the amount of time before you will get the pay-off. For writing an essay it is likely to be after the entire thing is written, watching TV is an almost instant hit. Impulsiveness is a more complex part which is covered in Piers Steel's &ldquo;Procrastination&rdquo;.</p>\n<p>If you are struggling to put effort in to a task then it is probably because your motivation for it is lower than its alternatives. For example, there are many things I could do at the moment: I could get some food, play computer games, go for a walk, write this essay, etc. and each has a certain level of motivation based on the above equation. If I make sure I'm not hungry then the first task is of such low value that it wont be a problem. If I delete my computer games so I have to find the CD to be able to play them it increases the delay making that less of an issue.</p>\n<p>Every time you get distracted take note of the task and see why you have a higher motivation for it than what you should be doing. Then go through each part of the equation in turn to see if you can reduce the motivation for tasks you don't want to do.</p>\n<h2>1.2 Self Control</h2>\n<p>Self-control is the effort you put in to a task to make sure you do it instead of one with a higher motivation. For example, getting a snack might have a higher motivation than continuing with my work but I can use up some self-control to make sure I keep working.</p>\n<p>Self control is like a muscle. Do too much and you will exhaust it, but if you successfully use it regularly it will get stronger.</p>\n<h2>2. How to Set goals to increase changes of success</h2>\n<p>Below are three methods to increase the chances of succeeding at a task. They are based on the ideas in section one.</p>\n<ol>\n<li>\n<p>The first method (triggers and mechanical starting points) works because you quickly program your subconscious to perform a task at a given time. It means you can sidestep the subconscious calculation of motivation until the task has started, and starting is often the hardest part.</p>\n</li>\n<li>\n<p>The second (mental contrasting) provides you with a more accurate guess for the value and expectancy of the task. This means you can work out whether you are likely to succeed before you have started. It also increases your motivation when doing the task.</p>\n</li>\n<li>\n<p>The third (prevention or promotion) frames the goal so that you get more motivation when you need it most. But you have to choose when you need it, there is a tradeoff.</p>\n</li>\n</ol>\n<p>It will still take a lot of self control to achieve worthwhile tasks but by using mechanisms such as this you can increase you chances.</p>\n<h3>2.1 Triggers and Mechanical Starting Points</h3>\n<p>The key to this method of goal setting is to set up simple triggers and a mechanical starting point. A trigger is something to tell you when to start working on your goal, a mechanical task is one that can be done with no thinking, or at least without conscious thought. By thinking of your goal in terms of triggers and starting points you can form a habit much quicker. For example if you goal is to become more flexible you should convert this into one with trigger and a mechanical starting point.</p>\n<p>The trigger should specify exactly where and when you start, a good example would be \"straight after brushing your teeth every weekday morning\" (this is only a good example if you already have the habit of brushing your teeth every morning), a bad example would be \"at 8:15am\", what happens if you wake up a bit late and are brushing your teeth at that time. A further problem with purely time based triggers is that you wont be able to tell it is exactly 8:15 unless you set an alarm and in a lot of cases it is not practical to have many alarms going off through the day.</p>\n<p>A mechanical starting point is something that gets you started without having to think. Setting your alarm for 10mins and swinging your leg back and forward would be a good starting point for the goal of become more flexible. If your goal was to write a book a good starting point would be to get out your notebook and read through the last bit of what you wrote last time. A bad starting point would be to continue writing where you left off last time, that requires conscious thought.</p>\n<p>Some goals do not have a time based trigger at all, for example &ldquo;not snacking&rdquo; applies all the time. You still want to form a habit that your subconscious can follow without having to deplete your self control. In that cause I would suggest framing the goal in terms of \"if ... then ...\" e.g. <strong>if </strong>I feel the urge to snack <strong>then </strong>I will eat a few nuts and seeds and drink a glass of water. For goals like this you can also factor in the brain taking the easiest option, if you make it a real effort to get to the snacks you will be less likely to do so.</p>\n<h3>2.2 Mental Contrasting</h3>\n<p>The next key point is to consider alternating points of view of the goal. You want to be overly pessimistic about how hard it will be (i.e. think it will be difficult) but optimistic about your chance of success. The idea is to get to a point where you think:</p>\n<blockquote>\"This will be hard to achieve but I know, with a lot of self control and effort I can do it, and it really will be worth it.\"</blockquote>\n<p>The way to do this is to alternate between thinking of reasons why the task will be difficult (or why you could fail) then think of why it is important to you to achieve.</p>\n<p>When you are thinking of the difficulties imagine yourself in a realistic situation where you are likely to give up. Think up a bad day where you are hungry, annoyed, lonely, and/or tired (H.A.L.T). After you have put yourself in that mindset imagine what would tip you over the edge into failing your goal in some way. By picturing you mental state it will help you have a more realistic view of you chances for success.</p>\n<p>Then imagine that you have already achieved the goal. Think of the reasons why it helped you or made your life better. You can also imagine what it would be like to not achieve the goal. Really think if this would make any difference or not. This will help you work out how important the goal is to you. It is important to be brutally honest.</p>\n<p>Repeat this cycle two or three times and you should have a good idea if you are likely to put in the effort when it counts, and if that effort is worth it. You should need very long to do this, just make sure you do each part in turn (think first of the difficulties, then the reasons).</p>\n<p>You might realise that when the chips are down and you are hungry, angry, lonely, tired or just having a bad day you are likely to give up. If this is the case (remember to be a bit pessimistic about this) then don&rsquo;t bother working on the goal. You have already decided that it is not worth the effort. If you think you cannot succeed when you are planning you will have no hope when some unexpected challenge pops up.</p>\n<p>It is also worth quickly thinking through an &ldquo;if-then&rdquo; reaction for your sticking points that you have identified. For example, if you are trying to run every day but you know some days you feel tired and find it difficult to start. In that case you could set the response to be \"<strong>If</strong> I ever feel too tired to start running <strong>then </strong>I will put my running shoes on and leave the house whether I actually run or not.\" The problem is likely to be self-control depletion not exhaustion, by getting to the point of actually running you have lowered the mental effort of starting.</p>\n<h3>2.3 Prevention or Promotion</h3>\n<p>The different ways you can think about a goal change where you will put effort in to it. You can either have a goal where you must not fail (prevention) or a goal where you want to conquer (promotion). The way you phrase your goal can change it from one to the other and cause you to have more motivation when you need it. For example lets say you go rock climbing for fun with some friends, if you are worried that you are holding the group back so must train harder then your focus is prevention. If you want to beat everyone else then your focus is promotion.</p>\n<p>If you have a prevention focus then you get more motivation when you are failing but less when you are doing well. Taking the example of rock climbing, if you had a really bad training session where you were actually worse than before then you would feel like you had to train even harder because you really cannot afford to fail. On the other hand you will put less effort in when succeeding, after all your goal is simply not to fail and if you are doing great then you can afford to put in less effort.</p>\n<p>If you have a promotion focus you get more motivation when you are doing well but are likely to give up when failing. For example if you have had a great training session and managed to beat someone on a new climb then you will feel elated and want to do more. Yet if you do badly then you are likely to put less effort in and hence do even worse and give up. If you were trying to win but if looks like you can't then why bother to put the effort in.</p>\n<p>You can use this to your advantage. Think of whether you want more motivation when failing or succeeding and frame your goal accordingly. For any task you can choose promotion or prevention not both. If you decide you want a promotion goal and start to fail then you should realise it, feel bad about it and make a conscious decision to put more effort in.</p>\n<h2>3 Summary</h2>\n<p>The subconscious gives you a motivation value for every thing you could do. You can change the motivation by working out why your subconscious is choosing distracting things and changing those factors. By spending a few minutes examining your goals you can increase the likelihood of achieving them. The three ways discussed were:</p>\n<ol>\n<li>\n<p>\"If-then\" triggers,</p>\n</li>\n<li>\n<p>Mental contrasting,</p>\n</li>\n<li>\n<p>Changing the wording of the goal to give either a prevention or promotion focus.</p>\n</li>\n</ol>\n<p>Remember that even with this type of goal setting you are vulnerable to self-control depletion. Sometimes you have bad days and on those days it will take a lot of mental effort and a lot of thinking through why you wanted to achieve the goal to actually succeed. The check-list below is meant to take the ideas above and form them into questions to improve the chances of succeeding.</p>\n<p>I suggest having a written list of goals somewhere where you will look at them regularly. For example, a bedside table where you can look at them morning and night. It depends on your routine. There is no point in having these triggers if they are not regularly refreshed in your mind. The best way to do that is to read them and perform them regularly.</p>\n<h3>3.1 Goal Check-list</h3>\n<ol>\n<li>\n<p>I want more motivation when [failing, succeeding] therefore focus will be [prevention, promotion]?</p>\n</li>\n<li>\n<p>It will be done when &hellip;</p>\n</li>\n<li>\n<p>I am doing this because &hellip;</p>\n</li>\n<li>\n<p>My sticking point will be &hellip;</p>\n</li>\n<li>\n<p>I want this to be done because &hellip;</p>\n</li>\n<li>\n<p>I risk failure when &hellip;</p>\n</li>\n<li>\n<p>The trigger is &hellip;</p>\n</li>\n<li>\n<p>The very first step is ...</p>\n</li>\n</ol>\n<h3>3.2 Distraction Check List</h3>\n<ol>\n<li>\n<p>Notice you are becoming distracted.</p>\n</li>\n<li>\n<p>Examine each factor of the motivation equation in turn for both the distracting task and the task you want to perform.</p>\n</li>\n<li>\n<p>Think how to change the goal or your environment to change the factors in your favour.</p>\n</li>\n<li>\n<p>If you are regularly coming up against distractions for the same goal then re-evaluate the value and expectancy. It might be worth changing or dropping the goal.</p>\n</li>\n</ol>\n<h2>PostScript</h2>\n<p>The sources are from the list below (plus two years of small tweaks by myself):</p>\n<ul>\n<li>What Color is your parachute</li>\n<li><a href=\"https://www.stephencovey.com/7habits/7habits.php\" target=\"_blank\">The Seven Habits of Highly Effective People by Stephen Covey</a></li>\n<li><a href=\"http://www.heidigranthalvorson.com/\" target=\"_blank\">Succeed: How We Can Reach Our Goals by Heidi Grant Halvorson</a></li>\n<li>The Procrastination Eqn. --&nbsp;<a href=\"http://procrastinus.com/\" target=\"_blank\">http://procrastinus.com/</a></li>\n<li><a href=\"http://gettingresults.com/wiki/How_To_-_Set_Goals_and_Achieve_Them\" target=\"_blank\">http://gettingresults.com/wiki/How_To_-_Set_Goals_and_Achieve_Them</a></li>\n</ul>\n<p><em>I wrote this article before reading the more recent work on how mindset changes whether people perform as if self-control was a resource that could be depleted. I plan to make a new article that contains this information as well as a lot more on mindset and emotional responses but I want to test it out on the London LW community first.</em></p>\n<p><em>Although this is based in research it does diverge somewhat. It is more a measure of what has worked well for me. If you want the truth in the research read the research papers in the back of the books I have referenced.</em></p>\n<p>This post is also available on my personal <a href=\"http://kerspoon.com/blog/2013/goal-setting-and-achievement/\">blog</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SQAosQR38nEJMvaiY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 3, "extendedScore": null, "score": 1.3986728573682456e-06, "legacy": true, "legacyId": "24503", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The reason people succeed is well-focused, regular, effort.</p>\n<p>The reason for failure is more often lack of effort rather than the direction of the effort. For example, more people put on weight because they couldn't stick to their diet than those that chose a diet that wouldn't have caused weight loss.</p>\n<h2 id=\"1__How_the_Brain_Decides_what_Task_you_Will_Perform\">1. How the Brain Decides what Task you Will Perform</h2>\n<p>You subconscious decides on your motivation for a task, but you can consciously choose to use your self-control and not do the easiest task. For example, my subconscious is saying that watching TV requires the least motivation but I have chosen to use up some of my self-control and write this article instead.</p>\n<p>I'm going to discuss a few tactics to reduce the self-control required to complete a task (such as setting up a trigger) as well as ways of making your subconscious disfavour your distractions (for example by removing the plug from your TV).</p>\n<h3 id=\"1_1_Motivation\">1.1 Motivation</h3>\n<p>A good guideline for the motivation that your subconscious gives you for a task is given in the equation below.</p>\n<p><img src=\"http://kerspoon.com/img/2013-goals/procrastination-equation.png\" alt=\"\" width=\"548\" height=\"99\"> <br> The Expectancy and Value can be thought of in terms of gambling. The Value is how much you get paid if you win and the Expectancy is the probability of winning. The Value of a task changes constantly as it is how much you value completing the task at any given moment. Delay is the amount of time before you will get the pay-off. For writing an essay it is likely to be after the entire thing is written, watching TV is an almost instant hit. Impulsiveness is a more complex part which is covered in Piers Steel's \u201cProcrastination\u201d.</p>\n<p>If you are struggling to put effort in to a task then it is probably because your motivation for it is lower than its alternatives. For example, there are many things I could do at the moment: I could get some food, play computer games, go for a walk, write this essay, etc. and each has a certain level of motivation based on the above equation. If I make sure I'm not hungry then the first task is of such low value that it wont be a problem. If I delete my computer games so I have to find the CD to be able to play them it increases the delay making that less of an issue.</p>\n<p>Every time you get distracted take note of the task and see why you have a higher motivation for it than what you should be doing. Then go through each part of the equation in turn to see if you can reduce the motivation for tasks you don't want to do.</p>\n<h2 id=\"1_2_Self_Control\">1.2 Self Control</h2>\n<p>Self-control is the effort you put in to a task to make sure you do it instead of one with a higher motivation. For example, getting a snack might have a higher motivation than continuing with my work but I can use up some self-control to make sure I keep working.</p>\n<p>Self control is like a muscle. Do too much and you will exhaust it, but if you successfully use it regularly it will get stronger.</p>\n<h2 id=\"2__How_to_Set_goals_to_increase_changes_of_success\">2. How to Set goals to increase changes of success</h2>\n<p>Below are three methods to increase the chances of succeeding at a task. They are based on the ideas in section one.</p>\n<ol>\n<li>\n<p>The first method (triggers and mechanical starting points) works because you quickly program your subconscious to perform a task at a given time. It means you can sidestep the subconscious calculation of motivation until the task has started, and starting is often the hardest part.</p>\n</li>\n<li>\n<p>The second (mental contrasting) provides you with a more accurate guess for the value and expectancy of the task. This means you can work out whether you are likely to succeed before you have started. It also increases your motivation when doing the task.</p>\n</li>\n<li>\n<p>The third (prevention or promotion) frames the goal so that you get more motivation when you need it most. But you have to choose when you need it, there is a tradeoff.</p>\n</li>\n</ol>\n<p>It will still take a lot of self control to achieve worthwhile tasks but by using mechanisms such as this you can increase you chances.</p>\n<h3 id=\"2_1_Triggers_and_Mechanical_Starting_Points\">2.1 Triggers and Mechanical Starting Points</h3>\n<p>The key to this method of goal setting is to set up simple triggers and a mechanical starting point. A trigger is something to tell you when to start working on your goal, a mechanical task is one that can be done with no thinking, or at least without conscious thought. By thinking of your goal in terms of triggers and starting points you can form a habit much quicker. For example if you goal is to become more flexible you should convert this into one with trigger and a mechanical starting point.</p>\n<p>The trigger should specify exactly where and when you start, a good example would be \"straight after brushing your teeth every weekday morning\" (this is only a good example if you already have the habit of brushing your teeth every morning), a bad example would be \"at 8:15am\", what happens if you wake up a bit late and are brushing your teeth at that time. A further problem with purely time based triggers is that you wont be able to tell it is exactly 8:15 unless you set an alarm and in a lot of cases it is not practical to have many alarms going off through the day.</p>\n<p>A mechanical starting point is something that gets you started without having to think. Setting your alarm for 10mins and swinging your leg back and forward would be a good starting point for the goal of become more flexible. If your goal was to write a book a good starting point would be to get out your notebook and read through the last bit of what you wrote last time. A bad starting point would be to continue writing where you left off last time, that requires conscious thought.</p>\n<p>Some goals do not have a time based trigger at all, for example \u201cnot snacking\u201d applies all the time. You still want to form a habit that your subconscious can follow without having to deplete your self control. In that cause I would suggest framing the goal in terms of \"if ... then ...\" e.g. <strong>if </strong>I feel the urge to snack <strong>then </strong>I will eat a few nuts and seeds and drink a glass of water. For goals like this you can also factor in the brain taking the easiest option, if you make it a real effort to get to the snacks you will be less likely to do so.</p>\n<h3 id=\"2_2_Mental_Contrasting\">2.2 Mental Contrasting</h3>\n<p>The next key point is to consider alternating points of view of the goal. You want to be overly pessimistic about how hard it will be (i.e. think it will be difficult) but optimistic about your chance of success. The idea is to get to a point where you think:</p>\n<blockquote>\"This will be hard to achieve but I know, with a lot of self control and effort I can do it, and it really will be worth it.\"</blockquote>\n<p>The way to do this is to alternate between thinking of reasons why the task will be difficult (or why you could fail) then think of why it is important to you to achieve.</p>\n<p>When you are thinking of the difficulties imagine yourself in a realistic situation where you are likely to give up. Think up a bad day where you are hungry, annoyed, lonely, and/or tired (H.A.L.T). After you have put yourself in that mindset imagine what would tip you over the edge into failing your goal in some way. By picturing you mental state it will help you have a more realistic view of you chances for success.</p>\n<p>Then imagine that you have already achieved the goal. Think of the reasons why it helped you or made your life better. You can also imagine what it would be like to not achieve the goal. Really think if this would make any difference or not. This will help you work out how important the goal is to you. It is important to be brutally honest.</p>\n<p>Repeat this cycle two or three times and you should have a good idea if you are likely to put in the effort when it counts, and if that effort is worth it. You should need very long to do this, just make sure you do each part in turn (think first of the difficulties, then the reasons).</p>\n<p>You might realise that when the chips are down and you are hungry, angry, lonely, tired or just having a bad day you are likely to give up. If this is the case (remember to be a bit pessimistic about this) then don\u2019t bother working on the goal. You have already decided that it is not worth the effort. If you think you cannot succeed when you are planning you will have no hope when some unexpected challenge pops up.</p>\n<p>It is also worth quickly thinking through an \u201cif-then\u201d reaction for your sticking points that you have identified. For example, if you are trying to run every day but you know some days you feel tired and find it difficult to start. In that case you could set the response to be \"<strong>If</strong> I ever feel too tired to start running <strong>then </strong>I will put my running shoes on and leave the house whether I actually run or not.\" The problem is likely to be self-control depletion not exhaustion, by getting to the point of actually running you have lowered the mental effort of starting.</p>\n<h3 id=\"2_3_Prevention_or_Promotion\">2.3 Prevention or Promotion</h3>\n<p>The different ways you can think about a goal change where you will put effort in to it. You can either have a goal where you must not fail (prevention) or a goal where you want to conquer (promotion). The way you phrase your goal can change it from one to the other and cause you to have more motivation when you need it. For example lets say you go rock climbing for fun with some friends, if you are worried that you are holding the group back so must train harder then your focus is prevention. If you want to beat everyone else then your focus is promotion.</p>\n<p>If you have a prevention focus then you get more motivation when you are failing but less when you are doing well. Taking the example of rock climbing, if you had a really bad training session where you were actually worse than before then you would feel like you had to train even harder because you really cannot afford to fail. On the other hand you will put less effort in when succeeding, after all your goal is simply not to fail and if you are doing great then you can afford to put in less effort.</p>\n<p>If you have a promotion focus you get more motivation when you are doing well but are likely to give up when failing. For example if you have had a great training session and managed to beat someone on a new climb then you will feel elated and want to do more. Yet if you do badly then you are likely to put less effort in and hence do even worse and give up. If you were trying to win but if looks like you can't then why bother to put the effort in.</p>\n<p>You can use this to your advantage. Think of whether you want more motivation when failing or succeeding and frame your goal accordingly. For any task you can choose promotion or prevention not both. If you decide you want a promotion goal and start to fail then you should realise it, feel bad about it and make a conscious decision to put more effort in.</p>\n<h2 id=\"3_Summary\">3 Summary</h2>\n<p>The subconscious gives you a motivation value for every thing you could do. You can change the motivation by working out why your subconscious is choosing distracting things and changing those factors. By spending a few minutes examining your goals you can increase the likelihood of achieving them. The three ways discussed were:</p>\n<ol>\n<li>\n<p>\"If-then\" triggers,</p>\n</li>\n<li>\n<p>Mental contrasting,</p>\n</li>\n<li>\n<p>Changing the wording of the goal to give either a prevention or promotion focus.</p>\n</li>\n</ol>\n<p>Remember that even with this type of goal setting you are vulnerable to self-control depletion. Sometimes you have bad days and on those days it will take a lot of mental effort and a lot of thinking through why you wanted to achieve the goal to actually succeed. The check-list below is meant to take the ideas above and form them into questions to improve the chances of succeeding.</p>\n<p>I suggest having a written list of goals somewhere where you will look at them regularly. For example, a bedside table where you can look at them morning and night. It depends on your routine. There is no point in having these triggers if they are not regularly refreshed in your mind. The best way to do that is to read them and perform them regularly.</p>\n<h3 id=\"3_1_Goal_Check_list\">3.1 Goal Check-list</h3>\n<ol>\n<li>\n<p>I want more motivation when [failing, succeeding] therefore focus will be [prevention, promotion]?</p>\n</li>\n<li>\n<p>It will be done when \u2026</p>\n</li>\n<li>\n<p>I am doing this because \u2026</p>\n</li>\n<li>\n<p>My sticking point will be \u2026</p>\n</li>\n<li>\n<p>I want this to be done because \u2026</p>\n</li>\n<li>\n<p>I risk failure when \u2026</p>\n</li>\n<li>\n<p>The trigger is \u2026</p>\n</li>\n<li>\n<p>The very first step is ...</p>\n</li>\n</ol>\n<h3 id=\"3_2_Distraction_Check_List\">3.2 Distraction Check List</h3>\n<ol>\n<li>\n<p>Notice you are becoming distracted.</p>\n</li>\n<li>\n<p>Examine each factor of the motivation equation in turn for both the distracting task and the task you want to perform.</p>\n</li>\n<li>\n<p>Think how to change the goal or your environment to change the factors in your favour.</p>\n</li>\n<li>\n<p>If you are regularly coming up against distractions for the same goal then re-evaluate the value and expectancy. It might be worth changing or dropping the goal.</p>\n</li>\n</ol>\n<h2 id=\"PostScript\">PostScript</h2>\n<p>The sources are from the list below (plus two years of small tweaks by myself):</p>\n<ul>\n<li>What Color is your parachute</li>\n<li><a href=\"https://www.stephencovey.com/7habits/7habits.php\" target=\"_blank\">The Seven Habits of Highly Effective People by Stephen Covey</a></li>\n<li><a href=\"http://www.heidigranthalvorson.com/\" target=\"_blank\">Succeed: How We Can Reach Our Goals by Heidi Grant Halvorson</a></li>\n<li>The Procrastination Eqn. --&nbsp;<a href=\"http://procrastinus.com/\" target=\"_blank\">http://procrastinus.com/</a></li>\n<li><a href=\"http://gettingresults.com/wiki/How_To_-_Set_Goals_and_Achieve_Them\" target=\"_blank\">http://gettingresults.com/wiki/How_To_-_Set_Goals_and_Achieve_Them</a></li>\n</ul>\n<p><em>I wrote this article before reading the more recent work on how mindset changes whether people perform as if self-control was a resource that could be depleted. I plan to make a new article that contains this information as well as a lot more on mindset and emotional responses but I want to test it out on the London LW community first.</em></p>\n<p><em>Although this is based in research it does diverge somewhat. It is more a measure of what has worked well for me. If you want the truth in the research read the research papers in the back of the books I have referenced.</em></p>\n<p>This post is also available on my personal <a href=\"http://kerspoon.com/blog/2013/goal-setting-and-achievement/\">blog</a></p>", "sections": [{"title": "1. How the Brain Decides what Task you Will Perform", "anchor": "1__How_the_Brain_Decides_what_Task_you_Will_Perform", "level": 1}, {"title": "1.1 Motivation", "anchor": "1_1_Motivation", "level": 2}, {"title": "1.2 Self Control", "anchor": "1_2_Self_Control", "level": 1}, {"title": "2. How to Set goals to increase changes of success", "anchor": "2__How_to_Set_goals_to_increase_changes_of_success", "level": 1}, {"title": "2.1 Triggers and Mechanical Starting Points", "anchor": "2_1_Triggers_and_Mechanical_Starting_Points", "level": 2}, {"title": "2.2 Mental Contrasting", "anchor": "2_2_Mental_Contrasting", "level": 2}, {"title": "2.3 Prevention or Promotion", "anchor": "2_3_Prevention_or_Promotion", "level": 2}, {"title": "3 Summary", "anchor": "3_Summary", "level": 1}, {"title": "3.1 Goal Check-list", "anchor": "3_1_Goal_Check_list", "level": 2}, {"title": "3.2 Distraction Check List", "anchor": "3_2_Distraction_Check_List", "level": 2}, {"title": "PostScript", "anchor": "PostScript", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-28T15:33:10.040Z", "modifiedAt": null, "url": null, "title": "MIRI strategy", "slug": "miri-strategy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.001Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ColonelMustard", "createdAt": "2013-10-07T00:51:38.473Z", "isAdmin": false, "displayName": "ColonelMustard"}, "userId": "Sf7RN5NSnaiEC4rAF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/re4RsZnPDEYueSAbf/miri-strategy", "pageUrlRelative": "/posts/re4RsZnPDEYueSAbf/miri-strategy", "linkUrl": "https://www.lesswrong.com/posts/re4RsZnPDEYueSAbf/miri-strategy", "postedAtFormatted": "Monday, October 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIRI%20strategy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIRI%20strategy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fre4RsZnPDEYueSAbf%2Fmiri-strategy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIRI%20strategy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fre4RsZnPDEYueSAbf%2Fmiri-strategy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fre4RsZnPDEYueSAbf%2Fmiri-strategy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 565, "htmlBody": "<p>Summary: I do not understand why MIRI hasn&rsquo;t produced a non-technical (pamphlet/blog post/video) to persuade people that UFAI is a serious concern. Creating and distributing this document should be MIRI&rsquo;s top priority.</p>\n<p>If you want to make sure the first AGI is FAI, one way to do so is to be the first to create an AI, and ensure it is FAI. Another is to persuade people that UFAI is a legitimate concern, and do so in large numbers. Ideally this would become a real concern, so nobody runs into the trap of Eliezer<sub style=\"font-family: mceinline;\">1999ish</sub> of &ldquo;I&rsquo;m going to build an AI and see how it works&rdquo;.</p>\n<p>1) is tough for an organisation of MIRI&rsquo;s size. 2) is a realistic goal. It benefits from:&nbsp;</p>\n<p>Funding:&nbsp;MIRI&rsquo;s funding almost certainly goes up if more people are concerned with AI x-risk. Ditto FHI.<br />Scalability: If MIRI has a new math finding, that's one new theorem. If MIRI creates a convincing demonstration that we have to worry about AI, spreading this message to a million people is plausible.<br />Partial goal completion: making a math breakthrough that reduces the time to AI might be counter-productive. Persuading an additional person of the dangers of UFAI <a href=\"https://www.google.com.sg/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Flesswrong.com%2Flw%2F1e%2Fraising_the_sanity_waterline%2F&amp;ei=8-JtUuGzFoXL0QWquIHgCA&amp;usg=AFQjCNHjQD5krYO1OJC3ShtBy0gDJI3YRg&amp;bvm=bv.55123115,d.d2k\" target=\"_blank\">raises the sanity waterline</a>.<br />Task difficulty: creating an AI is hard. Persuading people that &ldquo;UFAI is a possible extinction risk. Take it seriously&rdquo; is nothing like as difficult. (I was persuaded of this in about 20 minutes of conversation.)</p>\n<p>One possible response is &ldquo;it&rsquo;s not possible to persuade people without math backgrounds, training in rationality, engineering degrees, etc&rdquo;. To which I reply: what&rsquo;s the data supporting that hypothesis? How much effort has MIRI expended in trying to explain to intelligent non-LW readers what they&rsquo;re doing and why they&rsquo;re doing it? And what were the results?</p>\n<p>Another possible response is &ldquo;We have done this, and it's available on our website. Read the Five Theses&rdquo;. To which I reply: Is this is in the ideal form to persuade a McKinsey consultant who&rsquo;s never read Less Wrong? If an entrepreneur with net worth $20m but no math background wants to donate to the most efficient charity he finds, would he be convinced? What efforts has MIRI made to test the hypothesis that <a href=\"https://www.google.com.sg/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fintelligence.org%2F2013%2F05%2F05%2Ffive-theses-two-lemmas-and-a-couple-of-strategic-implications%2F&amp;ei=JuNtUoXHBMK80QX7koCYCA&amp;usg=AFQjCNGpBFtt1qtIWTITWOoo_Jcz124InA&amp;bvm=bv.55123115,d.d2k\" target=\"_blank\">the Five Theses</a>, or <a href=\"http://intelligence.org/files/IE-EI.pdf\" target=\"_blank\">Evidence and Import</a>, or any other document, has been tailored to optimise the chance of convincing others?<br />(Further &ndash; if MIRI _does_ think this is as persuasive as it can possibly be, why doesn't&nbsp;it shift focus to get the Five Theses read by as many people as possible?)</p>\n<p>Here&rsquo;s one way to go about accomplishing this. Write up an explanation of the concerns MIRI has and how it is trying to allay them, and do so in clear English. (The Five Theses are available in Up-Goer Five form. Writing them in language readable by the average college graduate should be a cinch compared to that). Send it out to a few of the target market and find the points that could be expanded, clarified, or made more convinced. Maybe provide two versions and see which one gets the most positive response. Continue this process until the document has been through a series of iterations and shows no signs of improvement. Then shift focus to getting that link read by as many people as possible. Ask all of MIRI&rsquo;s donors, all LW readers, HPMOR subscribers, friends and family etc, to forward that one document to their friends.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "re4RsZnPDEYueSAbf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 3, "extendedScore": null, "score": 1.3988591871430657e-06, "legacy": true, "legacyId": "24500", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-28T15:42:08.514Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup Bolder CO", "slug": "meetup-meetup-bolder-co-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yakurbe0112", "createdAt": "2012-07-01T23:40:26.855Z", "isAdmin": false, "displayName": "yakurbe0112"}, "userId": "xXr6Jngw57uMXveQ9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cyN4AXs5tYA4oEME5/meetup-meetup-bolder-co-0", "pageUrlRelative": "/posts/cyN4AXs5tYA4oEME5/meetup-meetup-bolder-co-0", "linkUrl": "https://www.lesswrong.com/posts/cyN4AXs5tYA4oEME5/meetup-meetup-bolder-co-0", "postedAtFormatted": "Monday, October 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%20Bolder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%20Bolder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyN4AXs5tYA4oEME5%2Fmeetup-meetup-bolder-co-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%20Bolder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyN4AXs5tYA4oEME5%2Fmeetup-meetup-bolder-co-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyN4AXs5tYA4oEME5%2Fmeetup-meetup-bolder-co-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sv'>Meetup Bolder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 October 2013 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Old Chicago 1102 Pearl St, Boulder, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last time we decided that the topic for this meeting would be what we want from lesswrong and what we should do to make it happen.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sv'>Meetup Bolder CO</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cyN4AXs5tYA4oEME5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.398867724636014e-06, "legacy": true, "legacyId": "24505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup_Bolder_CO\">Discussion article for the meetup : <a href=\"/meetups/sv\">Meetup Bolder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 October 2013 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Old Chicago 1102 Pearl St, Boulder, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last time we decided that the topic for this meeting would be what we want from lesswrong and what we should do to make it happen.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup_Bolder_CO1\">Discussion article for the meetup : <a href=\"/meetups/sv\">Meetup Bolder CO</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup Bolder CO", "anchor": "Discussion_article_for_the_meetup___Meetup_Bolder_CO", "level": 1}, {"title": "Discussion article for the meetup : Meetup Bolder CO", "anchor": "Discussion_article_for_the_meetup___Meetup_Bolder_CO1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-29T23:04:25.408Z", "modifiedAt": null, "url": null, "title": "Why didn't people (apparently?) understand the metaethics sequence?", "slug": "why-didn-t-people-apparently-understand-the-metaethics", "viewCount": null, "lastCommentedAt": "2021-05-08T17:38:35.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KZfoF7MdqJPcrhXaX/why-didn-t-people-apparently-understand-the-metaethics", "pageUrlRelative": "/posts/KZfoF7MdqJPcrhXaX/why-didn-t-people-apparently-understand-the-metaethics", "linkUrl": "https://www.lesswrong.com/posts/KZfoF7MdqJPcrhXaX/why-didn-t-people-apparently-understand-the-metaethics", "postedAtFormatted": "Tuesday, October 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20didn't%20people%20(apparently%3F)%20understand%20the%20metaethics%20sequence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20didn't%20people%20(apparently%3F)%20understand%20the%20metaethics%20sequence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZfoF7MdqJPcrhXaX%2Fwhy-didn-t-people-apparently-understand-the-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20didn't%20people%20(apparently%3F)%20understand%20the%20metaethics%20sequence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZfoF7MdqJPcrhXaX%2Fwhy-didn-t-people-apparently-understand-the-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZfoF7MdqJPcrhXaX%2Fwhy-didn-t-people-apparently-understand-the-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>There seems to be a widespread impression that <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">the metaethics sequence</a>&nbsp;was not very&nbsp;successful as an explanation of Eliezer Yudkowsky's views. <a href=\"http://wiki.lesswrong.com/wiki/Sequences#The_Metaethics_Sequence\">It even says so on the wiki</a>. And frankly, I'm puzzled by this... hence the \"apparently\" in this post's title. When I read the metaethics sequence, it seemed to make perfect sense to me. I can think of a couple things that may have made me different from the average OB/LW reader in this regard:</p>\n<ol>\n<li>I read <a href=\"/lw/y4/\">Three Worlds Collide</a>&nbsp;<em>before </em>doing my systematic read-through of the sequences.</li>\n<li>I have a background in academic philosophy, so I had a similar thought to <a href=\"/lw/435/what_is_eliezer_yudkowskys_metaethical_theory/3foq\">Richard Chapell's</a>&nbsp;linking of Eliezer's metaethics to rigid designators independently of Richard.</li>\n</ol>\n<div>Reading the comments on the metaethics sequence, though, hasn't enlightened me about what exactly people had a problem with, aside from a lot of arguing about definitions over whether Eliezer counts as a relativist.</div>\n<div><br /></div>\n<div>What's going on here? I ask mainly because I'm thinking of trying to write a post (or sequence?) explaining the metaethics sequence, and I'm wondering what points I should address, what issues I should look out for, etc.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1, "Z8wZZLeLMJ3NSK7kR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KZfoF7MdqJPcrhXaX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 23, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "24514", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 231, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-29T23:54:14.890Z", "modifiedAt": null, "url": null, "title": "Bayesianism for Humans", "slug": "bayesianism-for-humans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.184Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XCwPQzoe9hmQikMiY/bayesianism-for-humans", "pageUrlRelative": "/posts/XCwPQzoe9hmQikMiY/bayesianism-for-humans", "linkUrl": "https://www.lesswrong.com/posts/XCwPQzoe9hmQikMiY/bayesianism-for-humans", "postedAtFormatted": "Tuesday, October 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesianism%20for%20Humans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesianism%20for%20Humans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCwPQzoe9hmQikMiY%2Fbayesianism-for-humans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesianism%20for%20Humans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCwPQzoe9hmQikMiY%2Fbayesianism-for-humans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXCwPQzoe9hmQikMiY%2Fbayesianism-for-humans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 930, "htmlBody": "<p>Recently, I completed my first systematic read-through of <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a>. One of the biggest effects this had on me was considerably warming my attitude towards Bayesianism. Not long ago, if you'd asked me my opinion of Bayesianism, I'd probably have said something like, \"Bayes' theorem is all well and good when you know what numbers to plug in, but all too often you don't.\"</p>\n<p>Now I realize that that objection is based on a misunderstanding of Bayesianism, or at least Bayesianism-as-advocated-by-Eliezer-Yudkowsky. <a href=\"/lw/sg/when_not_to_use_probabilities/\">\"When (Not) To Use Probabilities\"</a>&nbsp;is all about this issue, but a cleaner expression of Eliezer's true view may be this quote from <a href=\"/lw/mt/beautiful_probability/\">\"Beautiful Probability\"</a>:</p>\n<blockquote>\n<p>No, you can't always do the exact Bayesian calculation for a problem. &nbsp;Sometimes you must seek an approximation; often, indeed. &nbsp;This doesn't mean that probability theory has ceased to apply, any more than your inability to calculate the aerodynamics of a 747 on an atom-by-atom basis implies that the 747 is not made out of atoms. &nbsp;Whatever approximation you use, it works to the extent that it approximates the ideal Bayesian calculation - and fails to the extent that it departs.</p>\n</blockquote>\n<p>The practical upshot of seeing Bayesianism as an ideal to be approximated, I think, is this: <strong>you should avoid engaging in any reasoning that's demonstrably nonsensical in Bayesian terms. </strong>Furthermore,<strong>&nbsp;Bayesian reasoning can be fruitfully mined for heuristics that are useful in the real world.</strong>&nbsp;That's an idea that actually has real-world applications for human beings, hence the title of this post, \"Bayesianism for Humans.\"</p>\n<p><a id=\"more\"></a>Here's my attempt to make an initial list of more directly applicable corollaries to Bayesianism. Many of these corollaries are non-obvious, yet eminently sensible once you think about them, which I think makes for a far better argument for Bayesianism than Dutch Book-type arguments with little real-world relevance. Most (but not all) of the links are to posts within the sequences, which hopefully will allow this post to double as a decent introductory guide to the parts of the sequences that explain Bayesianism.</p>\n<ul>\n<li>Watch out for <a href=\"http://en.wikipedia.org/wiki/Base_rate_fallacy\">base rate neglect</a>. It's why even experts screw up in one of the standard problems <a href=\"http://yudkowsky.net/rational/bayes/\">used to explain Bayes' Theorem</a>. Even when you don't know what the base rate is, there are times when you ought to expect it to be low, particularly if you're trying to detect a rare phenomenon <a href=\"http://books.google.com/books?id=x1Q9TxhYA3sC&amp;pg=PA128&amp;lpg=PA128&amp;dq=cory+doctorow+super-aids&amp;source=bl&amp;ots=uNFt0vGivo&amp;sig=1lTHCbnIu5mX47FxSLeIcnhmJS0&amp;hl=en&amp;sa=X&amp;ei=G59uUqLPL8_d4AO8zYGgDw&amp;ved=0CDcQ6AEwAQ#v=onepage&amp;q=cory%20doctorow%20super-aids&amp;f=false\">like a new disease or IDing terrorists</a>.</li>\n<li><a href=\"/lw/ih/\">Absence of Evidence is Evidence of Absence</a>. If observing E would increase the probability of H, observing not-E should decrease the probability of H. E and not-E sure as hell shouldn't <em>both </em>increase the probability of H.</li>\n<li>Relatedly, there's <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a>: roughly, if you think more evidence would probably increase your confidence in a belief, you should think there's small chance it would cause a larger change in the opposite direction.&nbsp;</li>\n<li>Conservation of expected evidence means that a rational person can't seek to confirm their beliefs, only to test them. If your expectation of how a test will affect your belief violates conservation of expected evidence, you should update your beliefs <em>now </em>based on how you expect the test to turn out.</li>\n<li>Also related closely related to the above, when it comes to gathering evidence,&nbsp;<a href=\"/lw/ju/rationalization/\">\"If you know your destination, you are already there.\"</a>&nbsp;Evidence gathered through a biased method designed to turn out one way is worthless.</li>\n<li>On the other hand, <a href=\"/lw/1ph/youre_entitled_to_arguments_but_not_that/\">you can't dismiss a hypothesis due to a lack of a <em>particular </em>piece of evidence that you wouldn't expect to have even if the hypothesis were true</a>.</li>\n<li>Even when an argument on one side is overcome by a stronger argument on the other side, you still need to take the first argument into account when assigning confidence to your belief, lest you <a href=\"/lw/ik/one_argument_against_an_army/\">gradually dismiss each piece of evidence on the other side because no piece is (individually) as strong as the one piece of evidence on your side.</a></li>\n<li><a href=\"/lw/jk/burdensome_details/\">Burdensome Details</a>: Every detail added to a claim makes it less probable.</li>\n<li><a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">Reversed Stupidity Is Not Intelligence</a>:&nbsp;If you would expect to see flying saucer cults regardless of whether or not extraterrestrials were visiting us, flying saucer cults are not evidence against extraterrestrials.</li>\n<li>Don't get caught up in arguing about definitions when you should be looking at what's actually indicative of what. (I take it that that's the Bayesian take-away from&nbsp;<a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">the sequence on words</a>, though Eliezer doesn't quite put it that way.)</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence#Rationality_and_Science\">Rationality and the rules of Science are not the same thing.</a> The latter are <a href=\"/lw/qb/science_doesnt_trust_your_rationality/\">social rules designed to make science work in spite of the irrationality of its practitioners</a>. They're not the same as the rules of rationality an ideal reasoner would follow.&nbsp;</li>\n<li>An example of the science vs. rationality issue: to an ideal reasoner, <a href=\"/lw/iy/my_wild_and_reckless_youth/\">successful retrospective predictions are as valuable as prospective predictions</a>. (To us non-ideal reasoners, prospective predictions are can be extra valuable as protection against fooling ourselves, but we still shouldn't discount retrospective predictions entirely.)&nbsp;</li>\n<li><a href=\"/lw/mj/rational_vs_scientific_evpsych/\">This is also reason to be careful about dismissing evo psych claims as just-so stories.</a></li>\n<li>Another example: contrary to old-fashioned statistical procedure, <a href=\"/lw/mt/beautiful_probability/\">a researcher's state of mind shouldn't affect the significance of their results</a>.</li>\n<li>Last example is from <a href=\"/lw/iu0/trusting_expert_consensus/\">a post of my own</a>:&nbsp;Expert opinion should be discounted when the expert's opinions could be predicted solely from information not relevant to the truth of the claims. But when the state of expert opinion surprises you, beware discounting their opinions just because you can think of <em>some </em>explanation for why they'd be wrong.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XCwPQzoe9hmQikMiY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 89, "extendedScore": null, "score": 0.000221, "legacy": true, "legacyId": "24491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 89, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AJ9dX59QXokZb35fk", "bkSkRwo9SRYxJMiSY", "mnS2WYLCGJP2kQkRn", "jiBFC7DcCrZjGmZnJ", "SFZoEBpLo9frSJGkc", "vqbieD9PHG8RRJddu", "WN73eiLQkuDtSC8Ag", "Yq6aA4M3JKWaQepPJ", "qNZM3EGoE5ZeMdCRt", "FaJaCgqBKphrDzDSj", "5bJyRMZzwMov5u3hW", "DwtYPRuCxpXTrzG9m", "pL3To6G42AeihNtaN", "R8YpYTq8LoD3k948L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-30T06:35:32.743Z", "modifiedAt": null, "url": null, "title": "Mental Context for Model Theory", "slug": "mental-context-for-model-theory", "viewCount": null, "lastCommentedAt": "2019-09-17T00:28:40.646Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MG8Yhsxqu9JY4xRPr/mental-context-for-model-theory", "pageUrlRelative": "/posts/MG8Yhsxqu9JY4xRPr/mental-context-for-model-theory", "linkUrl": "https://www.lesswrong.com/posts/MG8Yhsxqu9JY4xRPr/mental-context-for-model-theory", "postedAtFormatted": "Wednesday, October 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mental%20Context%20for%20Model%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMental%20Context%20for%20Model%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG8Yhsxqu9JY4xRPr%2Fmental-context-for-model-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mental%20Context%20for%20Model%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG8Yhsxqu9JY4xRPr%2Fmental-context-for-model-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMG8Yhsxqu9JY4xRPr%2Fmental-context-for-model-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2370, "htmlBody": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After <a>my</a> <a href=\"/lw/ii0/book_review_heuristics_and_biases_miri_course_list/\">first</a> <a href=\"/lw/il1/book_review_cognitive_science_miri_course_list/\">four</a> <a href=\"/lw/ioo/book_review_basic_category_theory_for_computer/\">book</a>&nbsp;<a href=\"/lw/ir6/book_review_na%C3%AFve_set_theory_miri_course_list/\">reviews</a>&nbsp;I took a week off, followed up on some dangling questions, and upkept other side projects. Then I dove into <em>Model Theory</em>, by Chang and Keisler.</p>\n<p>It has been three weeks. I have gained a decent foundation in model theory (by my own assessment), but I have not come close to completing the textbook. There are a number of other topics I want to touch upon before December, so I'm putting <em>Model Theory</em> aside for now. I'll be revisiting it in either January or March to finish the job.</p>\n<p>In the meantime, I do not have a complete book review for you. Instead, this is the first of three posts on my experience with model theory thus far.</p>\n<p>This post will give you some framing and context for model theory. I had to hop a number of conceptual hurdles before model theory started making sense &mdash; this post will contain some pointers that I wish I'd had three weeks ago. These tips and realizations are somewhat general to learning any logic or math; hopefully some of you will find them useful.</p>\n<p>Shortly, I'll post a summary of what I've learned so far. For the casual reader, this may help demystify some heavily advanced parts of the <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Heavily Advanced Epistemology</a> sequence (if you find it mysterious), and it may shed some light on some of the recent MIRI papers. On a personal note, there's a lot I want to write down &amp; solidify before moving on.</p>\n<p>In follow-up post, I'll discuss my experience struggling to learn something difficult on my own&nbsp;&mdash; model theory has required significantly more cognitive effort than did the previous textbooks.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"whatdoyoumean\">Between what was meant and what was said</h2>\n<p>Model theory is an abstract branch of mathematical logic, which itself is already too abstract for most. So allow me to motivate model theory a bit.</p>\n<p>At its core, model theory is the study of what you <em>said</em>, as opposed to what you <em>meant</em>. To give some intuition for this, I'll re-tell an overtold story about an ancient branch of math.</p>\n<p>In olden times, Euclid built Geometry upon five axioms:</p>\n<ol>\n<li>You can draw a straight line segment between two points.</li>\n<li>You can extend line segments into infinitely straight lines.</li>\n<li>You can draw a circle from a straight line segment, with the center at one end and radius the line segment.</li>\n<li>All right angles are congruent.</li>\n<li>If two lines are drawn which intersect a third in such a way that the sum of the inner angles on one side is less than two right angles, then the two lines inevitably must intersect each other on that side if extended far enough.</li>\n</ol>\n<p>One of these things is not like the other. The fifth axiom is the only one which requires some effort to understand. Intuitively, it states that parallel lines do not intersect. This statement irked Euclid for reasons apart from the ugliness of the axiom.</p>\n<p>The fact that parallel lines do not intersect seems like it should follow from the definition of lines and angles. It doesn't seem like something we should have to specify <em>in addition</em>. That we must <em>assume</em>&nbsp;parallel lines do not intersect (rather than <em>proving</em>&nbsp;it) was long seen as a wart on geometry.</p>\n<p>This wart irked mathematicians for millennia, until finally it was discovered that the fifth axiom is independent of the other four. You can build consistent systems where parallel lines intersect. You can build consistent systems where they diverge.</p>\n<p>This seemed crazy, at the time: parallel straight lines cannot diverge! Surely, a geometry in which they do is absurd!</p>\n<p>The problem is that mathematicians were imagining \"straight lines\" in their head that did not match the mathematical objects specified by the first four axioms of Euclid.</p>\n<p>This mistake was invited by names which Euclid chose. \"Straight lines\" invoke a mental image that is more specific than that which the axioms describe. If you detach the provocative words from the axioms</p>\n<ol>\n<li>You can make a <code>LUME</code> between any two <code>PTARS</code></li>\n<li>You can extend a <code>LUME</code> into a <code>SLUME</code></li>\n<li>&hellip;</li>\n</ol>\n<p>and so on, then it's much easier to understand that the <code>LUME</code>s which Euclid's axioms describe may not match up with the image of a \"straight line\" in your head. It is much easier to understand that there may be interpretations of <code>LUME</code> which do not obey the fifth postulate.</p>\n<p>In fact, if you take Euclid's first four postulates, there are many possible interpretations in which \"straight line\" takes on a multitude of meanings. This ability to disconnect the <em>intended</em> interpretation from the <em>available </em>interpretations is the bedrock of model theory. Model theory is the study of <em>all</em> interpretations of a theory, not just the ones that the original author intended.</p>\n<p>Of course, model theory isn't really about finding surprising new interpretations &mdash; it's much more general than that. It's about exploring the breadth of interpretations that a given theory makes available. It's about discerning properties that hold in all possible interpretations of a theory. It's about discovering how well (or poorly) a given theory constrains its interpretations. It's a toolset used to discuss interpretations in general.</p>\n<p>At its core, model theory is the study of what a mathematical theory actually says, when you strip the intent from the symbols.</p>\n<h2 id=\"ironwalls\">Iron walls</h2>\n<p>Before you can do model theory, you have to erect iron walls between four different concepts.</p>\n<ol>\n<li>Logics</li>\n<li>Languages</li>\n<li>Theories</li>\n<li>Models</li>\n</ol>\n<h3 id=\"logics\" style=\"color:gray\">Logics</h3>\n<p>A logic is a formal system for building and manipulating sentences. Traditionally, this logic defines a number of symbols (<code>( ) &and; &not; &forall; &exist; &equiv; &nu; '</code>, for example) and rules for building sentences from those symbols.</p>\n<p>Note that <em>you cannot generate sentences from a logic alone</em>. Rather, you <em>use</em> a logic to generate sentences <em>from</em> a language.</p>\n<p>Also, remember that the rules of a logic are <em>syntactic</em>, such as \"if <code>&phi;</code> is a sentence then <code>(&not;&phi;)</code> is a sentence\".</p>\n<p>Finally, remember that logics are just rules for generating sentences. A logic is perfectly happy to generate sentences shaped like <code>x&and;(&not;x)</code>, in spite of all your protests about contradictions.</p>\n<h3 id=\"languages\" style=\"color:gray\">Languages</h3>\n<p>A language is a collection of symbols. <em>From</em> those symbols, <em>using</em> a logic, you can start generating sentences.</p>\n<p>For example, in the propositional logic, using the language <code>{x, y}</code>, the string <code>hello</code> is surely not a sentence (for it fails to use the appropriate symbols). Nor is the string&nbsp;<code>&not;xy</code>&nbsp;a sentence: it fails to follow the rules of the logic. <code>((&not;x)&and;y)</code> is a sentence, for it uses the appropriate symbols and follows the given rules.</p>\n<p>Many results in model theory are achieved by holding the logic fixed and varying the language, so it's essential that these concepts be distinct in your mind.</p>\n<h3 id=\"theories\" style=\"color:gray\">Theories</h3>\n<p>A theory is a collection of sentences written in one language. For example, in the language <code>{&le;}</code> under first-order logic, we can discuss the theory</p>\n<ol>\n<li><code>(&forall;x)(x&le;x)</code></li>\n<li><code>(&forall;xy)(x&le;y)&and;(y&le;x)&rarr;(y&equiv;x)</code></li>\n<li><code>(&forall;xyz)(x&le;y)&and;(y&le;z)&rarr;(x&le;z)</code></li>\n</ol>\n<p>which is the theory of <em>order</em>. (The axioms above are reflexivity, antisymmetry, and transitivity).</p>\n<p>Remember that a theory is just a set of sentences drawn from all available sentences. These sentences aren't particularly special unless you make them special. Sentences like <code>(&exist;x)&not;(x&le;x)</code> are fine sentences built from the language <code>{&le;}</code>, even though they directly contradict the theory. Theories don't affect the sentences of a language &mdash; they're just a grab-bag of some sentences that seemed interesting to someone.</p>\n<h3 id=\"models\" style=\"color:gray\">Models</h3>\n<p>A model is an <em>interpretation</em> of the sentences generated by a language. A model is a structure which assigns a truth value to each sentence generated by some language under some logic.</p>\n<p>(More specifically, it's a structure that assigns binary values to sentences in such a way that we're justified in the name \"truth value\": for example, we require that a model says &phi;&nbsp;is true if and only if it says that&nbsp;&not;&phi; is false, and so on.)</p>\n<p>Only once we start interpreting sentences is it meaningful to talk about valid or refutable sentences. Once you have a model of <code>{&le;}</code> that happens to say that the axioms 1, 2, and 3 above are true, <em>then</em> you can start talking about how the theory of order rules out the sentence <code>(&exist;x)&not;(x&le;x)</code> &mdash; because there is no model of the theory of order which is also a model of this sentence.</p>\n<p>(You can actually talk about how <code>(&exist;x)&not;(x&le;x)</code> is inconsistent with the theory of order without appealing to model theory, but I find it helpful to treat everything as raw symbols until interpreted by a model.)</p>\n<p>To give a concrete example, in <em>first order logic,</em>&nbsp;using the <em>language </em>{S, +, *, 0}, the <em>theory</em>&nbsp;of arithmetic is the theory laid out by the [Peano axioms](<a href=\"http://en.wikipedia.org/wiki/Peano_axioms#First-order_theory_of_arithmetic\">http://en.wikipedia.org/wiki/Peano_axioms#First-order_theory_of_arithmetic</a>). The actual natural numbers zero, one, two, ... are a model of this theory (where zero is the interpretation of 0, one is the interpretation of S0, etc.).</p>\n<p>Also, it's worth noting that&nbsp;<em>any</em>&nbsp;object that interprets sentences and follows the rules of the logic qualifies as a model. There are often many non-isomorphic objects that interpret the same sentences in the same way. For example, rational numbers and real numbers are models of group theory that agree on every sentence in the language of groups, despite being different models.</p>\n<p>Distinctions between these four points is something that seems obvious to me in hindsight, but I explicitly remember expending cognitive effort to separate these concepts mentally, so there you go. Make sure these distinctions are wrought in iron before attempting model theory.</p>\n<h2 id=\"therighttousethatname\">The Right to use a name</h2>\n<p>There's something about math education in general that has troubled me for quite some time, and which I'm finally able to articulate. It's quite possible that this is a personal nit, since nobody else seems to care&nbsp;&mdash;&nbsp;but I'll share it anyway.</p>\n<p>Many math textbooks treat properties that <em>justify a name</em>&nbsp;of a thing as statements about the thing <em>after</em> naming it.</p>\n<p>This is a little abstract, so I'll make a silly example. Imagine someone is trying to show that, in category theory, composition of arrows is associative. They shouldn't appeal to visual intuition or any diagrams of arrows.</p>\n<p>The concept that following arrows is an associative operation is so ingrained in the concept of \"arrow\" that it's difficult to describe the property in English without sounding dumb.</p>\n<blockquote>\n<p>If you move from A to B, then move B-to-D-through-C in one step, and if I follow the same paths but move A-to-C-through-B in one step and then from C to D, then we will end up at the same place.</p>\n</blockquote>\n<p>This property of arrows is so stupidly obvious that the statement is frustrating. Further, it hides the following fact:</p>\n<p><em>Associative composition between thingies is&nbsp;</em><em>something we must have before we're justified in calling the thingies \"Arrows\"</em>.</p>\n<p>Associative composition is what <em>allows</em> you to use the name \"arrow\" and draw visual diagrams. You can't appeal to my intuition about \"arrows\" to show that composition is associative. It's the other way around! Only&nbsp;<em>after</em>&nbsp;you show that your thingies have associative composition are you allowed to label them as \"arrows\".</p>\n<p>As another example, the axioms of order (above) are what <em>allow</em> us to use the <code>&le;</code> symbol, which appeals to our intuitive idea of order. Really, it's more honest to say \"We have a binary relation <code>R</code>, satisfying</p>\n<ol>\n<li><code>(&forall;x)R(xx)</code></li>\n<li><code>(&forall;xy)R(xy)&and;R(yx)&rarr;(y&equiv;x)</code></li>\n<li><code>(&forall;xyz)R(xy)&and;R(yz)&rarr;R(xz)</code></li>\n</ol>\n<p>which <em>justifies</em> our use of the <code>&le;</code> symbol for <code>R</code>.\"</p>\n<p>I imagine this is not a problem for experienced mathematicians, for whom it goes without saying that you must formally specify (or disregard) all intuitive baggage that comes attached to the names. However, I remember distinctly a number of times when I gnashed my teeth with boredom as teachers made obvious statements (<em>of course</em> <code>&le;</code> is reflexive, why do we even need to say this?), simply because I didn't understand this idea.</p>\n<p>I mention this because the first few sections of the&nbsp;<em>Model Theory</em>&nbsp;textbook make statements that seem quite obvious. It's easy to grind your teeth and say \"duh, hurry up\". It's a little harder to understand exactly why such things must be said. In that light, I think this is a good piece of advice for learning mathematics in general:</p>\n<p>If you find yourself wondering why a statement must be said, check whether the statement is justifying any names.</p>\n<p><span style=\"color: #808080; font-size: 15px; font-weight: bold;\">Binding meaning</span></p>\n<p>The early parts of <em>Model Theory</em>&nbsp;will go down much easier if you realize that they're binding logical symbols to the appropriate meaning (and thus justifying the name \"model\").</p>\n<p>For example, when we state \"M models&nbsp;<code>&phi;&and;&psi;</code>&nbsp;if and only if it models&nbsp;<code>&phi;</code>&nbsp;and it models&nbsp;<code>&psi;</code>\", it's easy to say \"well duh\". It's a little harder to understand that <em>this is the mechanism by which the symbol</em>&nbsp;<code>&and;</code>&nbsp;<em>is bound to the interpretation \"and\".</em></p>\n<p>Also, note that the ability to distinguish between \"the symbol&nbsp;<code>+</code>&nbsp;in the language L\" from \"the addition function as interpreted by the model M\" is absolutely crucial.</p>\n<h2 id=\"totality\">Totality</h2>\n<p>Something that kept on biting me was this: <em>Models of first-order logic are \"total\"</em>. They have something to say about <em>every</em> sentence in a language. Even where a <em>theory</em> is incomplete, any individual <em>model</em> is \"complete\". A model of first-order logic interprets function symbols by total functions and relations by set-theoretic relations. The relationship <code>\u22a7</code> is total: for every sentence, either <code>M\u22a7&phi;</code> or <code>M\u22a7&not;&phi;</code>.</p>\n<p>This is a point where my intuitive notion of \"models as interpretations\" departed from the actual mathematical objects under consideration &mdash; functions are firmly partial-by-default in my mind's eye.</p>\n<p>It's important to hold firm the distinction between \"model\" and \"theory\" here. Remember that the number<em> theory</em>&nbsp;is incomplete, while the standard<em> model</em>&nbsp;of number theory is the one that picks \"true\" for all G&ouml;del sentences, has no infinite numbers, etc. (The difficulties in pinpointing such a model is exactly what the incompleteness theorem is all about.)</p>\n<p>Be aware that the mathematical definition of a model may not match your intuitive idea of \"a structure which interprets a theory\", <em>especially</em>&nbsp;if you're coming from computer science (or other constructive fields).</p>\n<hr />\n<p>None of this is particularly novel. Rather, this is a collection of distinctions and clarifications that would have made my life a bit easier when beginning the textbook.</p>\n<p>In my case, I didn't have any of these concepts wrong, per se &mdash; rather, I had them fuzzy. The above distinctions were not yet fleshed out in my mind. This post provides a context for model theory; a taste of the type of thinking you must be ready to think.</p>\n<p>I was originally going to use this as context for what I've learned in model theory so far, but this post took longer than expected. I'll follow up tomorrow.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MG8Yhsxqu9JY4xRPr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 90, "extendedScore": null, "score": 0.000234, "legacy": true, "legacyId": "24521", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 91, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After <a>my</a> <a href=\"/lw/ii0/book_review_heuristics_and_biases_miri_course_list/\">first</a> <a href=\"/lw/il1/book_review_cognitive_science_miri_course_list/\">four</a> <a href=\"/lw/ioo/book_review_basic_category_theory_for_computer/\">book</a>&nbsp;<a href=\"/lw/ir6/book_review_na%C3%AFve_set_theory_miri_course_list/\">reviews</a>&nbsp;I took a week off, followed up on some dangling questions, and upkept other side projects. Then I dove into <em>Model Theory</em>, by Chang and Keisler.</p>\n<p>It has been three weeks. I have gained a decent foundation in model theory (by my own assessment), but I have not come close to completing the textbook. There are a number of other topics I want to touch upon before December, so I'm putting <em>Model Theory</em> aside for now. I'll be revisiting it in either January or March to finish the job.</p>\n<p>In the meantime, I do not have a complete book review for you. Instead, this is the first of three posts on my experience with model theory thus far.</p>\n<p>This post will give you some framing and context for model theory. I had to hop a number of conceptual hurdles before model theory started making sense \u2014 this post will contain some pointers that I wish I'd had three weeks ago. These tips and realizations are somewhat general to learning any logic or math; hopefully some of you will find them useful.</p>\n<p>Shortly, I'll post a summary of what I've learned so far. For the casual reader, this may help demystify some heavily advanced parts of the <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Heavily Advanced Epistemology</a> sequence (if you find it mysterious), and it may shed some light on some of the recent MIRI papers. On a personal note, there's a lot I want to write down &amp; solidify before moving on.</p>\n<p>In follow-up post, I'll discuss my experience struggling to learn something difficult on my own&nbsp;\u2014 model theory has required significantly more cognitive effort than did the previous textbooks.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Between_what_was_meant_and_what_was_said\">Between what was meant and what was said</h2>\n<p>Model theory is an abstract branch of mathematical logic, which itself is already too abstract for most. So allow me to motivate model theory a bit.</p>\n<p>At its core, model theory is the study of what you <em>said</em>, as opposed to what you <em>meant</em>. To give some intuition for this, I'll re-tell an overtold story about an ancient branch of math.</p>\n<p>In olden times, Euclid built Geometry upon five axioms:</p>\n<ol>\n<li>You can draw a straight line segment between two points.</li>\n<li>You can extend line segments into infinitely straight lines.</li>\n<li>You can draw a circle from a straight line segment, with the center at one end and radius the line segment.</li>\n<li>All right angles are congruent.</li>\n<li>If two lines are drawn which intersect a third in such a way that the sum of the inner angles on one side is less than two right angles, then the two lines inevitably must intersect each other on that side if extended far enough.</li>\n</ol>\n<p>One of these things is not like the other. The fifth axiom is the only one which requires some effort to understand. Intuitively, it states that parallel lines do not intersect. This statement irked Euclid for reasons apart from the ugliness of the axiom.</p>\n<p>The fact that parallel lines do not intersect seems like it should follow from the definition of lines and angles. It doesn't seem like something we should have to specify <em>in addition</em>. That we must <em>assume</em>&nbsp;parallel lines do not intersect (rather than <em>proving</em>&nbsp;it) was long seen as a wart on geometry.</p>\n<p>This wart irked mathematicians for millennia, until finally it was discovered that the fifth axiom is independent of the other four. You can build consistent systems where parallel lines intersect. You can build consistent systems where they diverge.</p>\n<p>This seemed crazy, at the time: parallel straight lines cannot diverge! Surely, a geometry in which they do is absurd!</p>\n<p>The problem is that mathematicians were imagining \"straight lines\" in their head that did not match the mathematical objects specified by the first four axioms of Euclid.</p>\n<p>This mistake was invited by names which Euclid chose. \"Straight lines\" invoke a mental image that is more specific than that which the axioms describe. If you detach the provocative words from the axioms</p>\n<ol>\n<li>You can make a <code>LUME</code> between any two <code>PTARS</code></li>\n<li>You can extend a <code>LUME</code> into a <code>SLUME</code></li>\n<li>\u2026</li>\n</ol>\n<p>and so on, then it's much easier to understand that the <code>LUME</code>s which Euclid's axioms describe may not match up with the image of a \"straight line\" in your head. It is much easier to understand that there may be interpretations of <code>LUME</code> which do not obey the fifth postulate.</p>\n<p>In fact, if you take Euclid's first four postulates, there are many possible interpretations in which \"straight line\" takes on a multitude of meanings. This ability to disconnect the <em>intended</em> interpretation from the <em>available </em>interpretations is the bedrock of model theory. Model theory is the study of <em>all</em> interpretations of a theory, not just the ones that the original author intended.</p>\n<p>Of course, model theory isn't really about finding surprising new interpretations \u2014 it's much more general than that. It's about exploring the breadth of interpretations that a given theory makes available. It's about discerning properties that hold in all possible interpretations of a theory. It's about discovering how well (or poorly) a given theory constrains its interpretations. It's a toolset used to discuss interpretations in general.</p>\n<p>At its core, model theory is the study of what a mathematical theory actually says, when you strip the intent from the symbols.</p>\n<h2 id=\"Iron_walls\">Iron walls</h2>\n<p>Before you can do model theory, you have to erect iron walls between four different concepts.</p>\n<ol>\n<li>Logics</li>\n<li>Languages</li>\n<li>Theories</li>\n<li>Models</li>\n</ol>\n<h3 id=\"Logics\" style=\"color:gray\">Logics</h3>\n<p>A logic is a formal system for building and manipulating sentences. Traditionally, this logic defines a number of symbols (<code>( ) \u2227 \u00ac \u2200 \u2203 \u2261 \u03bd '</code>, for example) and rules for building sentences from those symbols.</p>\n<p>Note that <em>you cannot generate sentences from a logic alone</em>. Rather, you <em>use</em> a logic to generate sentences <em>from</em> a language.</p>\n<p>Also, remember that the rules of a logic are <em>syntactic</em>, such as \"if <code>\u03c6</code> is a sentence then <code>(\u00ac\u03c6)</code> is a sentence\".</p>\n<p>Finally, remember that logics are just rules for generating sentences. A logic is perfectly happy to generate sentences shaped like <code>x\u2227(\u00acx)</code>, in spite of all your protests about contradictions.</p>\n<h3 id=\"Languages\" style=\"color:gray\">Languages</h3>\n<p>A language is a collection of symbols. <em>From</em> those symbols, <em>using</em> a logic, you can start generating sentences.</p>\n<p>For example, in the propositional logic, using the language <code>{x, y}</code>, the string <code>hello</code> is surely not a sentence (for it fails to use the appropriate symbols). Nor is the string&nbsp;<code>\u00acxy</code>&nbsp;a sentence: it fails to follow the rules of the logic. <code>((\u00acx)\u2227y)</code> is a sentence, for it uses the appropriate symbols and follows the given rules.</p>\n<p>Many results in model theory are achieved by holding the logic fixed and varying the language, so it's essential that these concepts be distinct in your mind.</p>\n<h3 id=\"Theories\" style=\"color:gray\">Theories</h3>\n<p>A theory is a collection of sentences written in one language. For example, in the language <code>{\u2264}</code> under first-order logic, we can discuss the theory</p>\n<ol>\n<li><code>(\u2200x)(x\u2264x)</code></li>\n<li><code>(\u2200xy)(x\u2264y)\u2227(y\u2264x)\u2192(y\u2261x)</code></li>\n<li><code>(\u2200xyz)(x\u2264y)\u2227(y\u2264z)\u2192(x\u2264z)</code></li>\n</ol>\n<p>which is the theory of <em>order</em>. (The axioms above are reflexivity, antisymmetry, and transitivity).</p>\n<p>Remember that a theory is just a set of sentences drawn from all available sentences. These sentences aren't particularly special unless you make them special. Sentences like <code>(\u2203x)\u00ac(x\u2264x)</code> are fine sentences built from the language <code>{\u2264}</code>, even though they directly contradict the theory. Theories don't affect the sentences of a language \u2014 they're just a grab-bag of some sentences that seemed interesting to someone.</p>\n<h3 id=\"Models\" style=\"color:gray\">Models</h3>\n<p>A model is an <em>interpretation</em> of the sentences generated by a language. A model is a structure which assigns a truth value to each sentence generated by some language under some logic.</p>\n<p>(More specifically, it's a structure that assigns binary values to sentences in such a way that we're justified in the name \"truth value\": for example, we require that a model says \u03c6&nbsp;is true if and only if it says that&nbsp;\u00ac\u03c6 is false, and so on.)</p>\n<p>Only once we start interpreting sentences is it meaningful to talk about valid or refutable sentences. Once you have a model of <code>{\u2264}</code> that happens to say that the axioms 1, 2, and 3 above are true, <em>then</em> you can start talking about how the theory of order rules out the sentence <code>(\u2203x)\u00ac(x\u2264x)</code> \u2014 because there is no model of the theory of order which is also a model of this sentence.</p>\n<p>(You can actually talk about how <code>(\u2203x)\u00ac(x\u2264x)</code> is inconsistent with the theory of order without appealing to model theory, but I find it helpful to treat everything as raw symbols until interpreted by a model.)</p>\n<p>To give a concrete example, in <em>first order logic,</em>&nbsp;using the <em>language </em>{S, +, *, 0}, the <em>theory</em>&nbsp;of arithmetic is the theory laid out by the [Peano axioms](<a href=\"http://en.wikipedia.org/wiki/Peano_axioms#First-order_theory_of_arithmetic\">http://en.wikipedia.org/wiki/Peano_axioms#First-order_theory_of_arithmetic</a>). The actual natural numbers zero, one, two, ... are a model of this theory (where zero is the interpretation of 0, one is the interpretation of S0, etc.).</p>\n<p>Also, it's worth noting that&nbsp;<em>any</em>&nbsp;object that interprets sentences and follows the rules of the logic qualifies as a model. There are often many non-isomorphic objects that interpret the same sentences in the same way. For example, rational numbers and real numbers are models of group theory that agree on every sentence in the language of groups, despite being different models.</p>\n<p>Distinctions between these four points is something that seems obvious to me in hindsight, but I explicitly remember expending cognitive effort to separate these concepts mentally, so there you go. Make sure these distinctions are wrought in iron before attempting model theory.</p>\n<h2 id=\"The_Right_to_use_a_name\">The Right to use a name</h2>\n<p>There's something about math education in general that has troubled me for quite some time, and which I'm finally able to articulate. It's quite possible that this is a personal nit, since nobody else seems to care&nbsp;\u2014&nbsp;but I'll share it anyway.</p>\n<p>Many math textbooks treat properties that <em>justify a name</em>&nbsp;of a thing as statements about the thing <em>after</em> naming it.</p>\n<p>This is a little abstract, so I'll make a silly example. Imagine someone is trying to show that, in category theory, composition of arrows is associative. They shouldn't appeal to visual intuition or any diagrams of arrows.</p>\n<p>The concept that following arrows is an associative operation is so ingrained in the concept of \"arrow\" that it's difficult to describe the property in English without sounding dumb.</p>\n<blockquote>\n<p>If you move from A to B, then move B-to-D-through-C in one step, and if I follow the same paths but move A-to-C-through-B in one step and then from C to D, then we will end up at the same place.</p>\n</blockquote>\n<p>This property of arrows is so stupidly obvious that the statement is frustrating. Further, it hides the following fact:</p>\n<p><em>Associative composition between thingies is&nbsp;</em><em>something we must have before we're justified in calling the thingies \"Arrows\"</em>.</p>\n<p>Associative composition is what <em>allows</em> you to use the name \"arrow\" and draw visual diagrams. You can't appeal to my intuition about \"arrows\" to show that composition is associative. It's the other way around! Only&nbsp;<em>after</em>&nbsp;you show that your thingies have associative composition are you allowed to label them as \"arrows\".</p>\n<p>As another example, the axioms of order (above) are what <em>allow</em> us to use the <code>\u2264</code> symbol, which appeals to our intuitive idea of order. Really, it's more honest to say \"We have a binary relation <code>R</code>, satisfying</p>\n<ol>\n<li><code>(\u2200x)R(xx)</code></li>\n<li><code>(\u2200xy)R(xy)\u2227R(yx)\u2192(y\u2261x)</code></li>\n<li><code>(\u2200xyz)R(xy)\u2227R(yz)\u2192R(xz)</code></li>\n</ol>\n<p>which <em>justifies</em> our use of the <code>\u2264</code> symbol for <code>R</code>.\"</p>\n<p>I imagine this is not a problem for experienced mathematicians, for whom it goes without saying that you must formally specify (or disregard) all intuitive baggage that comes attached to the names. However, I remember distinctly a number of times when I gnashed my teeth with boredom as teachers made obvious statements (<em>of course</em> <code>\u2264</code> is reflexive, why do we even need to say this?), simply because I didn't understand this idea.</p>\n<p>I mention this because the first few sections of the&nbsp;<em>Model Theory</em>&nbsp;textbook make statements that seem quite obvious. It's easy to grind your teeth and say \"duh, hurry up\". It's a little harder to understand exactly why such things must be said. In that light, I think this is a good piece of advice for learning mathematics in general:</p>\n<p>If you find yourself wondering why a statement must be said, check whether the statement is justifying any names.</p>\n<p><span style=\"color: #808080; font-size: 15px; font-weight: bold;\">Binding meaning</span></p>\n<p>The early parts of <em>Model Theory</em>&nbsp;will go down much easier if you realize that they're binding logical symbols to the appropriate meaning (and thus justifying the name \"model\").</p>\n<p>For example, when we state \"M models&nbsp;<code>\u03c6\u2227\u03c8</code>&nbsp;if and only if it models&nbsp;<code>\u03c6</code>&nbsp;and it models&nbsp;<code>\u03c8</code>\", it's easy to say \"well duh\". It's a little harder to understand that <em>this is the mechanism by which the symbol</em>&nbsp;<code>\u2227</code>&nbsp;<em>is bound to the interpretation \"and\".</em></p>\n<p>Also, note that the ability to distinguish between \"the symbol&nbsp;<code>+</code>&nbsp;in the language L\" from \"the addition function as interpreted by the model M\" is absolutely crucial.</p>\n<h2 id=\"Totality\">Totality</h2>\n<p>Something that kept on biting me was this: <em>Models of first-order logic are \"total\"</em>. They have something to say about <em>every</em> sentence in a language. Even where a <em>theory</em> is incomplete, any individual <em>model</em> is \"complete\". A model of first-order logic interprets function symbols by total functions and relations by set-theoretic relations. The relationship <code>\u22a7</code> is total: for every sentence, either <code>M\u22a7\u03c6</code> or <code>M\u22a7\u00ac\u03c6</code>.</p>\n<p>This is a point where my intuitive notion of \"models as interpretations\" departed from the actual mathematical objects under consideration \u2014 functions are firmly partial-by-default in my mind's eye.</p>\n<p>It's important to hold firm the distinction between \"model\" and \"theory\" here. Remember that the number<em> theory</em>&nbsp;is incomplete, while the standard<em> model</em>&nbsp;of number theory is the one that picks \"true\" for all G\u00f6del sentences, has no infinite numbers, etc. (The difficulties in pinpointing such a model is exactly what the incompleteness theorem is all about.)</p>\n<p>Be aware that the mathematical definition of a model may not match your intuitive idea of \"a structure which interprets a theory\", <em>especially</em>&nbsp;if you're coming from computer science (or other constructive fields).</p>\n<hr>\n<p>None of this is particularly novel. Rather, this is a collection of distinctions and clarifications that would have made my life a bit easier when beginning the textbook.</p>\n<p>In my case, I didn't have any of these concepts wrong, per se \u2014 rather, I had them fuzzy. The above distinctions were not yet fleshed out in my mind. This post provides a context for model theory; a taste of the type of thinking you must be ready to think.</p>\n<p>I was originally going to use this as context for what I've learned in model theory so far, but this post took longer than expected. I'll follow up tomorrow.</p>", "sections": [{"title": "Between what was meant and what was said", "anchor": "Between_what_was_meant_and_what_was_said", "level": 1}, {"title": "Iron walls", "anchor": "Iron_walls", "level": 1}, {"title": "Logics", "anchor": "Logics", "level": 2}, {"title": "Languages", "anchor": "Languages", "level": 2}, {"title": "Theories", "anchor": "Theories", "level": 2}, {"title": "Models", "anchor": "Models", "level": 2}, {"title": "The Right to use a name", "anchor": "The_Right_to_use_a_name", "level": 1}, {"title": "Totality", "anchor": "Totality", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gc6foBcbozvEJ3HbG", "ucpD7YtcvKo6C9fBK", "Jar4BGrJ7BiQemBDM", "Ee8CZW7wzaNdCENYG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-30T12:07:24.727Z", "modifiedAt": null, "url": null, "title": "Democracy and rationality", "slug": "democracy-and-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.224Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "homunq", "createdAt": "2009-05-08T21:56:32.475Z", "isAdmin": false, "displayName": "homunq"}, "userId": "FZoKpxRbKw5g32eiY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WwzCKRFLfD678Fo7t/democracy-and-rationality", "pageUrlRelative": "/posts/WwzCKRFLfD678Fo7t/democracy-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/WwzCKRFLfD678Fo7t/democracy-and-rationality", "postedAtFormatted": "Wednesday, October 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Democracy%20and%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADemocracy%20and%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWwzCKRFLfD678Fo7t%2Fdemocracy-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Democracy%20and%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWwzCKRFLfD678Fo7t%2Fdemocracy-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWwzCKRFLfD678Fo7t%2Fdemocracy-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6574, "htmlBody": "<p style=\"padding-left: 30px;\">Note: This is a draft; so far, about the first half is complete. I'm posting it to Discussion for now; when it's finished, I'll move it to Main. In the mean time, I'd appreciate comments, including suggestions on style and/or format. In particular, if you think I should(n't) try to post this as a sequence of separate sections, let me know.</p>\n<p style=\"padding-left: 30px;\"><em>Summary: <strong>You want to find the truth? You want to win? You're gonna have to learn the right way to vote.</strong> Plurality voting sucks; better voting systems are built from the blocks of approval,&nbsp;</em><em>medians (Bucklin cutoffs),&nbsp;</em><em>delegation, and pairwise opposition. I'm working to promote these systems and <strong><a href=\"http://www.electology.org/\">I want your help</a></strong>.</em></p>\n<p style=\"padding-left: 30px;\"><em>Contents: 1. Overblown</em><em>&sup1;</em><em>&nbsp;rhetorical setup ... 2. Condorcet's idea</em><em>ls and Arrow's problem ...&nbsp;3. Further issues for politics ...&nbsp;4. Rating versus ranking; a solution? ... 5. Delegation and SODA ... 6. Criteria and pathologies ...&nbsp;7. Representation, Proportional representation, and Sortition ... 8. What I'm doing about it and what you can ... 9. Conclusions and future directions ... 10. Appendix: voting systems table ... 11. Footnotes</em></p>\n<p>1.</p>\n<p>This is a website focused on becoming more rational. But that can't just mean getting a black belt in individual epistemic rationality. In a situation where you're not the one making the decision, that black belt is just a recipe for frustration.</p>\n<p>Of course, there's also plenty of content here about how to interact rationally; how to argue for truth, including both hacking yourself to give in when you're wrong and hacking others to give in when they are. You can learn plenty here about <a href=\"/lw/gr/the_modesty_argument/\">Aumann's Agreement Theorem</a> on how two rational Bayesians should never knowingly disagree.</p>\n<p>But \"two rational Bayesians\" isn't a whole lot better as a model for society than \"one rational Bayesian\". Aspiring to be rational is well and good, but the Socratic ideal of a world tied together by two-person dialogue alone is as unrealistic as the sociopath's ideal of a world where their own voice rules alone. Society needs structures for more than two people to interact. And just as we need techniques for checking irrationality in one- and two-person contexts, we need them, perhaps all the more, in multi-person contexts.</p>\n<p>Most of the basic individual and dialogical rationality techniques carry over. Things like noticing when you are confused, or making your opponent's arguments into a steel man, are still perfectly applicable. But there's also a new set of issues when n&gt;2: the issues of democracy and voting. For a group of aspiring rationalists to come to a working consensus, of course they need to begin by evaluating and discussing the evidence, but eventually it will be time to cut off the discussion and just vote. When they do so, they should understand the strengths and pitfalls of voting in general and of their chosen voting method in particular.</p>\n<p>And voting's not just useful for an aspiring rationalist community. As it happens, it's an important part of how governments are run. Discussing <a href=\"/lw/gw/\">politics may be a mind-killer</a> in many contexts, but there are an awful lot of domains where politics is a part of the road to winning.&sup2; Understanding voting processes a little bit can help you navigate that road; understanding them deeply opens the possibility of improving that road and thus winning more often.</p>\n<p><strong>2. Collective rationality:&nbsp;Condorcet's ideals and Arrow's problem</strong></p>\n<p>Imagine it's 1785, and you're a member of the French Academy of Sciences. You're rubbing elbows with most of the giants of science and mathematics of your day: Coulomb, Fourier, Lalande, Lagrange, Laplace, Lavoisier, Monge; even the odd foreign notable like Franklin with his ideas to unify electrostatics and electric flow.</p>\n<p><img src=\"http://m5.paperblog.com/i/32/322089/the-eiffel-tower-as-i-saw-it-L-4zTFlS.jpeg\" alt=\"They'll remember your names\" width=\"640\" height=\"268\" /></p>\n<p><em>One day, they'll put your names in front of lots of cameras (even though that foreign yokel Franklin will be in more pictures)</em></p>\n<p>And this academy, with many of the smartest people in the world, has votes on stuff. Who will be our next president; who should edit and schedule our publications; etc. You're sure that if you all could just find the right way to do the voting, you'd get the right answer. In fact, you can easily prove that, or something like it: if a group is deciding between one right and one wrong option, and each member is independently more than 50% likely to get it right, then as the group size grows the chance of a majority vote choosing the right option goes to 1.</p>\n<p>But somehow, there's still annoying politics getting in the way. Some people seem to win the elections simply because everyone expects them to win. So last year, the academy decided on a new election system to use, proposed by your rival, Charles de Borda, in which candidates get different points for being a voters first, second, or third choice, and the one with the most points wins. But you're convinced that this new system will lead to the opposite problem: people who win the election precisely because nobody expected them to win, by getting the points that voters strategically don't want to give to a strong rival. But when people point that possibility out to Borda, he only huffs that \"my system is meant for honest men!\"</p>\n<p>So with your proof of the above intuitive, useful result about two-way elections, you try to figure out how to reduce an n-way election to the two-candidate case. Clearly, you can show that Borda's system will frequently give the wrong results from that perspective. But frustratingly, you find that there could sometimes be no right answer; that there will be no candidate who would beat all the others in one-on-one races. A crack has opened up; could it be that the collective decisions of intelligent individual rational agents could be irrational?</p>\n<p>Of course, the \"you\" in this story is the Marquis de Condorcet, and the year 1785 is when he published his&nbsp;<em style=\"font-family: sans-serif; font-size: 13.333333969116211px; line-height: 19.18402862548828px;\">Essai sur l&rsquo;application de l&rsquo;analyse &agrave; la probabilit&eacute; des d&eacute;cisions rendues &agrave; la pluralit&eacute; des voix, </em>a work devoted to the question of how to acheive collective rationality. The theorem referenced above is Condorcet's Jury Theorem, which seems to offer hope that democracy can point the way from individually-imperfect rationality towards an ever-more-perfect collective rationality. Just as&nbsp;<a href=\"/lw/gr/the_modesty_argument/\">Aumann's Agreement Theorem</a>&nbsp;shows that two rational agents should always move towards consensus, the Condorcet Jury Theorem apparently shows that if you have enough rational agents, the resulting consensus will be correct.</p>\n<p>But as I said, Condorcet also opened a crack in that hope: the possibility that collective preferences will be cyclical. If the assumptions of the jury theorem don't hold &mdash; if each voter doesn't have a &gt;50% chance of being right on a randomly-selected question, OR if the correctness of two randomly-selected voters is not independent and uncorrelated &mdash; then individually-sensible choices can lead to collectively-ridiculous ones.&nbsp;</p>\n<p>What do I mean by \"collectively-ridiculous\"? Let's imagine that the Rationalist Marching Band is choosing the colors for their summer, winter, and spring uniforms, and that they all agree that the only goal is to have as much as possible of the best possible colors. The summer-style uniforms come in red or blue, and they vote and pick blue; the winter-style ones come in blue or green, and they pick green; and the spring ones come in green or red, and they pick red.</p>\n<p>Obviously, this makes us doubt their collective rationality. If, as they all agree they should, they had a consistent favorite color, they should have chosen that color both times that it was available, rather than choosing three different colors in the three cases. Theoretically, the salesperson could use such a fact to pump money out of them; for instance, offering to let them \"trade up\" their spring uniform from red to blue, then to green, then back to red, charging them a small fee each time; if they voted consistently as above, they would agree to each trade (though of course in reality human voters would probably catch on to the trick pretty soon, so the abstract ideal of an unending circular money pump wouldn't work).</p>\n<p>This is the kind of irrationality that Condorcet showed was possible in collective decisionmaking. He also realized that there was a related issue with logical inconsistencies. If you were take a vote on 3 logically related propositions &mdash; say, \"Should we have a Minister of Silly Walks, to be appointed by the Chancellor of the Excalibur\", \"Should we have a Minister of Silly Walks, but not appointed by the Chancellor of the Excalibur\", and \"Should we in fact have a Minister of Silly Walks at all\", where the third cannot be true unless one of the first is &mdash; then you could easily get majority votes for inconsistent results &mdash; in this case, no, no, and yes, respectively. Obviously, there are many ways to fix the problem in this simple case &mdash; probably many less-wrong'ers would suggest some Bayesian tricks related to logical networks and treating votes as evidence\u2078 &mdash; but it's a tough problem in general even today, especially when the logical relationships can be complex, and Condorcet was quite right to be worried about its implications for collective rationality.&sup3;</p>\n<p>And that's not the only tough problem he correctly foresaw. Nearly 200 years later and an ocean away, in the 1960s, Kenneth Arrow showed that it was impossible for a preferential voting system to avoid the problem of a \"Condorcet cycle\" of preferences. Arrows theorem shows that any voting system which can consistently give the same winner (or, in ties, winners) for the same voter preferences; which does not make one voter the effective dictator; which is sure to elect a candidate if all voters prefer them; and which will switch the results for two candidates if you switch their names on all the votes... must exhibit, in at least some situation, the pathology that befell the Rationalist Marching Band above, or in other words, must fail \"independence of irrelevant alternatives\".</p>\n<p>Arrow's theorem is far from obvious a priori, but proof is not hard to understand intuitively using Condorcet's insight. Say that there are three candidates, X, Y, and Z, with roughly equal bases of support; and that they form a Condorcet cycle, because in two-way races, X would beat Y with help from Z supporters, Y would beat Z with help from X supporters, and Z would beat X with help from Y supporters. So whoever wins in the three-way race &mdash; say, X &mdash; just remove the one who would have lost to them &mdash; Y in this case &mdash; and that \"irrelevant\" change will change the winner to be the third &mdash; Z in this case.</p>\n<p><em>Summary of above: Collective rationality is harder than individual or two-way rationality. Condorcet saw the problem and tried to solve it, but Arrow saw that Condorcet had been doomed to fail.</em></p>\n<p><strong>3. Further issues for politics</strong></p>\n<p>So Condorcet's ideals of better rationality through voting appear to be in ruins. But at least we can hope that voting is a good way to do politics, right?</p>\n<p>Not so fast. Arrow's theorem quickly led to further disturbing results. Alan Gibbard (and also Mark Satterthwaite) extended that there is no voting system which doesn't encourage voting strategy. That is, if you view an voting system as a class of games where the finite players and finite available strategies are fixed, no player is effectively a dictator, and the only thing that varies are the payoffs for each player from each outcome, there is no voting system where you can derive your best strategic vote purely by looking \"honestly\" at your own preferences; there is always the possibility of situations where you have to second-guess what others will do.</p>\n<p>Amartya Sen piled on with another depressing extension of Arrows' logic. He showed that there is no possible way of aggregating individual choices into collective choice that satisfies two simple criteria. First, it shouldn't choose pareto-dominated outcomes; if everyone prefers situation XYZ to ABC, that they don't do XYZ. Second, it is \"minimally liberal\"; that is, there are at least two people who each get to freely make their own decision on at least one specific issue each, no matter what, so for instance I always get to decide between X and A (in Gibbard's\u2074 example, colors for my house), and you always get to decide between Y and B (colors for your own house). The problem is that if you nosily care more about my house's color, the decision that should have been mine, and I nosily care about yours, more than we each care about our own, then the pareto-dominant situation is the one where we don't decide our own houses; and that nosiness could, in theory, be the case for any specific choice that, a priori, someone might have labelled as our Inalienable Right. It's not such a surprising result when you think about it that way, but it does clearly show that unswerving ideals of Democracy and Liberty will never truly be compatible.</p>\n<p>Meanwhile, \"public choice\" theorists\u2075 like Duncan Black, James Buchanan, etc. were busy undermining the idea of democratic government from another direction: the motivations of the politicians and bureaucrats who are supposed to keep it running. They showed that various incentives, including the strange voting scenarios explored by Condorcet and Arrow, would tend open a gap between the motives of the people and those of the government, and that strategic voting and agenda-setting within a legislature would tend to extend the impact of that gap. Where Gibbard and Sen had proved general results, these theorists worked from specific examples. And in one aspect, at least, their analysis is devastatingly unanswerable: the near-ubiquitous \"democratic\" system of plurality voting, also known as first-past-the-post or vote-for-one or biggest-minority-wins, is terrible in both theory and practice.</p>\n<p>So, by the 1980s, things looked pretty depressing for the theory of democracy. Politics, the theory went, was doomed forever to be a worse than sausage factory; disgusting on the inside and distasteful even from outside.</p>\n<p>Should an ethical rationalist just give up on politics, then? Of course not. As long as the results it produces are important, it's worth trying to optimize. And as soon as you take the engineer's attitude of optimizing, instead of dogmatically searching for perfection or uselessly whining about the problems, the results above don't seem nearly as bad.</p>\n<p>From this engineer's perspective, public choice theory serves as an unsurprising warning that tradeoffs are necessary, but more usefully, as a map of where those tradeoffs can go particularly wrong. In particular, its clearest lesson, in all-caps bold with a blink tag, that PLURALITY IS BAD, can be seen as a hopeful suggestion that other voting systems may be better. Meanwhile, the logic of both Sen's and Gibbard's theorems are built on Arrow's earlier result. So if we could find a way around Arrow, it might help resolve the whole issue.</p>\n<p><em>Summary of above: Democracy is the worst political system... (...except for all the others?) But perhaps it doesn't have to be quite so bad as it is today.</em></p>\n<p><strong>4. Rating versus ranking</strong></p>\n<p>So finding a way around Arrow's theorem could be key to this whole matter. As a mathematical theorem, of course, the logic is bulletproof. But it does make one crucial assumption: that the only inputs to a voting system are rankings, that is, voters' ordinal preference orders for the candidates. No distinctions can be made using ratings or grades; that is, as long as you prefer X to Y to Z, the strength of those preferences can't matter. Whether you put Y almost up near X or way down next to Z, the result must be the same.</p>\n<p>Relax that assumption, and it's easy to create a voting system which meets Arrow's criteria. It's called Score voting\u2076, and it just means rating each candidate with a number from some fixed interval (abstractly speaking, a real number; but in practice, usually an integer); the scores are added up and the highest total or average wins. (Unless there are missing values, of course, total or average amount to the same thing.) You've probably used it yourself on Yelp, IMDB, or similar sites. And it clearly passes all of Arrow's criteria. Non-dictatorship? Check. Unanimity? Check. Symmetry over switching candidate names? Check. Independence of irrelevant alternatives? In the mathematical sense &mdash; that is, as long as the scores for other candidates are unchanged &mdash; check.</p>\n<p>So score voting is an ideal system? Well, it's certainly a far sight better than plurality. But let's check it against Sen and against Gibbard.</p>\n<p>Sen's theorem was based on a logic similar to Arrow. However, while Arrow's theorem deals with broad outcomes like which candidate wins, Sen's deals with finely-grained outcomes like (in the example we discussed) how each separate house should be painted. Extending the cardinal numerical logic of score voting to such finely-grained outcomes, we find we've simply reinvented markets. While markets can be great things and often work well in practice, Sen's result still holds in this case; if everything is on the market, then there is no decision which is always yours to make. But since, in practice, as long as you aren't destitute, you tend to be able to make the decisions you care the most about, Sen's theorem seems to have lost its bite in this context.</p>\n<p>What about Gibbard's theorem on strategy? Here, things are not so easy. Yes, Gibbard, like Sen, parallels Arrow. But while Arrow deals with what's written on the ballot, Gibbard deals with what's in the voters head. In particular, if a voter prefers X to Y by even the tiniest margin, Gibbard assumes (not unreasonably) that they may be willing to vote however they need to, if by doing so they can ensure X wins instead of Y. Thus, the internal preferences Gibbard treats are, effectively, just ordinal rankings; and the cardinal trick by which score voting avoided Arrovian problems no longer works.</p>\n<p>How does score voting deal with strategic issues in practice? The answer to that has two sides. On the one hand, score never requires voters to be actually dishonest. Unlike the situation in a ranked system such as plurality, where we all know that the strategic vote may be to dishonestly ignore your true favorite and vote for a \"lesser evil\" among the two frontrunners, in score voting you never need to vote a less-preferred option above a more-preferred option. At worst, all you have to do is exaggerate some distinctions and minimize others, so that you might end giving equal votes to less- and more-preferred options.</p>\n<p>Did I say \"at worst\"? I meant, \"almost always\". Voting strategy only matters to the result when, aside from your vote, two or more candidates are within one vote of being tied for first. Except in unrealistic, perfectly-balanced conditions, as the number of voters rises, the probability that anyone but the two <em>a priori</em> frontrunner candidates is in on this tie falls to zero.\u2077 Thus, in score voting, the optimal strategy is nearly always to vote your preferred frontrunner and all candidate above at the maximum, and your less-preferred frontrunner and all candidates below at the minimum. In other words, strategic score voting is basically equivalent to approval voting, where you give each candidate a 1 or 0 and the highest total wins.</p>\n<p>In one sense, score voting reducing to approval OK. Approval voting is not a bad system at all. For instance, if there's a known majority Condorcet winner &mdash; a candidate who could beat any other by a majority in a one-on-one race &mdash; and voters are strategic &mdash; they anticipate the unique strong Nash equilibrium, the situation where no group of voters could improve the outcome for all its members by changing their votes, whenever such a unique equilibrium exists &mdash; then the Condorcet winner will win under approval. That's a lot of words to say that approval will get the \"democratic\" results you'd expect in most cases.</p>\n<p>But in another sense, it's a problem. If one side of an issue is more inclined to be strategic than the other side, the more-strategic faction could win even if it's a minority. That clashes with many people's ideals of democracy; and worse, it encourages mind-killing political attitudes, where arguments are used as soldiers rather than as ways to seek the truth.</p>\n<p>But score and approval voting are not the only systems which escape Arrow's theorem through the trapdoor of ratings. If score voting, using the average of voter ratings, too-strongly encourages voters to strategically seek extreme ratings, then why not use the median rating instead? We know that medians are less sensitive to outliers than averages. And indeed, median-based systems are more resistant to one-sided strategy than average-based ones, giving better hope for reasonable discussion to prosper. That is to say, in a <a href=\"http://www.rangevoting.org/MedianAvg1side.html\">simple model</a>, a minority would need twice as much strategic coordination under median as under average, in order to overcome a majority; and there's good reason to believe that, because of natural factional separation, reality is even more favorable to median systems than that model.</p>\n<p>There are several different median systems available. In the US during the 1910-1925 Progressive Era, early versions collectively called \"<a href=\"https://en.wikipedia.org/wiki/Bucklin_voting\">Bucklin voting</a>\" were used briefly in over a dozen cities. These reforms, based on counting all top preferences, then adding lower preferences one level at a time until some candidate(s) reach a majority, were all rolled back soon after, principally by party machines upset at upstart challenges or victories. The possibility of multiple, simultaneous majorities is a principal reason for the variety of Bucklin/Median systems. Modern proposals of median systems include <a href=\"http://wiki.electorama.com/wiki/MAV\">Majority Approval Voting</a>, <a href=\"http://wiki.electorama.com/wiki/Majority_Judgment\">Majority Judgment</a>, and <a href=\"http://wiki.electorama.com/wiki/GMJ\">Graduated Majority Judgment</a>, which would probably give the same winners almost all of the time. An important detail is that most median system ballots use verbal or letter grades rather than numeric scores. This is justifiable because the median is preserved under any monotonic transformation, and studies suggest that it would help discourage strategic voting.</p>\n<p>Serious attention to rated systems like approval, score, and median systems barely began in the 1980s, and didn't really pick up until 2000. Meanwhile, the increased amateur interest in voting systems in this period &mdash; perhaps partially attributable to the anomalous 2000 US presidential election, or to more-recent anomalies in the UK, Canada, and Australia &mdash; has led to new discoveries in ranked systems as well. Though such systems are still clearly subject to Arrow's theorem, new \"improved Condorcet\" methods which use certain tricks to count a voter's equal preferences between to candidates on either side of the ledger depending on the strategic needs, seem to offer promise that Arrovian pathologies can be kept to a minimum.</p>\n<p>With this embarrassment of riches of systems to choose from, how should we evaluate which is best? Well, at least one thing is a clear consensus: plurality is a horrible system. Beyond that, things are more controversial; there are dozens of possible objective criteria one could formulate, and any system's inventor and/or supporters can usually formulate some criterion by which it shines.</p>\n<p>Ideally, we'd like to measure the utility of each voting system in the real world. Since that's impossible &mdash; it would take not just a statistically-significant sample of large-scale real-world elections for each system, but also some way to measure the true internal utility of a result in situations where voters are inevitably strategically motivated to lie about that utility &mdash; we must do the next best thing, and measure it in a computer, with simulated voters whose utilities are assigned measurable values. Unfortunately, that requires assumptions about how those utilities are distributed, how voter turnout is decided, and how and whether voters strategize. At best, those assumptions can be varied, to see if findings are robust.</p>\n<p>In 2000, Warren Smith <a href=\"http://scorevoting.net/WarrenSmithPages/homepage/rangevote.pdf\">performed such simulations</a> for a number of voting systems. He found that score voting had, very robustly, one of the top expected social utilities (or, as he termed it, lowest Bayesian regret). Close on its heels were a median system and approval voting. Unfortunately, though he explored a wide parameter space in terms of voter utility models and inherent strategic inclination of the voters, his simulations did not include voters who were more inclined to be strategic when strategy was more effective. His strategic assumptions were also unfavorable to ranked systems, and slightly unrealistic in other ways. Still, though certain of his numbers must be taken with a grain of salt, some of his results were large and robust enough to be trusted. For instance, he found that plurality voting and instant runoff voting were clearly inferior to rated systems; and that approval voting, even at its worst, offered over half the benefits compared to plurality of any other system.</p>\n<p><em>Summary of above: Rated systems, such as approval voting, score voting, and Majority Approval Voting, can avoid the problems of Arrow's theorem. Though they are certainly not immune to issues of strategic voting, they are a clear step up from plurality. Starting with this section, the opinions are my own; the two prior sections were based on general expert views on the topic.</em></p>\n<p><strong>5. Delegation and SODA</strong></p>\n<p>Rated systems are not the only way to try to beat the problems of Arrow and Gibbard (/Satterthwaite).</p>\n<p><em>Summary of above:</em></p>\n<p><strong>6. Criteria and pathologies</strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><strong>7. Representation, proportionality, and sortition</strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><strong>8. What I'm doing about it and what you can</strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><strong>9. Conclusions and future directions </strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><em></em><strong>10. Appendix: voting systems table</strong></p>\n<h4 style=\"background-image: none; margin: 0px 0px 0.3em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-style: none; font-size: 14.44444465637207px; font-family: sans-serif; line-height: 19.18402862548828px;\"><span id=\"Compliance_of_selected_systems_.28table.29\" class=\"mw-headline\">Compliance of selected systems (table)</span></h4>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">The following table shows which of the above criteria are met by several single-winner systems. Note: contains some errors; I'll carefully vet this when I'm finished with the writing. Still generally reliable though.</p>\n<table class=\"wikitable sortable jquery-tablesorter\" style=\"font-size: 13.333333969116211px; margin: 1em 0px; background-color: #f9f9f9; border: 1px solid #aaaaaa; border-collapse: collapse; color: #000000; font-family: sans-serif; line-height: 19.18402862548828px; text-align: center;\" border=\"0\">\n<thead> \n<tr style=\"font-size: 11.111111640930176px;\">\n<th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\">&nbsp;</th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion\">Major&shy;ity</a>/<br /><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Mutual majority criterion\" href=\"https://en.wikipedia.org/wiki/Mutual_majority_criterion\">MMC</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion\">Condorcet</a>/<br /><strong>Majority Condorcet</strong></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet loser criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_loser_criterion\">Cond.<br />loser</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Monotonicity criterion\" href=\"https://en.wikipedia.org/wiki/Monotonicity_criterion\">Mono&shy;tone</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Consistency criterion\" href=\"https://en.wikipedia.org/wiki/Consistency_criterion\">Consist&shy;ency</a>/<br /><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Participation criterion\" href=\"https://en.wikipedia.org/wiki/Participation_criterion\">Particip&shy;ation</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Reversal symmetry\" href=\"https://en.wikipedia.org/wiki/Reversal_symmetry\">Rever&shy;sal<br />sym&shy;metry</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of irrelevant alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives\">IIA</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Clone independence\" href=\"https://en.wikipedia.org/wiki/Clone_independence\">Cloneproof</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Polynomial time\" href=\"https://en.wikipedia.org/wiki/Polynomial_time\">Poly&shy;time</a>/<br /><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Resolvability criterion\" href=\"https://en.wikipedia.org/wiki/Resolvability_criterion\">Resolv&shy;able</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Voting machine\" href=\"https://en.wikipedia.org/wiki/Voting_machine#Vote-tabulation_Technologies\">Summ&shy;able</a><br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\">Equal rankings<br />allowed<br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\">Later<br />prefs<br />allowed<br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\" colspan=\"2\" align=\"center\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm\" href=\"https://en.wikipedia.org/wiki/Later-no-harm\">Later-no-harm</a>&shy;/<br />Later-no-help<br /></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><strong>FBC:No<br />favorite<br />betrayal</strong></th>\n</tr>\n</thead> \n<tbody>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Approval voting\" href=\"https://en.wikipedia.org/wiki/Approval_voting\">Approval</a><sup id=\"cite_ref-approvalrangecriteria_1-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalrangecriteria-1\">[nb 1]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Approval_voting\">Ambig&shy;uous</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Approval_voting\">No</a></span>/&shy;<span style=\"background-color: #ddffdd;\">Strategic yes<sup id=\"cite_ref-approvalnash_2-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes<sup id=\"cite_ref-approvalnash_2-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\">Ambig&shy;uous</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\">Ambig.&shy;<sup id=\"cite_ref-ambiguous_3-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-ambiguous-3\">[nb 3]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><sup id=\"cite_ref-approvalLNH_4-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalLNH-4\">[nb 4]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Borda count\" href=\"https://en.wikipedia.org/wiki/Borda_count\">Borda count</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Borda_count\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Borda_count\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">teaming</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Copeland's method\" href=\"https://en.wikipedia.org/wiki/Copeland%27s_method\">Copeland</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">crowding</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #bbffbb;\">Yes</span>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Instant-runoff voting\" href=\"https://en.wikipedia.org/wiki/Instant-runoff_voting\">IRV</a>&nbsp;(AV)</th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Instant-runoff_voting\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Instant-runoff_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Monotonicity criterion\" href=\"https://en.wikipedia.org/wiki/Monotonicity_criterion#Instant-runoff_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">O(N!)&shy;<sup id=\"cite_ref-5\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-5\">[nb 5]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Complying_methods\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Kemeny-Young method\" href=\"https://en.wikipedia.org/wiki/Kemeny-Young_method\">Kemeny-Young</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Kemeny-Young_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Kemeny-Young_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">teaming</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\">No</span>/<span style=\"background-color: #bbffbb;\">Yes</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)&shy;<sup id=\"cite_ref-6\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-6\">[nb 6]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Kemeny-Young method\" href=\"https://en.wikipedia.org/wiki/Kemeny-Young_method#Failed_criteria_for_all_Condorcet_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority Judgment\" href=\"https://en.wikipedia.org/wiki/Majority_Judgment\">Majority Judg&shy;ment</a><sup id=\"cite_ref-mjbucklin_7-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjbucklin-7\">[nb 7]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Majority_Judgment\">Yes</a><sup id=\"cite_ref-mjmajority_8-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjmajority-8\">[nb 8]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Approval_voting\">No</a></span>/&shy;<span style=\"background-color: #ddffdd;\">Strategic yes<sup id=\"cite_ref-approvalnash_2-2\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-mjcondloser_9-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjcondloser-9\">[nb 9]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-mjconsistency_10-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjconsistency-10\">[nb 10]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-mjreversal_11-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjreversal-11\">[nb 11]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)&shy;<sup id=\"cite_ref-12\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-12\">[nb 12]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" align=\"center\" bgcolor=\"#FFDDDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a><sup id=\"cite_ref-mjlnh_13-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjlnh-13\">[nb 13]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" align=\"center\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Minimax Condorcet\" href=\"https://en.wikipedia.org/wiki/Minimax_Condorcet\">Minimax</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Minimax_Condorcet\">Yes</a>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Minimax_Condorcet\">Yes</a><sup id=\"cite_ref-minimaxvariant_14-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-minimaxvariant-14\">[nb 14]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Vote-splitting\" href=\"https://en.wikipedia.org/wiki/Vote-splitting\">spoilers</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFFFFF\">Some variants</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFDDDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Complying_methods\">No</a><sup id=\"cite_ref-minimaxvariant_14-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-minimaxvariant-14\">[nb 14]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Plurality voting system\" href=\"https://en.wikipedia.org/wiki/Plurality_voting_system\">Plurality</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Plurality_voting_system\">Yes</a>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Plurality_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Vote-splitting\" href=\"https://en.wikipedia.org/wiki/Vote-splitting\">spoilers</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><sup id=\"cite_ref-approvalLNH_4-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalLNH-4\">[nb 4]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Range voting\" href=\"https://en.wikipedia.org/wiki/Range_voting\">Range voting</a><sup id=\"cite_ref-approvalrangecriteria_1-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalrangecriteria-1\">[nb 1]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Range_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Approval_voting\">No</a></span>/&shy;<span style=\"background-color: #ddffdd;\">Strategic yes<sup id=\"cite_ref-approvalnash_2-3\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes<sup id=\"cite_ref-approvalnash_2-4\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes<sup id=\"cite_ref-rangeIIA_15-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-rangeIIA-15\">[nb 15]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\">Ambig.&shy;<sup id=\"cite_ref-ambiguous_3-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-ambiguous-3\">[nb 3]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Ranked pairs\" href=\"https://en.wikipedia.org/wiki/Ranked_pairs\">Ranked pairs</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Ranked_Pairs\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Ranked_Pairs\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Two-round system\" href=\"https://en.wikipedia.org/wiki/Two-round_system\">Runoff voting</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Two-round_system\">Yes</a>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Two-round_system\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Vote-splitting\" href=\"https://en.wikipedia.org/wiki/Vote-splitting\">spoilers</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N)&shy;<sup id=\"cite_ref-16\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-16\">[nb 16]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-17\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-17\">[nb 17]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#DDFFDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Complying_methods\">Yes</a><sup id=\"cite_ref-18\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-18\">[nb 18]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Schulze method\" href=\"https://en.wikipedia.org/wiki/Schulze_method\">Schulze</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Schulze_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Schulze_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a class=\"new\" style=\"text-decoration: none; color: #a55858; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"SODA voting (page does not exist)\" href=\"https://en.wikipedia.org/w/index.php?title=SODA_voting&amp;action=edit&amp;redlink=1\">SODA voting</a><sup id=\"cite_ref-sodabase_19-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-sodabase-19\">[nb 19]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><span style=\"background-color: #ffdddd;\">Strategic yes</span>/<span style=\"background-color: #bbffbb;\">yes</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Ambig&shy;uous<sup id=\"cite_ref-sodamono_20-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-sodamono-20\">[nb 20]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><span style=\"background-color: #bbffbb;\">Yes</span>/Up to 4 cand.&nbsp;<sup id=\"cite_ref-soda4_21-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-soda4-21\">[nb 21]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes<sup id=\"cite_ref-22\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-22\">[nb 22]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">Up to 4 candidates<sup id=\"cite_ref-soda4_21-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-soda4-21\">[nb 21]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">Up to 4 cand. (then crowds)&nbsp;<sup id=\"cite_ref-soda4_21-2\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-soda4-21\">[nb 21]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes<sup id=\"cite_ref-23\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-23\">[nb 23]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFDDDD\">Limited<sup id=\"cite_ref-24\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-24\">[nb 24]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Sortition\" href=\"https://en.wikipedia.org/wiki/Sortition\">Random winner</a>/<br />arbitrary winner<sup id=\"cite_ref-25\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-25\">[nb 25]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">NA</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">NA</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDBBBB\"><span style=\"background-color: #99dd99;\">Yes</span>/<span style=\"background-color: #dd9999;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#77DD77\">O(1)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">&nbsp;</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Random ballot\" href=\"https://en.wikipedia.org/wiki/Random_ballot\">Random ballot</a><sup id=\"cite_ref-26\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-26\">[nb 26]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDBBBB\"><span style=\"background-color: #99dd99;\">Yes</span>/<span style=\"background-color: #dd9999;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">&nbsp;</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n</tr>\n</tbody>\n<tfoot></tfoot>\n</table>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\"><small>\"Yes/No\", in a column which covers two related criteria, signifies that the given system passes the first criterion and not the second one.</small></p>\n<div class=\"reflist\" style=\"font-size: 11.111111640930176px; margin-bottom: 0.5em; font-family: sans-serif; line-height: 19.18402862548828px; list-style-type: decimal;\"><ol class=\"references\" style=\"line-height: 1.5em; margin: 0.3em 0px 0.5em 3.2em; padding: 0px; list-style-image: none; list-style-type: inherit;\">\n<li id=\"cite_note-approvalrangecriteria-1\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalrangecriteria_1-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalrangecriteria_1-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">These criteria assume that all voters vote their true preference order. This is problematic for Approval and Range, where various votes are consistent with the same order. See&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Approval voting\" href=\"https://en.wikipedia.org/wiki/Approval_voting#Compliance_with_voting_system_criteria\">approval voting</a>&nbsp;for compliance under various voter models.</span></li>\n<li id=\"cite_note-approvalnash-2\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-2\"><sup style=\"line-height: 1em;\"><em><strong>c</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-3\"><sup style=\"line-height: 1em;\"><em><strong>d</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-4\"><sup style=\"line-height: 1em;\"><em><strong>e</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">In Approval, Range, and Majority Judgment, if all voters have perfect information about each other's true preferences and use rational strategy, any Majority Condorcet or Majority winner will be strategically forced &ndash; that is, win in the unique&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strong Nash equilibrium\" href=\"https://en.wikipedia.org/wiki/Strong_Nash_equilibrium\">Strong Nash equilibrium</a>. In particular if every voter knows that \"A or B are the two most-likely to win\" and places their \"approval threshold\" between the two, then the Condorcet winner, if one exists and is in the set {A,B}, will always win. These systems also satisfy the majority criterion in the weaker sense that any majority can force their candidate to win, if it so desires. (However, as the Condorcet criterion is incompatible with the participation criterion and the consistency criterion, these systems cannot satisfy these criteria in this Nash-equilibrium sense. Laslier, J.-F. (2006)&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(https://upload.wikimedia.org/wikipedia/commons/2/23/Icons-mini-file_acrobat.gif); padding-right: 18px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://halshs.archives-ouvertes.fr/docs/00/12/17/51/PDF/stratapproval4.pdf\">\"Strategic approval voting in a large electorate,\"</a><em>IDEP Working Papers</em>&nbsp;No. 405 (Marseille, France: Institut D'Economie Publique).)</span></li>\n<li id=\"cite_note-ambiguous-3\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-ambiguous_3-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-ambiguous_3-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">The original&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">independence of clones criterion</a>&nbsp;applied only to ranked voting methods. (T. Nicolaus Tideman, \"Independence of clones as a criterion for voting rules\",&nbsp;<em>Social Choice and Welfare</em>&nbsp;Vol. 4, No. 3 (1987), pp. 185&ndash;206.) There is some disagreement about how to extend it to unranked methods, and this disagreement affects whether approval and range voting are considered independent of clones. If the definition of \"clones\" is that \"every voter scores them within &plusmn;&epsilon; in the limit &epsilon;&rarr;0+\", then range voting is immune to clones.</span></li>\n<li id=\"cite_note-approvalLNH-4\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalLNH_4-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalLNH_4-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">Approval and Plurality do not allow later preferences. Technically speaking, this means that they pass the technical definition of the LNH criteria - if later preferences or ratings are impossible, then such preferences can not help or harm. However, from the perspective of a voter, these systems do not pass these criteria. Approval, in particular, encourages the voter to give the&nbsp;<em>same</em>&nbsp;ballot rating to a candidate who, in another voting system, would get a&nbsp;<em>later</em>&nbsp;rating or ranking. Thus, for approval, the practically meaningful criterion would be not \"later-no-harm\" but \"same-no-harm\" - something neither approval nor any other system satisfies.</span></li>\n<li id=\"cite_note-5\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-5\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">The number of piles that can be summed from various precincts is&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Floor and ceiling functions\" href=\"https://en.wikipedia.org/wiki/Floor_and_ceiling_functions\">floor</a>((e-1) N!) - 1.</span></li>\n<li id=\"cite_note-6\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-6\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Each prospective Kemeny-Young ordering has score equal to the sum of the pairwise entries that agree with it, and so the best ordering can be found using the pairwise matrix.</span></li>\n<li id=\"cite_note-mjbucklin-7\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjbucklin_7-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Bucklin voting, with skipped and equal-rankings allowed, meets the same criteria as Majority Judgment; in fact, Majority Judgment may be considered a form of Bucklin voting. Without allowing equal rankings, Bucklin's criteria compliance is worse; in particular, it fails Independence of Irrelevant Alternatives, which for a ranked method like this variant is incompatible with the Majority Criterion.</span></li>\n<li id=\"cite_note-mjmajority-8\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjmajority_8-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment passes the&nbsp;<em>rated</em>&nbsp;majority criterion (a candidate rated solo-top by a majority must win). It does not pass the&nbsp;<em>ranked</em>&nbsp;majority criterion, which is incompatible with Independence of Irrelevant Alternatives.</span></li>\n<li id=\"cite_note-mjcondloser-9\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjcondloser_9-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment passes the \"majority condorcet loser\" criterion; that is, a candidate who loses to all others by a&nbsp;<em>majority</em>&nbsp;cannot win. However, if some of the losses are not by a majority (including equal-rankings), the Condorcet loser can, theoretically, win in MJ, although such scenarios are rare.</span></li>\n<li id=\"cite_note-mjconsistency-10\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjconsistency_10-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Balinski and Laraki, Majority Judgment's inventors, point out that it meets a weaker criterion they call \"grade consistency\": if two electorates give the same rating for a candidate, then so will the combined electorate. Majority Judgment explicitly requires that ratings be expressed in a \"common language\", that is, that each rating have an absolute meaning. They claim that this is what makes \"grade consistency\" significant. MJ. Balinski M. and R. Laraki (2007) &laquo;A theory of measuring, electing and ranking&raquo;. Proceedings of the National Academy of Sciences USA, vol. 104, no. 21, 8720-8725.</span></li>\n<li id=\"cite_note-mjreversal-11\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjreversal_11-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment can actually pass or fail reversal symmetry depending on the rounding method used to find the median when there are even numbers of voters. For instance, in a two-candidate, two-voter race, if the ratings are converted to numbers and the two central ratings are averaged, then MJ meets reversal symmetry; but if the lower one is taken, it does not, because a candidate with [\"fair\",\"fair\"] would beat a candidate with [\"good\",\"poor\"] with or without reversal. However, for rounding methods which do not meet reversal symmetry, the chances of breaking it are on the order of the inverse of the number of voters; this is comparable with the probability of an exact tie in a two-candidate race, and when there's a tie, any method can break reversal symmetry.</span></li>\n<li id=\"cite_note-12\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-12\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority Judgment is summable at order KN, where K, the number of ranking categories, is set beforehand.</span></li>\n<li id=\"cite_note-mjlnh-13\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjlnh_13-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment meets a related, weaker criterion: ranking an additional candidate below the median grade (rather than your own grade) of your favorite candidate, cannot harm your favorite.</span></li>\n<li id=\"cite_note-minimaxvariant-14\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-minimaxvariant_14-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-minimaxvariant_14-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">A variant of Minimax that counts only pairwise opposition, not opposition minus support, fails the Condorcet criterion and meets later-no-harm.</span></li>\n<li id=\"cite_note-rangeIIA-15\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-rangeIIA_15-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Range satisfies the mathematical definition of IIA, that is, if each voter scores each candidate independently of which other candidates are in the race. However, since a given range score has no agreed-upon meaning, it is thought that most voters would either \"normalize\" or exaggerate their vote such that it votes at least one candidate each at the top and bottom possible ratings. In this case, Range would not be independent of irrelevant alternatives. Balinski M. and R. Laraki (2007) &laquo;A theory of measuring, electing and ranking&raquo;. Proceedings of the National Academy of Sciences USA, vol. 104, no. 21, 8720-8725.</span></li>\n<li id=\"cite_note-16\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-16\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Once for each round.</span></li>\n<li id=\"cite_note-17\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-17\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Later preferences are only possible between the two candidates who make it to the second round.</span></li>\n<li id=\"cite_note-18\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-18\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">That is, second-round votes cannot harm candidates already eliminated.</span></li>\n<li id=\"cite_note-sodabase-19\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-sodabase_19-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Unless otherwise noted, for SODA's compliances:</span> \n<ul style=\"line-height: 1.5em; margin: 0.3em 0px 0px 1.6em; padding: 0px; list-style-image: url(data;\">\n<li style=\"margin-bottom: 0.1em;\"><span class=\"reference-text\">Delegated votes are considered to be equivalent to voting the candidate's predeclared preferences.</span></li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"reference-text\">Ballots only are considered (In other words, voters are assumed not to have preferences that cannot be expressed by a delegated or approval vote.)</span></li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"reference-text\">Since at the time of assigning approvals on delegated votes there is always enough information to find an optimum strategy, candidates are assumed to use such a strategy.</span></li>\n</ul>\n</li>\n<li id=\"cite_note-sodamono-20\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-sodamono_20-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">For up to 4 candidates, SODA is monotonic. For more than 4 candidates, it is monotonic for adding an approval, for changing from an approval to a delegation ballot, and for changes in a candidate's preferences. However, if changes in a voter's preferences are executed as changes from a delegation to an approval ballot, such changes are not necessarily monotonic with more than 4 candidates.</span></li>\n<li id=\"cite_note-soda4-21\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-soda4_21-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-soda4_21-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-soda4_21-2\"><sup style=\"line-height: 1em;\"><em><strong>c</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">For up to 4 candidates, SODA meets the Participation, IIA, and Cloneproof criteria. It can fail these criteria in certain rare cases with more than 4 candidates. This is considered here as a qualified success for the Consistency and Participation criteria, which do not intrinsically have to do with numerous candidates, and as a qualified failure for the IIA and Cloneproof criteria, which do.</span></li>\n<li id=\"cite_note-22\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-22\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">SODA voting passes reversal symmetry for all scenarios that are reversible under SODA; that is, if each delegated ballot has a unique last choice. In other situations, it is not clear what it would mean to reverse the ballots, but there is always some possible interpretation under which SODA would pass the criterion.</span></li>\n<li id=\"cite_note-23\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-23\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">SODA voting is always polytime computable. There are some cases where the optimal strategy for a candidate assigning delegated votes may not be polytime computable; however, such cases are entirely implausible for a real-world election.</span></li>\n<li id=\"cite_note-24\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-24\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Later preferences are only possible through delegation, that is, if they agree with the predeclared preferences of the favorite.</span></li>\n<li id=\"cite_note-25\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-25\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Random winner: Uniformly randomly chosen candidate is winner. Arbitrary winner: some external entity, not a voter, chooses the winner. These systems are not, properly speaking, voting systems at all, but are included to show that even a horrible system can still pass some of the criteria.</span></li>\n<li id=\"cite_note-26\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-26\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Random ballot: Uniformly random-chosen ballot determines winner. This and closely related systems are of mathematical interest because they are the only possible systems which are truly strategy-free, that is, your best vote will never depend on anything about the other voters. They also satisfy both consistency and IIA, which is impossible for a deterministic ranked system. However, this system is not generally considered as a serious proposal for a practical method.</span></li>\n</ol></div>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\"><strong>11. Footnotes</strong></p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&sup1; When I call my introduction \"overblown\", I mean that I reserve the right to make broad generalizations there, without getting distracted by caveats. If you don't like this style, feel free to skip to section 2.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&sup2; Of course, the original \"politics is a mind killer\" sequence was perfectly clear about this: \"<span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Politics is an important domain to which we should individually apply our rationality&mdash;but it's a terrible domain in which to&nbsp;</span><em style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">learn</em><span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;rationality, or discuss rationality, unless all the discussants are already rational.\" The focus here is on the first part of that quote, because I think Less Wrong as a whole has moved too far in the direction of avoiding politics as not a domain for rationalists.</span></p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&sup3; Bayes developed his theorem decades before Condorcet's <em>Essai</em>, but Condorcet probably didn't know of it, as it wasn't popularized by Laplace until about 30 years later, after Condorcet was dead.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2074 Yes, this happens to be the same Alan Gibbard from the previous paragraph.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2075 Confusingly, \"public choice\" refers to a school of thought, while \"social choice\" is the name for the broader domain of study. Stop reading this footnote now if you don't want to hear mind-killing partisan identification. \"Public choice\" theorists are generally seen as politically conservative in the solutions they suggest. It seems to me that the broader \"social choice\" has avoided taking on a partisan connotation in this sense.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2076 Score voting is also called \"range voting\" by some. It is not a particularly new idea &mdash; for instance, the \"loudest cheer wins\" rule of ancient Sparta, and even aspects of honeybees' process for choosing new hives, can be seen as score voting &mdash; but it was first analyzed theoretically around 2000. Approval voting, which can be seen as a form of score voting where the scores are restricted to 0 and 1, had entered theory only about two decades earlier, though it too has a history of practical use back to antiquity.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2077 OK, fine, this is a simplification. As a voter, you have imperfect information about the true level of support and propensity to vote in the superpopulation of eligible voters, so in reality the chances of a decisive tie between other than your two expected frontrunners is non-zero. Still, in most cases, it's utterly negligible.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2078 This article will focus more on the literature on multi-player strategic voting (competing boundedly-instrumentally-rational agents) than on multi-player Aumann (cooperating boundedly-epistemically-rational agents). If you're interested in the latter, here are some starting points: <a href=\"http://www.scottaaronson.com/papers/agree-econ.pdf\">Scott Aaronson's work</a> is, as far as I know, the state of the art on 2-player Aumann, but its framework assumes that the players have a sophisticated ability to empathize and reason about each others' internal knowledge, and the problems with this that Aaronson plausibly handwaves away in the 2-player case are probably less tractable in the multi-player one. <a href=\"http://link.springer.com/chapter/10.1007/978-3-642-25510-6_13#page-1\">Dalkiran et al</a> deal with an Aumann-like problem over a social network; they find that attempts to \"jump ahead\" to a final consensus value instead of simply dumbly approaching it asymptotically can lead to failure to converge. And <a href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6481628&amp;tag=1\">Kanoria et al</a> have perhaps the most interesting result from the perspective of this article; they use the convergence of agents using a naive voting-based algorithm to give a nice upper bound on the difficulty of full Bayesian reasoning itself. None of these papers explicitly considers the problem of coming to consensus on more than one logically-related question at once, though Aaronson's work at least would clearly be easy to extend in that direction, and I think such extensions would be unsurprisingly Bayesian.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WwzCKRFLfD678Fo7t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 8, "extendedScore": null, "score": 1.4014072820394577e-06, "legacy": true, "legacyId": "24133", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px;\">Note: This is a draft; so far, about the first half is complete. I'm posting it to Discussion for now; when it's finished, I'll move it to Main. In the mean time, I'd appreciate comments, including suggestions on style and/or format. In particular, if you think I should(n't) try to post this as a sequence of separate sections, let me know.</p>\n<p style=\"padding-left: 30px;\"><em>Summary: <strong>You want to find the truth? You want to win? You're gonna have to learn the right way to vote.</strong> Plurality voting sucks; better voting systems are built from the blocks of approval,&nbsp;</em><em>medians (Bucklin cutoffs),&nbsp;</em><em>delegation, and pairwise opposition. I'm working to promote these systems and <strong><a href=\"http://www.electology.org/\">I want your help</a></strong>.</em></p>\n<p style=\"padding-left: 30px;\"><em>Contents: 1. Overblown</em><em>\u00b9</em><em>&nbsp;rhetorical setup ... 2. Condorcet's idea</em><em>ls and Arrow's problem ...&nbsp;3. Further issues for politics ...&nbsp;4. Rating versus ranking; a solution? ... 5. Delegation and SODA ... 6. Criteria and pathologies ...&nbsp;7. Representation, Proportional representation, and Sortition ... 8. What I'm doing about it and what you can ... 9. Conclusions and future directions ... 10. Appendix: voting systems table ... 11. Footnotes</em></p>\n<p>1.</p>\n<p>This is a website focused on becoming more rational. But that can't just mean getting a black belt in individual epistemic rationality. In a situation where you're not the one making the decision, that black belt is just a recipe for frustration.</p>\n<p>Of course, there's also plenty of content here about how to interact rationally; how to argue for truth, including both hacking yourself to give in when you're wrong and hacking others to give in when they are. You can learn plenty here about <a href=\"/lw/gr/the_modesty_argument/\">Aumann's Agreement Theorem</a> on how two rational Bayesians should never knowingly disagree.</p>\n<p>But \"two rational Bayesians\" isn't a whole lot better as a model for society than \"one rational Bayesian\". Aspiring to be rational is well and good, but the Socratic ideal of a world tied together by two-person dialogue alone is as unrealistic as the sociopath's ideal of a world where their own voice rules alone. Society needs structures for more than two people to interact. And just as we need techniques for checking irrationality in one- and two-person contexts, we need them, perhaps all the more, in multi-person contexts.</p>\n<p>Most of the basic individual and dialogical rationality techniques carry over. Things like noticing when you are confused, or making your opponent's arguments into a steel man, are still perfectly applicable. But there's also a new set of issues when n&gt;2: the issues of democracy and voting. For a group of aspiring rationalists to come to a working consensus, of course they need to begin by evaluating and discussing the evidence, but eventually it will be time to cut off the discussion and just vote. When they do so, they should understand the strengths and pitfalls of voting in general and of their chosen voting method in particular.</p>\n<p>And voting's not just useful for an aspiring rationalist community. As it happens, it's an important part of how governments are run. Discussing <a href=\"/lw/gw/\">politics may be a mind-killer</a> in many contexts, but there are an awful lot of domains where politics is a part of the road to winning.\u00b2 Understanding voting processes a little bit can help you navigate that road; understanding them deeply opens the possibility of improving that road and thus winning more often.</p>\n<p><strong id=\"2__Collective_rationality__Condorcet_s_ideals_and_Arrow_s_problem\">2. Collective rationality:&nbsp;Condorcet's ideals and Arrow's problem</strong></p>\n<p>Imagine it's 1785, and you're a member of the French Academy of Sciences. You're rubbing elbows with most of the giants of science and mathematics of your day: Coulomb, Fourier, Lalande, Lagrange, Laplace, Lavoisier, Monge; even the odd foreign notable like Franklin with his ideas to unify electrostatics and electric flow.</p>\n<p><img src=\"http://m5.paperblog.com/i/32/322089/the-eiffel-tower-as-i-saw-it-L-4zTFlS.jpeg\" alt=\"They'll remember your names\" width=\"640\" height=\"268\"></p>\n<p><em>One day, they'll put your names in front of lots of cameras (even though that foreign yokel Franklin will be in more pictures)</em></p>\n<p>And this academy, with many of the smartest people in the world, has votes on stuff. Who will be our next president; who should edit and schedule our publications; etc. You're sure that if you all could just find the right way to do the voting, you'd get the right answer. In fact, you can easily prove that, or something like it: if a group is deciding between one right and one wrong option, and each member is independently more than 50% likely to get it right, then as the group size grows the chance of a majority vote choosing the right option goes to 1.</p>\n<p>But somehow, there's still annoying politics getting in the way. Some people seem to win the elections simply because everyone expects them to win. So last year, the academy decided on a new election system to use, proposed by your rival, Charles de Borda, in which candidates get different points for being a voters first, second, or third choice, and the one with the most points wins. But you're convinced that this new system will lead to the opposite problem: people who win the election precisely because nobody expected them to win, by getting the points that voters strategically don't want to give to a strong rival. But when people point that possibility out to Borda, he only huffs that \"my system is meant for honest men!\"</p>\n<p>So with your proof of the above intuitive, useful result about two-way elections, you try to figure out how to reduce an n-way election to the two-candidate case. Clearly, you can show that Borda's system will frequently give the wrong results from that perspective. But frustratingly, you find that there could sometimes be no right answer; that there will be no candidate who would beat all the others in one-on-one races. A crack has opened up; could it be that the collective decisions of intelligent individual rational agents could be irrational?</p>\n<p>Of course, the \"you\" in this story is the Marquis de Condorcet, and the year 1785 is when he published his&nbsp;<em style=\"font-family: sans-serif; font-size: 13.333333969116211px; line-height: 19.18402862548828px;\">Essai sur l\u2019application de l\u2019analyse \u00e0 la probabilit\u00e9 des d\u00e9cisions rendues \u00e0 la pluralit\u00e9 des voix, </em>a work devoted to the question of how to acheive collective rationality. The theorem referenced above is Condorcet's Jury Theorem, which seems to offer hope that democracy can point the way from individually-imperfect rationality towards an ever-more-perfect collective rationality. Just as&nbsp;<a href=\"/lw/gr/the_modesty_argument/\">Aumann's Agreement Theorem</a>&nbsp;shows that two rational agents should always move towards consensus, the Condorcet Jury Theorem apparently shows that if you have enough rational agents, the resulting consensus will be correct.</p>\n<p>But as I said, Condorcet also opened a crack in that hope: the possibility that collective preferences will be cyclical. If the assumptions of the jury theorem don't hold \u2014 if each voter doesn't have a &gt;50% chance of being right on a randomly-selected question, OR if the correctness of two randomly-selected voters is not independent and uncorrelated \u2014 then individually-sensible choices can lead to collectively-ridiculous ones.&nbsp;</p>\n<p>What do I mean by \"collectively-ridiculous\"? Let's imagine that the Rationalist Marching Band is choosing the colors for their summer, winter, and spring uniforms, and that they all agree that the only goal is to have as much as possible of the best possible colors. The summer-style uniforms come in red or blue, and they vote and pick blue; the winter-style ones come in blue or green, and they pick green; and the spring ones come in green or red, and they pick red.</p>\n<p>Obviously, this makes us doubt their collective rationality. If, as they all agree they should, they had a consistent favorite color, they should have chosen that color both times that it was available, rather than choosing three different colors in the three cases. Theoretically, the salesperson could use such a fact to pump money out of them; for instance, offering to let them \"trade up\" their spring uniform from red to blue, then to green, then back to red, charging them a small fee each time; if they voted consistently as above, they would agree to each trade (though of course in reality human voters would probably catch on to the trick pretty soon, so the abstract ideal of an unending circular money pump wouldn't work).</p>\n<p>This is the kind of irrationality that Condorcet showed was possible in collective decisionmaking. He also realized that there was a related issue with logical inconsistencies. If you were take a vote on 3 logically related propositions \u2014 say, \"Should we have a Minister of Silly Walks, to be appointed by the Chancellor of the Excalibur\", \"Should we have a Minister of Silly Walks, but not appointed by the Chancellor of the Excalibur\", and \"Should we in fact have a Minister of Silly Walks at all\", where the third cannot be true unless one of the first is \u2014 then you could easily get majority votes for inconsistent results \u2014 in this case, no, no, and yes, respectively. Obviously, there are many ways to fix the problem in this simple case \u2014 probably many less-wrong'ers would suggest some Bayesian tricks related to logical networks and treating votes as evidence\u2078 \u2014 but it's a tough problem in general even today, especially when the logical relationships can be complex, and Condorcet was quite right to be worried about its implications for collective rationality.\u00b3</p>\n<p>And that's not the only tough problem he correctly foresaw. Nearly 200 years later and an ocean away, in the 1960s, Kenneth Arrow showed that it was impossible for a preferential voting system to avoid the problem of a \"Condorcet cycle\" of preferences. Arrows theorem shows that any voting system which can consistently give the same winner (or, in ties, winners) for the same voter preferences; which does not make one voter the effective dictator; which is sure to elect a candidate if all voters prefer them; and which will switch the results for two candidates if you switch their names on all the votes... must exhibit, in at least some situation, the pathology that befell the Rationalist Marching Band above, or in other words, must fail \"independence of irrelevant alternatives\".</p>\n<p>Arrow's theorem is far from obvious a priori, but proof is not hard to understand intuitively using Condorcet's insight. Say that there are three candidates, X, Y, and Z, with roughly equal bases of support; and that they form a Condorcet cycle, because in two-way races, X would beat Y with help from Z supporters, Y would beat Z with help from X supporters, and Z would beat X with help from Y supporters. So whoever wins in the three-way race \u2014 say, X \u2014 just remove the one who would have lost to them \u2014 Y in this case \u2014 and that \"irrelevant\" change will change the winner to be the third \u2014 Z in this case.</p>\n<p><em>Summary of above: Collective rationality is harder than individual or two-way rationality. Condorcet saw the problem and tried to solve it, but Arrow saw that Condorcet had been doomed to fail.</em></p>\n<p><strong id=\"3__Further_issues_for_politics\">3. Further issues for politics</strong></p>\n<p>So Condorcet's ideals of better rationality through voting appear to be in ruins. But at least we can hope that voting is a good way to do politics, right?</p>\n<p>Not so fast. Arrow's theorem quickly led to further disturbing results. Alan Gibbard (and also Mark Satterthwaite) extended that there is no voting system which doesn't encourage voting strategy. That is, if you view an voting system as a class of games where the finite players and finite available strategies are fixed, no player is effectively a dictator, and the only thing that varies are the payoffs for each player from each outcome, there is no voting system where you can derive your best strategic vote purely by looking \"honestly\" at your own preferences; there is always the possibility of situations where you have to second-guess what others will do.</p>\n<p>Amartya Sen piled on with another depressing extension of Arrows' logic. He showed that there is no possible way of aggregating individual choices into collective choice that satisfies two simple criteria. First, it shouldn't choose pareto-dominated outcomes; if everyone prefers situation XYZ to ABC, that they don't do XYZ. Second, it is \"minimally liberal\"; that is, there are at least two people who each get to freely make their own decision on at least one specific issue each, no matter what, so for instance I always get to decide between X and A (in Gibbard's\u2074 example, colors for my house), and you always get to decide between Y and B (colors for your own house). The problem is that if you nosily care more about my house's color, the decision that should have been mine, and I nosily care about yours, more than we each care about our own, then the pareto-dominant situation is the one where we don't decide our own houses; and that nosiness could, in theory, be the case for any specific choice that, a priori, someone might have labelled as our Inalienable Right. It's not such a surprising result when you think about it that way, but it does clearly show that unswerving ideals of Democracy and Liberty will never truly be compatible.</p>\n<p>Meanwhile, \"public choice\" theorists\u2075 like Duncan Black, James Buchanan, etc. were busy undermining the idea of democratic government from another direction: the motivations of the politicians and bureaucrats who are supposed to keep it running. They showed that various incentives, including the strange voting scenarios explored by Condorcet and Arrow, would tend open a gap between the motives of the people and those of the government, and that strategic voting and agenda-setting within a legislature would tend to extend the impact of that gap. Where Gibbard and Sen had proved general results, these theorists worked from specific examples. And in one aspect, at least, their analysis is devastatingly unanswerable: the near-ubiquitous \"democratic\" system of plurality voting, also known as first-past-the-post or vote-for-one or biggest-minority-wins, is terrible in both theory and practice.</p>\n<p>So, by the 1980s, things looked pretty depressing for the theory of democracy. Politics, the theory went, was doomed forever to be a worse than sausage factory; disgusting on the inside and distasteful even from outside.</p>\n<p>Should an ethical rationalist just give up on politics, then? Of course not. As long as the results it produces are important, it's worth trying to optimize. And as soon as you take the engineer's attitude of optimizing, instead of dogmatically searching for perfection or uselessly whining about the problems, the results above don't seem nearly as bad.</p>\n<p>From this engineer's perspective, public choice theory serves as an unsurprising warning that tradeoffs are necessary, but more usefully, as a map of where those tradeoffs can go particularly wrong. In particular, its clearest lesson, in all-caps bold with a blink tag, that PLURALITY IS BAD, can be seen as a hopeful suggestion that other voting systems may be better. Meanwhile, the logic of both Sen's and Gibbard's theorems are built on Arrow's earlier result. So if we could find a way around Arrow, it might help resolve the whole issue.</p>\n<p><em>Summary of above: Democracy is the worst political system... (...except for all the others?) But perhaps it doesn't have to be quite so bad as it is today.</em></p>\n<p><strong id=\"4__Rating_versus_ranking\">4. Rating versus ranking</strong></p>\n<p>So finding a way around Arrow's theorem could be key to this whole matter. As a mathematical theorem, of course, the logic is bulletproof. But it does make one crucial assumption: that the only inputs to a voting system are rankings, that is, voters' ordinal preference orders for the candidates. No distinctions can be made using ratings or grades; that is, as long as you prefer X to Y to Z, the strength of those preferences can't matter. Whether you put Y almost up near X or way down next to Z, the result must be the same.</p>\n<p>Relax that assumption, and it's easy to create a voting system which meets Arrow's criteria. It's called Score voting\u2076, and it just means rating each candidate with a number from some fixed interval (abstractly speaking, a real number; but in practice, usually an integer); the scores are added up and the highest total or average wins. (Unless there are missing values, of course, total or average amount to the same thing.) You've probably used it yourself on Yelp, IMDB, or similar sites. And it clearly passes all of Arrow's criteria. Non-dictatorship? Check. Unanimity? Check. Symmetry over switching candidate names? Check. Independence of irrelevant alternatives? In the mathematical sense \u2014 that is, as long as the scores for other candidates are unchanged \u2014 check.</p>\n<p>So score voting is an ideal system? Well, it's certainly a far sight better than plurality. But let's check it against Sen and against Gibbard.</p>\n<p>Sen's theorem was based on a logic similar to Arrow. However, while Arrow's theorem deals with broad outcomes like which candidate wins, Sen's deals with finely-grained outcomes like (in the example we discussed) how each separate house should be painted. Extending the cardinal numerical logic of score voting to such finely-grained outcomes, we find we've simply reinvented markets. While markets can be great things and often work well in practice, Sen's result still holds in this case; if everything is on the market, then there is no decision which is always yours to make. But since, in practice, as long as you aren't destitute, you tend to be able to make the decisions you care the most about, Sen's theorem seems to have lost its bite in this context.</p>\n<p>What about Gibbard's theorem on strategy? Here, things are not so easy. Yes, Gibbard, like Sen, parallels Arrow. But while Arrow deals with what's written on the ballot, Gibbard deals with what's in the voters head. In particular, if a voter prefers X to Y by even the tiniest margin, Gibbard assumes (not unreasonably) that they may be willing to vote however they need to, if by doing so they can ensure X wins instead of Y. Thus, the internal preferences Gibbard treats are, effectively, just ordinal rankings; and the cardinal trick by which score voting avoided Arrovian problems no longer works.</p>\n<p>How does score voting deal with strategic issues in practice? The answer to that has two sides. On the one hand, score never requires voters to be actually dishonest. Unlike the situation in a ranked system such as plurality, where we all know that the strategic vote may be to dishonestly ignore your true favorite and vote for a \"lesser evil\" among the two frontrunners, in score voting you never need to vote a less-preferred option above a more-preferred option. At worst, all you have to do is exaggerate some distinctions and minimize others, so that you might end giving equal votes to less- and more-preferred options.</p>\n<p>Did I say \"at worst\"? I meant, \"almost always\". Voting strategy only matters to the result when, aside from your vote, two or more candidates are within one vote of being tied for first. Except in unrealistic, perfectly-balanced conditions, as the number of voters rises, the probability that anyone but the two <em>a priori</em> frontrunner candidates is in on this tie falls to zero.\u2077 Thus, in score voting, the optimal strategy is nearly always to vote your preferred frontrunner and all candidate above at the maximum, and your less-preferred frontrunner and all candidates below at the minimum. In other words, strategic score voting is basically equivalent to approval voting, where you give each candidate a 1 or 0 and the highest total wins.</p>\n<p>In one sense, score voting reducing to approval OK. Approval voting is not a bad system at all. For instance, if there's a known majority Condorcet winner \u2014 a candidate who could beat any other by a majority in a one-on-one race \u2014 and voters are strategic \u2014 they anticipate the unique strong Nash equilibrium, the situation where no group of voters could improve the outcome for all its members by changing their votes, whenever such a unique equilibrium exists \u2014 then the Condorcet winner will win under approval. That's a lot of words to say that approval will get the \"democratic\" results you'd expect in most cases.</p>\n<p>But in another sense, it's a problem. If one side of an issue is more inclined to be strategic than the other side, the more-strategic faction could win even if it's a minority. That clashes with many people's ideals of democracy; and worse, it encourages mind-killing political attitudes, where arguments are used as soldiers rather than as ways to seek the truth.</p>\n<p>But score and approval voting are not the only systems which escape Arrow's theorem through the trapdoor of ratings. If score voting, using the average of voter ratings, too-strongly encourages voters to strategically seek extreme ratings, then why not use the median rating instead? We know that medians are less sensitive to outliers than averages. And indeed, median-based systems are more resistant to one-sided strategy than average-based ones, giving better hope for reasonable discussion to prosper. That is to say, in a <a href=\"http://www.rangevoting.org/MedianAvg1side.html\">simple model</a>, a minority would need twice as much strategic coordination under median as under average, in order to overcome a majority; and there's good reason to believe that, because of natural factional separation, reality is even more favorable to median systems than that model.</p>\n<p>There are several different median systems available. In the US during the 1910-1925 Progressive Era, early versions collectively called \"<a href=\"https://en.wikipedia.org/wiki/Bucklin_voting\">Bucklin voting</a>\" were used briefly in over a dozen cities. These reforms, based on counting all top preferences, then adding lower preferences one level at a time until some candidate(s) reach a majority, were all rolled back soon after, principally by party machines upset at upstart challenges or victories. The possibility of multiple, simultaneous majorities is a principal reason for the variety of Bucklin/Median systems. Modern proposals of median systems include <a href=\"http://wiki.electorama.com/wiki/MAV\">Majority Approval Voting</a>, <a href=\"http://wiki.electorama.com/wiki/Majority_Judgment\">Majority Judgment</a>, and <a href=\"http://wiki.electorama.com/wiki/GMJ\">Graduated Majority Judgment</a>, which would probably give the same winners almost all of the time. An important detail is that most median system ballots use verbal or letter grades rather than numeric scores. This is justifiable because the median is preserved under any monotonic transformation, and studies suggest that it would help discourage strategic voting.</p>\n<p>Serious attention to rated systems like approval, score, and median systems barely began in the 1980s, and didn't really pick up until 2000. Meanwhile, the increased amateur interest in voting systems in this period \u2014 perhaps partially attributable to the anomalous 2000 US presidential election, or to more-recent anomalies in the UK, Canada, and Australia \u2014 has led to new discoveries in ranked systems as well. Though such systems are still clearly subject to Arrow's theorem, new \"improved Condorcet\" methods which use certain tricks to count a voter's equal preferences between to candidates on either side of the ledger depending on the strategic needs, seem to offer promise that Arrovian pathologies can be kept to a minimum.</p>\n<p>With this embarrassment of riches of systems to choose from, how should we evaluate which is best? Well, at least one thing is a clear consensus: plurality is a horrible system. Beyond that, things are more controversial; there are dozens of possible objective criteria one could formulate, and any system's inventor and/or supporters can usually formulate some criterion by which it shines.</p>\n<p>Ideally, we'd like to measure the utility of each voting system in the real world. Since that's impossible \u2014 it would take not just a statistically-significant sample of large-scale real-world elections for each system, but also some way to measure the true internal utility of a result in situations where voters are inevitably strategically motivated to lie about that utility \u2014 we must do the next best thing, and measure it in a computer, with simulated voters whose utilities are assigned measurable values. Unfortunately, that requires assumptions about how those utilities are distributed, how voter turnout is decided, and how and whether voters strategize. At best, those assumptions can be varied, to see if findings are robust.</p>\n<p>In 2000, Warren Smith <a href=\"http://scorevoting.net/WarrenSmithPages/homepage/rangevote.pdf\">performed such simulations</a> for a number of voting systems. He found that score voting had, very robustly, one of the top expected social utilities (or, as he termed it, lowest Bayesian regret). Close on its heels were a median system and approval voting. Unfortunately, though he explored a wide parameter space in terms of voter utility models and inherent strategic inclination of the voters, his simulations did not include voters who were more inclined to be strategic when strategy was more effective. His strategic assumptions were also unfavorable to ranked systems, and slightly unrealistic in other ways. Still, though certain of his numbers must be taken with a grain of salt, some of his results were large and robust enough to be trusted. For instance, he found that plurality voting and instant runoff voting were clearly inferior to rated systems; and that approval voting, even at its worst, offered over half the benefits compared to plurality of any other system.</p>\n<p><em>Summary of above: Rated systems, such as approval voting, score voting, and Majority Approval Voting, can avoid the problems of Arrow's theorem. Though they are certainly not immune to issues of strategic voting, they are a clear step up from plurality. Starting with this section, the opinions are my own; the two prior sections were based on general expert views on the topic.</em></p>\n<p><strong id=\"5__Delegation_and_SODA\">5. Delegation and SODA</strong></p>\n<p>Rated systems are not the only way to try to beat the problems of Arrow and Gibbard (/Satterthwaite).</p>\n<p><em>Summary of above:</em></p>\n<p><strong id=\"6__Criteria_and_pathologies\">6. Criteria and pathologies</strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><strong id=\"7__Representation__proportionality__and_sortition\">7. Representation, proportionality, and sortition</strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><strong id=\"8__What_I_m_doing_about_it_and_what_you_can\">8. What I'm doing about it and what you can</strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><strong id=\"9__Conclusions_and_future_directions_\">9. Conclusions and future directions </strong></p>\n<p>do.</p>\n<p><em>Summary of above:</em></p>\n<p><em></em><strong>10. Appendix: voting systems table</strong></p>\n<h4 style=\"background-image: none; margin: 0px 0px 0.3em; overflow: hidden; padding-top: 0.5em; padding-bottom: 0.17em; border-bottom-style: none; font-size: 14.44444465637207px; font-family: sans-serif; line-height: 19.18402862548828px;\" id=\"Compliance_of_selected_systems__table_\"><span id=\"Compliance_of_selected_systems_.28table.29\" class=\"mw-headline\">Compliance of selected systems (table)</span></h4>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">The following table shows which of the above criteria are met by several single-winner systems. Note: contains some errors; I'll carefully vet this when I'm finished with the writing. Still generally reliable though.</p>\n<table class=\"wikitable sortable jquery-tablesorter\" style=\"font-size: 13.333333969116211px; margin: 1em 0px; background-color: #f9f9f9; border: 1px solid #aaaaaa; border-collapse: collapse; color: #000000; font-family: sans-serif; line-height: 19.18402862548828px; text-align: center;\" border=\"0\">\n<thead> \n<tr style=\"font-size: 11.111111640930176px;\">\n<th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\">&nbsp;</th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion\">Major\u00adity</a>/<br><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Mutual majority criterion\" href=\"https://en.wikipedia.org/wiki/Mutual_majority_criterion\">MMC</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion\">Condorcet</a>/<br><strong>Majority Condorcet</strong></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet loser criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_loser_criterion\">Cond.<br>loser</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Monotonicity criterion\" href=\"https://en.wikipedia.org/wiki/Monotonicity_criterion\">Mono\u00adtone</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Consistency criterion\" href=\"https://en.wikipedia.org/wiki/Consistency_criterion\">Consist\u00adency</a>/<br><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Participation criterion\" href=\"https://en.wikipedia.org/wiki/Participation_criterion\">Particip\u00adation</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Reversal symmetry\" href=\"https://en.wikipedia.org/wiki/Reversal_symmetry\">Rever\u00adsal<br>sym\u00admetry</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of irrelevant alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives\">IIA</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Clone independence\" href=\"https://en.wikipedia.org/wiki/Clone_independence\">Cloneproof</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Polynomial time\" href=\"https://en.wikipedia.org/wiki/Polynomial_time\">Poly\u00adtime</a>/<br><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Resolvability criterion\" href=\"https://en.wikipedia.org/wiki/Resolvability_criterion\">Resolv\u00adable</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Voting machine\" href=\"https://en.wikipedia.org/wiki/Voting_machine#Vote-tabulation_Technologies\">Summ\u00adable</a><br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\">Equal rankings<br>allowed<br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\">Later<br>prefs<br>allowed<br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\" colspan=\"2\" align=\"center\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm\" href=\"https://en.wikipedia.org/wiki/Later-no-harm\">Later-no-harm</a>\u00ad/<br>Later-no-help<br></th><th class=\"headerSort\" style=\"border: 1px solid #aaaaaa; padding: 0.2em 21px 0.2em 0.2em; background-color: #f2f2f2; background-image: url(data; cursor: pointer; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" title=\"Sort ascending\"><strong>FBC:No<br>favorite<br>betrayal</strong></th>\n</tr>\n</thead> \n<tbody>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Approval voting\" href=\"https://en.wikipedia.org/wiki/Approval_voting\">Approval</a><sup id=\"cite_ref-approvalrangecriteria_1-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalrangecriteria-1\">[nb 1]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Approval_voting\">Ambig\u00aduous</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Approval_voting\">No</a></span>/\u00ad<span style=\"background-color: #ddffdd;\">Strategic yes<sup id=\"cite_ref-approvalnash_2-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes<sup id=\"cite_ref-approvalnash_2-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\">Ambig\u00aduous</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\">Ambig.\u00ad<sup id=\"cite_ref-ambiguous_3-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-ambiguous-3\">[nb 3]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><sup id=\"cite_ref-approvalLNH_4-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalLNH-4\">[nb 4]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Borda count\" href=\"https://en.wikipedia.org/wiki/Borda_count\">Borda count</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Borda_count\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Borda_count\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">teaming</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Copeland's method\" href=\"https://en.wikipedia.org/wiki/Copeland%27s_method\">Copeland</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">crowding</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #bbffbb;\">Yes</span>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Instant-runoff voting\" href=\"https://en.wikipedia.org/wiki/Instant-runoff_voting\">IRV</a>&nbsp;(AV)</th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Instant-runoff_voting\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Instant-runoff_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Monotonicity criterion\" href=\"https://en.wikipedia.org/wiki/Monotonicity_criterion#Instant-runoff_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">O(N!)\u00ad<sup id=\"cite_ref-5\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-5\">[nb 5]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Complying_methods\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Kemeny-Young method\" href=\"https://en.wikipedia.org/wiki/Kemeny-Young_method\">Kemeny-Young</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Kemeny-Young_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Kemeny-Young_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">teaming</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\">No</span>/<span style=\"background-color: #bbffbb;\">Yes</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)\u00ad<sup id=\"cite_ref-6\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-6\">[nb 6]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Kemeny-Young method\" href=\"https://en.wikipedia.org/wiki/Kemeny-Young_method#Failed_criteria_for_all_Condorcet_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority Judgment\" href=\"https://en.wikipedia.org/wiki/Majority_Judgment\">Majority Judg\u00adment</a><sup id=\"cite_ref-mjbucklin_7-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjbucklin-7\">[nb 7]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Majority_Judgment\">Yes</a><sup id=\"cite_ref-mjmajority_8-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjmajority-8\">[nb 8]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Approval_voting\">No</a></span>/\u00ad<span style=\"background-color: #ddffdd;\">Strategic yes<sup id=\"cite_ref-approvalnash_2-2\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-mjcondloser_9-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjcondloser-9\">[nb 9]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-mjconsistency_10-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjconsistency-10\">[nb 10]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-mjreversal_11-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjreversal-11\">[nb 11]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)\u00ad<sup id=\"cite_ref-12\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-12\">[nb 12]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" align=\"center\" bgcolor=\"#FFDDDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a><sup id=\"cite_ref-mjlnh_13-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-mjlnh-13\">[nb 13]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" align=\"center\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Minimax Condorcet\" href=\"https://en.wikipedia.org/wiki/Minimax_Condorcet\">Minimax</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Minimax_Condorcet\">Yes</a>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Minimax_Condorcet\">Yes</a><sup id=\"cite_ref-minimaxvariant_14-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-minimaxvariant-14\">[nb 14]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Vote-splitting\" href=\"https://en.wikipedia.org/wiki/Vote-splitting\">spoilers</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFFFFF\">Some variants</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFDDDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Complying_methods\">No</a><sup id=\"cite_ref-minimaxvariant_14-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-minimaxvariant-14\">[nb 14]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Plurality voting system\" href=\"https://en.wikipedia.org/wiki/Plurality_voting_system\">Plurality</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Plurality_voting_system\">Yes</a>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Plurality_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Vote-splitting\" href=\"https://en.wikipedia.org/wiki/Vote-splitting\">spoilers</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><sup id=\"cite_ref-approvalLNH_4-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalLNH-4\">[nb 4]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Range voting\" href=\"https://en.wikipedia.org/wiki/Range_voting\">Range voting</a><sup id=\"cite_ref-approvalrangecriteria_1-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalrangecriteria-1\">[nb 1]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Range_voting\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\"><span style=\"background-color: #ffbbbb;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Approval_voting\">No</a></span>/\u00ad<span style=\"background-color: #ddffdd;\">Strategic yes<sup id=\"cite_ref-approvalnash_2-3\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes<sup id=\"cite_ref-approvalnash_2-4\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-approvalnash-2\">[nb 2]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes<sup id=\"cite_ref-rangeIIA_15-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-rangeIIA-15\">[nb 15]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\">Ambig.\u00ad<sup id=\"cite_ref-ambiguous_3-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-ambiguous-3\">[nb 3]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Ranked pairs\" href=\"https://en.wikipedia.org/wiki/Ranked_pairs\">Ranked pairs</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Ranked_Pairs\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Ranked_Pairs\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Two-round system\" href=\"https://en.wikipedia.org/wiki/Two-round_system\">Runoff voting</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Two-round_system\">Yes</a>/<span style=\"background-color: #ffbbbb;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Two-round_system\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No (<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Vote-splitting\" href=\"https://en.wikipedia.org/wiki/Vote-splitting\">spoilers</a>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N)\u00ad<sup id=\"cite_ref-16\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-16\">[nb 16]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No<sup id=\"cite_ref-17\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-17\">[nb 17]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#DDFFDD\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Complying_methods\">Yes</a><sup id=\"cite_ref-18\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-18\">[nb 18]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Schulze method\" href=\"https://en.wikipedia.org/wiki/Schulze_method\">Schulze</a></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Majority criterion\" href=\"https://en.wikipedia.org/wiki/Majority_criterion#Schulze_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Condorcet criterion\" href=\"https://en.wikipedia.org/wiki/Condorcet_criterion#Schulze_method\">Yes</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">No&nbsp;<small>(but&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Independence of Smith-dominated alternatives\" href=\"https://en.wikipedia.org/wiki/Independence_of_Smith-dominated_alternatives\">ISDA</a>)</small></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">O(N<sup style=\"line-height: 1em;\">2</sup>)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFBBBB\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Later-no-harm criterion\" href=\"https://en.wikipedia.org/wiki/Later-no-harm_criterion#Noncomplying_methods\">No</a></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFBBBB\">No</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a class=\"new\" style=\"text-decoration: none; color: #a55858; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"SODA voting (page does not exist)\" href=\"https://en.wikipedia.org/w/index.php?title=SODA_voting&amp;action=edit&amp;redlink=1\">SODA voting</a><sup id=\"cite_ref-sodabase_19-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-sodabase-19\">[nb 19]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><span style=\"background-color: #ffdddd;\">Strategic yes</span>/<span style=\"background-color: #bbffbb;\">yes</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Ambig\u00aduous<sup id=\"cite_ref-sodamono_20-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-sodamono-20\">[nb 20]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\"><span style=\"background-color: #bbffbb;\">Yes</span>/Up to 4 cand.&nbsp;<sup id=\"cite_ref-soda4_21-0\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-soda4-21\">[nb 21]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes<sup id=\"cite_ref-22\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-22\">[nb 22]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">Up to 4 candidates<sup id=\"cite_ref-soda4_21-1\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-soda4-21\">[nb 21]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#FFDDDD\">Up to 4 cand. (then crowds)&nbsp;<sup id=\"cite_ref-soda4_21-2\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-soda4-21\">[nb 21]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes<sup id=\"cite_ref-23\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-23\">[nb 23]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#FFDDDD\">Limited<sup id=\"cite_ref-24\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-24\">[nb 24]</a></sup></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#BBFFBB\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDFFDD\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Sortition\" href=\"https://en.wikipedia.org/wiki/Sortition\">Random winner</a>/<br>arbitrary winner<sup id=\"cite_ref-25\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-25\">[nb 25]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">NA</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">NA</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDBBBB\"><span style=\"background-color: #99dd99;\">Yes</span>/<span style=\"background-color: #dd9999;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#77DD77\">O(1)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">&nbsp;</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n</tr>\n<tr>\n<th style=\"border: 1px solid #aaaaaa; padding: 0.2em; background-color: #f2f2f2;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Random ballot\" href=\"https://en.wikipedia.org/wiki/Random_ballot\">Random ballot</a><sup id=\"cite_ref-26\" class=\"reference\" style=\"line-height: 1em; unicode-bidi: -webkit-isolate; font-weight: normal;\"><a style=\"text-decoration: none; color: #0b0080; background-image: none; white-space: nowrap; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_note-26\">[nb 26]</a></sup></th>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DDBBBB\"><span style=\"background-color: #99dd99;\">Yes</span>/<span style=\"background-color: #dd9999;\">No</span></td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">O(N)</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" colspan=\"2\" align=\"center\" bgcolor=\"#DD9999\">No</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#DD9999\">&nbsp;</td>\n<td style=\"border: 1px solid #aaaaaa; padding: 0.2em;\" bgcolor=\"#99DD99\">Yes</td>\n</tr>\n</tbody>\n<tfoot></tfoot>\n</table>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\"><small>\"Yes/No\", in a column which covers two related criteria, signifies that the given system passes the first criterion and not the second one.</small></p>\n<div class=\"reflist\" style=\"font-size: 11.111111640930176px; margin-bottom: 0.5em; font-family: sans-serif; line-height: 19.18402862548828px; list-style-type: decimal;\"><ol class=\"references\" style=\"line-height: 1.5em; margin: 0.3em 0px 0.5em 3.2em; padding: 0px; list-style-image: none; list-style-type: inherit;\">\n<li id=\"cite_note-approvalrangecriteria-1\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalrangecriteria_1-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalrangecriteria_1-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">These criteria assume that all voters vote their true preference order. This is problematic for Approval and Range, where various votes are consistent with the same order. See&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Approval voting\" href=\"https://en.wikipedia.org/wiki/Approval_voting#Compliance_with_voting_system_criteria\">approval voting</a>&nbsp;for compliance under various voter models.</span></li>\n<li id=\"cite_note-approvalnash-2\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-2\"><sup style=\"line-height: 1em;\"><em><strong>c</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-3\"><sup style=\"line-height: 1em;\"><em><strong>d</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalnash_2-4\"><sup style=\"line-height: 1em;\"><em><strong>e</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">In Approval, Range, and Majority Judgment, if all voters have perfect information about each other's true preferences and use rational strategy, any Majority Condorcet or Majority winner will be strategically forced \u2013 that is, win in the unique&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strong Nash equilibrium\" href=\"https://en.wikipedia.org/wiki/Strong_Nash_equilibrium\">Strong Nash equilibrium</a>. In particular if every voter knows that \"A or B are the two most-likely to win\" and places their \"approval threshold\" between the two, then the Condorcet winner, if one exists and is in the set {A,B}, will always win. These systems also satisfy the majority criterion in the weaker sense that any majority can force their candidate to win, if it so desires. (However, as the Condorcet criterion is incompatible with the participation criterion and the consistency criterion, these systems cannot satisfy these criteria in this Nash-equilibrium sense. Laslier, J.-F. (2006)&nbsp;<a class=\"external text\" style=\"text-decoration: none; color: #663366; background-image: url(https://upload.wikimedia.org/wikipedia/commons/2/23/Icons-mini-file_acrobat.gif); padding-right: 18px; background-position: 100% 50%; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://halshs.archives-ouvertes.fr/docs/00/12/17/51/PDF/stratapproval4.pdf\">\"Strategic approval voting in a large electorate,\"</a><em>IDEP Working Papers</em>&nbsp;No. 405 (Marseille, France: Institut D'Economie Publique).)</span></li>\n<li id=\"cite_note-ambiguous-3\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-ambiguous_3-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-ambiguous_3-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">The original&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Strategic nomination\" href=\"https://en.wikipedia.org/wiki/Strategic_nomination\">independence of clones criterion</a>&nbsp;applied only to ranked voting methods. (T. Nicolaus Tideman, \"Independence of clones as a criterion for voting rules\",&nbsp;<em>Social Choice and Welfare</em>&nbsp;Vol. 4, No. 3 (1987), pp. 185\u2013206.) There is some disagreement about how to extend it to unranked methods, and this disagreement affects whether approval and range voting are considered independent of clones. If the definition of \"clones\" is that \"every voter scores them within \u00b1\u03b5 in the limit \u03b5\u21920+\", then range voting is immune to clones.</span></li>\n<li id=\"cite_note-approvalLNH-4\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalLNH_4-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-approvalLNH_4-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">Approval and Plurality do not allow later preferences. Technically speaking, this means that they pass the technical definition of the LNH criteria - if later preferences or ratings are impossible, then such preferences can not help or harm. However, from the perspective of a voter, these systems do not pass these criteria. Approval, in particular, encourages the voter to give the&nbsp;<em>same</em>&nbsp;ballot rating to a candidate who, in another voting system, would get a&nbsp;<em>later</em>&nbsp;rating or ranking. Thus, for approval, the practically meaningful criterion would be not \"later-no-harm\" but \"same-no-harm\" - something neither approval nor any other system satisfies.</span></li>\n<li id=\"cite_note-5\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-5\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">The number of piles that can be summed from various precincts is&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" title=\"Floor and ceiling functions\" href=\"https://en.wikipedia.org/wiki/Floor_and_ceiling_functions\">floor</a>((e-1) N!) - 1.</span></li>\n<li id=\"cite_note-6\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-6\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Each prospective Kemeny-Young ordering has score equal to the sum of the pairwise entries that agree with it, and so the best ordering can be found using the pairwise matrix.</span></li>\n<li id=\"cite_note-mjbucklin-7\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjbucklin_7-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Bucklin voting, with skipped and equal-rankings allowed, meets the same criteria as Majority Judgment; in fact, Majority Judgment may be considered a form of Bucklin voting. Without allowing equal rankings, Bucklin's criteria compliance is worse; in particular, it fails Independence of Irrelevant Alternatives, which for a ranked method like this variant is incompatible with the Majority Criterion.</span></li>\n<li id=\"cite_note-mjmajority-8\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjmajority_8-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment passes the&nbsp;<em>rated</em>&nbsp;majority criterion (a candidate rated solo-top by a majority must win). It does not pass the&nbsp;<em>ranked</em>&nbsp;majority criterion, which is incompatible with Independence of Irrelevant Alternatives.</span></li>\n<li id=\"cite_note-mjcondloser-9\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjcondloser_9-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment passes the \"majority condorcet loser\" criterion; that is, a candidate who loses to all others by a&nbsp;<em>majority</em>&nbsp;cannot win. However, if some of the losses are not by a majority (including equal-rankings), the Condorcet loser can, theoretically, win in MJ, although such scenarios are rare.</span></li>\n<li id=\"cite_note-mjconsistency-10\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjconsistency_10-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Balinski and Laraki, Majority Judgment's inventors, point out that it meets a weaker criterion they call \"grade consistency\": if two electorates give the same rating for a candidate, then so will the combined electorate. Majority Judgment explicitly requires that ratings be expressed in a \"common language\", that is, that each rating have an absolute meaning. They claim that this is what makes \"grade consistency\" significant. MJ. Balinski M. and R. Laraki (2007) \u00abA theory of measuring, electing and ranking\u00bb. Proceedings of the National Academy of Sciences USA, vol. 104, no. 21, 8720-8725.</span></li>\n<li id=\"cite_note-mjreversal-11\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjreversal_11-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment can actually pass or fail reversal symmetry depending on the rounding method used to find the median when there are even numbers of voters. For instance, in a two-candidate, two-voter race, if the ratings are converted to numbers and the two central ratings are averaged, then MJ meets reversal symmetry; but if the lower one is taken, it does not, because a candidate with [\"fair\",\"fair\"] would beat a candidate with [\"good\",\"poor\"] with or without reversal. However, for rounding methods which do not meet reversal symmetry, the chances of breaking it are on the order of the inverse of the number of voters; this is comparable with the probability of an exact tie in a two-candidate race, and when there's a tie, any method can break reversal symmetry.</span></li>\n<li id=\"cite_note-12\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-12\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority Judgment is summable at order KN, where K, the number of ranking categories, is set beforehand.</span></li>\n<li id=\"cite_note-mjlnh-13\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-mjlnh_13-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Majority judgment meets a related, weaker criterion: ranking an additional candidate below the median grade (rather than your own grade) of your favorite candidate, cannot harm your favorite.</span></li>\n<li id=\"cite_note-minimaxvariant-14\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-minimaxvariant_14-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-minimaxvariant_14-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">A variant of Minimax that counts only pairwise opposition, not opposition minus support, fails the Condorcet criterion and meets later-no-harm.</span></li>\n<li id=\"cite_note-rangeIIA-15\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-rangeIIA_15-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Range satisfies the mathematical definition of IIA, that is, if each voter scores each candidate independently of which other candidates are in the race. However, since a given range score has no agreed-upon meaning, it is thought that most voters would either \"normalize\" or exaggerate their vote such that it votes at least one candidate each at the top and bottom possible ratings. In this case, Range would not be independent of irrelevant alternatives. Balinski M. and R. Laraki (2007) \u00abA theory of measuring, electing and ranking\u00bb. Proceedings of the National Academy of Sciences USA, vol. 104, no. 21, 8720-8725.</span></li>\n<li id=\"cite_note-16\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-16\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Once for each round.</span></li>\n<li id=\"cite_note-17\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-17\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Later preferences are only possible between the two candidates who make it to the second round.</span></li>\n<li id=\"cite_note-18\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-18\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">That is, second-round votes cannot harm candidates already eliminated.</span></li>\n<li id=\"cite_note-sodabase-19\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-sodabase_19-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Unless otherwise noted, for SODA's compliances:</span> \n<ul style=\"line-height: 1.5em; margin: 0.3em 0px 0px 1.6em; padding: 0px; list-style-image: url(data;\">\n<li style=\"margin-bottom: 0.1em;\"><span class=\"reference-text\">Delegated votes are considered to be equivalent to voting the candidate's predeclared preferences.</span></li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"reference-text\">Ballots only are considered (In other words, voters are assumed not to have preferences that cannot be expressed by a delegated or approval vote.)</span></li>\n<li style=\"margin-bottom: 0.1em;\"><span class=\"reference-text\">Since at the time of assigning approvals on delegated votes there is always enough information to find an optimum strategy, candidates are assumed to use such a strategy.</span></li>\n</ul>\n</li>\n<li id=\"cite_note-sodamono-20\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-sodamono_20-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">For up to 4 candidates, SODA is monotonic. For more than 4 candidates, it is monotonic for adding an approval, for changing from an approval to a delegation ballot, and for changes in a candidate's preferences. However, if changes in a voter's preferences are executed as changes from a delegation to an approval ballot, such changes are not necessarily monotonic with more than 4 candidates.</span></li>\n<li id=\"cite_note-soda4-21\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\">^&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-soda4_21-0\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up to:</span><sup style=\"line-height: 1em;\"><em><strong>a</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-soda4_21-1\"><sup style=\"line-height: 1em;\"><em><strong>b</strong></em></sup></a>&nbsp;<a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-soda4_21-2\"><sup style=\"line-height: 1em;\"><em><strong>c</strong></em></sup></a></span>&nbsp;<span class=\"reference-text\">For up to 4 candidates, SODA meets the Participation, IIA, and Cloneproof criteria. It can fail these criteria in certain rare cases with more than 4 candidates. This is considered here as a qualified success for the Consistency and Participation criteria, which do not intrinsically have to do with numerous candidates, and as a qualified failure for the IIA and Cloneproof criteria, which do.</span></li>\n<li id=\"cite_note-22\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-22\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">SODA voting passes reversal symmetry for all scenarios that are reversible under SODA; that is, if each delegated ballot has a unique last choice. In other situations, it is not clear what it would mean to reverse the ballots, but there is always some possible interpretation under which SODA would pass the criterion.</span></li>\n<li id=\"cite_note-23\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-23\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">SODA voting is always polytime computable. There are some cases where the optimal strategy for a candidate assigning delegated votes may not be polytime computable; however, such cases are entirely implausible for a real-world election.</span></li>\n<li id=\"cite_note-24\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-24\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Later preferences are only possible through delegation, that is, if they agree with the predeclared preferences of the favorite.</span></li>\n<li id=\"cite_note-25\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-25\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Random winner: Uniformly randomly chosen candidate is winner. Arbitrary winner: some external entity, not a voter, chooses the winner. These systems are not, properly speaking, voting systems at all, but are included to show that even a horrible system can still pass some of the criteria.</span></li>\n<li id=\"cite_note-26\" style=\"margin-bottom: 0.1em;\"><span class=\"mw-cite-backlink\"><strong><a style=\"text-decoration: none; color: #0b0080; background-image: none; background-position: initial initial; background-repeat: initial initial;\" href=\"https://en.wikipedia.org/wiki/User:Homunq#cite_ref-26\"><span class=\"cite-accessibility-label\" style=\"top: -99999px; clip: rect(1px 1px 1px 1px); overflow: hidden; position: absolute !important; padding: 0px !important; border: 0px !important; height: 1px !important; width: 1px !important;\">Jump up</span>^</a></strong></span>&nbsp;<span class=\"reference-text\">Random ballot: Uniformly random-chosen ballot determines winner. This and closely related systems are of mathematical interest because they are the only possible systems which are truly strategy-free, that is, your best vote will never depend on anything about the other voters. They also satisfy both consistency and IIA, which is impossible for a deterministic ranked system. However, this system is not generally considered as a serious proposal for a practical method.</span></li>\n</ol></div>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\"><strong id=\"11__Footnotes\">11. Footnotes</strong></p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u00b9 When I call my introduction \"overblown\", I mean that I reserve the right to make broad generalizations there, without getting distracted by caveats. If you don't like this style, feel free to skip to section 2.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u00b2 Of course, the original \"politics is a mind killer\" sequence was perfectly clear about this: \"<span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Politics is an important domain to which we should individually apply our rationality\u2014but it's a terrible domain in which to&nbsp;</span><em style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">learn</em><span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;rationality, or discuss rationality, unless all the discussants are already rational.\" The focus here is on the first part of that quote, because I think Less Wrong as a whole has moved too far in the direction of avoiding politics as not a domain for rationalists.</span></p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u00b3 Bayes developed his theorem decades before Condorcet's <em>Essai</em>, but Condorcet probably didn't know of it, as it wasn't popularized by Laplace until about 30 years later, after Condorcet was dead.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2074 Yes, this happens to be the same Alan Gibbard from the previous paragraph.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2075 Confusingly, \"public choice\" refers to a school of thought, while \"social choice\" is the name for the broader domain of study. Stop reading this footnote now if you don't want to hear mind-killing partisan identification. \"Public choice\" theorists are generally seen as politically conservative in the solutions they suggest. It seems to me that the broader \"social choice\" has avoided taking on a partisan connotation in this sense.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2076 Score voting is also called \"range voting\" by some. It is not a particularly new idea \u2014 for instance, the \"loudest cheer wins\" rule of ancient Sparta, and even aspects of honeybees' process for choosing new hives, can be seen as score voting \u2014 but it was first analyzed theoretically around 2000. Approval voting, which can be seen as a form of score voting where the scores are restricted to 0 and 1, had entered theory only about two decades earlier, though it too has a history of practical use back to antiquity.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2077 OK, fine, this is a simplification. As a voter, you have imperfect information about the true level of support and propensity to vote in the superpopulation of eligible voters, so in reality the chances of a decisive tie between other than your two expected frontrunners is non-zero. Still, in most cases, it's utterly negligible.</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em; line-height: 19.18402862548828px; font-family: sans-serif; font-size: 13.333333969116211px;\">\u2078 This article will focus more on the literature on multi-player strategic voting (competing boundedly-instrumentally-rational agents) than on multi-player Aumann (cooperating boundedly-epistemically-rational agents). If you're interested in the latter, here are some starting points: <a href=\"http://www.scottaaronson.com/papers/agree-econ.pdf\">Scott Aaronson's work</a> is, as far as I know, the state of the art on 2-player Aumann, but its framework assumes that the players have a sophisticated ability to empathize and reason about each others' internal knowledge, and the problems with this that Aaronson plausibly handwaves away in the 2-player case are probably less tractable in the multi-player one. <a href=\"http://link.springer.com/chapter/10.1007/978-3-642-25510-6_13#page-1\">Dalkiran et al</a> deal with an Aumann-like problem over a social network; they find that attempts to \"jump ahead\" to a final consensus value instead of simply dumbly approaching it asymptotically can lead to failure to converge. And <a href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6481628&amp;tag=1\">Kanoria et al</a> have perhaps the most interesting result from the perspective of this article; they use the convergence of agents using a naive voting-based algorithm to give a nice upper bound on the difficulty of full Bayesian reasoning itself. None of these papers explicitly considers the problem of coming to consensus on more than one logically-related question at once, though Aaronson's work at least would clearly be easy to extend in that direction, and I think such extensions would be unsurprisingly Bayesian.</p>", "sections": [{"title": "2. Collective rationality:\u00a0Condorcet's ideals and Arrow's problem", "anchor": "2__Collective_rationality__Condorcet_s_ideals_and_Arrow_s_problem", "level": 2}, {"title": "3. Further issues for politics", "anchor": "3__Further_issues_for_politics", "level": 2}, {"title": "4. Rating versus ranking", "anchor": "4__Rating_versus_ranking", "level": 2}, {"title": "5. Delegation and SODA", "anchor": "5__Delegation_and_SODA", "level": 2}, {"title": "6. Criteria and pathologies", "anchor": "6__Criteria_and_pathologies", "level": 2}, {"title": "7. Representation, proportionality, and sortition", "anchor": "7__Representation__proportionality__and_sortition", "level": 2}, {"title": "8. What I'm doing about it and what you can", "anchor": "8__What_I_m_doing_about_it_and_what_you_can", "level": 2}, {"title": "9. Conclusions and future directions ", "anchor": "9__Conclusions_and_future_directions_", "level": 2}, {"title": "Compliance of selected systems (table)", "anchor": "Compliance_of_selected_systems__table_", "level": 1}, {"title": "11. Footnotes", "anchor": "11__Footnotes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NKECtGX4RZPd7SqYp", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-30T17:44:33.128Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, chapters 94-96", "slug": "meetup-durham-rtlw-hpmor-discussion-chapters-94-96", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LH6FZoNyzTGLgxHHY/meetup-durham-rtlw-hpmor-discussion-chapters-94-96", "pageUrlRelative": "/posts/LH6FZoNyzTGLgxHHY/meetup-durham-rtlw-hpmor-discussion-chapters-94-96", "linkUrl": "https://www.lesswrong.com/posts/LH6FZoNyzTGLgxHHY/meetup-durham-rtlw-hpmor-discussion-chapters-94-96", "postedAtFormatted": "Wednesday, October 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20chapters%2094-96&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20chapters%2094-96%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLH6FZoNyzTGLgxHHY%2Fmeetup-durham-rtlw-hpmor-discussion-chapters-94-96%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20chapters%2094-96%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLH6FZoNyzTGLgxHHY%2Fmeetup-durham-rtlw-hpmor-discussion-chapters-94-96", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLH6FZoNyzTGLgxHHY%2Fmeetup-durham-rtlw-hpmor-discussion-chapters-94-96", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sw'>Durham/RTLW HPMoR discussion, chapters 94-96</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 November 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham, NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bring a question, an observation, or any other thing you might like to discuss from chapters 94-96 of HPMoR.</p>\n\n<p>We'll gather coffee around noon and have discussion 12:30-2ish.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sw'>Durham/RTLW HPMoR discussion, chapters 94-96</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LH6FZoNyzTGLgxHHY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24524", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__chapters_94_96\">Discussion article for the meetup : <a href=\"/meetups/sw\">Durham/RTLW HPMoR discussion, chapters 94-96</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 November 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham, NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bring a question, an observation, or any other thing you might like to discuss from chapters 94-96 of HPMoR.</p>\n\n<p>We'll gather coffee around noon and have discussion 12:30-2ish.</p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__chapters_94_961\">Discussion article for the meetup : <a href=\"/meetups/sw\">Durham/RTLW HPMoR discussion, chapters 94-96</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, chapters 94-96", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__chapters_94_96", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, chapters 94-96", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__chapters_94_961", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-30T18:37:48.529Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC/VA Games meetup", "slug": "meetup-washington-dc-va-games-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b6SF58thwXY9YLnkw/meetup-washington-dc-va-games-meetup", "pageUrlRelative": "/posts/b6SF58thwXY9YLnkw/meetup-washington-dc-va-games-meetup", "linkUrl": "https://www.lesswrong.com/posts/b6SF58thwXY9YLnkw/meetup-washington-dc-va-games-meetup", "postedAtFormatted": "Wednesday, October 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%2FVA%20Games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%2FVA%20Games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb6SF58thwXY9YLnkw%2Fmeetup-washington-dc-va-games-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%2FVA%20Games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb6SF58thwXY9YLnkw%2Fmeetup-washington-dc-va-games-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb6SF58thwXY9YLnkw%2Fmeetup-washington-dc-va-games-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sx'>Washington DC/VA Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 November 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">6305 Windward Dr., Burke VA 22015, 703-239-9660.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(The time should read 3:00 pm: there's been some weirdness with the display).</p>\n\n<p>We'll be meeting to hang out and play games. The meetup is at a house and not directly metro accessible, so there will be a van to pick people up from Franconia-Springfield metro at 2:55 pm. If you need a ride from the metro and suspect you will be unable to make the pickup time, please message <a href=\"http://lesswrong.com/message/compose/?to=rocurley\">me</a> or <a href=\"http://lesswrong.com/message/compose/?to=maia\">maia</a> and let us know ASAP.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sx'>Washington DC/VA Games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b6SF58thwXY9YLnkw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4017799498283445e-06, "legacy": true, "legacyId": "24525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_VA_Games_meetup\">Discussion article for the meetup : <a href=\"/meetups/sx\">Washington DC/VA Games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 November 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">6305 Windward Dr., Burke VA 22015, 703-239-9660.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(The time should read 3:00 pm: there's been some weirdness with the display).</p>\n\n<p>We'll be meeting to hang out and play games. The meetup is at a house and not directly metro accessible, so there will be a van to pick people up from Franconia-Springfield metro at 2:55 pm. If you need a ride from the metro and suspect you will be unable to make the pickup time, please message <a href=\"http://lesswrong.com/message/compose/?to=rocurley\">me</a> or <a href=\"http://lesswrong.com/message/compose/?to=maia\">maia</a> and let us know ASAP.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_VA_Games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/sx\">Washington DC/VA Games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC/VA Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_VA_Games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC/VA Games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_VA_Games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-30T20:14:52.156Z", "modifiedAt": null, "url": null, "title": "Why officers vs. enlisted?", "slug": "why-officers-vs-enlisted", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:38.186Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/98o4yRCT6GkogR4f2/why-officers-vs-enlisted", "pageUrlRelative": "/posts/98o4yRCT6GkogR4f2/why-officers-vs-enlisted", "linkUrl": "https://www.lesswrong.com/posts/98o4yRCT6GkogR4f2/why-officers-vs-enlisted", "postedAtFormatted": "Wednesday, October 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20officers%20vs.%20enlisted%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20officers%20vs.%20enlisted%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F98o4yRCT6GkogR4f2%2Fwhy-officers-vs-enlisted%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20officers%20vs.%20enlisted%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F98o4yRCT6GkogR4f2%2Fwhy-officers-vs-enlisted", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F98o4yRCT6GkogR4f2%2Fwhy-officers-vs-enlisted", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 913, "htmlBody": "<p>It's always puzzled me that, in armies, officers form a separate hierarchical ladder from the NCOs and enlisted soldiers.</p>\n<p><span style=\"text-align: center;\">Armies could have a single hierarchy, top to bottom, as in the simplified diagram below on the left. Instead, all armies have two&nbsp;</span><em style=\"text-align: center;\">distinct</em><span style=\"text-align: center;\">&nbsp;ladders, with one strictly above the other, as on the right. &nbsp;(Reminds me of those wacky non-standard integers.)</span></p>\n<p style=\"text-align: center;\">&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_ivk_2.png?v=889f5394ab36da1e54c30a6a5cd424db\" alt=\"\" height=\"150\" />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<img src=\"http://images.lesswrong.com/t3_ivk_3.png?v=3365fc473bad86d0f337132649a5d470\" alt=\"\" height=\"150\" />&nbsp;&nbsp;</p>\n<p>The usual answers are obvious but irrelevant: Yes, some people shoot straight to a position high on the ladder. You could do that with either model. Yes, even when those lower down on the ladder have more experience and wisdom, it can make practical sense to have a hierarchy. Yes, the higher someone is, the higher the level of the decisions they make. You could likewise do these on a one-ladder model.</p>\n<p>It's said that officers \"decide,\" while non-officers \"just carry out &nbsp;orders\"; or that officers choose strategy, and non-officers do tactics. But everyone makes decisions, on their own level. A private makes decisions for himself, a corporal for three soldiers, and a colonel for a thousand, each one in the context of their orders from above. One soldier's strategy is his superior's tactics. And the distinction is not based on command: New army doctors automatically become officers, even if they don't command anyone. Doctors are non-combatants, but fighter pilots are combatants <em>par excellence</em>, don't command anyone, and are all officers.</p>\n<p>These answers don't explain why there need to be two ladders.&nbsp;<a href=\"http://www.quora.com/Military/Why-are-armies-divided-into-officers-and-enlisted-men\">I asked at Quora </a>without a convincing answer. Historically, the distinction was based on social classes, but that doesn't explain why <em>every </em>army follows this arrangement, including those in very different societies.</p>\n<p>Similarly: What's a corporate executive? (I'm talking about large companies here; small companies and startups are different.) I understand that there is a management hierarchy, but why the arbitrary distinction between a senior manager and a junior executive? Aren't those just two rungs on the ladder? In corporate-speak, an executive is called a \"decision maker.\" What a strange term! Isn't a manager or even a lowly \"individual contributor\" also a decision maker -- at the scope that their own managers allow? (I should add that the two-ladder system is not as developed in business as it is in the army &nbsp;or in medicine. There is no career ladder for non-execs that extends arbitrarily high, though always below the execs.)</p>\n<p>Not all professions work that way. Actuaries have ten levels, based on passing a sequence of exams. And though some areas of engineering distinguish an engineer from a technician, software engineering has no such dichotomy: Some software engineers make more money, and some make broader decisions or manage others, but there is no two-way split.</p>\n<p>In medicine, on the other hand, there is a clear distinction between doctors and nurses. There are different status levels among doctors and among nurses, but a PhD in nursing stands on the other side of a clear border from a beginning MD. Similarly with lawyers and paralegals. These dichotomies stem from licensing restrictions, which in turn are descended from medieval guild practices. But why does it have to be this way? Why not just rank medical personnel, or legal personnel, in a single continuum from practical nurse through rockstar brain surgeon. (Is that a title?). There would still be the understanding that some people will never climb beyond a certain point, while others can jump straight to a higher rung.</p>\n<p>The answer lies in LessWrong's concept of \"agentiness\": Making \"<a href=\"/lw/5i8/the_power_of_agency/\">choices</a> so as to maximize the fulfillment of explicit desires, given explicit beliefs.\" Less abstractly, it is sometimes described as&nbsp;\"<a href=\"/lw/id2/to_what_degree_do_you_model_people_as_agents/\">reliability and responsibility</a>.\" Agenty types<a href=\"http://wiki.lesswrong.com/wiki/Jargon\"> get to be called</a> \"Player Characters\" or heroes. (\"Agenty\" and \"agentiness\" are made-up words, and the standard terminology is \"agent\" and&nbsp;\"agency.\") &nbsp;I think \"agenty\" was made up to point out that while all humans are agents to some extent, some do it far better than others.)</p>\n<p>In the organizational context, officers and executives are meant to be agenty, while enlisted/NCO and non-executives are not. The officers and executives plan towards achieving goals, while everyone else executes defined tasks. The officers and executives make high-variance decisions, with high risks and high returns, while everyone else has the job of just doing their job consistently and not messing up.</p>\n<p>Is agentiness a natural kind, <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">a cluster in thingspace</a>, a <a href=\"http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0174%3Atext%3DPhaedrus%3Asection%3D265e\">joint-carving</a> concept? Might agentiness just be a mix of features that occur to varying degrees in various contexts?</p>\n<p>We might say that agentiness is a continuum: Everyone has some, but some people have more than others. Lower-downs sometimes have goals, and higher-ups often act like cogs. Moreover, the agentiness of officers and executives is strictly in the context of their superiors' goals: They may be agenty, but not for their individual goals. It would be more accurate to say that in their roles they are <em>meant</em>&nbsp;to be agenty, on behalf of the organization.</p>\n<p>Some people are non-agenty in some of their social roles and agenty in others. For example, I know workers who readily admit to being lowly cogs in a machine, but who have tremendous achievements in setting up and leading non-profits outside work hours. Some hard-driving workaholics are milquetoasts at home. Some caring, wise, foresightful parents are limp rags at work.</p>\n<p>But agentiness is a real concept, at least so far as the officers and executives go. Their roles are implicitly defined by agentiness. Armies and corporations decide which people have it (or at least are meant to). These organizations agree with LessWrong that agentiness is a natural kind.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xXX3n22DQZuKqXEdT": 1, "2EFq8dJbxKNzforjM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "98o4yRCT6GkogR4f2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 23, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "24464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 144, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vbcjYg6h3XzuqaaN8", "cenSWez9Ddgsjd5Fc", "WBw8dDkAWohFjWQSk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-30T22:10:45.071Z", "modifiedAt": null, "url": null, "title": "I need some help debugging my approach to informal models and reasoning", "slug": "i-need-some-help-debugging-my-approach-to-informal-models", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.431Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WCTSWZhPz3iPkjEud/i-need-some-help-debugging-my-approach-to-informal-models", "pageUrlRelative": "/posts/WCTSWZhPz3iPkjEud/i-need-some-help-debugging-my-approach-to-informal-models", "linkUrl": "https://www.lesswrong.com/posts/WCTSWZhPz3iPkjEud/i-need-some-help-debugging-my-approach-to-informal-models", "postedAtFormatted": "Wednesday, October 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20need%20some%20help%20debugging%20my%20approach%20to%20informal%20models%20and%20reasoning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20need%20some%20help%20debugging%20my%20approach%20to%20informal%20models%20and%20reasoning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWCTSWZhPz3iPkjEud%2Fi-need-some-help-debugging-my-approach-to-informal-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20need%20some%20help%20debugging%20my%20approach%20to%20informal%20models%20and%20reasoning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWCTSWZhPz3iPkjEud%2Fi-need-some-help-debugging-my-approach-to-informal-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWCTSWZhPz3iPkjEud%2Fi-need-some-help-debugging-my-approach-to-informal-models", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 621, "htmlBody": "<p>I'm having trouble understanding the process I should use when I am considering new models as they might apply to old data, like memories. This is primarily when reasoning with respect to qualitative models, like those that come out of development psychology, business, or military strategy. These models can be either normative or descriptive, but the big trait that they all seem to share is that they were all conceptualized with reference to the inside view more than the outside view - they were based on either memories or intuition, so they will have a lot of implicit internal structure, or they will have a lot of bullshit. Re-framing my own experiences as a way of finding out whether these models are useful is thus reliant on system one more than system two. Unfortunately now we're in the realm of bias.<br /><br />My concrete examples of models that I am evaluating are (a) when I am attempting to digest the information contained in the \"Principles\" document (as discussed <a href=\"https://groups.google.com/forum/#%21searchin/vancouver-rationalists/principles/vancouver-rationalists/oQLE9tUjQeY/vrWn0UW3KbUJ\" target=\"_blank\">here</a>) and for which situations the information might apply in; (b) learning Alfred Adler's \"individual psychology\" from <a href=\"http://therawness.com/raw-concepts-the-theaters-of-operation/\" target=\"_blank\">The Rawness</a>, which also expands the ideas and (c) the mighty <a href=\"http://en.wikipedia.org/wiki/OODA_loop\" target=\"_blank\">OODA loop</a>.<br /><br />When I brought up the OODA loop during a meetup with the Vancouver Rationalists I ended up making some mistakes regarding the \"theories\" from which it was derived, adding the idea of \"clout\" to my mental toolkit. But it also makes me wary that my instinctive approach to learning about qualitative models such as this might have other weaknesses.<br /><br />I asked at another meetup, \"What is the best way to internalize advice from books?\" and someone responded with thinking about concrete situations where the idea might have been useful.&nbsp; <br /><br />As a strategy to evaluate the truth of a model I can see this backfiring. Due to the reliance on System One in both model structuring and model evaluation, hindsight bias is likely to be an issue, or a form of Forer effect. I could then make erroneous judgements on how effectively the model will predict an outcome, and use the model in ineffective ways (ironically this is brought up by the author on The Rawness). In most cases I believe that this is better than nothing, but I don't think it's good enough either. It does seem possible to be mindful of the actual conceptual points and just wait for relevance, but the reason why we reflect is so that we are primed to see certain patterns again when they come up, so that doesn't seem like enough either.<br /><br />As a way of evaluating model usefulness I can see this go two ways. On one hand, many long-standing problems exist due to mental ruts, and benefit from re-framing the issue in light of new information. When I read books I often experience a linkage between statements that a book makes and goals that I have, or situations I want to make sense of (similar to Josh Kaufman and his usage of the <a href=\"http://joshkaufman.net/3-simple-techniques-to-optimize-your-reading-comprehension-and-retention/\" target=\"_blank\">McDowell's Reading Grid)</a>. However, this experience has little to do with the model being correct.<br /><br />Here are three questions I have, although more will likely come up:</p>\n<ul>\n<li>What are the most common mistakes humans make when figuring out if a qualitative model applies to their experiences or not? </li>\n<li>How can they be worked around, removed, or compensated for? </li>\n<li>Can we make statements about when \"informal\" models (i.e. not specified in formal language or not mappable to mathematical descriptions other than in structures like semantic webs) are <em>generally</em> useful to have and when they <em>generally</em> fail?</li>\n<li>etc.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WCTSWZhPz3iPkjEud", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4019832958026007e-06, "legacy": true, "legacyId": "24526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-31T01:17:20.995Z", "modifiedAt": null, "url": null, "title": "Aliveness in Training", "slug": "aliveness-in-training", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.125Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3XmDRYcnXHbwuWCf7/aliveness-in-training", "pageUrlRelative": "/posts/3XmDRYcnXHbwuWCf7/aliveness-in-training", "linkUrl": "https://www.lesswrong.com/posts/3XmDRYcnXHbwuWCf7/aliveness-in-training", "postedAtFormatted": "Thursday, October 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aliveness%20in%20Training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAliveness%20in%20Training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3XmDRYcnXHbwuWCf7%2Faliveness-in-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aliveness%20in%20Training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3XmDRYcnXHbwuWCf7%2Faliveness-in-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3XmDRYcnXHbwuWCf7%2Faliveness-in-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 779, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/gn/the_martial_art_of_rationality/\">The Martial Art of Rationality</a></p>\n<p>One principle in the martial arts is that arts that are practiced with <em>aliveness</em> tend to be more effective.</p>\n<p>\"Aliveness\" in this case refers to a set of training principles focused on simulating conditions in an actual fight as closely as possible in training. Rather than train techniques in a vacuum or against a compliant opponent, alive training focuses on training with movement, timing, and energy under conditions that approximate those where the techniques will actually be used.<sup>[1]</sup></p>\n<p>A good example of training that <em>isn't</em> alive would be methods that focused entirely on practicing <a href=\"http://en.wikipedia.org/wiki/Kata\">kata</a> and forms without making contact with other practitioners; a good example of training that <em>is </em>alive would be methods that focused on verifying the efficacy of techniques through full-contact engagement with other practitioners.</p>\n<p>Aliveness tends to create an environment free from <a href=\"/lw/2i/epistemic_viciousness/\">epistemic viciousness</a>-- if your technique doesn't work, you'll know because you won't be able to use it against an opponent. Further, if your technique <em>does</em> work, you'll know that it works because you will have applied it against people trying to prevent you from doing so, and the added confidence will help you better apply that technique when you need it.</p>\n<p>Evidence from martial arts competitions indicates that those who practice with aliveness are more effective than others. One of the chief reasons that <a href=\"http://en.wikipedia.org/wiki/Brazilian_Jiu-Jitsu\">Brazilian jiu-jitsu</a> (BJJ) practitioners were so successful in early mixed martial arts tournaments was that BJJ-- a martial art that relies primarily on grappling and the use of submission holds and locks to defeat the opponent-- can be trained safely with almost complete aliveness, whereas many other martial arts cannot.<sup>[2]</sup></p>\n<p>Now, this is not to say that one should only attempt to practice martial arts under completely realistic conditions. For instance, no martial arts school that I am aware of randomly ambushes or attempts to mug its students on the streets outside of class in order to test how they would respond under truly realistic conditions.<sup>[3]</sup></p>\n<p>Even in the age of sword duels, people would train with blunt weapons and protective armor rather than sharp weapons and ordinary clothes. Would training with sharp weapons and ordinary clothes be more alive than training with blunt weapons and protective armor? Certainly, but the trainees wouldn't be! And yet training with blunt weapons is still useful-- the fact that training does not fully approximate realistic conditions does not intrinsically mean it is bad.</p>\n<p>That being said, generally speaking martial arts training that is more alive-- that better approximates realistic fighting conditions-- is more effective within reasonable safety margins. There is a growing consensus among students of martial arts who are looking for effective self-defense techniques that the specific martial art one practices is not hugely relevant, and that what matters more is the extent to which the training does or doesn't use aliveness.</p>\n<p>&nbsp;</p>\n<p><strong>Aliveness and Rationality</strong></p>\n<p>So, that's all well and good-- but how can we apply these principles to rationality practice?</p>\n<p>While martial arts training has very clear methods of measuring whether or not skills work (can I apply this technique against a resisting opponent?), rationality training is much murkier-- measuring rationality skills is a nontrivial problem.</p>\n<p>Further, under normal circumstances the opponent that you are resisting when applying rationality techniques is your own brain, not an external enemy.<sup>[4]</sup> This makes applying appropriate levels of resistance in training difficult, because it's very easy to cheat yourself. The best method that I have found thus far is <a href=\"/lw/i94/better_rationality_through_lucid_dreaming/\">lucid dreaming</a>, as forcing your dreaming brain to recognize its true state through the various hallucinations and constructed memories associated with dreaming is no easy task.</p>\n<p>That being said, I make no claims to special or unique knowledge in this area. If anyone has suggestions for useful methods of \"live\" rationality practice, I'd love to hear them.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>[1] For further explanation, see Matt Thornton's classic video <a href=\"http://www.youtube.com/watch?v=932XUCWlelQ\">\"Why Aliveness?\"</a></p>\n<p>[2] If your plan is to choke someone until they fall unconscious, it is possible to safely train for this with nearly complete aliveness by wrestling against an opponent and simply releasing the chokehold before they actually fall unconscious. By contrast, it is much harder to safely train to punch someone into unconsciousness, and harder still to safely train to break people's necks.</p>\n<p>[3] The game of <a href=\"http://en.wikipedia.org/wiki/Assassin_%28game%29\">Assassins</a> <em>does</em> do this, but usually follows rules that are constrained enough to make it a suboptimal method of training.</p>\n<p>[4] There are some contexts in which rationality techniques are applied in order to overcome an external enemy. Competitive games and some sports are a good method of finding practice in this respect. For instance, in order to be a competitive <a href=\"http://en.wikipedia.org/wiki/Magic_%28card_game%29\">Magic: The Gathering</a> player, you need to engage many epistemic and instrumental rationality skills. Competitive poker <a href=\"http://rationalpoker.com/\">can offer similar development.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3XmDRYcnXHbwuWCf7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 1.4021615281852193e-06, "legacy": true, "legacyId": "24363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["teaxCFgtmCQ3E9fy8", "T8ddXNtmNSHexhQh8", "6MNjzszLa2w8teDxs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-31T04:08:45.436Z", "modifiedAt": "2020-09-09T04:29:22.294Z", "url": null, "title": "Human Memory: Problem Set", "slug": "human-memory-problem-set", "viewCount": null, "lastCommentedAt": "2015-06-22T06:43:16.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K36Xmhu76Fhi9hNNA/human-memory-problem-set", "pageUrlRelative": "/posts/K36Xmhu76Fhi9hNNA/human-memory-problem-set", "linkUrl": "https://www.lesswrong.com/posts/K36Xmhu76Fhi9hNNA/human-memory-problem-set", "postedAtFormatted": "Thursday, October 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Human%20Memory%3A%20Problem%20Set&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHuman%20Memory%3A%20Problem%20Set%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK36Xmhu76Fhi9hNNA%2Fhuman-memory-problem-set%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Human%20Memory%3A%20Problem%20Set%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK36Xmhu76Fhi9hNNA%2Fhuman-memory-problem-set", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK36Xmhu76Fhi9hNNA%2Fhuman-memory-problem-set", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 913, "htmlBody": "<p>I'm working on a post about how best to use human memory<span style=\"color: #444444; font-family: arial, sans-serif; line-height: 16px;\">&mdash;</span>when it's good to store things in your own brain and why, when it's best to outsource your memory, what memory upgrades are worthwhile in what contexts, and how to integrate and apply memory systems in real life. I'm hoping the following set of memory problems will draw out approaches that haven't occurred to me so I can compare a wider range of methods.</p>\n<p>I'll post the first solutions I thought of myself later on, but for now I'd like to hear what you would do in each of these situations and what you believe to be the pros and cons of your answers. Can you think of ways to improve upon your first thoughts and the answers of others?</p>\n<p>(You don't have to respond to all of the questions; feel free to post as little or as much as comes to mind.)</p>\n<hr />\n<p><a id=\"more\"></a></p>\n<p style=\"padding-left: 30px;\"><img style=\"float: right;\" src=\"http://sandivand.files.wordpress.com/2012/02/remember.jpg\" alt=\"\" width=\"189\" height=\"293\" /><strong></strong></p>\n<p style=\"padding-left: 30px;\"><strong>1.</strong> I'm leaving for a week-long business trip. The last time this happened, I wasn't totally sure I'd remembered to lock my door. The worry wouldn't leave my mind and was making it hard to relax, so I eventually called a friend and asked them to stop by and make sure it was locked. Now, I'm standing in front of my door, suitcase in hand, all ready to go, and I'm about to lock it. What should I do?</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>2. </strong>I'm in the middle of a conversation with a friend. He mentions a book that really interests me. It's called <em>Antifragile</em> and I think, \"I've got to remember to Google that.\" But the conversation continues, and it would be rude to pull out my phone right this minute.</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>3.</strong> I bought a new fridge. This one closes differently than my old fridge: My old fridge would swing shut and seal nicely if I just left it open, while this one requires that I push on the door even when it's mostly closed to ensure it seals. For some reason, I'm having a hard time with this transition. I've had it for a week, and I've come home from work to find my more perishable food spoiled three times already. It's not that there's a problem with the latching mechanism or that the seal is bad. The problem is that I keep forgetting.</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>4.</strong> I'm driving in an unfamiliar city. My GPS says my destination is somewhere around here, but I suspect it's working with slightly outdated information. Rather than driving around aimlessly in hopes that I'll randomly run across that which I seek, I sensibly stop to ask a local for directions. Unfortunately, it seems that the business I'm looking for has moved to another part of the city, and I'm going to have to remember more than <a href=\"http://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two\">seven things</a> to get there.</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>5.</strong> I'm preparing a talk for an upcoming conference. It's not the kind of thing that would benefit much from a Power Point, so using one would be tacky. I don't care for the idea of flipping through note cards as I go either. And I definitely don't want to memorize an entire speech word-for-word. But neither do I want to just wing it and hope it turns out well-structured and non-rambly, and I'm afraid I'll forget one of the key points if I get nervous.</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>6.&nbsp;</strong>I've just started at a new school. I've been through this before, so I know that in the next two weeks I'm going to have to say or write my student ID number about a zillion times, after which point I'll still need it, but only occasionally. It's 12 digits long: 000458789625. I'll surely have it memorized by the end of the first two weeks just because I've repeated it so much, but it sure would be nice to not have to pull out my wallet, find my ID, switch my attention back and forth between that and what I'm writing, and then double check. Every damn time.</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>7.&nbsp;</strong>In an attempt to reduce stress, I'm trying to maintain a clear boundary between home and work by not working while I'm at home. But almost every evening, I think of several things related to work that I really want to remember. It's the worst when this happens as I'm trying to fall asleep, all snugly under the covers, and I don't even want to open my eyes to enter the thought into my phone, let alone to turn on a light, find my calendar, and pencil in a deadline.</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>8.&nbsp;</strong>This evening, I'll be going to a large and crowded party where my main goal is networking. I know of several important people who will be there (and whose favor I'd very much like to gain), and I know there will be dozens more I've never heard of with whom it will be useful to connect. But I'm getting anxious, because I'm terrible with names, and I don't want to make a fool of myself by forgetting something like which major startups one of the known important people has been funding.</p>\n<hr />\n<p style=\"padding-left: 30px;\"><strong>9.&nbsp;</strong>I'm studying for the medical licensing exam. This basically means somehow turning myself into an encyclopedia of modern medicine. It's a giant mess of mostly disconnected facts, weird jargon, and lengthy procedures. I cannot even imagine fitting all of this into a single brain. I stare blankly at the table of contents of a 700 page textbook, and I begin to panic.</p>\n<hr />\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K36Xmhu76Fhi9hNNA", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 23, "extendedScore": null, "score": 1.4023252854160535e-06, "legacy": true, "legacyId": "24506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-10-31T04:08:45.436Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-31T07:06:56.951Z", "modifiedAt": null, "url": null, "title": "Very Basic Model Theory", "slug": "very-basic-model-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:53.655Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F6BrJFkqEhh22rFsZ/very-basic-model-theory", "pageUrlRelative": "/posts/F6BrJFkqEhh22rFsZ/very-basic-model-theory", "linkUrl": "https://www.lesswrong.com/posts/F6BrJFkqEhh22rFsZ/very-basic-model-theory", "postedAtFormatted": "Thursday, October 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Very%20Basic%20Model%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVery%20Basic%20Model%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6BrJFkqEhh22rFsZ%2Fvery-basic-model-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Very%20Basic%20Model%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6BrJFkqEhh22rFsZ%2Fvery-basic-model-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6BrJFkqEhh22rFsZ%2Fvery-basic-model-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3146, "htmlBody": "<p>In this post I'll discuss some basic results of model theory. It may be helpful to read through <a href=\"/lw/ix5/mental_context_for_model_theory/\">my previous post</a> if you haven't yet. Model Theory is an implicit context for the Heavily Advanced Epistemology sequence and for a few of the recent MIRI papers, so casual readers may find this brief introduction useful. And who knows, maybe it will pique your interest:</p>\n<h2 id=\"afewlogics\">A tale of two logics</h2>\n<p><strong>propositional logic</strong> is the \"easy logic\", built from basic symbols and the connectives \"and\" and \"not\". Remember that all other connectives can be built from these two: With Enough NAND Gates You Can Rule The World and all that. Propositional logic is sometimes called the \"sentential logic\", because it's not like any other logics are \"of or relating to sentences\" (/sarcasm).</p>\n<p><strong>first order logic</strong> is the \"nice logic\". It has quantifiers (\"there exists\", \"for all\") and an internal notion of equality. Its sentences contain constants, functions, and relations. This lets you say lots of cool stuff that you can't say in propositional logic. First order logic turns out to be quite friendly (as we'll see below). However, it's not strong enough to talk about certain crazy/contrived ideas that humans cook up (such as \"the numbers\").</p>\n<p>There are many other logics available (second order logic AKA \"the heavy guns\", &omega;-logic AKA \"please just please can I talk about numbers\", and many more). In this post we'll focus on propositional and first-order logics.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"trainingwheels\">Training Wheels</h2>\n<p>As a warm-up, let's define \"models\" for propositional logic.</p>\n<p>Remember, logics <em>together</em> with languages produce sentences. Languages of propositional logic are sets of basic symbols. From these symbols and the two connectives of propositional logic (&and; and &not;) we construct our sentences.</p>\n<p>For example, from the language <code>{x, y}</code> we can build the sentences <code>x</code>, <code>&not;x</code>, <code>x&and;y</code>, and many more. We <em>cannot</em> build sentences like <code>hello</code> (which doesn't use the right symbols) or <code>&not;xy</code> (which breaks the rules of the logic).</p>\n<p>We want to study interpretations for such sentences, where an \"interpretation\" is some object which assigns a truth value to each sentence. This is denoted by the \u22a7 operator: <code>A\u22a7B</code> expresses some form of \"A models B as true\". This operator is used frequently and in many contexts throughout model theory.</p>\n<p>What we're looking for is some object (set) such that there is a relation (\u22a7) between the object and all sentences generated by the language under consideration. There are many object/relation pairs that assign truth values in a stupid way. For example, an interpretation that assigns \"true\" to every sentence is quite useless.</p>\n<p>Many other interpretations are somewhat useful but still \"wrong\". For example, there are interpretations of sentences which treat <code>&and;</code> like implication instead of conjunction. Somebody may have use for such things, but we certainly don't.</p>\n<p>We want to narrow our consideration to interpretations where <code>&and;</code> means \"and\" and <code>&not;</code> means \"not\", so we explicitly require object/relation pairs such that whenever the object models both <code>&phi;</code> and <code>&psi;</code>, it also models <code>&phi;&and;&psi;</code>. (And <code>X\u22a7&phi;</code> iff it is not the case that <code>X\u22a7&not;&phi;</code>, where <code>X</code> is the object under consideration.)</p>\n<p>If we can find any objects that work like this, we'll be justified in calling them \"models\". However, we haven't defined any such objects yet &mdash; we've only constrained their potential behavior.</p>\n<p>Any interpretation of the sentences generated from a language L of propositional logic must assign truth values to all basic sentence symbols in L. It turns out, once we select which sentence symbols are \"true\", we're done. The rest of the behavior is completely specified by the rules about <code>&and;</code> and <code>&not;</code>.</p>\n<p>In other words, any object/relation which assigns truth values to sentences according to the above rules is isomorphic to a set S of sentence symbols with the operator <code>\u22a7</code> defined in the obvious way (starting with <code>S\u22a7x iff x&isin;S</code>).</p>\n<p>Exploring this idea gives you a feel for how to define models of a logic.</p>\n<h2 id=\"morepower\">More power</h2>\n<p>Propositional logic is pretty boring. If you want to say anything interesting, you've got to be able to say more than just \"and\" and \"not\". Let's move on to a stronger logic.</p>\n<p>First-order logic uses the symbols <code>( ) &forall; &exist; &and; &not; &nu; ' &equiv;</code> or equivalent (where &nu; &nu;' &nu;'', etc. are variables) with the familiar syntactic rules. A language of first-order logic has three different types of symbols: relation symbols, function symbols, and constant symbols. The rules of logic are designed such that symbols act as you expect, given their names.</p>\n<p>An <em>interpretation</em> of the sentences generated by some language in first-order logic is (as before) an object that assigns each sentence a truth value (via a relation). We narrow these objects down to the ones that treat all the symbols in ways that justify their names.</p>\n<p>More specifically, we consider interpretations that have of some \"universe\" (set) of items. Constant symbols are interpreted as specific items in the universe. Relation/function symbols are interpreted as relations/functions on the universe.</p>\n<p>We further restrict consideration to interpretations where the logical symbols act in the intended fashion. For example, we require that the interpretation hold <code>c&equiv;d</code> true (where <code>c</code> and <code>d</code> are constant symbols) if and only if the interpretation of <code>c</code> in the universe is equal to the interpretation of <code>d</code> in the universe.</p>\n<p>All this is pretty mechanical. The resulting objects are \"models\". Some of the early results in model theory show that these mathematical objects have indeed earned the name.</p>\n<h2 id=\"completeness\">Completeness</h2>\n<p>Completeness is the poster-child of model theory, and it warrants quite a bit of exploration. It's actually saying a few different things. I'll break it down:</p>\n<p><strong>1. Theorems of first-order logic are true in every model of first-order logic.</strong></p>\n<p>A theorem of first-order logic is something we can prove from the logic alone, such as <code>(&forall;&nu;)(&nu;&equiv;&nu;)</code>. Contrast this with a sentence like <code>(&exist;&nu;)(F(&nu;)&equiv;c)</code>, where <code>F</code> is a function symbol and <code>c</code> is a constant symbol &mdash; the truth of this sentence depends entirely upon interpretation.</p>\n<p>So what (1) says is that there aren't any models that deny tautological truths of first-order logic. This result <em>should not be surprising</em>: this claim merely states that we picked a good definition for \"models\". If instead we found that there are models which deny theorems, this would not be some big grand proof about the behavior of first-order logic. Rather, it would mean that we put the \"model\" label on the wrong group of thingies, and that we should go back and try again.</p>\n<p><strong>2. Any sentence true in every model of first-order logic is a theorem of first-order logic.</strong></p>\n<p>This is the converse to (1), and it's quite a bit more powerful. This states that the theorems of a language are <em>only</em> the things true in every model of that language. There's no mystery sentence that is true in every model but not provable from the syntax of the logic.</p>\n<p>From one point of view, this says that there are no \"missing\" models that \"would have\" said the mystery sentence was false (hence \"completeness\"). From another, this says that all sentences are \"well behaved\": no sentence \"outwits\" all models.</p>\n<p>It's worth noting that this is where completeness fails for models of second order logic. From one perspective, there must be some \"missing models\" in second-order logic. From the other perspective, there must be certain sentences which can \"outwit\" their models (presumably G&ouml;del sentences). I haven't spent any time formally examining these intuitive claims yet; I have much to learn about second-order logic.</p>\n<p>I should also note that I've been implicitly assuming completeness for the last two posts by ignoring the syntactic rules of the logics under consideration. The completeness theorem lets me do this, because it states that the results interpreted by first-order models are exactly the same as the results derived from the syntactic rules of first-order logic. <em>Because the completeness theorem holds</em>, I'm allowed to treat the logical sentences as dead symbols and consider the binding <code>M\u22a7&phi;&and;&psi; iff M\u22a7&phi; and M\u22a7&psi;</code> to be the means by which <code>&and;</code> obtains its meaning.</p>\n<p>When the completeness theorem fails (as it does in stronger logics) then there is a gap between what the rules of the logic allow and what the models of the logic can say. In that case I must separately consider what the rules say and what the models model. This is the edge of my comfort zone; more exploration with second-order logic is necessary.</p>\n<p>Fortunately, everything is well-behaved in first order logic. In fact, (1) and (2) above can be made stronger:</p>\n<p><strong>3. A set &Sigma; of sentences is consistent if it has a model.</strong></p>\n<p>This is another sanity check that we've put the \"model\" label on the right thing. A set of sentences is inconsistent if it can derive both <code>&phi;</code> and <code>&not;&phi;</code> for some sentence &phi;. (More specifically, &Sigma; is inconsistent if it can derive everything. If there is <em>any sentence</em>&nbsp;that &Sigma; cannot derive, &Sigma; is consistent. Remember that from a contradiction, anything follows.)</p>\n<p>Our models are defined such that whenever they model <code>&phi;</code>, they do not model <code>&not;&phi;</code> (and vice versa). Thus, if a set of sentences has a model (&Sigma; \"has a model\" when there is some model that holds true every sentence &sigma; in &Sigma;) then there must be some sentences which &Sigma; cannot deduce (&not;&sigma; for each &sigma; in &Sigma;, for instance). Thus, &Sigma; is consistent.</p>\n<p><strong>4. A set &Sigma; of sentences has a model if it is consistent.</strong></p>\n<p>This is where things get interesting again. This is a stronger version of (2) above: <em>every consistent theory has a model</em>.</p>\n<p>This makes a lot of sense after you understand (2) and (3), but don't underestimate its importance. This tells us that for every consistent theory there is some interpretation following the rules which we laid out. Again, this says something positive about our models (they are strong enough to handle any consistent theory) and something negative about the logic (it's not strong enough to permit a consistent theory stating \"I cannot be modeled\").</p>\n<p>Note: I'm not sure what it would mean for a theory to state \"I cannot be modeled\", nor am I convinced that the idea is meaningful. Again, we're nearing the edge of my comfort zone.</p>\n<p>The point is, the completeness theorem for first order logic says \"if you hand me a consistent theory, I can build a model of it\". This is very useful. We don't have to waste any time worrying whether or not there's an interpretation available that satisfies all our stringent rules about how&nbsp;<span style=\"font-family: monospace;\">&equiv;</span>&nbsp;actually means \"equals\" and so on: if the theory is consistent, a model exists.</p>\n<h2 id=\"witnesses\">Witnesses</h2>\n<p>There's been a lot of hand-waving going on for the past four points. Let's get back to the models.</p>\n<p>In model theory, you're manipulating interpretations for theories. Due to the generality of such work, there are few tools available to manipulate any abstract model. One of the most useful tools in model theory is the ability to <em>extend a model</em>.</p>\n<p>Ideally, when you extend a model, you want to make it more manageable without changing its behavior. Our first example of such a technique involves extending a model to add \"witnesses\".</p>\n<p>A \"witness\" is a constant symbol that witnesses the truth of an existentially quantified sentence. For example, in the language <code>{S, +, \u2715, 0}</code> of arithmetic, the constant symbol <code>0</code> is a witness to the sentence <code>(&exist;&nu;)(S&nu;&equiv;S0)</code> (because <code>0</code> makes <code>(S&nu;&equiv;S0)</code> true). However, there is no <em>witness</em> to the sentence <code>(&exist;&nu;)(&nu;&equiv;S0)</code>, because <code>1</code> is not a constant symbol of the language.</p>\n<p>However, we can <em>extend</em> a language to add new constant symbols. Specifically, given any model, we can extend the language to contain one constant symbol <code>\u0101</code> for each element <code>a</code> in the universe. Then we can extend the model to interpret each <code>\u0101</code> by <code>a</code>. Such extension does not change the behavior of the model, but it does give the model a number of nice properties.</p>\n<p>A model is said to \"have witnesses\" if for every existentially quantified sentence shaped like&nbsp;<span style=\"font-family: monospace;\">(&exist;&nu;)</span><span style=\"font-family: monospace;\">&psi;</span>&nbsp;that it models, there is some constant c such that the model models <span style=\"font-family: monospace;\">&psi;</span><span style=\"font-family: monospace;\">(&nu;\\c)</span><span style=\"font-family: monospace;\">&nbsp;</span>(&psi; with &nu; replaced by c).</p>\n<p>Before we discuss them, note a few things:</p>\n<ol>\n<li>We can also <em>reduce</em> a model &amp; language extended in this way by eliminating the added constants.</li>\n<li>We can talk about sets of sentences with witnesses if, whenever a sentence shaped like <code>(&exist;&nu;)&psi;</code> is in the set of sentences &Sigma;, there is some constant <code>c</code> such that <code>&psi;(&nu;\\c)</code>&nbsp;is also in &Sigma;.</li>\n<li>Just as we can extend a model &amp; language so that the model has witnesses, we can extend a set of sentences and language so that the set of sentences has witnesses (by adding a new constant symbol for each existentially quantified sentence). Such extension does not threaten the consistency of a set of sentences.</li>\n<li>It's easy to construct a model from a consistent set of sentences that has witnesses. The universe is the set of all constant symbols (technically, one element for each equivalent set of constant symbols), and the behavior of function/relation symbols is forced by their behavior on the constant symbols.</li>\n</ol>\n<p>Formalize these four points and put them together, and you've just proved the completeness theorem. (Given any consistent set &Sigma; of sentences, add witnesses then make a model then reduce it to the original language, you now have a model of &Sigma;).</p>\n<h2 id=\"compactness\">Compactness</h2>\n<p>The compactness theorem states that <em>A set &Sigma; of sentences has a model iff every finite subset of &Sigma; has a model</em>.</p>\n<p>This leads to a pretty surprising result. Let's break it down.</p>\n<p><strong>1. If &Sigma; has a model then every finite subset of &Sigma; has a model.</strong></p>\n<p>This direction is obvious: any model of &Sigma; is also a model of all subsets of &Sigma;: if a model holds true all sentences &sigma; in &Sigma; then it obviously holds true all sentences &sigma; in any subset of &Sigma;.</p>\n<p><strong>2. If every finite subset of &Sigma; has a model then &Sigma; has a model.</strong></p>\n<p>This is where things get interesting. Note that we measure the size of a model by measuring the size of its universe. So when we say \"a countably infinite model\" we mean a model with a countably infinite universe.</p>\n<p>The proof of the above is actually quite easy: in first order logic, all proofs are finite. Therefore, any proof of contradiction (and thus inconsistency) must be finite. Since every finite subset of &Sigma; has a model, no finite subset of &Sigma; is inconsistent. Because all inconsistencies are finite, &Sigma; is not inconsistent. Then, by completeness, because &Sigma; is consistent it has a model.</p>\n<p>Or, in other words, the compactness theorem says</p>\n<blockquote>\n<p>If &Sigma; wants to be inconsistent it can do it in a finite subset.</p>\n</blockquote>\n<p>When we know that <em>no</em> finite subset of &Sigma; is inconsistent, we know that &Sigma; has a model.</p>\n<p>The surprising result that this leads to is as follows:</p>\n<p><em>If a theory T allows arbitrarily large finite models then it has an infinite model.</em></p>\n<p>If you hand me a theory that allows arbitrarily large finite models, I can extend the language by adding countably many constants to the language. Then I can consider the set &Sigma; of sentences built from T joined with sentences of the form \"there are at least n distinct constants\" for all finite n. Clearly, every finite subset of &Sigma; is satisfied by one of the finite models of T, which come in arbitrarily large sizes &mdash; just pick one that fits. Then, by compactness, &Sigma; has a model. The model of &Sigma; has infinitely many constants.</p>\n<p>I have just given a <em>fully general recipe</em> for building an infinite model given a theory T that admits arbitrarily large finite models. What does this mean? It means that no theory in first order logic is capable of saying \"I admit only finite models\". Sentences can say \"models must have no more than 10 elements\" or \"models are allowed to be infinite\", but sentences <em>cannot</em> say \"My model is finite\". This follows directly fro the fact that all proofs of contradiction are finite. You either have a <em>specific</em>&nbsp;limit, or you don't get a limit at all.</p>\n<h2 id=\"morepower\">MORE POWER</h2>\n<p>If you hand me a theory that allows arbitrarily sized finite models, I can hand you back an infinite model.</p>\n<p>It gets better.</p>\n<p>If you hand me an infinite theory, I can expand it.</p>\n<p>Using a method similar to the one above, I can expand T with sentences of the form \"there are at least &beta; distinct constants\", for all &beta; less than my chosen cardinal &alpha;. Following the same argument as above, all finite subsets of this theory have a model so this theory has a model, which obviously is of power &alpha;.</p>\n<p>In other words, a theory in first-order logic can either say \"I am at most <code>this</code> big\", where <code>this</code> is a specific finite number, or \"I am <code>very</code> big\", where <code>very</code> is can be <em>any infinite cardinal that you like</em>.</p>\n<p>Example: The theory of arithmetic doesn't have a finite cutoff point. There's no maximum number. Countably infinite models are allowed.</p>\n<p>Therefore, arbitrarily large infinite models are allowed.</p>\n<p>There are countable models of arithmetic (at least one of which you'll find familiar), and there are also models of arithmetic where there are as many \"numbers\" as there are reals. Then there are bigger models of arithmetic that are saturated, no, <em>dripping</em>&nbsp;with numbers.</p>\n<p>Now you understand why first order logic cannot discuss the \"standard\" model of number theory. No first-order theory can specifically pinpoint \"countable\" models! A first order theory must either specify a finite maximum size <em>explicitly</em>, or allow models of unfathomable size.</p>\n<h2 id=\"evenmore\">Even more</h2>\n<p>I was hoping to get farther than this, but this is a good stopping point. Hopefully you've learned something about model theory. There are many more interesting results yet.</p>\n<p>The book <em>Model Theory</em> by Chang and Keisler primarily focuses on introducing new ways to extend arbitrary models (giving them nice properties in the process) and then discusses results that can be gleaned from models extended thus. Focus is also given to special cases where models are well behaved, such as <em>atomic</em>&nbsp;models (which are a sort of minimal model for a given theory) and <em>saturated</em> models (which are a sort of maximal model for a given theory, <em>within</em> a given cardinality. There Are Always Bigger Models).</p>\n<p>There's also quite a bit of exploration into manipulating languages (<span style=\"font-family: monospace;\">Eliminate Quantifiers To Win Big Prizes!!!</span>) and even manipulating logics (Q. How far beyond first order logic we can go before sacrificing things like completeness and compactness? A. Not very.)</p>\n<p>If you're interested in learning more, then&hellip; well, I'm probably not the person to talk to. I'm not even halfway through this textbook. But if you're feeling gutsy, consider picking up <em>Model Theory</em>&nbsp;and getting started. Some of the harder concepts would be much easier to work through with other people rather than alone.</p>\n<p>In fact, I have a number of notes about trying to learn something difficult on my own (what worked, what didn't) that I plan to share. But that's a story for another day.</p>\n<p>Finally, please note that my entire exposure to model theory is half a textbook and some internetting. I guarantee I've misunderstood some things. If you see errors in my explanations,<em>&nbsp;</em>don't hesitate to let me know! My feedback loop isn't very strong right now.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F6BrJFkqEhh22rFsZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 39, "extendedScore": null, "score": 9.9e-05, "legacy": true, "legacyId": "24539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In this post I'll discuss some basic results of model theory. It may be helpful to read through <a href=\"/lw/ix5/mental_context_for_model_theory/\">my previous post</a> if you haven't yet. Model Theory is an implicit context for the Heavily Advanced Epistemology sequence and for a few of the recent MIRI papers, so casual readers may find this brief introduction useful. And who knows, maybe it will pique your interest:</p>\n<h2 id=\"A_tale_of_two_logics\">A tale of two logics</h2>\n<p><strong>propositional logic</strong> is the \"easy logic\", built from basic symbols and the connectives \"and\" and \"not\". Remember that all other connectives can be built from these two: With Enough NAND Gates You Can Rule The World and all that. Propositional logic is sometimes called the \"sentential logic\", because it's not like any other logics are \"of or relating to sentences\" (/sarcasm).</p>\n<p><strong>first order logic</strong> is the \"nice logic\". It has quantifiers (\"there exists\", \"for all\") and an internal notion of equality. Its sentences contain constants, functions, and relations. This lets you say lots of cool stuff that you can't say in propositional logic. First order logic turns out to be quite friendly (as we'll see below). However, it's not strong enough to talk about certain crazy/contrived ideas that humans cook up (such as \"the numbers\").</p>\n<p>There are many other logics available (second order logic AKA \"the heavy guns\", \u03c9-logic AKA \"please just please can I talk about numbers\", and many more). In this post we'll focus on propositional and first-order logics.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Training_Wheels\">Training Wheels</h2>\n<p>As a warm-up, let's define \"models\" for propositional logic.</p>\n<p>Remember, logics <em>together</em> with languages produce sentences. Languages of propositional logic are sets of basic symbols. From these symbols and the two connectives of propositional logic (\u2227 and \u00ac) we construct our sentences.</p>\n<p>For example, from the language <code>{x, y}</code> we can build the sentences <code>x</code>, <code>\u00acx</code>, <code>x\u2227y</code>, and many more. We <em>cannot</em> build sentences like <code>hello</code> (which doesn't use the right symbols) or <code>\u00acxy</code> (which breaks the rules of the logic).</p>\n<p>We want to study interpretations for such sentences, where an \"interpretation\" is some object which assigns a truth value to each sentence. This is denoted by the \u22a7 operator: <code>A\u22a7B</code> expresses some form of \"A models B as true\". This operator is used frequently and in many contexts throughout model theory.</p>\n<p>What we're looking for is some object (set) such that there is a relation (\u22a7) between the object and all sentences generated by the language under consideration. There are many object/relation pairs that assign truth values in a stupid way. For example, an interpretation that assigns \"true\" to every sentence is quite useless.</p>\n<p>Many other interpretations are somewhat useful but still \"wrong\". For example, there are interpretations of sentences which treat <code>\u2227</code> like implication instead of conjunction. Somebody may have use for such things, but we certainly don't.</p>\n<p>We want to narrow our consideration to interpretations where <code>\u2227</code> means \"and\" and <code>\u00ac</code> means \"not\", so we explicitly require object/relation pairs such that whenever the object models both <code>\u03c6</code> and <code>\u03c8</code>, it also models <code>\u03c6\u2227\u03c8</code>. (And <code>X\u22a7\u03c6</code> iff it is not the case that <code>X\u22a7\u00ac\u03c6</code>, where <code>X</code> is the object under consideration.)</p>\n<p>If we can find any objects that work like this, we'll be justified in calling them \"models\". However, we haven't defined any such objects yet \u2014 we've only constrained their potential behavior.</p>\n<p>Any interpretation of the sentences generated from a language L of propositional logic must assign truth values to all basic sentence symbols in L. It turns out, once we select which sentence symbols are \"true\", we're done. The rest of the behavior is completely specified by the rules about <code>\u2227</code> and <code>\u00ac</code>.</p>\n<p>In other words, any object/relation which assigns truth values to sentences according to the above rules is isomorphic to a set S of sentence symbols with the operator <code>\u22a7</code> defined in the obvious way (starting with <code>S\u22a7x iff x\u2208S</code>).</p>\n<p>Exploring this idea gives you a feel for how to define models of a logic.</p>\n<h2 id=\"More_power\">More power</h2>\n<p>Propositional logic is pretty boring. If you want to say anything interesting, you've got to be able to say more than just \"and\" and \"not\". Let's move on to a stronger logic.</p>\n<p>First-order logic uses the symbols <code>( ) \u2200 \u2203 \u2227 \u00ac \u03bd ' \u2261</code> or equivalent (where \u03bd \u03bd' \u03bd'', etc. are variables) with the familiar syntactic rules. A language of first-order logic has three different types of symbols: relation symbols, function symbols, and constant symbols. The rules of logic are designed such that symbols act as you expect, given their names.</p>\n<p>An <em>interpretation</em> of the sentences generated by some language in first-order logic is (as before) an object that assigns each sentence a truth value (via a relation). We narrow these objects down to the ones that treat all the symbols in ways that justify their names.</p>\n<p>More specifically, we consider interpretations that have of some \"universe\" (set) of items. Constant symbols are interpreted as specific items in the universe. Relation/function symbols are interpreted as relations/functions on the universe.</p>\n<p>We further restrict consideration to interpretations where the logical symbols act in the intended fashion. For example, we require that the interpretation hold <code>c\u2261d</code> true (where <code>c</code> and <code>d</code> are constant symbols) if and only if the interpretation of <code>c</code> in the universe is equal to the interpretation of <code>d</code> in the universe.</p>\n<p>All this is pretty mechanical. The resulting objects are \"models\". Some of the early results in model theory show that these mathematical objects have indeed earned the name.</p>\n<h2 id=\"Completeness\">Completeness</h2>\n<p>Completeness is the poster-child of model theory, and it warrants quite a bit of exploration. It's actually saying a few different things. I'll break it down:</p>\n<p><strong id=\"1__Theorems_of_first_order_logic_are_true_in_every_model_of_first_order_logic_\">1. Theorems of first-order logic are true in every model of first-order logic.</strong></p>\n<p>A theorem of first-order logic is something we can prove from the logic alone, such as <code>(\u2200\u03bd)(\u03bd\u2261\u03bd)</code>. Contrast this with a sentence like <code>(\u2203\u03bd)(F(\u03bd)\u2261c)</code>, where <code>F</code> is a function symbol and <code>c</code> is a constant symbol \u2014 the truth of this sentence depends entirely upon interpretation.</p>\n<p>So what (1) says is that there aren't any models that deny tautological truths of first-order logic. This result <em>should not be surprising</em>: this claim merely states that we picked a good definition for \"models\". If instead we found that there are models which deny theorems, this would not be some big grand proof about the behavior of first-order logic. Rather, it would mean that we put the \"model\" label on the wrong group of thingies, and that we should go back and try again.</p>\n<p><strong id=\"2__Any_sentence_true_in_every_model_of_first_order_logic_is_a_theorem_of_first_order_logic_\">2. Any sentence true in every model of first-order logic is a theorem of first-order logic.</strong></p>\n<p>This is the converse to (1), and it's quite a bit more powerful. This states that the theorems of a language are <em>only</em> the things true in every model of that language. There's no mystery sentence that is true in every model but not provable from the syntax of the logic.</p>\n<p>From one point of view, this says that there are no \"missing\" models that \"would have\" said the mystery sentence was false (hence \"completeness\"). From another, this says that all sentences are \"well behaved\": no sentence \"outwits\" all models.</p>\n<p>It's worth noting that this is where completeness fails for models of second order logic. From one perspective, there must be some \"missing models\" in second-order logic. From the other perspective, there must be certain sentences which can \"outwit\" their models (presumably G\u00f6del sentences). I haven't spent any time formally examining these intuitive claims yet; I have much to learn about second-order logic.</p>\n<p>I should also note that I've been implicitly assuming completeness for the last two posts by ignoring the syntactic rules of the logics under consideration. The completeness theorem lets me do this, because it states that the results interpreted by first-order models are exactly the same as the results derived from the syntactic rules of first-order logic. <em>Because the completeness theorem holds</em>, I'm allowed to treat the logical sentences as dead symbols and consider the binding <code>M\u22a7\u03c6\u2227\u03c8 iff M\u22a7\u03c6 and M\u22a7\u03c8</code> to be the means by which <code>\u2227</code> obtains its meaning.</p>\n<p>When the completeness theorem fails (as it does in stronger logics) then there is a gap between what the rules of the logic allow and what the models of the logic can say. In that case I must separately consider what the rules say and what the models model. This is the edge of my comfort zone; more exploration with second-order logic is necessary.</p>\n<p>Fortunately, everything is well-behaved in first order logic. In fact, (1) and (2) above can be made stronger:</p>\n<p><strong id=\"3__A_set___of_sentences_is_consistent_if_it_has_a_model_\">3. A set \u03a3 of sentences is consistent if it has a model.</strong></p>\n<p>This is another sanity check that we've put the \"model\" label on the right thing. A set of sentences is inconsistent if it can derive both <code>\u03c6</code> and <code>\u00ac\u03c6</code> for some sentence \u03c6. (More specifically, \u03a3 is inconsistent if it can derive everything. If there is <em>any sentence</em>&nbsp;that \u03a3 cannot derive, \u03a3 is consistent. Remember that from a contradiction, anything follows.)</p>\n<p>Our models are defined such that whenever they model <code>\u03c6</code>, they do not model <code>\u00ac\u03c6</code> (and vice versa). Thus, if a set of sentences has a model (\u03a3 \"has a model\" when there is some model that holds true every sentence \u03c3 in \u03a3) then there must be some sentences which \u03a3 cannot deduce (\u00ac\u03c3 for each \u03c3 in \u03a3, for instance). Thus, \u03a3 is consistent.</p>\n<p><strong id=\"4__A_set___of_sentences_has_a_model_if_it_is_consistent_\">4. A set \u03a3 of sentences has a model if it is consistent.</strong></p>\n<p>This is where things get interesting again. This is a stronger version of (2) above: <em>every consistent theory has a model</em>.</p>\n<p>This makes a lot of sense after you understand (2) and (3), but don't underestimate its importance. This tells us that for every consistent theory there is some interpretation following the rules which we laid out. Again, this says something positive about our models (they are strong enough to handle any consistent theory) and something negative about the logic (it's not strong enough to permit a consistent theory stating \"I cannot be modeled\").</p>\n<p>Note: I'm not sure what it would mean for a theory to state \"I cannot be modeled\", nor am I convinced that the idea is meaningful. Again, we're nearing the edge of my comfort zone.</p>\n<p>The point is, the completeness theorem for first order logic says \"if you hand me a consistent theory, I can build a model of it\". This is very useful. We don't have to waste any time worrying whether or not there's an interpretation available that satisfies all our stringent rules about how&nbsp;<span style=\"font-family: monospace;\">\u2261</span>&nbsp;actually means \"equals\" and so on: if the theory is consistent, a model exists.</p>\n<h2 id=\"Witnesses\">Witnesses</h2>\n<p>There's been a lot of hand-waving going on for the past four points. Let's get back to the models.</p>\n<p>In model theory, you're manipulating interpretations for theories. Due to the generality of such work, there are few tools available to manipulate any abstract model. One of the most useful tools in model theory is the ability to <em>extend a model</em>.</p>\n<p>Ideally, when you extend a model, you want to make it more manageable without changing its behavior. Our first example of such a technique involves extending a model to add \"witnesses\".</p>\n<p>A \"witness\" is a constant symbol that witnesses the truth of an existentially quantified sentence. For example, in the language <code>{S, +, \u2715, 0}</code> of arithmetic, the constant symbol <code>0</code> is a witness to the sentence <code>(\u2203\u03bd)(S\u03bd\u2261S0)</code> (because <code>0</code> makes <code>(S\u03bd\u2261S0)</code> true). However, there is no <em>witness</em> to the sentence <code>(\u2203\u03bd)(\u03bd\u2261S0)</code>, because <code>1</code> is not a constant symbol of the language.</p>\n<p>However, we can <em>extend</em> a language to add new constant symbols. Specifically, given any model, we can extend the language to contain one constant symbol <code>\u0101</code> for each element <code>a</code> in the universe. Then we can extend the model to interpret each <code>\u0101</code> by <code>a</code>. Such extension does not change the behavior of the model, but it does give the model a number of nice properties.</p>\n<p>A model is said to \"have witnesses\" if for every existentially quantified sentence shaped like&nbsp;<span style=\"font-family: monospace;\">(\u2203\u03bd)</span><span style=\"font-family: monospace;\">\u03c8</span>&nbsp;that it models, there is some constant c such that the model models <span style=\"font-family: monospace;\">\u03c8</span><span style=\"font-family: monospace;\">(\u03bd\\c)</span><span style=\"font-family: monospace;\">&nbsp;</span>(\u03c8 with \u03bd replaced by c).</p>\n<p>Before we discuss them, note a few things:</p>\n<ol>\n<li>We can also <em>reduce</em> a model &amp; language extended in this way by eliminating the added constants.</li>\n<li>We can talk about sets of sentences with witnesses if, whenever a sentence shaped like <code>(\u2203\u03bd)\u03c8</code> is in the set of sentences \u03a3, there is some constant <code>c</code> such that <code>\u03c8(\u03bd\\c)</code>&nbsp;is also in \u03a3.</li>\n<li>Just as we can extend a model &amp; language so that the model has witnesses, we can extend a set of sentences and language so that the set of sentences has witnesses (by adding a new constant symbol for each existentially quantified sentence). Such extension does not threaten the consistency of a set of sentences.</li>\n<li>It's easy to construct a model from a consistent set of sentences that has witnesses. The universe is the set of all constant symbols (technically, one element for each equivalent set of constant symbols), and the behavior of function/relation symbols is forced by their behavior on the constant symbols.</li>\n</ol>\n<p>Formalize these four points and put them together, and you've just proved the completeness theorem. (Given any consistent set \u03a3 of sentences, add witnesses then make a model then reduce it to the original language, you now have a model of \u03a3).</p>\n<h2 id=\"Compactness\">Compactness</h2>\n<p>The compactness theorem states that <em>A set \u03a3 of sentences has a model iff every finite subset of \u03a3 has a model</em>.</p>\n<p>This leads to a pretty surprising result. Let's break it down.</p>\n<p><strong id=\"1__If___has_a_model_then_every_finite_subset_of___has_a_model_\">1. If \u03a3 has a model then every finite subset of \u03a3 has a model.</strong></p>\n<p>This direction is obvious: any model of \u03a3 is also a model of all subsets of \u03a3: if a model holds true all sentences \u03c3 in \u03a3 then it obviously holds true all sentences \u03c3 in any subset of \u03a3.</p>\n<p><strong id=\"2__If_every_finite_subset_of___has_a_model_then___has_a_model_\">2. If every finite subset of \u03a3 has a model then \u03a3 has a model.</strong></p>\n<p>This is where things get interesting. Note that we measure the size of a model by measuring the size of its universe. So when we say \"a countably infinite model\" we mean a model with a countably infinite universe.</p>\n<p>The proof of the above is actually quite easy: in first order logic, all proofs are finite. Therefore, any proof of contradiction (and thus inconsistency) must be finite. Since every finite subset of \u03a3 has a model, no finite subset of \u03a3 is inconsistent. Because all inconsistencies are finite, \u03a3 is not inconsistent. Then, by completeness, because \u03a3 is consistent it has a model.</p>\n<p>Or, in other words, the compactness theorem says</p>\n<blockquote>\n<p>If \u03a3 wants to be inconsistent it can do it in a finite subset.</p>\n</blockquote>\n<p>When we know that <em>no</em> finite subset of \u03a3 is inconsistent, we know that \u03a3 has a model.</p>\n<p>The surprising result that this leads to is as follows:</p>\n<p><em>If a theory T allows arbitrarily large finite models then it has an infinite model.</em></p>\n<p>If you hand me a theory that allows arbitrarily large finite models, I can extend the language by adding countably many constants to the language. Then I can consider the set \u03a3 of sentences built from T joined with sentences of the form \"there are at least n distinct constants\" for all finite n. Clearly, every finite subset of \u03a3 is satisfied by one of the finite models of T, which come in arbitrarily large sizes \u2014 just pick one that fits. Then, by compactness, \u03a3 has a model. The model of \u03a3 has infinitely many constants.</p>\n<p>I have just given a <em>fully general recipe</em> for building an infinite model given a theory T that admits arbitrarily large finite models. What does this mean? It means that no theory in first order logic is capable of saying \"I admit only finite models\". Sentences can say \"models must have no more than 10 elements\" or \"models are allowed to be infinite\", but sentences <em>cannot</em> say \"My model is finite\". This follows directly fro the fact that all proofs of contradiction are finite. You either have a <em>specific</em>&nbsp;limit, or you don't get a limit at all.</p>\n<h2 id=\"MORE_POWER\">MORE POWER</h2>\n<p>If you hand me a theory that allows arbitrarily sized finite models, I can hand you back an infinite model.</p>\n<p>It gets better.</p>\n<p>If you hand me an infinite theory, I can expand it.</p>\n<p>Using a method similar to the one above, I can expand T with sentences of the form \"there are at least \u03b2 distinct constants\", for all \u03b2 less than my chosen cardinal \u03b1. Following the same argument as above, all finite subsets of this theory have a model so this theory has a model, which obviously is of power \u03b1.</p>\n<p>In other words, a theory in first-order logic can either say \"I am at most <code>this</code> big\", where <code>this</code> is a specific finite number, or \"I am <code>very</code> big\", where <code>very</code> is can be <em>any infinite cardinal that you like</em>.</p>\n<p>Example: The theory of arithmetic doesn't have a finite cutoff point. There's no maximum number. Countably infinite models are allowed.</p>\n<p>Therefore, arbitrarily large infinite models are allowed.</p>\n<p>There are countable models of arithmetic (at least one of which you'll find familiar), and there are also models of arithmetic where there are as many \"numbers\" as there are reals. Then there are bigger models of arithmetic that are saturated, no, <em>dripping</em>&nbsp;with numbers.</p>\n<p>Now you understand why first order logic cannot discuss the \"standard\" model of number theory. No first-order theory can specifically pinpoint \"countable\" models! A first order theory must either specify a finite maximum size <em>explicitly</em>, or allow models of unfathomable size.</p>\n<h2 id=\"Even_more\">Even more</h2>\n<p>I was hoping to get farther than this, but this is a good stopping point. Hopefully you've learned something about model theory. There are many more interesting results yet.</p>\n<p>The book <em>Model Theory</em> by Chang and Keisler primarily focuses on introducing new ways to extend arbitrary models (giving them nice properties in the process) and then discusses results that can be gleaned from models extended thus. Focus is also given to special cases where models are well behaved, such as <em>atomic</em>&nbsp;models (which are a sort of minimal model for a given theory) and <em>saturated</em> models (which are a sort of maximal model for a given theory, <em>within</em> a given cardinality. There Are Always Bigger Models).</p>\n<p>There's also quite a bit of exploration into manipulating languages (<span style=\"font-family: monospace;\">Eliminate Quantifiers To Win Big Prizes!!!</span>) and even manipulating logics (Q. How far beyond first order logic we can go before sacrificing things like completeness and compactness? A. Not very.)</p>\n<p>If you're interested in learning more, then\u2026 well, I'm probably not the person to talk to. I'm not even halfway through this textbook. But if you're feeling gutsy, consider picking up <em>Model Theory</em>&nbsp;and getting started. Some of the harder concepts would be much easier to work through with other people rather than alone.</p>\n<p>In fact, I have a number of notes about trying to learn something difficult on my own (what worked, what didn't) that I plan to share. But that's a story for another day.</p>\n<p>Finally, please note that my entire exposure to model theory is half a textbook and some internetting. I guarantee I've misunderstood some things. If you see errors in my explanations,<em>&nbsp;</em>don't hesitate to let me know! My feedback loop isn't very strong right now.</p>", "sections": [{"title": "A tale of two logics", "anchor": "A_tale_of_two_logics", "level": 1}, {"title": "Training Wheels", "anchor": "Training_Wheels", "level": 1}, {"title": "More power", "anchor": "More_power", "level": 1}, {"title": "Completeness", "anchor": "Completeness", "level": 1}, {"title": "1. Theorems of first-order logic are true in every model of first-order logic.", "anchor": "1__Theorems_of_first_order_logic_are_true_in_every_model_of_first_order_logic_", "level": 2}, {"title": "2. Any sentence true in every model of first-order logic is a theorem of first-order logic.", "anchor": "2__Any_sentence_true_in_every_model_of_first_order_logic_is_a_theorem_of_first_order_logic_", "level": 2}, {"title": "3. A set \u03a3 of sentences is consistent if it has a model.", "anchor": "3__A_set___of_sentences_is_consistent_if_it_has_a_model_", "level": 2}, {"title": "4. A set \u03a3 of sentences has a model if it is consistent.", "anchor": "4__A_set___of_sentences_has_a_model_if_it_is_consistent_", "level": 2}, {"title": "Witnesses", "anchor": "Witnesses", "level": 1}, {"title": "Compactness", "anchor": "Compactness", "level": 1}, {"title": "1. If \u03a3 has a model then every finite subset of \u03a3 has a model.", "anchor": "1__If___has_a_model_then_every_finite_subset_of___has_a_model_", "level": 2}, {"title": "2. If every finite subset of \u03a3 has a model then \u03a3 has a model.", "anchor": "2__If_every_finite_subset_of___has_a_model_then___has_a_model_", "level": 2}, {"title": "MORE POWER", "anchor": "MORE_POWER", "level": 1}, {"title": "Even more", "anchor": "Even_more", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 16}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MG8Yhsxqu9JY4xRPr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-31T13:01:17.011Z", "modifiedAt": null, "url": null, "title": "Does the universe contain a friendly artificial superintelligence?", "slug": "does-the-universe-contain-a-friendly-artificial", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:27.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DevilMaster", "createdAt": "2011-02-24T13:00:38.912Z", "isAdmin": false, "displayName": "DevilMaster"}, "userId": "Tyd9adyB6P8gBMFGh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gTDyEedMDDcvzEpJk/does-the-universe-contain-a-friendly-artificial", "pageUrlRelative": "/posts/gTDyEedMDDcvzEpJk/does-the-universe-contain-a-friendly-artificial", "linkUrl": "https://www.lesswrong.com/posts/gTDyEedMDDcvzEpJk/does-the-universe-contain-a-friendly-artificial", "postedAtFormatted": "Thursday, October 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20the%20universe%20contain%20a%20friendly%20artificial%20superintelligence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20the%20universe%20contain%20a%20friendly%20artificial%20superintelligence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTDyEedMDDcvzEpJk%2Fdoes-the-universe-contain-a-friendly-artificial%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20the%20universe%20contain%20a%20friendly%20artificial%20superintelligence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTDyEedMDDcvzEpJk%2Fdoes-the-universe-contain-a-friendly-artificial", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgTDyEedMDDcvzEpJk%2Fdoes-the-universe-contain-a-friendly-artificial", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 602, "htmlBody": "<p>First and foremost, let's give a definition of \"friendly artificial superintelligence\" (from now on, FASI). A FASI is a computer system that:</p>\n<ol>\n<li>is capable to deduct, reason and solve problems</li>\n<li>helps human progress, is incapable to harm anybody and does not allow anybody to come to any kind of harm</li>\n<li>is so much more intelligent than any human that it has developed molecular nanotechnology by itself, making it de facto omnipotent</li>\n</ol>\n<p>In order to find an answer to this question, we must check whether our observations on the universe match with what we would observe if the universe did, indeed, contain a FASI.</p>\n<p>If, somewhere in another solar system, an alien civilization had already developed a FASI, it would be reasonable to presume that, sooner or later, one or more members of that civilization would ask it to make them omnipotent. The FASI, being friendly by definition, would not refuse. [1]<br />It would also make sure that anybody who becomes omnipotent is also rendered incapable to harm anybody and incapable to allow anybody to come to any kind of harm.</p>\n<p>The new omnipotent beings would also do the same to anybody who asks them to become omnipotent. It would be a short time before they use their omnipotence to leave their own solar system, meet other intelligent civilizations and make them omnipotent too.</p>\n<p>In short, the ultimate consequence of the appearance of a FASI would be that every intelligent being in the universe would become omnipotent. This does not match with our observations, so we must conclude that <strong>a FASI does not exist anywhere in the universe</strong>.</p>\n<p>[1] We must assume that a FASI would not just reply \"You silly creature, becoming omnipotent is not in your best interest so I will not make you omnipotent because I know better\" (or an equivalent thereof). If we did, we would implicitly consider the absence of omnipotent beings as evidence for the presence of a FASI. This would force us to consider the eventual presence of omnipotent beings as evidence for the absence of a FASI, which would not make sense.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Based on this conclusion, let's try to answer another question: <strong>is our universe a computer simulation?</strong></p>\n<p>According to Nick Bostrom, if even just one civilization in the universe</p>\n<ol>\n<li>survives long enough to enter a posthuman stage, and</li>\n<li>is interested to create \"ancestor simulations\"</li>\n</ol>\n<p>then the probability that we are living in one is extremely high.</p>\n<p>However, if a civilization did actually reach a posthuman stage where it can create ancestor simulations, it would also be advanced enough to create a FASI.</p>\n<p>If a FASI existed in such a universe, the cheapest way it would have to make anybody else omnipotent would be to create a universe simulation that does not differ substantially from our universe, except for the presence of an omnipotent simulacrum of the individual who asked to be made omnipotent in our universe. Every subsequent request of omnipotence would result in another simulation being created, containing one more omnipotent being. Any eventual simulation where those beings are not omnipotent would be deactivated: keeping it running would lead to the existence of a universe where a request of omnipotence has not been granted, which would go against the modus operandi of the FASI.</p>\n<p>Thus, any simulation of a universe containing even just one friendly omnipotent being would always progress to a state where every intelligent being is omnipotent. Again, this does not match with our observations. Since we had already concluded that a FASI does not exist in our universe, we must come to the further conclusion that <strong>our universe is not a computer simulation.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gTDyEedMDDcvzEpJk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -21, "extendedScore": null, "score": 1.4028342612871884e-06, "legacy": true, "legacyId": "24541", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-31T15:54:39.861Z", "modifiedAt": null, "url": null, "title": "PSA for LW futurists/academics", "slug": "psa-for-lw-futurists-academics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ShWG7uGitaLQWTaZ/psa-for-lw-futurists-academics", "pageUrlRelative": "/posts/3ShWG7uGitaLQWTaZ/psa-for-lw-futurists-academics", "linkUrl": "https://www.lesswrong.com/posts/3ShWG7uGitaLQWTaZ/psa-for-lw-futurists-academics", "postedAtFormatted": "Thursday, October 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PSA%20for%20LW%20futurists%2Facademics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APSA%20for%20LW%20futurists%2Facademics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ShWG7uGitaLQWTaZ%2Fpsa-for-lw-futurists-academics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PSA%20for%20LW%20futurists%2Facademics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ShWG7uGitaLQWTaZ%2Fpsa-for-lw-futurists-academics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ShWG7uGitaLQWTaZ%2Fpsa-for-lw-futurists-academics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>Rutgers was planning to offer a \"Future of Humankind\" course via Coursera, but I received this announcement today:</p>\n<p>\"<span style=\"color: #444444; font-family: Arial; font-size: 14px; line-height: 22.390625px;\">Hello Future of Humankind registrants,</span></p>\n<div style=\"color: #444444; font-family: Arial; font-size: 14px; line-height: 22.390625px;\">It is with great sadness, that we must inform you that due to the untimely death of James Martin, we will be unable to offer the Future of Humankind course as had originally been planned. &nbsp;Rutgers University is interested in developing similar course material, and we will keep you abreast of any updates with developments in our courses.<br /><br /></div>\n<div style=\"color: #444444; font-family: Arial; font-size: 14px; line-height: 22.390625px;\">Thank you for your understanding,<br /></div>\n<p><span style=\"color: #444444; font-family: Arial; font-size: 14px; line-height: 22.390625px;\">The Future of Humankind Course Staff\"<br /></span></p>\n<p><span style=\"color: #444444; font-family: Arial; font-size: 14px; line-height: 22.390625px;\">Average Coursera offerings are in tens of thousands, and this might be a chance to make a large impact. Proper contacts with Rutgers could be easily established, I also have some direct contacts at Coursera.<br /><br />(I'm specifically thinking about you&nbsp;</span><a href=\"/user/James_Miller\">http://lesswrong.com/user/James_Miller</a>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ShWG7uGitaLQWTaZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 1.4030000450799146e-06, "legacy": true, "legacyId": "24542", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T00:38:40.868Z", "modifiedAt": null, "url": null, "title": "Lone Genius Bias and Returns on Additional Researchers", "slug": "lone-genius-bias-and-returns-on-additional-researchers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:34.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qZHn4rDBXdkPKnwNH/lone-genius-bias-and-returns-on-additional-researchers", "pageUrlRelative": "/posts/qZHn4rDBXdkPKnwNH/lone-genius-bias-and-returns-on-additional-researchers", "linkUrl": "https://www.lesswrong.com/posts/qZHn4rDBXdkPKnwNH/lone-genius-bias-and-returns-on-additional-researchers", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lone%20Genius%20Bias%20and%20Returns%20on%20Additional%20Researchers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALone%20Genius%20Bias%20and%20Returns%20on%20Additional%20Researchers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZHn4rDBXdkPKnwNH%2Flone-genius-bias-and-returns-on-additional-researchers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lone%20Genius%20Bias%20and%20Returns%20on%20Additional%20Researchers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZHn4rDBXdkPKnwNH%2Flone-genius-bias-and-returns-on-additional-researchers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZHn4rDBXdkPKnwNH%2Flone-genius-bias-and-returns-on-additional-researchers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1905, "htmlBody": "<p>One thing that most puzzles me about Eliezer's writings on AI is his apparent belief that a small organization like MIRI is likely to be able to beat larger organizations like Google or the US Department of Defense to building human-level AI. In fact, he seems to believe such larger organizations may have no advantage at all over a smaller one, and perhaps will even be at a disadvantage. In his 2011 debate with Robin Hanson, <a href=\"/lw/bug/partial_transcript_of_the_hansonyudkowsky_june/\">he said</a>:</p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>As far as I can tell what happens when the government tries to develop AI is nothing.</strong> But that could just be an artifact of our local technological level and it might change over the next few decades. To me it seems like a deeply confusing issue whose answer is probably not very complicated in an absolute sense. Like we know why it&rsquo;s difficult to build a star. You&rsquo;ve got to gather a very large amount of interstellar hydrogen in one place. So we understand what sort of labor goes into a star and we know why a star is difficult to build. When it comes to building a mind, we don&rsquo;t know how to do it so it seems very hard. We like query our brains to say &ldquo;map us a strategy to build this thing&rdquo; and it returns null so it feels like it&rsquo;s a very difficult problem. But in point of fact we don&rsquo;t actually know that the problem is difficult apart from being confusing. We understand the star-building problem so we know it&rsquo;s difficult. This one we don&rsquo;t know how difficult it&rsquo;s going to be after it&rsquo;s no longer confusing.</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">So to me the AI problem looks like a&mdash;it looks to me more like the sort of thing that the problem is finding bright enough researchers, bringing them together, letting them work on that problem instead of demanding that they work on something where they&rsquo;re going to produce a progress report in two years which will validate the person who approved the grant and advance their career. And so the government has historically been tremendously bad at producing basic research progress in AI, in part because the most senior people in AI are often people who got to be very senior by having failed to build it for the longest period of time. (This is not a universal statement. I&rsquo;ve met smart senior people in AI.)</p>\n<p class=\"MsoNormal\" style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>But nonetheless, basically I&rsquo;m not very afraid of the government because I don&rsquo;t think it&rsquo;s a throw warm bodies at the problem and I don&rsquo;t think it&rsquo;s a throw warm computers at the problem. I think it&rsquo;s a good methodology, good people selection, letting them do sufficiently blue sky stuff, and so far historically the government has been tremendously bad at producing that kind of progress.</strong> (When they have a great big project to try to build something it doesn&rsquo;t work. When they fund long-term research it works.)</p>\n</blockquote>\n<p>I admit, I don't feel like I fully grasp all the reasons for the disagreement between Eliezer and myself on this issue. Some of the disagreement, I suspect, comes from slightly different views on the nature of intelligence, though I'm having the trouble pinpointing what those differences might be. But some of the difference, I'm think, comes from the fact that I've become convinced humans suffer from a <em>Lone Genius Bias</em><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\">&mdash;a tendency to over-attribute scientific and technological progress to the efforts of lone geniuses.</span></p>\n<p><strong><em>Disclaimer: </em></strong><em>My understanding of <a href=\"/lw/cfc/how_can_we_ensure_that_a_friendly_ai_team_will_be/9o3a\">Luke's current strategy for MIRI</a> is that it does not hinge on whether or not MIRI itself eventually builds AI or not. It seems to me that as long as MIRI keeps <a href=\"http://intelligence.org/research/\">publishing research</a> that could potentially help other people build FAI, MIRI is doing important work. Therefore, I wouldn't advocate anything in this post being taken as a reason not to donate to MIRI. I've donated recently, and will probably [edit: see <a href=\"/r/lesswrong/lw/ixt/lone_genius_bias_and_returns_on_additional/9zls\">below</a>]&nbsp;continue to do so in the future.</em><a id=\"more\"></a></p>\n<p><a href=\"http://intelligence.org/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>&nbsp;has an interesting section labeled \"Returns on Population\" (section 3.4) where, among other things, Eliezer says:</p>\n<blockquote>\n<p>Although I expect that this section of my analysis will not be without controversy, it appears to the author to also be an important piece of data to be explained that human science and engineering seem to scale over time better than over population&mdash;an extra decade seems much more valuable than adding warm bodies.</p>\n<p>Indeed, it appears to the author that human science scales ludicrously poorly with increased numbers of scientists, and that this is a major reason there hasn&rsquo;t been more relative change from 1970&ndash;2010 than from 1930&ndash;1970 despite the vastly increased num- ber of scientists. The rate of real progress seems mostly constant with respect to time, times a small factor more or less. I admit that in trying to make this judgment I am trying to summarize an overwhelmingly distant grasp on all the fields outside my own handful. Even so, a complete halt to science or a truly exponential (or even quadratic) speedup of real progress both seem like they would be hard to miss, and the exponential increase of published papers is measurable. Real scientific progress is continuing over time, so we haven&rsquo;t run out of things to investigate; and yet somehow real scientific progress isn&rsquo;t scaling anywhere near as fast as professional scientists are being added.</p>\n<p>The most charitable interpretation of this phenomenon would be that science problems are getting harder and fields are adding scientists at a combined pace which produces more or less constant progress. It seems plausible that, for example, Intel adds new researchers at around the pace required to keep up with its accustomed exponential growth...</p>\n</blockquote>\n<p>Eliezer goes on to suggest, however, that Intel is not at all typical, and proposes some <em>other </em>explanations, two of which (\"science is inherently bounded by serial causal depth\" and that scientific progress is limited by the need to wait for the last generation to die) suggest that progress doesn't scale at <em>all </em>with added researchers, at least past a certain point.</p>\n<p>I'm inclined to think that that Eliezer's basic claim here&mdash;that research progress scales better with time than population&mdash;is probably correct. Doubling the number of researchers working on a problem rarely means solving the problem twice as fast. However, I doubt the scaling is as <em>ludicrously </em>bad as Eliezer suggests. I suspect the case of Intel is fairly typical, and the \"science problems are getting harder\" theory of the history of science has a lot more going for it than Eliezer wants to grant.</p>\n<p>For one thing, there seems to be a human bias in favor of attributing scientific and technological progress to lone geniuses&mdash;call it the Lone Genius Bias. In fiction, it's common for the cast to have a single \"smart guy,\" a Reed Richards type, who does everything important in the the science and technology area, pulling off miraculous achievements all by himself. (If you're lucky, this role will be shared by <em>two </em>characters, like Fitz-Simmons on Joss Whedon's new <em>S.H.I.L.D. </em>TV show.) Similarly, villainous plots often hinge on kidnapping <em>one single scientist </em>who will be able to fulfill all the villain with all the villain's technical know-how needs.</p>\n<p>There's some reason to chalk this up to peculiarities of fiction (see TVTtropes articles on the <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/OmnidisciplinaryScientist\">Omnidisciplinary Scientist</a> and <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/TheMainCharactersDoEverything\">The Main Characters Do Everything</a> generally). But it often seems to bleed over into perceptions of real-life scientists and engineers. Saul Kripke, in the course of making a point about proper names, <a href=\"http://books.google.com/books?id=9vvAlOBfq0kC&amp;pg=PA85&amp;lpg=PA85&amp;dq=Saul+Kripke+einstein+atom+bomb&amp;source=bl&amp;ots=MTck8mFtwF&amp;sig=S8dWKlrdOrwjJdkvg2YXbcyQve0&amp;hl=en&amp;sa=X&amp;ei=btFyUpKAA_Td4APfv4HwBQ&amp;ved=0CDYQ6AEwAw#v=onepage&amp;q=Saul%20Kripke%20einstein%20atom%20bomb&amp;f=false\">once claimed</a> that he often met people who identified Einstein as the inventor of the atom bomb.</p>\n<p>Of course, in reality, Einstein just provided the initial theoretical basis for the atom bomb. Not only did the bomb itself require the Manhattan Project (which involved over 100,000 people) to build, but there there was a fair amount of basic science that had to take place after Einstein's original statement of mass-energy equivalence in 1905 before the Manhattan Project could even be conceived of.</p>\n<p>Or: in the popular imagination, Thomas Edison was an amazingly brilliant inventor, almost on par with Reed Richards. A contrarian view, popular among tech geeks, says that actually Edison was a jerk who got famous taking credit for other people's work, and also he depended on having a lot of other people working for him at Menlo Park. But then there's a <a href=\"/lw/2pv/intellectual_hipsters_and_metacontrarianism/\">meta-contrarian</a>&nbsp;view that argues that Menlo Park&nbsp;was <a href=\"http://en.wikipedia.org/wiki/Thomas_Edison#Menlo_Park\">\"the first industrial research lab,\" and industrial research labs are very important, to the point that Menlo Park itself was Edison's \"major innovation.\"</a>&nbsp;On this view, it's not <em>Edison's</em> fault that Lone Genius Bias leads people to misunderstand what his true contribution was.</p>\n<p>It's easy to see, in evolutionary terms, why humans might suffer from Lone Genius Bias. In the ancestral environment, major achievements would often have been the work of a single individual. Theoretically, there might have been the occasional achievement that required the cooperation of a whole entire hunter-gatherer band, but major achievements were <em>never </em>the work of Intel-sized R&amp;D departments or 100,000 person Manhattan Projects. (The is an instance of the more general principle that humans have trouble fully grokking complex modern societies.)</p>\n<p>Once you know about Lone Genius Bias, you should be suspicious when you find yourself gravitating towards future scenarios where the key innovations are the work of a few geniuses. Furthermore, it's not just that big projects are more common now than they were in the ancestral environment. The tendency of major advances to be the work of large groups seems to have noticeably increased over just the last century or so, and that trend may only continue even further in the future.</p>\n<p>Consider Nobel Prizes. The first Nobel Prizes were awarded in 1901. When people think of Nobel Prize winners they tend to think of <em>unshared </em>Nobel Prizes, like Einstein's, but in fact a Nobel Prize can be shared by up to three people. And when you look at <a href=\"http://en.wikipedia.org/wiki/List_of_Nobel_laureates\">the list of Nobel Prize winners over the years</a>, the tendency towards giving out more and more shared prizes as time goes on is obvious.</p>\n<p>In fact, given the way science currently works, many people find the rule rule that no more than three people can share a prize too restrictive. The Nobel for the discovery of the Higgs Boson, for example, went to two theoreticians who predicted the particle decades ago, while ignoring the contributions of the large number of experimental scientists whose work was required to confirm the particle's existence. An <em>IEEE Spectrum </em>headline went as far as to state the prize <a href=\"http://spectrum.ieee.org/tech-talk/aerospace/astrophysics/nobel-for-higgs-boson-discovery-ignores-how-modern-science-works\">\"ignores how modern science works.\"</a></p>\n<p><a href=\"http://spectrum.ieee.org/tech-talk/aerospace/astrophysics/nobel-for-higgs-boson-discovery-ignores-how-modern-science-works\"></a>You can reach the same conclusion just looking at the bylines on scientific papers. The single-author scientific paper <a href=\"http://www.nature.com/nature/history/full/nature06243.html\">\"has all but disappeared.\"</a>&nbsp;Some of that may be due to people gaming the citation-count-as-measure-of-scientific-productivity system, but my impression is that the typical university science lab's PI (principle investigator) really couldn't be nearly as productive without their miniature army of postdocs, grad students, and paid staff. (Consider also that gaming of citation counts <em>hasn't</em> led to an explosion of authors-per-paper in fields like philosophy, where there are obviously fewer benefits to collaboration.)</p>\n<p>And if you need one more argument that scientific problems are getting harder, and increasingly unlikely to be solved by lone geniuses... what does anyone <em>honestly </em>think the chances are that the Next Big Thing in science will come in the form some <a href=\"http://en.wikipedia.org/wiki/Annus_Mirabilis_papers\">26 year old publishing a few single-author papers in the same year he got his PhD</a>?</p>\n<p><strong>Update: </strong>Luke's comments on this post are awesome and I recommend people read them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "ZpG9rheyAkgCoEQea": 1, "zHjC29kkPmsdo7WTr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qZHn4rDBXdkPKnwNH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 37, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "24545", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XvmrwJsRMogXrYG3h", "9kcTNWopvXFncXgPy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T11:24:32.538Z", "modifiedAt": null, "url": null, "title": "Historical/Rationalistic Assesment Question", "slug": "historical-rationalistic-assesment-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.617Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b7X3DBFNFpJtYnTdd/historical-rationalistic-assesment-question", "pageUrlRelative": "/posts/b7X3DBFNFpJtYnTdd/historical-rationalistic-assesment-question", "linkUrl": "https://www.lesswrong.com/posts/b7X3DBFNFpJtYnTdd/historical-rationalistic-assesment-question", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Historical%2FRationalistic%20Assesment%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHistorical%2FRationalistic%20Assesment%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb7X3DBFNFpJtYnTdd%2Fhistorical-rationalistic-assesment-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Historical%2FRationalistic%20Assesment%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb7X3DBFNFpJtYnTdd%2Fhistorical-rationalistic-assesment-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb7X3DBFNFpJtYnTdd%2Fhistorical-rationalistic-assesment-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<p>Assume a highly rational actor with as much knowledge of the world as they could realistically have. Roughly what point is the 'turning point' in history after which they should be able to clearly realise that Western&nbsp;democracy is superior to European-style monarchy from a perspective of human welfare?</p>\r\n<p>Clarification: By superior, I mean 'overall superior'- that a variant of Western democracy is a better sort of system to think about when trying to make an ideal system for a country than a European-style monarchy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b7X3DBFNFpJtYnTdd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -15, "extendedScore": null, "score": 1.4041195688906736e-06, "legacy": true, "legacyId": "24556", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T12:51:39.673Z", "modifiedAt": null, "url": null, "title": "Change the labels, undo infinitely good improvements", "slug": "change-the-labels-undo-infinitely-good-improvements", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pzxMrvAoDP464SEuJ/change-the-labels-undo-infinitely-good-improvements", "pageUrlRelative": "/posts/pzxMrvAoDP464SEuJ/change-the-labels-undo-infinitely-good-improvements", "linkUrl": "https://www.lesswrong.com/posts/pzxMrvAoDP464SEuJ/change-the-labels-undo-infinitely-good-improvements", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Change%20the%20labels%2C%20undo%20infinitely%20good%20improvements&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChange%20the%20labels%2C%20undo%20infinitely%20good%20improvements%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzxMrvAoDP464SEuJ%2Fchange-the-labels-undo-infinitely-good-improvements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Change%20the%20labels%2C%20undo%20infinitely%20good%20improvements%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzxMrvAoDP464SEuJ%2Fchange-the-labels-undo-infinitely-good-improvements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzxMrvAoDP464SEuJ%2Fchange-the-labels-undo-infinitely-good-improvements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1159, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/The_Hitchhiker's_Guide_to_the_Galaxy\">Infinity is big</a>.&nbsp;You just won't believe how vastly, hugely, mindbogglingly big it is. I mean, you may think it's a long way down the road to the chemist's, but that's just peanuts to infinity.</p>\n<p>And there are a lot of paradoxes connected with infinity. Here we'll be looking at a small selection of them, connected with infinite ethics.</p>\n<p>Suppose that you had some ethical principles that you would want to spread to infinitely many different agents - maybe through acausal decision making, maybe through some sort of Kantian <a href=\"http://en.wikipedia.org/wiki/Categorical_imperative\">categorical imperative</a>. So even if the universe is infinite, filled with infinitely many agents, you have potentially infinite influence (which is more than most of us have most days). What would you do with this influence - what kind of decisions would you like to impose across the universe(s)'s population? What would count as an improvement?</p>\n<p>There are many different ethical theories you could use - but one thing you'd want is that your improvements are actual improvements. You wouldn't want to implement improvements that turn out to be illusionary. And you certainly wouldn't want to implement improvements that could be undone by relabeling people.</p>\n<p>How so? Well, imagine that you have a <a href=\"http://en.wikipedia.org/wiki/Countable_set\">countable</a> infinity of agents, with utilities (..., -3, -2, -1, 0, 1, 2, 3, ...). Then suppose everyone gets +1 utility. You'd think that giving an infinity of agents one extra utility each would be fabulous - but the utilities are exactly the same as before. The current -1 utility belongs to the person who had -2 before, but there's still currently someone with -1, just as there was someone with -1 before the change. And this holds for every utility value: an infinity of improvements has accomplished... nothing. As soon as you relabel who is who, you're in exactly the same position as before.</p>\n<p>But things can get worse. Subtracting one utility from everyone also leaves the outcome the same, after relabeling everyone. So this universal improvement is completely indistinguishable from a universal regression.<a id=\"more\"></a></p>\n<h2>Conditions for improvement</h2>\n<p>So the question is, under what conditions can we be sure that an improvement is genuine?</p>\n<p>We'll assume that we have a countable infinity of agents, and we'll make a few <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">highly</a> <a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">non-trivial</a> assumptions (the only real justification being that these assumptions are traditional). First, we'll assume that everyone's personal preferences/welfare/hedonistic levels (or whatever we're using) are expressed in terms of a utility function (unrealistic). Secondly, we'll assume that each of these utility functions has a defined zero point (dubious). Finally, we'll assume that these utility functions can be put on a common scale, so they can be compared with each other (extremely dubious).</p>\n<p>So, what counts as an improvement? We've already seen that adding +1 to everyone's utility is not an improvement. What about multiplication? If everyone's utility is above 0, then surely multiplying everyone's utility by 2 must make things better?</p>\n<p>Not so. Assume everyone's utility is (..., 1/8, 1/4, 1/2, 1, 2, 4, 8, ...), then it's clear that multiplying by 2 (or dividing by 2) has no impact on the overall situation.</p>\n<p>Since addition and multiplication are out, what about increasing the number of happy people? It is even easier to see that this can have no impact: simply assume that everyone's utility is some constant c&gt;0. Then if we get everyone to construct a copy of themselves with same utility, we end up with twice as many people with utility c - but since we started with infinitely many people and ended up with infinitely many people, we've accomplished nothing.</p>\n<p>&nbsp;</p>\n<h2>Bounding the individual</h2>\n<p>To avoid being completely ineffective, we need to make some stronger assumptions. For instance, we could bound the personal utilities. If the utilities are bounded above (or, indeed, below), then adding +1 will have definite impact: we can't undo that effect by relabeling people. This is because the set of utilities now has a <a href=\"http://en.wikipedia.org/wiki/Supremum\">supremum</a>&nbsp;(a generalisation of maximum) or an <a href=\"http://en.wikipedia.org/wiki/Infimum\">infimum</a> (a generalisation of minimum). And when we add +1, we increase the supremum or infimum by +1, so the two collections of utilities are no longer comparable.</p>\n<p>Bounding people's utilities on one side is not enough to ensure multiplication has an impact, as we saw above with the example, which had everyone's utility above 0. But if people's utilities are bounded above <em>and</em> below, then we can ensure that multiplying will change the overall situation (unless everyone's utility is at zero). The argument for supremum and infimum will work just as above, as long as one of them is non-zero.</p>\n<p>Ok, so adding +1 utility to everyone is now a definite improvement (or at the least a definite change, and it certainly feels like an improvement). What about adding different amounts to different people? Is this an improvement?</p>\n<p>Not so. Assume people utilities are (..., 1/8, 1/4, 1/2, 1, 3/2, 7/4, 15/8, ...). This is bounded below (by 0) and bounded above (by 2). Yet if you move everyone's utility up to the amount of the person just above them, you will have increased everyone's utility and changed... absolutely nothing.</p>\n<p>To make sure that your improvement is genuine, you need to ensure that you increase everyone's utility by <em>at least</em>&nbsp;&epsilon;, for any given &epsilon;&gt;0. But you need not increase <em>everyone's</em> utility - for instance, you can skip a finite number of people and still get a clear improvement.</p>\n<p>What about skipping an infinite number of people? This won't work in general. Assume you have infinitely many people at utility 1, and infinitely many at utility 2. Then if you move infinitely many people from 1 to 2 (while still leaving infinitely many at 1), you will have accomplished nothing.</p>\n<p>This leads to a rather curious form of egalitarianism: the only way of ensuring that you've got an improvement overall is to ensure that (almost) everyone shares in the improvement - to at least a small extent.</p>\n<p>Duplicating happy people is still ineffective in the bounded cases.</p>\n<p>&nbsp;</p>\n<h2>Bounding the collective</h2>\n<p>What if not only the individual utilities are bounded, but the sum of utilities is also bounded - just as 1+1/2+1/4+1/8+... sum to 2? This is a very unlikely situation (most people's utilities would be arbitrarily close to zero). But, if it were to occur, everything becomes easy. Any time you increase anyone's utility by any amount, the overall situation is not longer equivalent with the initial one. Same goes for increasing everyone's utility by any amount, whether or not this is above an &epsilon;&gt;0. We can slightly generalise this situation by changing the zero point of everyone's utility: if the sum of everyone's utility is bounded, for any choice of the zero point, then any increase of utility changes the situation. Relabeling cannot undo these improvements.</p>\n<p>Similarly, making finitely many extra copies of happy people is now finally an inarguable change. Unlike above, however, this no longer holds true if we move the zero point.</p>\n<p>&nbsp;</p>\n<h2>More versus happier people</h2>\n<p>It is interesting that improving overall happiness is successful in more situations that duplicating happy people. Infinity, it seems, often wants happier people, nor more of them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pzxMrvAoDP464SEuJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 4, "extendedScore": null, "score": 1.404203001255274e-06, "legacy": true, "legacyId": "24544", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://en.wikipedia.org/wiki/The_Hitchhiker's_Guide_to_the_Galaxy\">Infinity is big</a>.&nbsp;You just won't believe how vastly, hugely, mindbogglingly big it is. I mean, you may think it's a long way down the road to the chemist's, but that's just peanuts to infinity.</p>\n<p>And there are a lot of paradoxes connected with infinity. Here we'll be looking at a small selection of them, connected with infinite ethics.</p>\n<p>Suppose that you had some ethical principles that you would want to spread to infinitely many different agents - maybe through acausal decision making, maybe through some sort of Kantian <a href=\"http://en.wikipedia.org/wiki/Categorical_imperative\">categorical imperative</a>. So even if the universe is infinite, filled with infinitely many agents, you have potentially infinite influence (which is more than most of us have most days). What would you do with this influence - what kind of decisions would you like to impose across the universe(s)'s population? What would count as an improvement?</p>\n<p>There are many different ethical theories you could use - but one thing you'd want is that your improvements are actual improvements. You wouldn't want to implement improvements that turn out to be illusionary. And you certainly wouldn't want to implement improvements that could be undone by relabeling people.</p>\n<p>How so? Well, imagine that you have a <a href=\"http://en.wikipedia.org/wiki/Countable_set\">countable</a> infinity of agents, with utilities (..., -3, -2, -1, 0, 1, 2, 3, ...). Then suppose everyone gets +1 utility. You'd think that giving an infinity of agents one extra utility each would be fabulous - but the utilities are exactly the same as before. The current -1 utility belongs to the person who had -2 before, but there's still currently someone with -1, just as there was someone with -1 before the change. And this holds for every utility value: an infinity of improvements has accomplished... nothing. As soon as you relabel who is who, you're in exactly the same position as before.</p>\n<p>But things can get worse. Subtracting one utility from everyone also leaves the outcome the same, after relabeling everyone. So this universal improvement is completely indistinguishable from a universal regression.<a id=\"more\"></a></p>\n<h2 id=\"Conditions_for_improvement\">Conditions for improvement</h2>\n<p>So the question is, under what conditions can we be sure that an improvement is genuine?</p>\n<p>We'll assume that we have a countable infinity of agents, and we'll make a few <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">highly</a> <a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">non-trivial</a> assumptions (the only real justification being that these assumptions are traditional). First, we'll assume that everyone's personal preferences/welfare/hedonistic levels (or whatever we're using) are expressed in terms of a utility function (unrealistic). Secondly, we'll assume that each of these utility functions has a defined zero point (dubious). Finally, we'll assume that these utility functions can be put on a common scale, so they can be compared with each other (extremely dubious).</p>\n<p>So, what counts as an improvement? We've already seen that adding +1 to everyone's utility is not an improvement. What about multiplication? If everyone's utility is above 0, then surely multiplying everyone's utility by 2 must make things better?</p>\n<p>Not so. Assume everyone's utility is (..., 1/8, 1/4, 1/2, 1, 2, 4, 8, ...), then it's clear that multiplying by 2 (or dividing by 2) has no impact on the overall situation.</p>\n<p>Since addition and multiplication are out, what about increasing the number of happy people? It is even easier to see that this can have no impact: simply assume that everyone's utility is some constant c&gt;0. Then if we get everyone to construct a copy of themselves with same utility, we end up with twice as many people with utility c - but since we started with infinitely many people and ended up with infinitely many people, we've accomplished nothing.</p>\n<p>&nbsp;</p>\n<h2 id=\"Bounding_the_individual\">Bounding the individual</h2>\n<p>To avoid being completely ineffective, we need to make some stronger assumptions. For instance, we could bound the personal utilities. If the utilities are bounded above (or, indeed, below), then adding +1 will have definite impact: we can't undo that effect by relabeling people. This is because the set of utilities now has a <a href=\"http://en.wikipedia.org/wiki/Supremum\">supremum</a>&nbsp;(a generalisation of maximum) or an <a href=\"http://en.wikipedia.org/wiki/Infimum\">infimum</a> (a generalisation of minimum). And when we add +1, we increase the supremum or infimum by +1, so the two collections of utilities are no longer comparable.</p>\n<p>Bounding people's utilities on one side is not enough to ensure multiplication has an impact, as we saw above with the example, which had everyone's utility above 0. But if people's utilities are bounded above <em>and</em> below, then we can ensure that multiplying will change the overall situation (unless everyone's utility is at zero). The argument for supremum and infimum will work just as above, as long as one of them is non-zero.</p>\n<p>Ok, so adding +1 utility to everyone is now a definite improvement (or at the least a definite change, and it certainly feels like an improvement). What about adding different amounts to different people? Is this an improvement?</p>\n<p>Not so. Assume people utilities are (..., 1/8, 1/4, 1/2, 1, 3/2, 7/4, 15/8, ...). This is bounded below (by 0) and bounded above (by 2). Yet if you move everyone's utility up to the amount of the person just above them, you will have increased everyone's utility and changed... absolutely nothing.</p>\n<p>To make sure that your improvement is genuine, you need to ensure that you increase everyone's utility by <em>at least</em>&nbsp;\u03b5, for any given \u03b5&gt;0. But you need not increase <em>everyone's</em> utility - for instance, you can skip a finite number of people and still get a clear improvement.</p>\n<p>What about skipping an infinite number of people? This won't work in general. Assume you have infinitely many people at utility 1, and infinitely many at utility 2. Then if you move infinitely many people from 1 to 2 (while still leaving infinitely many at 1), you will have accomplished nothing.</p>\n<p>This leads to a rather curious form of egalitarianism: the only way of ensuring that you've got an improvement overall is to ensure that (almost) everyone shares in the improvement - to at least a small extent.</p>\n<p>Duplicating happy people is still ineffective in the bounded cases.</p>\n<p>&nbsp;</p>\n<h2 id=\"Bounding_the_collective\">Bounding the collective</h2>\n<p>What if not only the individual utilities are bounded, but the sum of utilities is also bounded - just as 1+1/2+1/4+1/8+... sum to 2? This is a very unlikely situation (most people's utilities would be arbitrarily close to zero). But, if it were to occur, everything becomes easy. Any time you increase anyone's utility by any amount, the overall situation is not longer equivalent with the initial one. Same goes for increasing everyone's utility by any amount, whether or not this is above an \u03b5&gt;0. We can slightly generalise this situation by changing the zero point of everyone's utility: if the sum of everyone's utility is bounded, for any choice of the zero point, then any increase of utility changes the situation. Relabeling cannot undo these improvements.</p>\n<p>Similarly, making finitely many extra copies of happy people is now finally an inarguable change. Unlike above, however, this no longer holds true if we move the zero point.</p>\n<p>&nbsp;</p>\n<h2 id=\"More_versus_happier_people\">More versus happier people</h2>\n<p>It is interesting that improving overall happiness is successful in more situations that duplicating happy people. Infinity, it seems, often wants happier people, nor more of them.</p>", "sections": [{"title": "Conditions for improvement", "anchor": "Conditions_for_improvement", "level": 1}, {"title": "Bounding the individual", "anchor": "Bounding_the_individual", "level": 1}, {"title": "Bounding the collective", "anchor": "Bounding_the_collective", "level": 1}, {"title": "More versus happier people", "anchor": "More_versus_happier_people", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pyTuR4ZbLfqpS2oMh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T14:26:49.443Z", "modifiedAt": null, "url": null, "title": "[Link] Lost and Found ", "slug": "link-lost-and-found", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zu5u5wtgjg9r2Q3HH/link-lost-and-found", "pageUrlRelative": "/posts/zu5u5wtgjg9r2Q3HH/link-lost-and-found", "linkUrl": "https://www.lesswrong.com/posts/zu5u5wtgjg9r2Q3HH/link-lost-and-found", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Lost%20and%20Found%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Lost%20and%20Found%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzu5u5wtgjg9r2Q3HH%2Flink-lost-and-found%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Lost%20and%20Found%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzu5u5wtgjg9r2Q3HH%2Flink-lost-and-found", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzu5u5wtgjg9r2Q3HH%2Flink-lost-and-found", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 417, "htmlBody": "<p><strong>Related:</strong> <a href=\"http://lesswrong.com/r/discussion/lw/h4u/link_son_of_lowhanging_fruit/\">Son of Low Hanging Fruit</a>, <a href=\"/r/discussion/lw/iud/link_lowhanging_poop/\">Low Hanging Poop </a></p>\n<p>A <a href=\"http://westhunt.wordpress.com/2013/10/31/lost-and-found/\">post</a> by <a href=\"http://en.wikipedia.org/wiki/Gregory_Cochran\">Gregory Cochran</a>'s and <a href=\"http://en.wikipedia.org/wiki/Henry_Harpending\">Henry Harpending</a>'s blog <a href=\"http://westhunt.wordpress.com/\">West Hunter</a>.</p>\n<blockquote>\n<p>Marcus Terentius Varro&nbsp; was called the most learned of the Romans.&nbsp;  But what did he know, and how did he know it? I ask because of this  quote, from <em>Rerum rusticarum libri III&nbsp; </em>(Agricultural Topics in Three Books):</p>\n<p>&ldquo;Especial care should be taken, in locating the steading, to place it  at the foot of a wooded hill, where there are broad pastures, and so as  to be exposed to the most healthful winds that blow in the region. A  steading facing the east has the best situation, as it has the shade in  summer and the sun in winter. If you are forced to build on the bank of a  river, be careful not to let the steading face the river, as it will be  extremely cold in winter, and unwholesome in summer. 2 Precautions must  also be taken in the neighbourhood of swamps, both for the reasons  given, and because there are bred certain minute creatures which cannot  be seen by the eyes, which float in the air and enter the body through  the mouth and nose and there cause serious diseases.&rdquo; &ldquo;What can I do,&rdquo;  asked Fundanius, &ldquo;to prevent disease if&nbsp; I should inherit a farm of that  kind?&rdquo; &ldquo;Even I can answer that question,&rdquo; replied Agrius; &ldquo;sell it for  the highest cash price; or if you can&rsquo;t sell it, abandon it.&rdquo;</p>\n<p>I get the distinct impression that someone (probably someone other  than Varro) came up with an approximation of germ theory 1500 years  before&nbsp;<a href=\"http://en.wikipedia.org/wiki/Girolamo_Fracastoro\">Girolamo Fracastoro</a>.&nbsp; But his work was lost.</p>\n<p>Everybody knows, or should know, that the vast majority of Classical  literature has not been preserved.&nbsp; Those lost works contained facts and  ideas that might have value today &ndash; certainly there are topics that we  understand much better because of insights from Classical literature.  For example,&nbsp; Reich and Patterson find that some of the Indian castes  have existed for something like three thousand years:&nbsp; this is easier to  believe when you consider that Megasthenes wrote about the caste system  as early as 300 BC.</p>\n<p>We don&rsquo;t put much effort into recovering lost Classical literature.&nbsp;  But there are ways in which we could push harder &ndash; by increased funding  for work on the Herculaneum scrolls, or the Oxyrhynchus papyri  collection, for example.&nbsp; Some old-fashioned motivated archaeology might  get lucky and find another set of Amarna cuneiform letters, or a new  Antikythera&nbsp; mechanism.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zu5u5wtgjg9r2Q3HH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 20, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "24557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cw6CpcBfyTyPWH3Ny", "9CGAffRSj4TD7LkcH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T16:05:11.207Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Princeton NJ", "slug": "new-lw-meetup-princeton-nj", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2zQortoMjphzw9ox5/new-lw-meetup-princeton-nj", "pageUrlRelative": "/posts/2zQortoMjphzw9ox5/new-lw-meetup-princeton-nj", "linkUrl": "https://www.lesswrong.com/posts/2zQortoMjphzw9ox5/new-lw-meetup-princeton-nj", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Princeton%20NJ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Princeton%20NJ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zQortoMjphzw9ox5%2Fnew-lw-meetup-princeton-nj%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Princeton%20NJ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zQortoMjphzw9ox5%2Fnew-lw-meetup-princeton-nj", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2zQortoMjphzw9ox5%2Fnew-lw-meetup-princeton-nj", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 556, "htmlBody": "<p><strong>This summary was posted to LW main on October 25th. The following week's summary is <a href=\"/lw/iy6/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/sa\"></a><a href=\"/meetups/sm\">Princeton NJ Meetup:&nbsp;<span class=\"date\">09 November 2013 02:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/s9\">Atlanta: Rationalist Movie Night!:&nbsp;<span class=\"date\">26 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/sa\"></a><a href=\"/meetups/sa\">Cologne (K&ouml;ln):&nbsp;<span class=\"date\">10 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/rq\">Frankfurt (including effective altruism presentation):&nbsp;<span class=\"date\">27 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/sk\">Moscow, BeliefBusters:&nbsp;<span class=\"date\">27 October 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/s0\">Saint Petersburg, Russia:&nbsp;<span class=\"date\">27 October 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/so\">Saskatoon - Rhetorical Fallacies!:&nbsp;<span class=\"date\">26 October 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/sq\">Tempe, AZ: How to Measure Anything I:&nbsp;<span class=\"date\">25 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/sr\">Urbana-Champaign: Dark Arts Meetup Sunday October 27, Illini Union Courtyard Cafe (Ground Floor), 2PM:&nbsp;<span class=\"date\">27 October 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li> <a href=\"/meetups/sn\">London Social Meetup (and AskMeAnything about the CFAR workshop):&nbsp;<span class=\"date\">27 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/sl\">Washington DC: Anthropics with Robin Hanson:&nbsp;<span class=\"date\">27 October 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2zQortoMjphzw9ox5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4043883689054547e-06, "legacy": true, "legacyId": "24482", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6D49FhNsvSLh5Tgcb", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T16:24:55.602Z", "modifiedAt": null, "url": null, "title": "Meetup : Tempe, AZ: How to Measure Anything II", "slug": "meetup-tempe-az-how-to-measure-anything-ii", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7TZJa4m5tTqzbp9bm/meetup-tempe-az-how-to-measure-anything-ii", "pageUrlRelative": "/posts/7TZJa4m5tTqzbp9bm/meetup-tempe-az-how-to-measure-anything-ii", "linkUrl": "https://www.lesswrong.com/posts/7TZJa4m5tTqzbp9bm/meetup-tempe-az-how-to-measure-anything-ii", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tempe%2C%20AZ%3A%20How%20to%20Measure%20Anything%20II&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tempe%2C%20AZ%3A%20How%20to%20Measure%20Anything%20II%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TZJa4m5tTqzbp9bm%2Fmeetup-tempe-az-how-to-measure-anything-ii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tempe%2C%20AZ%3A%20How%20to%20Measure%20Anything%20II%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TZJa4m5tTqzbp9bm%2Fmeetup-tempe-az-how-to-measure-anything-ii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TZJa4m5tTqzbp9bm%2Fmeetup-tempe-az-how-to-measure-anything-ii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sy'>Tempe, AZ: How to Measure Anything II</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 November 2013 03:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting near the entrance to Hayden Library. This week, we will play a round of Zendo and discuss Section II of How to Measure Anything. Bring something you may want to measure but aren't sure how.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sy'>Tempe, AZ: How to Measure Anything II</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7TZJa4m5tTqzbp9bm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.4044072791237639e-06, "legacy": true, "legacyId": "24559", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_II\">Discussion article for the meetup : <a href=\"/meetups/sy\">Tempe, AZ: How to Measure Anything II</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 November 2013 03:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting near the entrance to Hayden Library. This week, we will play a round of Zendo and discuss Section II of How to Measure Anything. Bring something you may want to measure but aren't sure how.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_II1\">Discussion article for the meetup : <a href=\"/meetups/sy\">Tempe, AZ: How to Measure Anything II</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tempe, AZ: How to Measure Anything II", "anchor": "Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_II", "level": 1}, {"title": "Discussion article for the meetup : Tempe, AZ: How to Measure Anything II", "anchor": "Discussion_article_for_the_meetup___Tempe__AZ__How_to_Measure_Anything_II1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T20:31:47.702Z", "modifiedAt": null, "url": null, "title": "November 2013 Media Thread", "slug": "november-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:06.838Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TBhkYoWXrauTpMLDg/november-2013-media-thread", "pageUrlRelative": "/posts/TBhkYoWXrauTpMLDg/november-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/TBhkYoWXrauTpMLDg/november-2013-media-thread", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20November%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANovember%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBhkYoWXrauTpMLDg%2Fnovember-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=November%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBhkYoWXrauTpMLDg%2Fnovember-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBhkYoWXrauTpMLDg%2Fnovember-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TBhkYoWXrauTpMLDg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.404643809429266e-06, "legacy": true, "legacyId": "24560", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T20:37:18.265Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, November 1-15", "slug": "group-rationality-diary-november-1-15-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.737Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AmfDFXFQguDF42wfm/group-rationality-diary-november-1-15-0", "pageUrlRelative": "/posts/AmfDFXFQguDF42wfm/group-rationality-diary-november-1-15-0", "linkUrl": "https://www.lesswrong.com/posts/AmfDFXFQguDF42wfm/group-rationality-diary-november-1-15-0", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20November%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20November%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmfDFXFQguDF42wfm%2Fgroup-rationality-diary-november-1-15-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20November%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmfDFXFQguDF42wfm%2Fgroup-rationality-diary-november-1-15-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmfDFXFQguDF42wfm%2Fgroup-rationality-diary-november-1-15-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 250, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the public group instrumental rationality diary for November 1-15.&nbsp;</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The poll earlier this month seems to be sufficiently in favor of maintaining the current schedule that extra votes are unlikely to change things much, but if you'd really like to register your opinion, you are welcome to do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/ir8/group_rationality_diary_october_115_plus/9tol\">here</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Immediate past diary: &nbsp;<a href=\"/lw/iuj/group_rationality_diary_october_1631/\">October 16-31</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AmfDFXFQguDF42wfm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "24561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nrMwsYBqxZvJj4LPG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T20:48:13.857Z", "modifiedAt": null, "url": null, "title": "\"Singularity or Bust\" full documentary ", "slug": "singularity-or-bust-full-documentary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.017Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Torello", "createdAt": "2013-07-01T17:38:37.441Z", "isAdmin": false, "displayName": "Torello"}, "userId": "xoRpeFN7K5MgDRcvM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bHrNAGABeCF7Jn7wA/singularity-or-bust-full-documentary", "pageUrlRelative": "/posts/bHrNAGABeCF7Jn7wA/singularity-or-bust-full-documentary", "linkUrl": "https://www.lesswrong.com/posts/bHrNAGABeCF7Jn7wA/singularity-or-bust-full-documentary", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Singularity%20or%20Bust%22%20full%20documentary%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Singularity%20or%20Bust%22%20full%20documentary%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbHrNAGABeCF7Jn7wA%2Fsingularity-or-bust-full-documentary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Singularity%20or%20Bust%22%20full%20documentary%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbHrNAGABeCF7Jn7wA%2Fsingularity-or-bust-full-documentary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbHrNAGABeCF7Jn7wA%2Fsingularity-or-bust-full-documentary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>http://www.3quarksdaily.com/3quarksdaily/2013/11/singularity-or-bust-.html</p>\n<p>&nbsp;</p>\n<p>I've never heard of this before, and have only watched 7 minutes so far, but I'd imagine many people here would be interested in this video.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bHrNAGABeCF7Jn7wA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 1.404659559488858e-06, "legacy": true, "legacyId": "24562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-01T22:07:56.442Z", "modifiedAt": null, "url": null, "title": "Halloween thread - rationalist's horrors.", "slug": "halloween-thread-rationalist-s-horrors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:38.613Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wwa", "createdAt": "2011-12-12T01:43:43.227Z", "isAdmin": false, "displayName": "wwa"}, "userId": "ceumxcAj9mE4tNZJE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NPqBcdqdh9rauZd8W/halloween-thread-rationalist-s-horrors", "pageUrlRelative": "/posts/NPqBcdqdh9rauZd8W/halloween-thread-rationalist-s-horrors", "linkUrl": "https://www.lesswrong.com/posts/NPqBcdqdh9rauZd8W/halloween-thread-rationalist-s-horrors", "postedAtFormatted": "Friday, November 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Halloween%20thread%20-%20rationalist's%20horrors.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHalloween%20thread%20-%20rationalist's%20horrors.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPqBcdqdh9rauZd8W%2Fhalloween-thread-rationalist-s-horrors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Halloween%20thread%20-%20rationalist's%20horrors.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPqBcdqdh9rauZd8W%2Fhalloween-thread-rationalist-s-horrors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPqBcdqdh9rauZd8W%2Fhalloween-thread-rationalist-s-horrors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>This is a kind of \"X files\" thread.</p>\n<p>Post experiences which spooked you, which made you doubt reality, mathematical or physical laws, your sanity, memory or perception. The more improbable the better, but no second-hand legends please, share only what you personally experienced. If you had the event later explained rationally please use rot13 to avoid spoilers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NPqBcdqdh9rauZd8W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 35, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "24563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 124, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-02T01:48:02.502Z", "modifiedAt": null, "url": null, "title": "How to choose a country/city?", "slug": "how-to-choose-a-country-city", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:36.988Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "joaolkf", "createdAt": "2010-02-24T18:52:27.966Z", "isAdmin": false, "displayName": "joaolkf"}, "userId": "woC2b5rav5sGrAo3E", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uAvBv3cCcCqv52uoq/how-to-choose-a-country-city", "pageUrlRelative": "/posts/uAvBv3cCcCqv52uoq/how-to-choose-a-country-city", "linkUrl": "https://www.lesswrong.com/posts/uAvBv3cCcCqv52uoq/how-to-choose-a-country-city", "postedAtFormatted": "Saturday, November 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20choose%20a%20country%2Fcity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20choose%20a%20country%2Fcity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuAvBv3cCcCqv52uoq%2Fhow-to-choose-a-country-city%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20choose%20a%20country%2Fcity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuAvBv3cCcCqv52uoq%2Fhow-to-choose-a-country-city", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuAvBv3cCcCqv52uoq%2Fhow-to-choose-a-country-city", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1495, "htmlBody": "<p class=\"MsoNormal\">EDIT: I've found a very relevant indicator for my question, see \"Quality of life\" criteria below.</p>\n<p class=\"MsoNormal\"><br />My main question is: <strong>which <em>non-academic </em>factors should I consider when moving to another country/city for a PhD?</strong> Further, I would also like to evaluate each country/city<sup>1</sup> according to those criteria, but first I need to know which are the relevant criteria. If you know any (any at all) scientific literature on moving to another country and well being, let me know.</p>\n<p class=\"MsoNormal\">I've lived in Brazil all my life, I really like it here for many reasons. Mostly, by how personal relationships are established and maintained. However, Brazil's inability to construct a stable well developed society have crippled my intellectual development, and I simply cannot take it anymore - my brain will die here. Moreover, I feel like most of my high level desires(values) are much more in line with countries on the other end of the <a href=\"http://www.worldvaluessurvey.org/wvs/articles/folder_published/article_base_54\" target=\"_blank\">World Values Survey graphic</a>. I have rational/secular and self-expressing values, instead of traditional-survival oriented ones. For all those reasons, I will be applying for my PhD aboard. I have pondered many of the career and academic factors involved, and I've had the help of many good and objective indexes available (e.g.: <a href=\"http://www.philosophicalgourmet.com/overall.asp\" target=\"_blank\">here</a> and <a href=\"http://www.topuniversities.com/university-rankings/university-subject-rankings/2013/philosophy\" target=\"_blank\">here</a>). I've mapped most of the Departments of Philosophy in which I could research my topic (<a href=\"http://philosophynow.org/issues/91/Moral_Enhancement\" target=\"_blank\">moral enhancement</a>), and I believe these are the major factors. However, there is one other important factor I'm a bit clueless about: which country/city is better in all other aspects already not accounted by academic criteria?</p>\n<p class=\"MsoNormal\">My main options are<sup>2</sup>:</p>\n<ul>\n<li><strong>1st: Oxford</strong> (no need to explain)</li>\n<li><strong>2nd: Manchester</strong> (it's near Oxford, John Harris is there, one of the foremost researchers on moral enhancement)</li>\n<li><strong>3rd: Stockholm</strong> (where everyone is born a transhumanist)</li>\n<li><strong>3rd: Wellington</strong>, New Zealand (Nicholas Agar is there, one of the foremost researchers on moral enhancement)</li>\n<li><strong>4th</strong>:<strong> Some places in continental Europe</strong> I'm still investigating (e.g.: Zurich , Munich)</li>\n<li><strong>4th: Brazil </strong>(bioethics program in Rio de Janeiro)</li>\n</ul>\n<p class=\"MsoNormal\">However, this list is solely based on academic criteria. I need to factor in non-academic criteria. In fact, I do not even know which are the relevant non-academic criteria. That would be my first question. &nbsp;I got fixated on the World Values Survey factors, but I might be wrong. I would gather the happiness index is important, but it might not vary for the same individual between countries, or it might covary oddly with the happiness index of the destination country. &nbsp;My second question would be how each country/city is ranked according to these criteria.<br /><br />There are many things that will be affected by accessing these other factors. First, I think Oxford is far, far above the 2nd option. But it is above enough that if I do not get in there on the first time (80% probability), I should wait and apply next year again instead of going to somewhere else where I did get accepted? Second, my current plan is to build the strongest possible application for Oxford and use it elsewhere. But if Oxford is not so clearly the undisputed 1st place, then I should be more concerned with building a good application that also accounted for other countries specific criteria. Furthermore, right now, I think I have a major bias against New Zealand. In terms of moral enhancement research it would be the second best after Oxford, it has huge human development, freedom and happiness indexes. However, the fact it is in the freaking middle of nowhere is very discouraging. Am I wrong about this? What are the correct factors I should be accounting for?</p>\n<p class=\"MsoNormal\">Here is a list of the factors I could gather from the comments, mostly <a href=\"/lw/iyf/how_to_choose_a_countrycity/9ztr\" target=\"_blank\">the one</a> by&nbsp;MathiasZaman:</p>\n<ul>\n<li><strong>World Values <a href=\"http://www.worldvaluessurvey.org/wvs/articles/folder_published/article_base_54\" target=\"_blank\">Survey</a></strong>: Already explained above, I believe is one of the most important. But I wonder if I'm not biased and fixated on this. I would also like to have a Cities Values Survey, since in reality I'm choosing cities.</li>\n</ul>\n<ul>\n<li><strong>Quality of life</strong>: It should matter. But I haven't found a good index for not-huge cities. The <a href=\"http://en.wikipedia.org/wiki/Quality-of-life_Index\" target=\"_blank\">index for countries are well know</a>. Sweden and New Zealand take the lead, then England and after a while Brazil. However, obviously, being an expatriate changes things a lot. If you know of an expatriates' quality of life index for cities or countries, please, let me know. However, there's one <a href=\"http://www.expatexplorer.hsbc.com/files/pdfs/overall-reports/2012/report.pdf \" target=\"_blank\">good indicator for expatriates</a> available, but it is only for countries though.</li>\n</ul>\n<ul>\n<li><strong>Happiness</strong>: It should matter. Or, it might not vary for the same individual between countries. I don't know. <a href=\"http://en.wikipedia.org/wiki/Satisfaction_with_Life_Index\" target=\"_blank\">It is more or less the same as for quality of life</a>, since it is a major component of it.</li>\n</ul>\n<ul>\n<li><strong>Relative closeness to other countries</strong>: I'm having a hard time spelling out this one, but check <a href=\"/lw/iyf/how_to_choose_a_countrycity/9zte\" target=\"_blank\">this comment</a> by Kaj.</li>\n</ul>\n<ul>\n<li><strong>Language barrier</strong>: This is hard to account for. I'm expecting that in no developed country I would be put in a situation where relevant people (from my university) would not be talking in English if I'm on the conversation. If it is not true, this is majorly relevant. If it is true, this is mildly relevant. I would expect this would be both a function of <a href=\"http://en.wikipedia.org/wiki/EF_English_Proficiency_Index\" target=\"_blank\">English proficiency</a> and willingness to talk in English. Note Sweden is the highest in proficiency and the rest of continental Europe is the lowest. However, I do not know how to find the \"willingness\" factor.</li>\n</ul>\n<ul>\n<li><strong>Socio-economic system</strong>: Highly relevant. I believe this is accounted for on the World Values Survey, as type of government strongly covaries with values. More modern (rational-secular/self-expressing) have more liberal systems, while less modern have more strong governments. (while the really ancient ones have almost no State).</li>\n</ul>\n<ul>\n<li><strong>Public transport and real estate</strong>: Highly practical and I would not have thought if not for the comments. Commuting times and cost are very important. Real estate also, one of the many reasons I have not considered London was because of extremely high rents. Also, this brings back to mind why I posted this. I remember reading a very useful post on how to choose a house, where it pointed out to many relevant but unaccounted factors, commuting was one of them. What I want is something similar for cities.</li>\n</ul>\n<ul>\n<li><strong>Finances</strong>: It is mildly relevant, I do not believe I will have a desire for anything else besides researching, specially in Oxford. But I might be wrong. How I will finance myself is still a bit uncertain. For high ranking universities I will probably have a scholarship from Brazil, otherwise I will need a scholarship from elsewhere. With the probabilities in brackets, and some living costs factored in: \n<ul>\n<li>Oxford: Brazilian government scholarship. They will give me 1100 EUR per month besides paying for all the fees and accommodation. They pay one international travel per year. (90%) High living costs.</li>\n<li>Manchester, same as above. (70%)</li>\n<li>Stockholm: Swedish government salary (there a PhD is a job). For an Physics position it was ~2500 EUR per month.(100%) It has a <a href=\"http://en.wikipedia.org/wiki/List_of_most_expensive_cities_for_expatriate_employees#ECA_International\" target=\"_blank\">very high living cost for expatriates</a></li>\n<li>Wellington: I don't know, but will find out.</li>\n<li>Brazil: 950 EUR per month (70%). Low living costs.&nbsp;</li>\n</ul>\n</li>\n<li><strong>International status</strong>: Makes a huge difference if one lives in a city by desire or by merely being born there. Prima facie, one should be more interesting if she is there by desire. Thus, I should give priority to more international cities. I will have to use anecdotal evidence here, since on normal datasets low skilled immigrants will dominate the sample. If I were less busy, I would compile data on an university-by-university basis. \n<ul>\n</ul>\n</li>\n</ul>\n<p class=\"MsoNormal\">Finally, please remember this not a competition between countries or cities and refrain for expressing any, however tiny, nationalism on the comments. I'm not expressing my subjective feelings either, I'm merely trying to find out the relevant factors and how countries or cities rank according to them.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong>Footnotes:</strong></p>\n<p class=\"MsoNormal\"><strong>1.</strong> I would mostly like to be comparing cities, which was what I did when accounting for academic criteria, however (a) some datas are only available for countries, (b) in some cases I do not know to which city I will go and (c) this makes the analysis more complex.</p>\n<p class=\"MsoNormal\"><strong>2.</strong> US is out of the table for 4 reasons: (1) I would have to throw my MPhil on the garbage and start over. (2) Isn't that far away from a survival-traditional oriented society. (3) GRE (philosophy is the most competitive PhD program, I would have to nearly ace it, and I simply can't do that at the present time) (4) Doesn't have many transhumanistic oriented philosophy departments, specially on the top universities. Canada is out for (1), (3) and (4).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uAvBv3cCcCqv52uoq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 15, "extendedScore": null, "score": 1.4049469155580062e-06, "legacy": true, "legacyId": "24567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-02T16:37:23.797Z", "modifiedAt": null, "url": null, "title": "Open Thread, November 1 - 7, 2013 ", "slug": "open-thread-november-1-7-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:36.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "witzvo", "createdAt": "2012-05-10T07:45:38.575Z", "isAdmin": false, "displayName": "witzvo"}, "userId": "efsbsXkkuRESNruDe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/etwE7Tq8wPr5KkGfc/open-thread-november-1-7-2013", "pageUrlRelative": "/posts/etwE7Tq8wPr5KkGfc/open-thread-november-1-7-2013", "linkUrl": "https://www.lesswrong.com/posts/etwE7Tq8wPr5KkGfc/open-thread-november-1-7-2013", "postedAtFormatted": "Saturday, November 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20November%201%20-%207%2C%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20November%201%20-%207%2C%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FetwE7Tq8wPr5KkGfc%2Fopen-thread-november-1-7-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20November%201%20-%207%2C%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FetwE7Tq8wPr5KkGfc%2Fopen-thread-november-1-7-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FetwE7Tq8wPr5KkGfc%2Fopen-thread-november-1-7-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"line-height: 12.660714149475098px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "etwE7Tq8wPr5KkGfc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.405799936944451e-06, "legacy": true, "legacyId": "24571", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 301, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-02T20:35:55.780Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes November 2013", "slug": "rationality-quotes-november-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:02.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malcolmocean", "createdAt": "2012-05-16T15:13:58.012Z", "isAdmin": false, "displayName": "MalcolmOcean"}, "userId": "urN5htciqMuEeghc4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sRLQw9Woe8TMqpRLT/rationality-quotes-november-2013", "pageUrlRelative": "/posts/sRLQw9Woe8TMqpRLT/rationality-quotes-november-2013", "linkUrl": "https://www.lesswrong.com/posts/sRLQw9Woe8TMqpRLT/rationality-quotes-november-2013", "postedAtFormatted": "Saturday, November 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20November%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20November%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRLQw9Woe8TMqpRLT%2Frationality-quotes-november-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20November%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRLQw9Woe8TMqpRLT%2Frationality-quotes-november-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRLQw9Woe8TMqpRLT%2Frationality-quotes-november-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sRLQw9Woe8TMqpRLT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.4060288810103616e-06, "legacy": true, "legacyId": "24555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 390, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-03T06:29:09.725Z", "modifiedAt": null, "url": null, "title": "Kidnapping and the game of Chicken", "slug": "kidnapping-and-the-game-of-chicken", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:33.959Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GQy2BSQG9Dd6vPhs8/kidnapping-and-the-game-of-chicken", "pageUrlRelative": "/posts/GQy2BSQG9Dd6vPhs8/kidnapping-and-the-game-of-chicken", "linkUrl": "https://www.lesswrong.com/posts/GQy2BSQG9Dd6vPhs8/kidnapping-and-the-game-of-chicken", "postedAtFormatted": "Sunday, November 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kidnapping%20and%20the%20game%20of%20Chicken&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKidnapping%20and%20the%20game%20of%20Chicken%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQy2BSQG9Dd6vPhs8%2Fkidnapping-and-the-game-of-chicken%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kidnapping%20and%20the%20game%20of%20Chicken%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQy2BSQG9Dd6vPhs8%2Fkidnapping-and-the-game-of-chicken", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQy2BSQG9Dd6vPhs8%2Fkidnapping-and-the-game-of-chicken", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1255, "htmlBody": "<p><br /><img style=\"float: right; margin: 10px;\" src=\"http://images.lesswrong.com/t3_ixl_0.png\" alt=\"(0,0) (1,2) \\n (2,1) (0,0)\" width=\"239\" height=\"188\" />Observe the payoff matrix at right (the unit of reward? Cookies.). Each player wants to play 'A', but only so long as the two players play different moves.</p>\n<p>Suppose that Red got to move first. There are some games where moving first is terrible - take Rock Paper Scissors for example. But in this game, moving first is great, because you get to narrow down your opponent's options! If Red goes first, Red picks 'A', and then Blue has to pick 'B' to get a cookie.</p>\n<p>This is basically kidnapping. Red has taken all three cookies hostage, and nobody gets any cookies unless Blue agrees to Red's demands for two cookies. Whoever gets to move first plays the kidnapper, and the other player has to decide whether to accede to their ransom demand in exchange for a cookie.</p>\n<p>&nbsp;</p>\n<p>What if neither player gets to move before the other, but instead they have their moves revealed at the same time?</p>\n<p><em>Pre-Move Chat:</em></p>\n<p><em>Red: \"I'm going to pick A, you'd better pick B.\"</em></p>\n<p><em>Blue: \"I don't care what you pick, I'm picking A. You can pick A too if you really want to get 0 cookies.\"</em></p>\n<p><em>Red: \"Okay I'm really seriously going to pick A. Please pick B.\"</em></p>\n<p><em>Blue: \"Nah, don't think so. I'll just pick A. You should just pick B.\"</em></p>\n<p>And so on. They are now playing a game of Chicken. Whoever swerves first is worse off, but if neither of them give in, they crash into each other and die and get no cookies.</p>\n<p>&nbsp;</p>\n<p>So, The Question: is it better to play A, or to play B?</p>\n<div>This is definitely a trick question, but it can't be <em>too</em>&nbsp;trickish because at some point Red and Blue will have to figure out what to do. So why is it a trick question?</div>\n<div><br /></div>\n<div>Because this is a two-player game, and whether it's good to play A or not depends on what your opponent will do.</div>\n<div><br /></div>\n<div><br /></div>\n<div>A thought experiment: suppose we threw a party where you could only get dessert (cookies!) by playing this game. At the start, people are unfamiliar with the game, but they recognize that A has higher payoffs than B, so they all pick A all the time. But alas! When both people pick A, neither get anything, so no cookies are distributed. We decide that everyone can play as much as they want until we run out of cookies.</div>\n<div><br /></div>\n<div>Quite soon, one kind soul decides that they will play B, even though it has a lower payoff. A new round of games is begun, and each person gets a turn to play against our kind altruist. Soon, each other person has won their game, and they have 2 cookies each, while our selfless altruist has just one cookie per match they played. So, er, 11 or so cookies?</div>\n<div><br /></div>\n<div>Many of the other party-goers are enlightened by this example. They, too, want to be selfless and altruistic so that they can acquire 11 cookies / win at kidnapping. But a funny thing happens as each additional person plays B - the people playing A win two more cookies per round (one round is everyone getting to play everyone else once), and the people playing B win<em>&nbsp;</em>one <em>fewer</em>&nbsp;cookie, since nobody gets cookies when both play B either. Eventually, there are eight people playing A and four people playing B, and all of them nom 8 cookies per round.</div>\n<div><br /></div>\n<div><br /></div>\n<div>It's inevitable that the people playing B eventually get the same number of cookies as the people playing A - if there was a cookie imbalance, then people would switch to the better strategy until cookies were balanced again. Playing A has a higher payoff, but all that really means is that there are eight people playing A and only 4 playing B. It's like B has an&nbsp;<em>ecological niche</em>, and that niche is only of a certain size.</div>\n<div><br /></div>\n<div>What does the party case say about what Red and Blue should do when playing a one-shot game? The ratios of players turn into probabilities: if you're less than 67% sure the other person will play A, you should play A. If you're more than 67% sure, you should play B. This plan only works for situations similar to drawing an opponent out of a pool of deterministic players, though.</div>\n<div><br /></div>\n<div><br /></div>\n<div>Stage two of the problem: what if we allow players access to each others' source code?</div>\n<div><br /></div>\n<div>While you can still have A-players and B-players, you can now have a third strategy, which is to play B against A-players and play A against B-players. This strategy will have a niche size in between playing A and playing B.</div>\n<div><br /></div>\n<div>What's really great about reading source code, though, is that running into a copy of yourself no longer means duplicate moves and no cookies. The best \"A-players\" and \"B-players\" now choose moves against their copies by flipping coins, so that half the time they get at least one cookie. Flipping a coin against a copy of yourself averages 3/4 of a cookie, which is almost good enough to put B-players out of business. In fact, if we'd chosen our payoff matrix to have a bigger reward for playing A, we actually <em>could</em>&nbsp;put B-players out of business. Fun question: is it possible to decrease the total number of cookies won by increasing the reward for playing A?</div>\n<div><br /></div>\n<div>An interesting issue is how this modification changes the advice for the one-shot game. Our advice against simpler opponents was basically the \"predictor\" strategy, but that strategy is now in equilibrium with the other two! Good advice now is more like a meta-strategy. If the opponent is likely to be an A-player or a B-player, be a predictor, if the opponent is likely to be a predictor, be an A-player. Now that we've been this cycle before, it should be clearer that this \"advice\" is really a new strategy that will be introduced when we take the game one meta-level up. The effect on the game is really to introduce gradations of players, where some play A more often and some play B more often, but the populations can be balanced such that each player gets the same average reward.</div>\n<div><br /></div>\n<div><br /></div>\n<div>An interesting facet of a competition between predictors is what we might call \"stupidity envy\" (See <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP</a>). If we use the straightforward algorithm for our predictors (simulate what the opponent will do, then choose the best strategy), then a dumb predictor cannot predict the move of a smarter predictor. This is because the smarter predictor is predicting the dumb one, and you can't predict yourself in less time than you take to run. So the dumber predictor has to use some kind of default move. If its default move is A, then the smarter predictor has no good choice but to take B, and the dumber predictor wins.</div>\n<div><br /></div>\n<div>It's like the dumber predictor has gotten to move first. Being dumb / moving first isn't always good - imagine having to move first in rock paper scissors - but in games where moving first is better, and even a dumb predictor can see why, it's better to be the dumber predictor.</div>\n<div><br /></div>\n<div>On our other axis of smartness, though, the \"meta-level,\" more meta usually produces better head-to-head results - yet the humble A-player gets the best results of all. It's only the fact that A-players do poorly against other A-players that allows a diverse ecology on the B-playing side of the spectrum.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GQy2BSQG9Dd6vPhs8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 23, "extendedScore": null, "score": 1.406598551771171e-06, "legacy": true, "legacyId": "24537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q9DbfYfFzkotno9hG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-03T07:07:34.000Z", "modifiedAt": null, "url": null, "title": "The Witching Hour", "slug": "the-witching-hour", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8KHR3tfa4SJjMSkXd/the-witching-hour", "pageUrlRelative": "/posts/8KHR3tfa4SJjMSkXd/the-witching-hour", "linkUrl": "https://www.lesswrong.com/posts/8KHR3tfa4SJjMSkXd/the-witching-hour", "postedAtFormatted": "Sunday, November 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Witching%20Hour&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Witching%20Hour%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KHR3tfa4SJjMSkXd%2Fthe-witching-hour%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Witching%20Hour%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KHR3tfa4SJjMSkXd%2Fthe-witching-hour", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KHR3tfa4SJjMSkXd%2Fthe-witching-hour", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1896, "htmlBody": "<p>On an ordinary evening, Tal Aivon was lively and pleasant. The collection of longhouses and yurts within its tall brick walls shone bright with kerosene &#8211; not just torches, real kerosene &#8211; and its communal meeting area was noisy with conversation and song. The children would be playing their games, and on the eves of holy days the Lorekeepers would chant their stories of the Lost World, accompanied by lyres and the town&#8217;s one decaying gyitar.</p>\n<p>Tonight, though, a pall lay on Tal Aivon. The six gates of its tall brick walls were barred and shut, and foreboding warriors dressed in odd combinations of Kevlar and steel armor stood just within them, brandishing their swords. Families locked themselves in their yurts and longhouses, huddled around little kerosene lanterns. In the temple, the priests knelt before the stone idols of St. Christ and St. Mahomet, chanting plaintive prayers for protection.</p>\n<p>&#8220;I still don&#8217;t understand,&#8221; Meical Dorn complained, from inside the longest longhouse &#8220;what this is all about. &#8220;None of the wildlings are anywhere nearby &#8211; I should know, I&#8217;ve came through two hundred miles of forest to get here &#8211; and the only three towns in this area are at peace with you. In Great Rabda, even an impending attack couldn&#8217;t make us cower inside like this. I have half a mind to think there&#8217;s something you&#8217;re not telling me, Fin. Something that might&#8230;threaten our deal.&#8221;</p>\n<p>Fin Lerisas, Chief Lorekeeper for Tal Aivon, sighed. &#8220;Nothing that would threaten our deal, Meical. Great Rabda has gold. We have sunblessings. Just stay here long enough for our bankers to figure out the price, and you&#8217;ll have timers and mathers and lighters of your very own.&#8221;</p>\n<p>Meical glanced longingly at the Chief Lorekeeper&#8217;s own sunblessing, a timer that stood on the shelf of his private room. 1:52 AM gleamed on its face, with an maddeningly smooth red glow unlike sunlight or moonlight or firelight. Yet Meical knew it was sunlight, or something like. He was the Lorekeeper of Great Rabda. The Lorekeepers of Tal Aivon were far wiser than he &#8211; how could they not be with the town&#8217;s close proximity to ruined Diteroi and its trove of artifacts from the Lost World &#8211; but even he knew how sunblessings worked. You took them outside and the blue tiles on their surface fed on sunlight. Then they worked various miracles. Timers would tell you the time far more precisely than any sundial &#8211; invaluable in keeping the schedule of sacred prayer decreed by St. Mahomet. Mathers would add and subtract quantities more quickly than the fastest savant. Lighters would shine at night without wood or kerosene. </p>\n<p>Meical had no doubt that the Lorekeepers of Tal Aivon &#8211; the wisest on the Great Peninsula &#8211; knew of still other sunblessings, ones that mighty but lore-deficient Great Rabda had never heard of. He himself would be happy with anything &#8211; even the meanest timer. Of all the millions of wonders built by the Lost World, only the sunblessings still worked, and they were in fiendishly short supply. While lore-rich Tal Aivon had a timer upon each of its six gates, Great Rabda, for all its bountiful gold and grain, had not a single sunblessing to call its own. As its Lorekeeper, it would aid his status immensely if this trade mission was successful and he could bring something back to demonstrate the power of the Lost World and, incidentally, his own importance as keeper of its Lore.</p>\n<p>But even his greed for power did not override his concern for his own safety. &#8220;I&#8217;m serious, Fin. I want to know what&#8217;s going on. I can&#8217;t deal with a city that won&#8217;t even tell me why it&#8217;s on high alert.&#8221;</p>\n<p>Fin Lerisas, Chief Lorekeeper of Tal Aivon and wisest in ancient matters on the whole Great Peninsula, gave another sigh. &#8220;If you were not a Lorekeeper yourself, I would not sure such secrets with a foreigner. But if it threatens the deal, very well. Only know that you will be no happier with this knowledge, and that you may not sleep quite as soundly on autumn nights from now on.&#8221;</p>\n<p>Meical gave a nod, indicating he wanted the old man to continue.</p>\n<p>&#8220;In Great Rabda you have no sunblessings, and so you must keep the time like wildlings, by watching the course of the sun. Here in Tal Aivon we have six timers, one on each of the city gates, and so everyone down to the meanest peasant knows the time, down to the second. To most, they check the time when they enter the city, and the time when they leave the city, and they never think any more of it. We Lorekeepers are more astute, but not infinitely so. And so it was only forty years ago, in the time when my uncle Derech was Chief Lorekeeper, that we noticed&#8221; (and here his voice changed to a whisper) &#8220;that <i>there is something wrong with Time</i>.&#8221;</p>\n<p>&#8220;The stars,&#8221; he continued &#8220;sometimes match the time as told by the timers, and sometimes they do not. At first we thought the flaw was in the heavens themselves, so perfect are the devices of the Lost World. But this so discomfited my uncle that for three months he sat in front of this very timer, handing it off to an acolyte only when he slept. And one night, his watch bore fruit.&#8221;</p>\n<p>&#8220;What happened?&#8221; asked Meical, breathlessly.</p>\n<p>&#8220;Time moved backwards,&#8221; said Fin.</p>\n<p>&#8220;Impossible,&#8221; said Meical.</p>\n<p>&#8220;It was on this very night,&#8221; said Fin. &#8220;Time, which three hundred sixty four days of the year moves only in one direction, suddenly jumped backwards. And you yourself will witness it.&#8221;</p>\n<p>He pointed to the timer on his shelf, which now read 1:59 AM. Its red glow suddenly looked unfriendly, even eerie. Even though Meical knew it had to be sunlight at its root, it held none of the wholesomeness of the sun.</p>\n<p>And then it changed. 1:59 turned to 1:00.</p>\n<p>Meical gasped, and his fingers instantly formed the cross of St. Jesus and then the crescent of St. Mahomet. &#8220;Madness!&#8221; he whispered.</p>\n<p>&#8220;Something,&#8221; said Fin, &#8220;is wrong with this night. It is not always this night &#8211; it can come as early as three days before, or as late as three days after. My uncle worked out the formula after several years. But every year, it happens. Time jumps backwards.&#8221;</p>\n<p>&#8220;But why?&#8221; asked Meical. &#8220;Why would the gods do such a thing? Why would they break the symmetry of the True Time and the heavens?&#8221;</p>\n<p>&#8220;That&#8217;s the worst part,&#8221; said Fin. &#8220;When I was younger, I looked over my uncle&#8217;s formula &#8211; the one for calculating the day when the time skip would happen &#8211; and found what he had missed. The day of the time skip is fixed to the seven day calendar of the Lost World. To the ancients, it would always occur on the same day of the week. Sunday. Their holy day.&#8221;</p>\n<p>Meical felt his blood run cold. &#8220;That&#8217;s&#8230;some coincidence.&#8221;</p>\n<p>&#8220;Perhaps,&#8221; said Fin. &#8220;But I don&#8217;t think it <i>is</i> a coincidence. The gods are just. They would not play with Time as children play with blocks, picking one up here, then putting it down far away. I think the ancients of the Lost World, the ones who could build the great glass towers, the ones who manufactured sunblessings, the ones who made Diteroi-That-Was &#8211; I think they took their magic and threw it against time, and broke it. I think they wanted to become lords of time itself.&#8221;</p>\n<p>&#8220;But they failed,&#8221; guessed Meical.</p>\n<p>&#8220;They created a single hour,&#8221; said Fin. &#8220;Of the nine thousand hours in a year, all but one were made by the gods, but one was made by Man. What stopped them from creating more, from creating an infinite number of hours, from becoming immortal by arresting the progression of Time? We will never know. But it is my belief that when they saw what men had done, the gods stopped them before they could do worse. Meical, I believe that is how the Lost World ended. A last ditch effort by the gods to save Time itself from the hubris of Man.&#8221;</p>\n<p>Meical was silent. For all their wisdom, none of the Lorekeepers claimed to know how the Lost World ended. Surely the gods had pulverized it for some offense, but what sin could have been so dire as to doom those magnificent glass towers, those great black roads as smooth as water? Meical looked at the clock, gleaming 1:03 AM, and knew. Knew in his heart that Fin was right.</p>\n<p>&#8220;There is a day in the very early springtime,&#8221; said Fin, &#8220;when an hour disappears. The gods are stingy. They would not grant the ancients their victory. What they did with that hour in springtime, I do not know. But their message is clear.&#8221;</p>\n<p>Meical shuddered again. Like all the inhabitants of Great Rabda, he had told the time with the sun and the stars. But it had always been an approximation, not the to-the-second True Time displayed on the six gates of Tal Aivon. And so in their ignorance they had missed no fewer than two violations of Time, and it had fallen to the people of Tal Aivon alone to guard these terrible secrets.</p>\n<p>&#8220;You ask why we extinguish our fires and pray this night. Nine thousand hours in the year were made by the gods, but one was made by Man. I cannot help but wonder what walks abroad, during the hour no god made. I cannot help but wonder what spirits awake on the anniversary of the old world&#8217;s death. When time itself stands stagnant, what sorts of things breed within it? I prefer not to think about such things. That is why for the past forty years, ever since my discovery, I have knelt with the priests in the temple, and joined in their prayers. With an honored guest such as yourself here, I thought to entertain you instead, to avoid worrying you. Now I see that thought was vain. Will you come to the temple and pray with me?&#8221;</p>\n<p>And so on the longest night of the year, Fin Lerisas, Chief Lorekeeper of Tal Aivon, and Meical Dorn, Lorekeeper of Great Rabda, knelt in the temple and prayed to St. Jesus and St. Mahomet that time continue, that 1:59 AM be followed by 2:00 AM just as it always had in the past, and that the people be forgiven the sins of the Lost World, which had dared to change Time itself. And lo, at the appointed hour the six clocks on the six gates of Tal Aivon showed 2 AM, and the people rejoiced, and the kerosene lights were lit and the city of Tal Aivon was lively and pleasant once again.</p>\n<p>Three days later, Meical Dorn left Tal Aivon minus the gold he had brought but with a sunblessing of his own, a beautiful slate-gray mather that would have the engineers of Great Rabda dancing with glee. They had offered him a timer instead, a beautiful digital timer that even played short tunes at different hours, but Meical had refused. He bore a secret that need not trouble the people of Great Rabda. They would have a mather, and calculate things lightning-quick, and never know that there was a flaw in Time that even the gods themselves could not resolve.</p>\n<p>But until the day he died, every so often on chill autumn nights Meical Dorn would look up at the stars and shudder.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8KHR3tfa4SJjMSkXd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": "The Codex", "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "rNuPrZvabXe2MaZv8", "canonicalCollectionSlug": "codex", "canonicalBookId": "kcCvSNNZd8pfQvf9E", "canonicalNextPostSlug": "against-tulip-subsidies", "canonicalPrevPostSlug": "a-philosopher-walks-into-a-coffee-shop", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-03T13:30:45.766Z", "modifiedAt": null, "url": null, "title": "Meetup : Saint Petersburg. Why rationality?", "slug": "meetup-saint-petersburg-why-rationality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "efim", "createdAt": "2013-04-14T00:57:28.743Z", "isAdmin": false, "displayName": "efim"}, "userId": "Y8azdhZD6fvWdGwaB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jTZeqefkaNcA3C99j/meetup-saint-petersburg-why-rationality", "pageUrlRelative": "/posts/jTZeqefkaNcA3C99j/meetup-saint-petersburg-why-rationality", "linkUrl": "https://www.lesswrong.com/posts/jTZeqefkaNcA3C99j/meetup-saint-petersburg-why-rationality", "postedAtFormatted": "Sunday, November 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saint%20Petersburg.%20Why%20rationality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saint%20Petersburg.%20Why%20rationality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTZeqefkaNcA3C99j%2Fmeetup-saint-petersburg-why-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saint%20Petersburg.%20Why%20rationality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTZeqefkaNcA3C99j%2Fmeetup-saint-petersburg-why-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTZeqefkaNcA3C99j%2Fmeetup-saint-petersburg-why-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/sz'>Saint Petersburg. Why rationality?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>From 16:00 till 21:30.\nWe will be located in cafe \"PMG\" - for detailed description please read mailing list announcement.</p>\n\n<p>Come to our meet up! Whether you liked first one or missed it we would be very happy to see you.\nOn this one we plan to discuss why we personally learned and want to learn rational methods of thinking, thru this we will refine our picture of Rationality and bond.\nThere also will be a little report on logarithmic probabilities, fair scoring rules and exercise on calibration.</p>\n\n<p>There will be required reading\n(not exactly required, but if at all possible - please get acquainted - to get everyone on fairly equal groud).</p>\n\n<p>This three on first topic:\n1)<a href=\"http://yudkowsky.net/rational/virtues/\" rel=\"nofollow\">http://yudkowsky.net/rational/virtues/</a>\n2)<a href=\"http://lesswrong.com/lw/nb/something_to_protect/\" rel=\"nofollow\">http://lesswrong.com/lw/nb/something_to_protect/</a>\n3)<a href=\"http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\" rel=\"nofollow\">http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/</a>\nAlso I personally recommend to read at least original letter:\n4)<a href=\"http://yudkowsky.net/other/yehuda/\" rel=\"nofollow\">http://yudkowsky.net/other/yehuda/</a></p>\n\n<p>On the second topic it would be better to read these:\n1)<a href=\"http://lesswrong.com/lw/mp/0_and_1_are_not_probabilities/\" rel=\"nofollow\">http://lesswrong.com/lw/mp/0_and_1_are_not_probabilities/</a>\n2)<a href=\"http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/\" rel=\"nofollow\">http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/</a>\n(or\\and maybe <a href=\"http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/\" rel=\"nofollow\">http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/</a>)</p>\n\n<p>Subscribe to mailing list for notifications and possibly more information: <a href=\"https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg</a>\nOr contact me on 8 911 843 56 44. Every day from 18-00 to 00-00.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/sz'>Saint Petersburg. Why rationality?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jTZeqefkaNcA3C99j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.40700365582426e-06, "legacy": true, "legacyId": "24572", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Why_rationality_\">Discussion article for the meetup : <a href=\"/meetups/sz\">Saint Petersburg. Why rationality?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>From 16:00 till 21:30.\nWe will be located in cafe \"PMG\" - for detailed description please read mailing list announcement.</p>\n\n<p>Come to our meet up! Whether you liked first one or missed it we would be very happy to see you.\nOn this one we plan to discuss why we personally learned and want to learn rational methods of thinking, thru this we will refine our picture of Rationality and bond.\nThere also will be a little report on logarithmic probabilities, fair scoring rules and exercise on calibration.</p>\n\n<p>There will be required reading\n(not exactly required, but if at all possible - please get acquainted - to get everyone on fairly equal groud).</p>\n\n<p>This three on first topic:\n1)<a href=\"http://yudkowsky.net/rational/virtues/\" rel=\"nofollow\">http://yudkowsky.net/rational/virtues/</a>\n2)<a href=\"http://lesswrong.com/lw/nb/something_to_protect/\" rel=\"nofollow\">http://lesswrong.com/lw/nb/something_to_protect/</a>\n3)<a href=\"http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\" rel=\"nofollow\">http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/</a>\nAlso I personally recommend to read at least original letter:\n4)<a href=\"http://yudkowsky.net/other/yehuda/\" rel=\"nofollow\">http://yudkowsky.net/other/yehuda/</a></p>\n\n<p>On the second topic it would be better to read these:\n1)<a href=\"http://lesswrong.com/lw/mp/0_and_1_are_not_probabilities/\" rel=\"nofollow\">http://lesswrong.com/lw/mp/0_and_1_are_not_probabilities/</a>\n2)<a href=\"http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/\" rel=\"nofollow\">http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/</a>\n(or\\and maybe <a href=\"http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/\" rel=\"nofollow\">http://lesswrong.com/lw/jn/how_much_evidence_does_it_take/</a>)</p>\n\n<p>Subscribe to mailing list for notifications and possibly more information: <a href=\"https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg</a>\nOr contact me on 8 911 843 56 44. Every day from 18-00 to 00-00.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Why_rationality_1\">Discussion article for the meetup : <a href=\"/meetups/sz\">Saint Petersburg. Why rationality?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saint Petersburg. Why rationality?", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Why_rationality_", "level": 1}, {"title": "Discussion article for the meetup : Saint Petersburg. Why rationality?", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Why_rationality_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB", "DoLQN5ryZ9XkZjq5h", "QGkYCwyC7wTDyt3yT", "nj8JKFoLSMEmD3RGp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-03T15:58:41.117Z", "modifiedAt": null, "url": null, "title": "A Workflow with Spaced Repetition", "slug": "a-workflow-with-spaced-repetition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.195Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pkBzPGreRssNfDBnA/a-workflow-with-spaced-repetition", "pageUrlRelative": "/posts/pkBzPGreRssNfDBnA/a-workflow-with-spaced-repetition", "linkUrl": "https://www.lesswrong.com/posts/pkBzPGreRssNfDBnA/a-workflow-with-spaced-repetition", "postedAtFormatted": "Sunday, November 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Workflow%20with%20Spaced%20Repetition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Workflow%20with%20Spaced%20Repetition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpkBzPGreRssNfDBnA%2Fa-workflow-with-spaced-repetition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Workflow%20with%20Spaced%20Repetition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpkBzPGreRssNfDBnA%2Fa-workflow-with-spaced-repetition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpkBzPGreRssNfDBnA%2Fa-workflow-with-spaced-repetition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1103, "htmlBody": "<p>This is a detailed description of my reading and learning workflow. You may find ideas to adopt, or maybe you can tell me what I could be doing differently!</p>\n<h1>Overview</h1>\n<p>I've been using Spaced Repetition on and Off for the past few years, and have built a solid Anki habit this last three months, to the point where now I wonder how I could read books without entering the important points into Anki.</p>\n<p>I recommend getting a habit of using Spaced Repetition, it's a small habit that doesn't require too much willpower (it can feel like a game, if done right!), and is useful in the long term.</p>\n<h1>Daily routine: transit</h1>\n<p>I have a dozen or so Anki decks. Some I consider &ldquo;valuable&rdquo; (Algorithms, Driving Code, Git commands), some less so (Paris Metro, Hiragana and Katakana, Vim commands, &hellip;). I also carry around a book, notebook and four-color pen.</p>\n<p>On any downtime (waiting for transit, waiting in line in a store, standing in crowded transit&hellip;), I&rsquo;ll review my decks, starting with those with the most due cards.</p>\n<p>On some days I may not finish all the decks, but that&rsquo;s no big deal; with an hour and a half of transit per day, I&rsquo;ll get to them eventually.</p>\n<p>If I can sit for a bit of time, and don&rsquo;t have too many outstanding cards, I&rsquo;ll usually read a book (or work on stuff in my notebook if I have some stuff that needs brainstorming).</p>\n<h1><strong>Reading books</strong></h1>\n<p>If I&rsquo;m reading fiction, I&rsquo;m relaxing, I don&rsquo;t need to try to remember anything :)</p>\n<p>If I&rsquo;m reading non-fiction, I&rsquo;ll usually have an index card as a bookmark and place to take notes - things to look up, summaries and rephrasings, diagrams, page numbers of parts to come back to, and of course things to enter in Anki (though I&rsquo;ll sometimes just directly enter them in my phone).</p>\n<p>I&rsquo;ll reread my notes when I finished the book or a big chapter, or when I come back to the book after a long time, and eventually enter them in Anki (usually with Anki's web interface, which is quicker than typing on a phone).</p>\n<h1><strong>Reading online material</strong></h1>\n<p>I have a bunch of Google Docs where I take notes on various topics (why Google Docs? I can search them, share them if needed, work with them from various places). If I&rsquo;m reading something I want to remember, I&rsquo;ll usually have a corresponding google doc open in another window (so I can see both at the same time - hunting through tabs breaks the flow). My notes will be a mix of</p>\n<ul>\n<li>URLs marked as &ldquo;to read&rdquo; or &ldquo;read&rdquo; (with maybe a summary of what it&rsquo;s about)</li>\n<li>Verbatim quotes</li>\n<li>Rephrasings, insights, questions, brainstoriming</li>\n<li>&ldquo;anki format&rdquo; cards (pairs of question, then answer), for example, from my Haskell deck:</li>\n</ul>\n<blockquote>\n<pre>How do I declare that Integer is of class Eq, using IntegerEq?<br />instance Eq Integer where<br />&nbsp; x == y&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp; x `integerEq` y</pre>\n</blockquote>\n<p>(note that in this case it's three lines, when entering into Anki I'll have to put the first line as question and the two other ones as answer)</p>\n<p>Building the anki cards in Google docs makes it easier to make related cards by copying and pasting the same question and changing little bits (\"Question: ???, B and C\", \"Question: A, ??? and C\", \"Question: A, B and ???\")</p>\n<p>In the evening, when I don&rsquo;t have the energy for something more difficult, I&rsquo;ll occasionally copy batches of stuff from Google Docs into Anki. To do that first I copy everything into a plain text file (to strip all formatting, otherwise things look weird in Anki and it&rsquo;s distracting), and then cut-paste the cards into Anki by alt-tabbing between the text file and the Anki web interface (this sounds cumbersome but can be done fairly quickly using pretty much only the keyboard).</p>\n<h1><strong>What if I get behind?</strong></h1>\n<p>No big deal, I&rsquo;ll review the &ldquo;important&rdquo; decks first, and then eventually catch up on the rest (Some people recommend using one big deck for everything; I prefer having several small decks because it makes it easier to catch up with what matters if I &ldquo;fall of the bandwagon&rdquo;).</p>\n<h1><strong>What I learned</strong></h1>\n<ul>\n<li>Make Stupid and easy cards; I aim for having answers that are a single word</li>\n<li>I delete or suspend cards that I suspect are a waste of time (because I don&rsquo;t care about learning that; because it&rsquo;s too difficult; because I suspect it&rsquo;s wrong).</li>\n<li>Double-sided cards are useful for learning languages (I used to make both directions independently)</li>\n<li>If you're learning a foreign language with a weird alphabet, it's worth the extra effort of finding an imput system on your phone (or computer) that handles that alphabet.</li>\n</ul>\n<h1><strong>What I&rsquo;d like to improve</strong></h1>\n<p>Batch-entering data is a bit complicated, I wish I could just select a bunch of text in google docs and say \"just put all this in Anki\". However, as a low-energy habit batch-copying stuff feels a bit like a game so I don't mind that much.</p>\n<ul>\n<li>I wish I could put some decks at &ldquo;low throttle&rdquo; and some at &ldquo;high throttle&rdquo; (say, I want to learn 20 driving code cards a day, but only 3 vim cards). Anki has a setting that says how many new cards you get, but it's global; so either I change that setting all the time (which can be done fairly quickly), or control the influx by leaving stuff in Google Docs.</li>\n<li>I wish I could control randomization: just select a bunch of cards and say \"randomize these\". There's some cards I want to see in a random order, and some where I'd rather see them in the original order.</li>\n<li>Anki is bad at handling synchronization, if I used Anki on my phone and want to use the web interface, I need to synchronize first, which takes a few minutes and may fail; otherwise there will be a conflict and I will have to pick which of the two datasets I keep. This is another reason why I prefer to use Google Docs for staging: waiting for synchronization breaks my flow.</li>\n<li>How do people use evernote or supermemo?</li>\n</ul>\n<h1>More resources on Spaced Repetition<br /></h1>\n<p>The <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">article on the Wiki</a> points to a few discussions here of Spaced Repetition (which are worth reading if you want to see how other people use it), including <a href=\"http://www.gwern.net/Spaced%20repetition\">Gwern's excellent article</a>.</p>\n<p>How about you? Do you use Spaced Repetition? Have you tried, but give up? Do you have a workflow with some bits that differ from mine? Do you have any tips of things I could do better?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pkBzPGreRssNfDBnA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "24573", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is a detailed description of my reading and learning workflow. You may find ideas to adopt, or maybe you can tell me what I could be doing differently!</p>\n<h1 id=\"Overview\">Overview</h1>\n<p>I've been using Spaced Repetition on and Off for the past few years, and have built a solid Anki habit this last three months, to the point where now I wonder how I could read books without entering the important points into Anki.</p>\n<p>I recommend getting a habit of using Spaced Repetition, it's a small habit that doesn't require too much willpower (it can feel like a game, if done right!), and is useful in the long term.</p>\n<h1 id=\"Daily_routine__transit\">Daily routine: transit</h1>\n<p>I have a dozen or so Anki decks. Some I consider \u201cvaluable\u201d (Algorithms, Driving Code, Git commands), some less so (Paris Metro, Hiragana and Katakana, Vim commands, \u2026). I also carry around a book, notebook and four-color pen.</p>\n<p>On any downtime (waiting for transit, waiting in line in a store, standing in crowded transit\u2026), I\u2019ll review my decks, starting with those with the most due cards.</p>\n<p>On some days I may not finish all the decks, but that\u2019s no big deal; with an hour and a half of transit per day, I\u2019ll get to them eventually.</p>\n<p>If I can sit for a bit of time, and don\u2019t have too many outstanding cards, I\u2019ll usually read a book (or work on stuff in my notebook if I have some stuff that needs brainstorming).</p>\n<h1 id=\"Reading_books\"><strong>Reading books</strong></h1>\n<p>If I\u2019m reading fiction, I\u2019m relaxing, I don\u2019t need to try to remember anything :)</p>\n<p>If I\u2019m reading non-fiction, I\u2019ll usually have an index card as a bookmark and place to take notes - things to look up, summaries and rephrasings, diagrams, page numbers of parts to come back to, and of course things to enter in Anki (though I\u2019ll sometimes just directly enter them in my phone).</p>\n<p>I\u2019ll reread my notes when I finished the book or a big chapter, or when I come back to the book after a long time, and eventually enter them in Anki (usually with Anki's web interface, which is quicker than typing on a phone).</p>\n<h1 id=\"Reading_online_material\"><strong>Reading online material</strong></h1>\n<p>I have a bunch of Google Docs where I take notes on various topics (why Google Docs? I can search them, share them if needed, work with them from various places). If I\u2019m reading something I want to remember, I\u2019ll usually have a corresponding google doc open in another window (so I can see both at the same time - hunting through tabs breaks the flow). My notes will be a mix of</p>\n<ul>\n<li>URLs marked as \u201cto read\u201d or \u201cread\u201d (with maybe a summary of what it\u2019s about)</li>\n<li>Verbatim quotes</li>\n<li>Rephrasings, insights, questions, brainstoriming</li>\n<li>\u201canki format\u201d cards (pairs of question, then answer), for example, from my Haskell deck:</li>\n</ul>\n<blockquote>\n<pre>How do I declare that Integer is of class Eq, using IntegerEq?<br>instance Eq Integer where<br>&nbsp; x == y&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp; x `integerEq` y</pre>\n</blockquote>\n<p>(note that in this case it's three lines, when entering into Anki I'll have to put the first line as question and the two other ones as answer)</p>\n<p>Building the anki cards in Google docs makes it easier to make related cards by copying and pasting the same question and changing little bits (\"Question: ???, B and C\", \"Question: A, ??? and C\", \"Question: A, B and ???\")</p>\n<p>In the evening, when I don\u2019t have the energy for something more difficult, I\u2019ll occasionally copy batches of stuff from Google Docs into Anki. To do that first I copy everything into a plain text file (to strip all formatting, otherwise things look weird in Anki and it\u2019s distracting), and then cut-paste the cards into Anki by alt-tabbing between the text file and the Anki web interface (this sounds cumbersome but can be done fairly quickly using pretty much only the keyboard).</p>\n<h1 id=\"What_if_I_get_behind_\"><strong>What if I get behind?</strong></h1>\n<p>No big deal, I\u2019ll review the \u201cimportant\u201d decks first, and then eventually catch up on the rest (Some people recommend using one big deck for everything; I prefer having several small decks because it makes it easier to catch up with what matters if I \u201cfall of the bandwagon\u201d).</p>\n<h1 id=\"What_I_learned\"><strong>What I learned</strong></h1>\n<ul>\n<li>Make Stupid and easy cards; I aim for having answers that are a single word</li>\n<li>I delete or suspend cards that I suspect are a waste of time (because I don\u2019t care about learning that; because it\u2019s too difficult; because I suspect it\u2019s wrong).</li>\n<li>Double-sided cards are useful for learning languages (I used to make both directions independently)</li>\n<li>If you're learning a foreign language with a weird alphabet, it's worth the extra effort of finding an imput system on your phone (or computer) that handles that alphabet.</li>\n</ul>\n<h1 id=\"What_I_d_like_to_improve\"><strong>What I\u2019d like to improve</strong></h1>\n<p>Batch-entering data is a bit complicated, I wish I could just select a bunch of text in google docs and say \"just put all this in Anki\". However, as a low-energy habit batch-copying stuff feels a bit like a game so I don't mind that much.</p>\n<ul>\n<li>I wish I could put some decks at \u201clow throttle\u201d and some at \u201chigh throttle\u201d (say, I want to learn 20 driving code cards a day, but only 3 vim cards). Anki has a setting that says how many new cards you get, but it's global; so either I change that setting all the time (which can be done fairly quickly), or control the influx by leaving stuff in Google Docs.</li>\n<li>I wish I could control randomization: just select a bunch of cards and say \"randomize these\". There's some cards I want to see in a random order, and some where I'd rather see them in the original order.</li>\n<li>Anki is bad at handling synchronization, if I used Anki on my phone and want to use the web interface, I need to synchronize first, which takes a few minutes and may fail; otherwise there will be a conflict and I will have to pick which of the two datasets I keep. This is another reason why I prefer to use Google Docs for staging: waiting for synchronization breaks my flow.</li>\n<li>How do people use evernote or supermemo?</li>\n</ul>\n<h1 id=\"More_resources_on_Spaced_Repetition\">More resources on Spaced Repetition<br></h1>\n<p>The <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">article on the Wiki</a> points to a few discussions here of Spaced Repetition (which are worth reading if you want to see how other people use it), including <a href=\"http://www.gwern.net/Spaced%20repetition\">Gwern's excellent article</a>.</p>\n<p>How about you? Do you use Spaced Repetition? Have you tried, but give up? Do you have a workflow with some bits that differ from mine? Do you have any tips of things I could do better?</p>", "sections": [{"title": "Overview", "anchor": "Overview", "level": 1}, {"title": "Daily routine: transit", "anchor": "Daily_routine__transit", "level": 1}, {"title": "Reading books", "anchor": "Reading_books", "level": 1}, {"title": "Reading online material", "anchor": "Reading_online_material", "level": 1}, {"title": "What if I get behind?", "anchor": "What_if_I_get_behind_", "level": 1}, {"title": "What I learned", "anchor": "What_I_learned", "level": 1}, {"title": "What I\u2019d like to improve", "anchor": "What_I_d_like_to_improve", "level": 1}, {"title": "More resources on Spaced Repetition", "anchor": "More_resources_on_Spaced_Repetition", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-03T17:27:18.049Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup, 10/11/13", "slug": "meetup-london-social-meetup-10-11-13", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2yDP3Sdk4fd7XDaBm/meetup-london-social-meetup-10-11-13", "pageUrlRelative": "/posts/2yDP3Sdk4fd7XDaBm/meetup-london-social-meetup-10-11-13", "linkUrl": "https://www.lesswrong.com/posts/2yDP3Sdk4fd7XDaBm/meetup-london-social-meetup-10-11-13", "postedAtFormatted": "Sunday, November 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup%2C%2010%2F11%2F13&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%2C%2010%2F11%2F13%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yDP3Sdk4fd7XDaBm%2Fmeetup-london-social-meetup-10-11-13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%2C%2010%2F11%2F13%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yDP3Sdk4fd7XDaBm%2Fmeetup-london-social-meetup-10-11-13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2yDP3Sdk4fd7XDaBm%2Fmeetup-london-social-meetup-10-11-13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t0'>London social meetup, 10/11/13</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LW London is having another social meetup, and you should come! There's no fixed topic of discussion, so we talk about whatever strikes our fancy, which can range from fanfiction to cryonics by way of logic puzzles and utilitarian population ethics.</p>\n\n<p>We'll meet in the Shakespeare's Head near Holborn tube station, at 2pm. There'll be a sign identifying us; also, feel free to contact me through email (philip.hazelden@gmail.com) or phone (07792009646) if you have questions or difficulty finding us.</p>\n\n<p>Also check out our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t0'>London social meetup, 10/11/13</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2yDP3Sdk4fd7XDaBm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "24574", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup__10_11_13\">Discussion article for the meetup : <a href=\"/meetups/t0\">London social meetup, 10/11/13</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LW London is having another social meetup, and you should come! There's no fixed topic of discussion, so we talk about whatever strikes our fancy, which can range from fanfiction to cryonics by way of logic puzzles and utilitarian population ethics.</p>\n\n<p>We'll meet in the Shakespeare's Head near Holborn tube station, at 2pm. There'll be a sign identifying us; also, feel free to contact me through email (philip.hazelden@gmail.com) or phone (07792009646) if you have questions or difficulty finding us.</p>\n\n<p>Also check out our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup__10_11_131\">Discussion article for the meetup : <a href=\"/meetups/t0\">London social meetup, 10/11/13</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup, 10/11/13", "anchor": "Discussion_article_for_the_meetup___London_social_meetup__10_11_13", "level": 1}, {"title": "Discussion article for the meetup : London social meetup, 10/11/13", "anchor": "Discussion_article_for_the_meetup___London_social_meetup__10_11_131", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-03T21:26:52.468Z", "modifiedAt": null, "url": null, "title": "The Inefficiency of Theoretical Discovery", "slug": "the-inefficiency-of-theoretical-discovery", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:07.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jZtyt67PS2QfehkE8/the-inefficiency-of-theoretical-discovery", "pageUrlRelative": "/posts/jZtyt67PS2QfehkE8/the-inefficiency-of-theoretical-discovery", "linkUrl": "https://www.lesswrong.com/posts/jZtyt67PS2QfehkE8/the-inefficiency-of-theoretical-discovery", "postedAtFormatted": "Sunday, November 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Inefficiency%20of%20Theoretical%20Discovery&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Inefficiency%20of%20Theoretical%20Discovery%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZtyt67PS2QfehkE8%2Fthe-inefficiency-of-theoretical-discovery%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Inefficiency%20of%20Theoretical%20Discovery%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZtyt67PS2QfehkE8%2Fthe-inefficiency-of-theoretical-discovery", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZtyt67PS2QfehkE8%2Fthe-inefficiency-of-theoretical-discovery", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 559, "htmlBody": "<p><small>Previously: <a href=\"http://www.overcomingbias.com/2009/11/why-neglect-big-topics.html\">Why Neglect Big Topics</a>.</small></p>\n<p>Why was there no serious philosophical discussion of <a href=\"http://wiki.lesswrong.com/wiki/Moral_uncertainty\">normative uncertainty</a> until <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/04/Hudson-Subjectivization-in-Ethics.pdf\">1989</a>, given that all the necessary ideas and tools were present at the time of Jeremy Bentham?</p>\n<p>Why did no professional philosopher analyze I.J. Good&rsquo;s important &ldquo;intelligence explosion&rdquo; thesis (from 1959<sup><a id=\"fnref:1\" href=\"#fn:1\">1</a></sup>) until <a href=\"http://www.consc.net/papers/singularityjcs.pdf\">2010</a>?</p>\n<p>Why was <a href=\"/lw/h1k/reflection_in_probabilistic_logic/\">reflectively consistent probabilistic metamathematics</a> not described until 2013, given that the ideas it builds on go back at least to the 1940s?</p>\n<p>Why did it take <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/09/Spohn-Dependency-equilibria-and-the-causal-structure-of-decision-and-game-situations.pdf\">until 2003</a> for professional philosophers to begin updating causal decision theory for the age of causal Bayes nets, and until <a href=\"http://prestongreene.com/Papers_files/Greene,%20Rationality%20and%20Success.pdf\">2013</a> to formulate a reliabilist metatheory of rationality?</p>\n<p>By analogy to <a href=\"http://en.wikipedia.org/wiki/Efficient-market_hypothesis\">financial market efficiency</a>, I like to say that &ldquo;theoretical discovery is fairly inefficient.&rdquo; That is: there are often large, unnecessary delays in theoretical discovery.</p>\n<p>This shouldn&rsquo;t surprise us. For one thing, there aren&rsquo;t necessarily large personal rewards for making theoretical progress. But it does mean that those who <em>do</em> care about certain kinds of theoretical progress shouldn&rsquo;t necessarily think that progress will be hard. There is often low-hanging fruit to be plucked by investigators who know where to look.</p>\n<p>Where should we look for low-hanging fruit? I&rsquo;d guess that theoretical progress may be relatively easy where:</p>\n<ol>\n<li>Progress has no obvious, immediately profitable applications.</li>\n<li>Relatively few quality-adjusted researcher hours have been devoted to the problem.</li>\n<li>New tools or theoretical advances open up promising new angles of attack.</li>\n<li>Progress is only valuable to those with unusual views.</li>\n</ol>\n<p>These guesses make sense of the abundant low-hanging fruit in much of MIRI&rsquo;s theoretical research, with the glaring exception of decision theory. Our September decision theory workshop revealed plenty of low-hanging fruit, but why should that be? Decision theory is widely applied in <a href=\"http://en.wikipedia.org/wiki/Multi-agent_system\">multi-agent systems</a>, and in philosophy it&rsquo;s clear that visible progress in decision theory is one way to &ldquo;make a name&rdquo; for oneself and advance one&rsquo;s career. Tons of quality-adjusted researcher hours have been devoted to the problem. Yes, new theoretical advances (e.g. causal Bayes nets and program equilibrium) open up promising new angles of attack, but they don&rsquo;t seem necessary to much of the low-hanging fruit discovered thus far. And progress in decision theory is definitely not valuable only to those with unusual views. What gives?</p>\n<p>Anyway, three questions:</p>\n<ol>\n<li>Do you agree about the relative inefficiency of theoretical discovery?</li>\n<li>What are some other signs of likely low-hanging fruit for theoretical progress?</li>\n<li>What&rsquo;s up with decision theory having so much low-hanging fruit?</li>\n</ol>\n<div><br /></div>\n<p><sup>1</sup><small> <a id=\"fn:1\" href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">Good (1959)</a> is the earliest statement of the intelligence explosion: &ldquo;Once a machine is designed that is good enough&hellip; it can be put to work designing an even better machine. At this point an &rdquo;explosion&ldquo; will clearly occur; all the problems of science and technology will be handed over to machines and it will no longer be necessary for people to work. Whether this will lead to a Utopia or to the extermination of the human race will depend on how the problem is handled by the machines. The important thing will be to give them the aim of serving human beings.&rdquo; The term itself, &ldquo;intelligence explosion,&rdquo; originates with <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\">Good (1965)</a>. Technically, artist and philosopher&nbsp;<a href=\"http://en.wikipedia.org/wiki/Stefan_Themerson\">Stefan Themerson</a> wrote a \"philosophical analysis\" of Good's intelligence explosion thesis called&nbsp;<em><a href=\"http://www.amazon.com/gp/product/0852471033/\">Special Branch</a>,</em>&nbsp;published in 1972, but by \"philosophical analysis\" I have in mind a more analytic, argumentative kind of philosophical analysis than is found in Themerson's literary&nbsp;<em>Special Branch</em>.&nbsp;<a href=\"#fnref:1\">&nbsp;\u21a9</a></small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jZtyt67PS2QfehkE8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 29, "extendedScore": null, "score": 1.4074613861056305e-06, "legacy": true, "legacyId": "24576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["duAkuSqJhGDcfMaTA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-04T07:05:56.800Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels monthly meetup: memory!", "slug": "meetup-brussels-monthly-meetup-memory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:29.253Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sc7R5sJzzYvZwd3YM/meetup-brussels-monthly-meetup-memory", "pageUrlRelative": "/posts/sc7R5sJzzYvZwd3YM/meetup-brussels-monthly-meetup-memory", "linkUrl": "https://www.lesswrong.com/posts/sc7R5sJzzYvZwd3YM/meetup-brussels-monthly-meetup-memory", "postedAtFormatted": "Monday, November 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20memory!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20monthly%20meetup%3A%20memory!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsc7R5sJzzYvZwd3YM%2Fmeetup-brussels-monthly-meetup-memory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20memory!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsc7R5sJzzYvZwd3YM%2Fmeetup-brussels-monthly-meetup-memory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsc7R5sJzzYvZwd3YM%2Fmeetup-brussels-monthly-meetup-memory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t1'>Brussels monthly meetup: memory!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 November 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's topic: memory. Read some interesting research? Have a niggling worry that your entire life started last week and any memory prior to that is a fabrication? Come share! I'm experimenting with the memory palace/method of loci; if I'm successful I might try and do the party trick.</p>\n\n<p>As on every second Saturday of the month, we will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</p>\n\n<p>The Brussels meetups use a Google Group: https://groups.google.com/forum/#!forum/lesswrong-brussels</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t1'>Brussels monthly meetup: memory!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sc7R5sJzzYvZwd3YM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4080184570708681e-06, "legacy": true, "legacyId": "24580", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__memory_\">Discussion article for the meetup : <a href=\"/meetups/t1\">Brussels monthly meetup: memory!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 November 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's topic: memory. Read some interesting research? Have a niggling worry that your entire life started last week and any memory prior to that is a fabrication? Come share! I'm experimenting with the memory palace/method of loci; if I'm successful I might try and do the party trick.</p>\n\n<p>As on every second Saturday of the month, we will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</p>\n\n<p>The Brussels meetups use a Google Group: https://groups.google.com/forum/#!forum/lesswrong-brussels</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__memory_1\">Discussion article for the meetup : <a href=\"/meetups/t1\">Brussels monthly meetup: memory!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels monthly meetup: memory!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__memory_", "level": 1}, {"title": "Discussion article for the meetup : Brussels monthly meetup: memory!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__memory_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-04T10:39:19.077Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup", "slug": "meetup-urbana-champaign-thinking-fast-and-slow-pages-70-140", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c6NzqeGo36NEimj9u/meetup-urbana-champaign-thinking-fast-and-slow-pages-70-140", "pageUrlRelative": "/posts/c6NzqeGo36NEimj9u/meetup-urbana-champaign-thinking-fast-and-slow-pages-70-140", "linkUrl": "https://www.lesswrong.com/posts/c6NzqeGo36NEimj9u/meetup-urbana-champaign-thinking-fast-and-slow-pages-70-140", "postedAtFormatted": "Monday, November 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Pages%2070-140%20Discussion%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Pages%2070-140%20Discussion%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6NzqeGo36NEimj9u%2Fmeetup-urbana-champaign-thinking-fast-and-slow-pages-70-140%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Pages%2070-140%20Discussion%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6NzqeGo36NEimj9u%2Fmeetup-urbana-champaign-thinking-fast-and-slow-pages-70-140", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6NzqeGo36NEimj9u%2Fmeetup-urbana-champaign-thinking-fast-and-slow-pages-70-140", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t2'>Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coordinates are: 40.109545,-88.227318 Meetup will be held in the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Discussion will be on pages 70-140 of Thinking Fast and Slow.</p>\n\n<p>We will probably also discuss the raven paradox after having a week to think about it and read outside opinions, because we were unsatisfied with our response to it last time.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/K6gCQ4yWk90\">Cross-posted on the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t2'>Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c6NzqeGo36NEimj9u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.4082238198231371e-06, "legacy": true, "legacyId": "24584", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Pages_70_140_Discussion_Meetup\">Discussion article for the meetup : <a href=\"/meetups/t2\">Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coordinates are: 40.109545,-88.227318 Meetup will be held in the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Discussion will be on pages 70-140 of Thinking Fast and Slow.</p>\n\n<p>We will probably also discuss the raven paradox after having a week to think about it and read outside opinions, because we were unsatisfied with our response to it last time.</p>\n\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/K6gCQ4yWk90\">Cross-posted on the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Pages_70_140_Discussion_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/t2\">Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Pages_70_140_Discussion_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Pages_70_140_Discussion_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-04T15:43:55.704Z", "modifiedAt": null, "url": null, "title": "From Philosophy to Math to Engineering", "slug": "from-philosophy-to-math-to-engineering", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TDTnut9pkT6hdKrf8/from-philosophy-to-math-to-engineering", "pageUrlRelative": "/posts/TDTnut9pkT6hdKrf8/from-philosophy-to-math-to-engineering", "linkUrl": "https://www.lesswrong.com/posts/TDTnut9pkT6hdKrf8/from-philosophy-to-math-to-engineering", "postedAtFormatted": "Monday, November 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20From%20Philosophy%20to%20Math%20to%20Engineering&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrom%20Philosophy%20to%20Math%20to%20Engineering%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDTnut9pkT6hdKrf8%2Ffrom-philosophy-to-math-to-engineering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=From%20Philosophy%20to%20Math%20to%20Engineering%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDTnut9pkT6hdKrf8%2Ffrom-philosophy-to-math-to-engineering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDTnut9pkT6hdKrf8%2Ffrom-philosophy-to-math-to-engineering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 541, "htmlBody": "<p><small>Cross-posted from the <a href=\"http://intelligence.org/2013/11/04/from-philosophy-to-math-to-engineering/\">MIRI blog</a>.</small></p>\n<p>For centuries, philosophers wondered how we could learn what causes what. Some argued it was impossible, or possible only via experiment. Others kept <a href=\"/lw/8ns/hack_away_at_the_edges/\">hacking away</a> at the problem, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/10/Pearl-The-Art-and-Science-of-Cause-and-Effect.pdf\">clarifying ideas</a> like <em>counterfactual</em> and <em>probability</em> and  <em>correlation</em> by making them more precise and coherent.</p>\n<p>Then, in the 1990s, a breakthrough: Judea Pearl and others <a href=\"http://www.amazon.com/Causality-ebook/dp/B00AKE1VYK/\">showed</a> that, in principle, we can sometimes infer causal relations from data even without experiment, via the mathematical machinery of probabilistic graphical models.</p>\n<p>Next, engineers used this mathematical insight to write <a href=\"http://cran.r-project.org/web/packages/pcalg/index.html\">software</a> that can, in seconds, infer causal relations from a data set of observations.</p>\n<p>Across the centuries, researchers had toiled away, pushing our understanding of causality from philosophy to math to engineering.</p>\n<p align=\"center\"><a href=\"https://intelligence.org/wp-content/uploads/2013/10/From-Philosophy-to-Math-to-Engineering.jpg\"><img class=\"aligncenter size-full wp-image-10570\" src=\"https://intelligence.org/wp-content/uploads/2013/10/From-Philosophy-to-Math-to-Engineering-small.jpg\" alt=\"From Philosophy to Math to Engineering (small)\" /></a></p>\n<p>And so it is with&nbsp;<a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a>&nbsp;research. Current progress on each sub-problem of Friendly AI lies somewhere on a spectrum from philosophy to math to engineering.</p>\n<!--more-->\n<p>We began with some fuzzy philosophical ideas of what we want from a Friendly AI (FAI).&nbsp;We want it to be benevolent and powerful enough to eliminate suffering, protect us from natural catastrophes, help us explore the universe, and otherwise make life&nbsp;<em>awesome</em>. We want FAI to allow for moral progress, rather than immediately reshape the galaxy according to whatever our current values happen to be. We want FAI to remain beneficent even as it rewrites its core algorithms to become smarter and smarter. And so on.</p>\n<p>Small pieces of this philosophical puzzle have been broken off and <a href=\"/lw/hok/link_scott_aaronson_on_free_will/9546?context=1#comments\">turned into</a> math, e.g. <a href=\"http://www.amazon.com/Causality-ebook/dp/B00AKE1VYK/\">Pearlian causal analysis</a> and <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff induction</a>. Pearl's math has since been used to produce causal inference software that can be run on today's computers, whereas engineers have thus far succeeded in implementing (tractable approximations of) Solomonoff induction&nbsp;only for <a href=\"http://arxiv.org/pdf/0909.0801v2.pdf\">very limited applications</a>.</p>\n<p>Toy versions of two pieces of the \"stable self-modification\" problem were transformed into math problems in <a href=\"http://intelligence.org/files/OntologicalCrises.pdf\">de Blanc (2011)</a> and <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky &amp; Herreshoff (2013)</a>, though this was done to enable further insight via formal analysis, not to assert that these small pieces of the philosophical problem had been&nbsp;<em>solved</em>&nbsp;to the level of math.</p>\n<p>Thanks to Patrick LaVictoire and other <a href=\"http://intelligence.org/get-involved/#workshop\">MIRI workshop</a> participants,<sup>1</sup> Douglas Hofstadter's FAI-relevant philosophical idea of \"<a href=\"http://en.wikipedia.org/wiki/Superrationality\">superrationality</a>\" seems to have been, for the most part, successfully <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">transformed</a> into math, and a bit of the engineering work has also <a href=\"https://github.com/klao/provability/blob/master/modal.hs\">been done</a>.</p>\n<p>I say \"seems\" because, while humans are fairly skilled at turning math into feats of practical engineering, we seem to be <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">much</a> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/10/Brennan-Scepticism-about-philosophy.pdf\"><em>less</em></a> <a href=\"http://consc.net/papers/progress.pdf\">skilled</a> at turning philosophy into math, without leaving anything out. For example, some very sophisticated thinkers have <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/10/Rathmanner-Hutter-A-philosophical-treatise-of-universal-induction.pdf\">claimed</a>&nbsp;that \"Solomonoff induction solves the problem of inductive inference,\" or <a href=\"http://www.amazon.com/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/0387339981/\">that</a> \"Solomonoff has successfully invented a perfect theory of induction.\"&nbsp;And indeed, it certainly&nbsp;<em>seems</em> like a truly universal induction procedure. However, it <a href=\"/lw/cw1/open_problems_related_to_solomonoff_induction/\">turns out</a> that Solomonoff induction <em>doesn't</em> fully solve the problem of inductive inference, for relatively subtle reasons.<sup>2</sup></p>\n<p>Unfortunately, philosophical mistakes like this could be fatal when humanity builds the first self-improving AGI (<a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Yudkowsky 2008</a>).<sup>3</sup> FAI-relevant philosophical work is, as Nick Bostrom says, \"philosophy with a deadline.\"</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> <small>And before them, <a href=\"http://mechroom.technion.ac.il/~moshet/progeqnote4.pdf\">Moshe Tennenholtz</a>.</small></p>\n<p><sup>2</sup> <small>Yudkowsky plans to write more about how to improve on Solomonoff induction, later.</small></p>\n<p><small></small><sup>3</sup> <small>This is a specific instance of a problem Peter Ludlow described like <a href=\"http://leiterreports.typepad.com/blog/2013/10/progress-in-philosophy-revisited.html\">this</a>: \"the technological curve is pulling away from the philosophy curve very rapidly and is about to leave it completely behind.\"</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TDTnut9pkT6hdKrf8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 32, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "24586", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6bSHiD9TxsJwe2WqT", "Kyc5dFDzBg4WccrbK", "FwiPfF8Woe5JrzqEu", "fC248GwrWLT4Dkjf6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-04T17:05:17.789Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup Bolder CO", "slug": "meetup-meetup-bolder-co", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.442Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yakurbe0112", "createdAt": "2012-07-01T23:40:26.855Z", "isAdmin": false, "displayName": "yakurbe0112"}, "userId": "xXr6Jngw57uMXveQ9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gN2vnAW9tmqmF9eWm/meetup-meetup-bolder-co", "pageUrlRelative": "/posts/gN2vnAW9tmqmF9eWm/meetup-meetup-bolder-co", "linkUrl": "https://www.lesswrong.com/posts/gN2vnAW9tmqmF9eWm/meetup-meetup-bolder-co", "postedAtFormatted": "Monday, November 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%20Bolder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%20Bolder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgN2vnAW9tmqmF9eWm%2Fmeetup-meetup-bolder-co%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%20Bolder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgN2vnAW9tmqmF9eWm%2Fmeetup-meetup-bolder-co", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgN2vnAW9tmqmF9eWm%2Fmeetup-meetup-bolder-co", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t3'>Meetup Bolder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 November 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Old Chicago 1102 Pearl St, Boulder, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We didn't meet up last week for the discucion so we will do it this week. The topic is what we want from our meetup group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t3'>Meetup Bolder CO</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gN2vnAW9tmqmF9eWm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4085954461361378e-06, "legacy": true, "legacyId": "24587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup_Bolder_CO\">Discussion article for the meetup : <a href=\"/meetups/t3\">Meetup Bolder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 November 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Old Chicago 1102 Pearl St, Boulder, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We didn't meet up last week for the discucion so we will do it this week. The topic is what we want from our meetup group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup_Bolder_CO1\">Discussion article for the meetup : <a href=\"/meetups/t3\">Meetup Bolder CO</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup Bolder CO", "anchor": "Discussion_article_for_the_meetup___Meetup_Bolder_CO", "level": 1}, {"title": "Discussion article for the meetup : Meetup Bolder CO", "anchor": "Discussion_article_for_the_meetup___Meetup_Bolder_CO1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-04T23:33:57.741Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area Meetup: Productive Other-Optimization", "slug": "meetup-durham-nc-triangle-area-meetup-productive-other", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kP5FTXw6qrXMqjJPF/meetup-durham-nc-triangle-area-meetup-productive-other", "pageUrlRelative": "/posts/kP5FTXw6qrXMqjJPF/meetup-durham-nc-triangle-area-meetup-productive-other", "linkUrl": "https://www.lesswrong.com/posts/kP5FTXw6qrXMqjJPF/meetup-durham-nc-triangle-area-meetup-productive-other", "postedAtFormatted": "Monday, November 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%20Meetup%3A%20Productive%20Other-Optimization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%20Meetup%3A%20Productive%20Other-Optimization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkP5FTXw6qrXMqjJPF%2Fmeetup-durham-nc-triangle-area-meetup-productive-other%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%20Meetup%3A%20Productive%20Other-Optimization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkP5FTXw6qrXMqjJPF%2Fmeetup-durham-nc-triangle-area-meetup-productive-other", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkP5FTXw6qrXMqjJPF%2Fmeetup-durham-nc-triangle-area-meetup-productive-other", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t4'>Durham NC/Triangle Area Meetup: Productive Other-Optimization</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 November 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2411 N Roxboro St, Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bring your problems, worries, and knowledge gaps, great and small, and we will apply our communal knowledge to solving them - and try to recognize when we're committing the Typical Mind Fallacy.</p>\n\n<p>It will likely help to write down some problems beforehand!  Feel free to post them here as well, to get our thoughts going. If anyone is particularly eager to go first, they have priority - otherwise we will choose randomly, and if all goes well, continue working through others as part of future meetups. The format will be:</p>\n\n<ul>\n<li>Announce problem, get clarifications</li>\n<li>5 minutes of quiet solution-searching</li>\n<li>20-30 minutes of discussion</li>\n<li>Repeat</li>\n<li>After 3 cycles, 5 minutes more of discussion of each</li>\n</ul>\n\n<p>11/17, 7:00 PM, at the House with the Red Door, 2411 N Roxboro St, parking on Ellerbee. Hangouts, libations, and/or games to subsequently ensue in-situ or at Fullsteam!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t4'>Durham NC/Triangle Area Meetup: Productive Other-Optimization</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kP5FTXw6qrXMqjJPF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.4089698352197821e-06, "legacy": true, "legacyId": "24590", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup__Productive_Other_Optimization\">Discussion article for the meetup : <a href=\"/meetups/t4\">Durham NC/Triangle Area Meetup: Productive Other-Optimization</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 November 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2411 N Roxboro St, Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Bring your problems, worries, and knowledge gaps, great and small, and we will apply our communal knowledge to solving them - and try to recognize when we're committing the Typical Mind Fallacy.</p>\n\n<p>It will likely help to write down some problems beforehand!  Feel free to post them here as well, to get our thoughts going. If anyone is particularly eager to go first, they have priority - otherwise we will choose randomly, and if all goes well, continue working through others as part of future meetups. The format will be:</p>\n\n<ul>\n<li>Announce problem, get clarifications</li>\n<li>5 minutes of quiet solution-searching</li>\n<li>20-30 minutes of discussion</li>\n<li>Repeat</li>\n<li>After 3 cycles, 5 minutes more of discussion of each</li>\n</ul>\n\n<p>11/17, 7:00 PM, at the House with the Red Door, 2411 N Roxboro St, parking on Ellerbee. Hangouts, libations, and/or games to subsequently ensue in-situ or at Fullsteam!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup__Productive_Other_Optimization1\">Discussion article for the meetup : <a href=\"/meetups/t4\">Durham NC/Triangle Area Meetup: Productive Other-Optimization</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area Meetup: Productive Other-Optimization", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup__Productive_Other_Optimization", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area Meetup: Productive Other-Optimization", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area_Meetup__Productive_Other_Optimization1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T01:56:49.065Z", "modifiedAt": null, "url": null, "title": "Meetup : Rescheduled November Meetup", "slug": "meetup-rescheduled-november-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:29.775Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h4CPnXGbQAvyiMKzJ/meetup-rescheduled-november-meetup", "pageUrlRelative": "/posts/h4CPnXGbQAvyiMKzJ/meetup-rescheduled-november-meetup", "linkUrl": "https://www.lesswrong.com/posts/h4CPnXGbQAvyiMKzJ/meetup-rescheduled-november-meetup", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rescheduled%20November%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rescheduled%20November%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh4CPnXGbQAvyiMKzJ%2Fmeetup-rescheduled-november-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rescheduled%20November%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh4CPnXGbQAvyiMKzJ%2Fmeetup-rescheduled-november-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh4CPnXGbQAvyiMKzJ%2Fmeetup-rescheduled-november-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t5'>Rescheduled November Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Update, 11/24/13. <br />\nNew venue!  This is a private residence without cats.  The address, again is:</p>\n\n<p>Post Lindbergh Apartments\n491 Lindbergh Place NE Apt 618\nAtlanta, GA 30324</p>\n\n<p>Gate code: 607</p>\n\n<p>Once you enter the gate, make a left and continue till you reach a swimming pool. Apartment 618  is behind the pool. You can park wherever there's space.</p>\n\n<p>Please call Katie at 678-416-0248 with questions.</p>\n\n<hr />\n\n<p>We've had a last minute hitch in our plans for the meetup, courtesy of the flu! Unfortunately, one of the roommates at our host's house has come down with the flu, and in the interest of protecting our health from contagions, the meetup has been changed to SUNDAY, November 24th, at 7 (SEVEN) PM. Venue TBA soon.</p>\n\n<hr />\n\n<p>Come join us! We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times! Check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/Atlanta.Lesswrong/ where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t5'>Rescheduled November Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h4CPnXGbQAvyiMKzJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": -1, "extendedScore": null, "score": 1.4091074873665706e-06, "legacy": true, "legacyId": "24592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rescheduled_November_Meetup\">Discussion article for the meetup : <a href=\"/meetups/t5\">Rescheduled November Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Update, 11/24/13. <br>\nNew venue!  This is a private residence without cats.  The address, again is:</p>\n\n<p>Post Lindbergh Apartments\n491 Lindbergh Place NE Apt 618\nAtlanta, GA 30324</p>\n\n<p>Gate code: 607</p>\n\n<p>Once you enter the gate, make a left and continue till you reach a swimming pool. Apartment 618  is behind the pool. You can park wherever there's space.</p>\n\n<p>Please call Katie at 678-416-0248 with questions.</p>\n\n<hr>\n\n<p>We've had a last minute hitch in our plans for the meetup, courtesy of the flu! Unfortunately, one of the roommates at our host's house has come down with the flu, and in the interest of protecting our health from contagions, the meetup has been changed to SUNDAY, November 24th, at 7 (SEVEN) PM. Venue TBA soon.</p>\n\n<hr>\n\n<p>Come join us! We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times! Check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/Atlanta.Lesswrong/ where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rescheduled_November_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/t5\">Rescheduled November Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rescheduled November Meetup", "anchor": "Discussion_article_for_the_meetup___Rescheduled_November_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Rescheduled November Meetup", "anchor": "Discussion_article_for_the_meetup___Rescheduled_November_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T02:13:00.679Z", "modifiedAt": null, "url": null, "title": "New vs. Business-as-Usual Future", "slug": "new-vs-business-as-usual-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.867Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pwnsGPMwwen36tsRX/new-vs-business-as-usual-future", "pageUrlRelative": "/posts/pwnsGPMwwen36tsRX/new-vs-business-as-usual-future", "linkUrl": "https://www.lesswrong.com/posts/pwnsGPMwwen36tsRX/new-vs-business-as-usual-future", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20vs.%20Business-as-Usual%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20vs.%20Business-as-Usual%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwnsGPMwwen36tsRX%2Fnew-vs-business-as-usual-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20vs.%20Business-as-Usual%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwnsGPMwwen36tsRX%2Fnew-vs-business-as-usual-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpwnsGPMwwen36tsRX%2Fnew-vs-business-as-usual-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 433, "htmlBody": "<p>What, in a broad sense, does the future look like? We don't know, and while many have historically made predictions, the track record for such predictions is less than impressive. I have noted that there appear to be two main types of view about the future-- the \"new future\" and the \"business-as-usual future.\" In order to simplify this discussion, let's restrict it only to the coming century-- the period between 2013 and 2113.</p>\n<p>The \"new future\" is, generally speaking, the idea that the coming century is going to be very different from the present; the \"business-as-usual future\" is, generally speaking, the idea that the coming century is going to be very similar to the present.</p>\n<p>Here are some characteristics of the new future:</p>\n<ul>\n<li>Some large-scale event occurs that alters human experience forever-- an intelligence explosion leading to a technological singularity, existential risks leading to human suppression or extinction, global climate change on a massive scale, etc.</li>\n<li>Society changes a lot, and in fundamental ways that are difficult to understand. Daily life is vastly altered.</li>\n<li>If future history even exists after the dramatic change, it views the coming century as being a critical moment where everything became vastly different, on par with or exceeding the significance of the development of agriculture.</li>\n</ul>\n<p>Here are some characteristics of the business-as-usual future:</p>\n<ul>\n<li>The intelligence explosion doesn't happen. AI continues to advance in much the same way that it has for the last several decades. More human-capable tasks become automated, but in slow and predictable ways. Intelligence amplification doesn't happen or doesn't yield generally useful results.</li>\n<li>The world doesn't end. Global warming ends up being just another doomsday scare. Perhaps a lot of people die in the Third World, but the rest of the world adapts and keeps going much like it has for the last ever. Yellowstone doesn't explode. No asteroids hit the earth. There isn't a nuclear war.</li>\n<li>Society doesn't change very much except in superficial ways. Daily life is more or less the same. </li>\n<li>Wars might happen. Nations might collapse. But wars have been happening and nations have been collapsing for thousands of years. By and large, the coming century is viewed by future history as not particularly unlike those that came before.</li>\n</ul>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">Reference class forecasting</a> seems to indicate that the business-as-usual future is quite likely. But <a href=\"/lw/1p5/outside_view_as_conversationhalter/\">as we know</a>, this is far from a textbook case of reference class forecasting, and applying such techniques may not be helpful. What, then, is a good method of establishing what you think the future will look like?</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pwnsGPMwwen36tsRX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 5, "extendedScore": null, "score": 1.4091230925861975e-06, "legacy": true, "legacyId": "24564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FsfnDfADftGDYeG4c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T03:10:53.855Z", "modifiedAt": null, "url": null, "title": "2013 Census/Survey: call for changes and additions", "slug": "2013-census-survey-call-for-changes-and-additions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:35.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xij43oLTBRnEQv2bT/2013-census-survey-call-for-changes-and-additions", "pageUrlRelative": "/posts/xij43oLTBRnEQv2bT/2013-census-survey-call-for-changes-and-additions", "linkUrl": "https://www.lesswrong.com/posts/xij43oLTBRnEQv2bT/2013-census-survey-call-for-changes-and-additions", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202013%20Census%2FSurvey%3A%20call%20for%20changes%20and%20additions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2013%20Census%2FSurvey%3A%20call%20for%20changes%20and%20additions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxij43oLTBRnEQv2bT%2F2013-census-survey-call-for-changes-and-additions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2013%20Census%2FSurvey%3A%20call%20for%20changes%20and%20additions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxij43oLTBRnEQv2bT%2F2013-census-survey-call-for-changes-and-additions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxij43oLTBRnEQv2bT%2F2013-census-survey-call-for-changes-and-additions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<p>I have finally gotten the survey to a point where I'm pretty happy with it. I have no big changes I want to make this year. But as is the tradition, please take a week to discuss what <em>minor</em> changes you want to the survey (within the limits of what Google Docs and finite time can do) and I will try to comply. In particular, we can continue the tradition that any question you request can be added to the Extra Credit section unless it's illegal or horribly offensive.</p>\n<p>You can find last year's survey results <a href=\"/lw/fp5/2012_survey_results/\">here</a> and you can find the very preliminary version of this year's survey (so far exactly the same as last year's) <a href=\"https://docs.google.com/spreadsheet/viewform?usp=drive_web&amp;formkey=dGZ6a1NfZ0V1SV9xdE1ma0pUMTc1S1E6MA#gid=0\">here</a>.</p>\n<p><strong>EDIT:</strong> I don't particularly like the IQ test or the Big Five test used last year. If you have any better replacements for either, tell me and I'll put them in.</p>\n<p><strong>EDIT2: </strong>CFAR, you added seven questions last year. Let me know what you want to do with those this year. Keep them? Remove them? Replace them?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xij43oLTBRnEQv2bT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 47, "extendedScore": null, "score": 0.000132, "legacy": true, "legacyId": "24597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 155, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x9FNKTEt68Rz6wQ6P"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T03:32:42.920Z", "modifiedAt": null, "url": null, "title": "No Universally Compelling Arguments in Math or Science", "slug": "no-universally-compelling-arguments-in-math-or-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.873Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FpupDqv4vbHSiawER/no-universally-compelling-arguments-in-math-or-science", "pageUrlRelative": "/posts/FpupDqv4vbHSiawER/no-universally-compelling-arguments-in-math-or-science", "linkUrl": "https://www.lesswrong.com/posts/FpupDqv4vbHSiawER/no-universally-compelling-arguments-in-math-or-science", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%20Universally%20Compelling%20Arguments%20in%20Math%20or%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%20Universally%20Compelling%20Arguments%20in%20Math%20or%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFpupDqv4vbHSiawER%2Fno-universally-compelling-arguments-in-math-or-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%20Universally%20Compelling%20Arguments%20in%20Math%20or%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFpupDqv4vbHSiawER%2Fno-universally-compelling-arguments-in-math-or-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFpupDqv4vbHSiawER%2Fno-universally-compelling-arguments-in-math-or-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1423, "htmlBody": "<p>Last week, I started a thread on <a href=\"/r/discussion/lw/iwy/why_didnt_people_apparently_understand_the/\">the widespread sentiment that people don't understand the metaethics sequence</a>. One of the things that surprised me most in the thread was <a href=\"/lw/iwy/why_didnt_people_apparently_understand_the/9zao\">this</a>&nbsp;exchange:</p>\n<p>Commenter: \"I happen to (mostly) agree that there aren't universally compelling arguments, but I still wish there were. The metaethics sequence failed to talk me out of valuing this.\"</p>\n<p>Me: \"But you realize that Eliezer is arguing that there aren't universally compelling arguments in any domain, including mathematics or science? So if that doesn't threaten the objectivity of mathematics or science, why should that threaten the objectivity of morality?\"</p>\n<p>Commenter: \"Waah? Of course there are universally compelling arguments in math and science.\"</p>\n<p>Now, I realize this is just one commenter. But <a href=\"/lw/iwy/why_didnt_people_apparently_understand_the/9z6s\">the most-upvoted comment in the thread</a> also perceived \"no universally compelling arguments\" as a major source of confusion, suggesting that it was perceived as conflicting with morality not being arbitrary. And <a href=\"/lw/iwy/why_didnt_people_apparently_understand_the/a054\">today</a>, someone mentioned having \"no universally compelling arguments\" cited at them as a decisive refutation of moral realism.</p>\n<p>After the exchange quoted above, I went back and read the original <a href=\"/lw/rn/no_universally_compelling_arguments/\">No Universally Compelling Arguments</a> post, and realized that while it had been obvious to <em>me </em>when I read it that Eliezer meant it to apply to everything, math and science included, it was rather short on concrete examples, perhaps in violation of <a href=\"/lw/nh/extensions_and_intensions/\">Eliezer's own advice</a>. The concrete examples can be found in the sequences, though... just not in that particular post.<a id=\"more\"></a></p>\n<p>First, I recommend reading&nbsp;<a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">The Design Space of Minds-In-General</a>&nbsp;if you haven't already. TLDR; the space of minds in general ginormous and includes some downright weird minds. The space of human minds is a teeny tiny dot in the larger space (in case this isn't clear, the diagram in that post isn't remotely drawn to scale). Now with that out of the way...</p>\n<p><strong>There are minds in the space of minds-in-general that do not recognize <em>modus ponens.</em></strong></p>\n<p><em>Modus ponens&nbsp;</em>is the rule of inference that says that if you have a statement of the form \"If A then B\", and also have \"A\", then you can derive \"B\". It's a fundamental part of logic. But there are possible mind that reject it. A brilliant illustration of this point can be found in Lewis Carroll's dialog&nbsp;<a href=\"http://www.ditext.com/carroll/tortoise.html\">\"What the Tortoise Said to Achilles\"</a>&nbsp;(for those not in the know, Carroll was a mathematician;&nbsp;<em>Alice in Wonderland </em>is secretly full of math jokes).</p>\n<p>Eliezer covers the dialog in his post <a href=\"/lw/rs/created_already_in_motion/\">Created Already In Motion</a>, but here's the short version: In Carroll's dialog, the tortoise asks Achilles to imagine someone rejecting a particular instance of <em>modus ponens </em>(drawn from Euclid's <em>Elements, </em>though that isn't important). The Tortoise suggests that such a person might be persuaded by adding an additional premise, and Achilles goes along with it&mdash;foolishly, because this quickly leads to an infinite regress when the Tortoise suggests that someone might reject the new argument in spite of accepting the premises (which leads to another round of trying to patch the argument, and then..)</p>\n<p>\"What the Tortoise Said to Achilles\" is one of the reasons I tend to think of the so-called \"problem of induction\" as a pseudo-problem. The \"problem of induction\" is often defined as the problem of how to justify induction, but it seems to make just as much senses to ask how to justify deduction. But speaking of induction...</p>\n<p><strong>There are minds in the space of minds-in-general that reason counter-inductively.</strong></p>\n<p>To quote Eliezer:</p>\n<blockquote>\n<p>There are <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">possible minds in mind design space</a> who have anti-Occamian and anti-Laplacian priors; they believe that simpler theories are less likely to be correct, and that the more often something happens, the less likely it is to happen again.</p>\n<p>And when you ask these strange beings why they keep using priors that never seem to work in real life... they reply, \"Because it's never worked for us before!\"</p>\n</blockquote>\n<p>If this bothers you, well, I refer you back to Lewis' Carroll's dialog. There are also minds in the mind design space that ignore the standard laws of logic, and are furthermore totally unbothered by (what we would regard as) the absurdities produced by doing so. Oh, but if you thought that was bad, consider this...</p>\n<p><strong>There are minds in the space of minds-in-general that use a maximum entropy prior, and never learn anything.</strong></p>\n<div>\n<p>Here's Eliezer again <a href=\"/lw/hg/inductive_bias/\">discussing</a> a problem where you have to predict whether a ball drawn out of an urn will be red or white, based on the color of the balls that have been previously drawn out of the urn:</p>\n<blockquote>\n<p>Suppose that your prior information about the urn is that a monkey tosses balls into the urn, selecting red balls with 1/4 probability and white balls with 3/4 probability, each ball selected independently. &nbsp;The urn contains 10 balls, and we sample without replacement. &nbsp;(E. T. Jaynes called this the \"binomial monkey prior\".) &nbsp;Now suppose that on the first three rounds, you see three red balls. &nbsp;What is the probability of seeing a red ball on the fourth round?</p>\n<p>First, we calculate the prior probability that the monkey tossed 0 red balls and 10 white balls into the urn; then the prior probability that the monkey tossed 1 red ball and 9 white balls into the urn; and so on. &nbsp;Then we take our evidence (three red balls, sampled without replacement) and calculate the likelihood of seeing that evidence, conditioned on each of the possible urn contents. &nbsp;Then we update and normalize the posterior probability of the possible remaining urn contents. &nbsp;Then we average over the probability of drawing a red ball from each possible urn, weighted by that urn's posterior probability. And the answer is... <em>(scribbles frantically for quite some time)</em>... 1/4!</p>\n<p>Of course it's 1/4. &nbsp;We specified that each ball was independently tossed into the urn, with a known 1/4 probability of being red. &nbsp;Imagine that the monkey is tossing the balls to you, one by one; if it tosses you a red ball on one round, that doesn't change the probability that it tosses you a red ball on the next round. &nbsp;When we withdraw one ball from the urn, it doesn't tell us anything about the other balls in the urn.</p>\n<p>If you start out with a maximum-entropy prior, then you never learn anything, ever, no matter how much evidence you observe. You do not even learn anything wrong - you always remain as ignorant as you began.</p>\n</blockquote>\n<p>You may think, while minds such as I've been describing are possible in theory, they're unlikely to evolve anywhere in the universe, and probably they wouldn't survive long if programmed as an AI. And you'd probably be right about that. On the other hand, it's not hard to imagine minds that are generally able to get along well in the world, but irredeemably crazy on particular questions. Sometimes, it's tempting to suspect some humans of being this way, and even if that isn't <em>literally </em>true of any humans, it's not hard to imagine as just a more extreme form of existing human tendencies. See e.g. Robin Hanson on <a href=\"http://www.overcomingbias.com/tag/nearfar\">near vs. far mode</a>, and imagine a mind that will literally never leave far mode on certain questions, regardless of the circumstances.</p>\n<p>It used to disturb me to think that there might be, say, young earth creationists in the world who couldn't be persuaded to give up their young earth creationism by any evidence or arguments, no matter how long they lived. Yet I've realized that, while there may or may not be actual human young earth creationists like that (it's an empirical question), there are certainly possible minds in the space of mind designs like that. And when I think about that fact, I'm forced to shrug my shoulders and say, \"oh well\" and leave it at that.</p>\n<p>That means I can understand why people would be bothered by a lack of universally compelling arguments for their moral views... but you shouldn't be any <em>more </em>bothered by that than by the lack of universally compelling arguments against young earth creationism. And if you don't think the lack of universally compelling arguments is a reason to think there's no objective truth about the age of the earth, you shouldn't think it's a reason to think there's no objective truth about morality.</p>\n<p>(Note: this may end up being just the first in a series of posts on the metaethics sequence. People are welcome to discuss what I should cover in subsequent posts in the comments.)</p>\n<p><strong>Added: </strong>Based on initial comments, I wonder if some people who describe themselves as being bothered the lack of universally compelling arguments would more accurately describe themselves as being bothered by the <a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\">orthogonality thesis</a>.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"QbdJ7SxLE2NT8gzr3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FpupDqv4vbHSiawER", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 49, "extendedScore": null, "score": 0.000205, "legacy": true, "legacyId": "24598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Last week, I started a thread on <a href=\"/r/discussion/lw/iwy/why_didnt_people_apparently_understand_the/\">the widespread sentiment that people don't understand the metaethics sequence</a>. One of the things that surprised me most in the thread was <a href=\"/lw/iwy/why_didnt_people_apparently_understand_the/9zao\">this</a>&nbsp;exchange:</p>\n<p>Commenter: \"I happen to (mostly) agree that there aren't universally compelling arguments, but I still wish there were. The metaethics sequence failed to talk me out of valuing this.\"</p>\n<p>Me: \"But you realize that Eliezer is arguing that there aren't universally compelling arguments in any domain, including mathematics or science? So if that doesn't threaten the objectivity of mathematics or science, why should that threaten the objectivity of morality?\"</p>\n<p>Commenter: \"Waah? Of course there are universally compelling arguments in math and science.\"</p>\n<p>Now, I realize this is just one commenter. But <a href=\"/lw/iwy/why_didnt_people_apparently_understand_the/9z6s\">the most-upvoted comment in the thread</a> also perceived \"no universally compelling arguments\" as a major source of confusion, suggesting that it was perceived as conflicting with morality not being arbitrary. And <a href=\"/lw/iwy/why_didnt_people_apparently_understand_the/a054\">today</a>, someone mentioned having \"no universally compelling arguments\" cited at them as a decisive refutation of moral realism.</p>\n<p>After the exchange quoted above, I went back and read the original <a href=\"/lw/rn/no_universally_compelling_arguments/\">No Universally Compelling Arguments</a> post, and realized that while it had been obvious to <em>me </em>when I read it that Eliezer meant it to apply to everything, math and science included, it was rather short on concrete examples, perhaps in violation of <a href=\"/lw/nh/extensions_and_intensions/\">Eliezer's own advice</a>. The concrete examples can be found in the sequences, though... just not in that particular post.<a id=\"more\"></a></p>\n<p>First, I recommend reading&nbsp;<a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">The Design Space of Minds-In-General</a>&nbsp;if you haven't already. TLDR; the space of minds in general ginormous and includes some downright weird minds. The space of human minds is a teeny tiny dot in the larger space (in case this isn't clear, the diagram in that post isn't remotely drawn to scale). Now with that out of the way...</p>\n<p><strong id=\"There_are_minds_in_the_space_of_minds_in_general_that_do_not_recognize_modus_ponens_\">There are minds in the space of minds-in-general that do not recognize <em>modus ponens.</em></strong></p>\n<p><em>Modus ponens&nbsp;</em>is the rule of inference that says that if you have a statement of the form \"If A then B\", and also have \"A\", then you can derive \"B\". It's a fundamental part of logic. But there are possible mind that reject it. A brilliant illustration of this point can be found in Lewis Carroll's dialog&nbsp;<a href=\"http://www.ditext.com/carroll/tortoise.html\">\"What the Tortoise Said to Achilles\"</a>&nbsp;(for those not in the know, Carroll was a mathematician;&nbsp;<em>Alice in Wonderland </em>is secretly full of math jokes).</p>\n<p>Eliezer covers the dialog in his post <a href=\"/lw/rs/created_already_in_motion/\">Created Already In Motion</a>, but here's the short version: In Carroll's dialog, the tortoise asks Achilles to imagine someone rejecting a particular instance of <em>modus ponens </em>(drawn from Euclid's <em>Elements, </em>though that isn't important). The Tortoise suggests that such a person might be persuaded by adding an additional premise, and Achilles goes along with it\u2014foolishly, because this quickly leads to an infinite regress when the Tortoise suggests that someone might reject the new argument in spite of accepting the premises (which leads to another round of trying to patch the argument, and then..)</p>\n<p>\"What the Tortoise Said to Achilles\" is one of the reasons I tend to think of the so-called \"problem of induction\" as a pseudo-problem. The \"problem of induction\" is often defined as the problem of how to justify induction, but it seems to make just as much senses to ask how to justify deduction. But speaking of induction...</p>\n<p><strong id=\"There_are_minds_in_the_space_of_minds_in_general_that_reason_counter_inductively_\">There are minds in the space of minds-in-general that reason counter-inductively.</strong></p>\n<p>To quote Eliezer:</p>\n<blockquote>\n<p>There are <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">possible minds in mind design space</a> who have anti-Occamian and anti-Laplacian priors; they believe that simpler theories are less likely to be correct, and that the more often something happens, the less likely it is to happen again.</p>\n<p>And when you ask these strange beings why they keep using priors that never seem to work in real life... they reply, \"Because it's never worked for us before!\"</p>\n</blockquote>\n<p>If this bothers you, well, I refer you back to Lewis' Carroll's dialog. There are also minds in the mind design space that ignore the standard laws of logic, and are furthermore totally unbothered by (what we would regard as) the absurdities produced by doing so. Oh, but if you thought that was bad, consider this...</p>\n<p><strong id=\"There_are_minds_in_the_space_of_minds_in_general_that_use_a_maximum_entropy_prior__and_never_learn_anything_\">There are minds in the space of minds-in-general that use a maximum entropy prior, and never learn anything.</strong></p>\n<div>\n<p>Here's Eliezer again <a href=\"/lw/hg/inductive_bias/\">discussing</a> a problem where you have to predict whether a ball drawn out of an urn will be red or white, based on the color of the balls that have been previously drawn out of the urn:</p>\n<blockquote>\n<p>Suppose that your prior information about the urn is that a monkey tosses balls into the urn, selecting red balls with 1/4 probability and white balls with 3/4 probability, each ball selected independently. &nbsp;The urn contains 10 balls, and we sample without replacement. &nbsp;(E. T. Jaynes called this the \"binomial monkey prior\".) &nbsp;Now suppose that on the first three rounds, you see three red balls. &nbsp;What is the probability of seeing a red ball on the fourth round?</p>\n<p>First, we calculate the prior probability that the monkey tossed 0 red balls and 10 white balls into the urn; then the prior probability that the monkey tossed 1 red ball and 9 white balls into the urn; and so on. &nbsp;Then we take our evidence (three red balls, sampled without replacement) and calculate the likelihood of seeing that evidence, conditioned on each of the possible urn contents. &nbsp;Then we update and normalize the posterior probability of the possible remaining urn contents. &nbsp;Then we average over the probability of drawing a red ball from each possible urn, weighted by that urn's posterior probability. And the answer is... <em>(scribbles frantically for quite some time)</em>... 1/4!</p>\n<p>Of course it's 1/4. &nbsp;We specified that each ball was independently tossed into the urn, with a known 1/4 probability of being red. &nbsp;Imagine that the monkey is tossing the balls to you, one by one; if it tosses you a red ball on one round, that doesn't change the probability that it tosses you a red ball on the next round. &nbsp;When we withdraw one ball from the urn, it doesn't tell us anything about the other balls in the urn.</p>\n<p>If you start out with a maximum-entropy prior, then you never learn anything, ever, no matter how much evidence you observe. You do not even learn anything wrong - you always remain as ignorant as you began.</p>\n</blockquote>\n<p>You may think, while minds such as I've been describing are possible in theory, they're unlikely to evolve anywhere in the universe, and probably they wouldn't survive long if programmed as an AI. And you'd probably be right about that. On the other hand, it's not hard to imagine minds that are generally able to get along well in the world, but irredeemably crazy on particular questions. Sometimes, it's tempting to suspect some humans of being this way, and even if that isn't <em>literally </em>true of any humans, it's not hard to imagine as just a more extreme form of existing human tendencies. See e.g. Robin Hanson on <a href=\"http://www.overcomingbias.com/tag/nearfar\">near vs. far mode</a>, and imagine a mind that will literally never leave far mode on certain questions, regardless of the circumstances.</p>\n<p>It used to disturb me to think that there might be, say, young earth creationists in the world who couldn't be persuaded to give up their young earth creationism by any evidence or arguments, no matter how long they lived. Yet I've realized that, while there may or may not be actual human young earth creationists like that (it's an empirical question), there are certainly possible minds in the space of mind designs like that. And when I think about that fact, I'm forced to shrug my shoulders and say, \"oh well\" and leave it at that.</p>\n<p>That means I can understand why people would be bothered by a lack of universally compelling arguments for their moral views... but you shouldn't be any <em>more </em>bothered by that than by the lack of universally compelling arguments against young earth creationism. And if you don't think the lack of universally compelling arguments is a reason to think there's no objective truth about the age of the earth, you shouldn't think it's a reason to think there's no objective truth about morality.</p>\n<p>(Note: this may end up being just the first in a series of posts on the metaethics sequence. People are welcome to discuss what I should cover in subsequent posts in the comments.)</p>\n<p><strong>Added: </strong>Based on initial comments, I wonder if some people who describe themselves as being bothered the lack of universally compelling arguments would more accurately describe themselves as being bothered by the <a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\">orthogonality thesis</a>.</p>\n</div>", "sections": [{"title": "There are minds in the space of minds-in-general that do not recognize modus ponens.", "anchor": "There_are_minds_in_the_space_of_minds_in_general_that_do_not_recognize_modus_ponens_", "level": 1}, {"title": "There are minds in the space of minds-in-general that reason counter-inductively.", "anchor": "There_are_minds_in_the_space_of_minds_in_general_that_reason_counter_inductively_", "level": 1}, {"title": "There are minds in the space of minds-in-general that use a maximum entropy prior, and never learn anything.", "anchor": "There_are_minds_in_the_space_of_minds_in_general_that_use_a_maximum_entropy_prior__and_never_learn_anything_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "230 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 230, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KZfoF7MdqJPcrhXaX", "PtoQdG7E8MxYJrigu", "HsznWM9A7NiuGsp28", "tnWRXkcDi5Tw9rzXw", "CuSTqHgeK4CMpWYTe", "H59YqogX94z5jb8xx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T10:33:46.570Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Now 2 Sigma More Awesome", "slug": "meetup-moscow-now-2-sigma-more-awesome", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:29.987Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "berekuk", "createdAt": "2009-09-25T22:53:16.659Z", "isAdmin": false, "displayName": "berekuk"}, "userId": "7B8mbMEHF5qW9tfxk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/azCykmTWoxygPAzDx/meetup-moscow-now-2-sigma-more-awesome", "pageUrlRelative": "/posts/azCykmTWoxygPAzDx/meetup-moscow-now-2-sigma-more-awesome", "linkUrl": "https://www.lesswrong.com/posts/azCykmTWoxygPAzDx/meetup-moscow-now-2-sigma-more-awesome", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Now%202%20Sigma%20More%20Awesome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Now%202%20Sigma%20More%20Awesome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FazCykmTWoxygPAzDx%2Fmeetup-moscow-now-2-sigma-more-awesome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Now%202%20Sigma%20More%20Awesome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FazCykmTWoxygPAzDx%2Fmeetup-moscow-now-2-sigma-more-awesome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FazCykmTWoxygPAzDx%2Fmeetup-moscow-now-2-sigma-more-awesome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t6'>Moscow, Now 2 Sigma More Awesome</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have lots of various content prepared this time, 2 standard deviations more than usual.</p>\n\n<p>We're planning to have (not necessarily in this order):</p>\n\n<ul>\n<li>a report from a participant of our ongoing willpower group;</li>\n<li>a report about \"Feeling Good\" by David Burns;</li>\n<li>an exercise on discovering your terminal values;</li>\n<li>a section on how to make conversations more productive by learning to recognize several failure modes of arguing.</li>\n</ul>\n\n<p>There might be a calibration session if we have time for it.</p>\n\n<p>There might also be a cake, but it can turn out to be a lie.</p>\n\n<p>As usual, we're going to have our ongoing <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131110_meet_up&amp;utm_content=20131110_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a>, <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131110_meet_up&amp;utm_content=20131110_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>, positive reinforcements (now with cashew as an option, not just chocolate!) and pizza.</p>\n\n<p>We intend to broadcast this meetup on ustream or justin.tv, but considering the amount of chaos and the fact we never did it before, it's unlikely to turn out well. So join us in person if you can.</p>\n\n<p><em>If you are going for the first time:</em></p>\n\n<p>We gather in the Yandex office. Look the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance</a>. You need to pass the first entrance and the bicycle parking on your way. Here is an additional guide on how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian) to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t6'>Moscow, Now 2 Sigma More Awesome</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "azCykmTWoxygPAzDx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4096058152612638e-06, "legacy": true, "legacyId": "24604", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Now_2_Sigma_More_Awesome\">Discussion article for the meetup : <a href=\"/meetups/t6\">Moscow, Now 2 Sigma More Awesome</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have lots of various content prepared this time, 2 standard deviations more than usual.</p>\n\n<p>We're planning to have (not necessarily in this order):</p>\n\n<ul>\n<li>a report from a participant of our ongoing willpower group;</li>\n<li>a report about \"Feeling Good\" by David Burns;</li>\n<li>an exercise on discovering your terminal values;</li>\n<li>a section on how to make conversations more productive by learning to recognize several failure modes of arguing.</li>\n</ul>\n\n<p>There might be a calibration session if we have time for it.</p>\n\n<p>There might also be a cake, but it can turn out to be a lie.</p>\n\n<p>As usual, we're going to have our ongoing <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131110_meet_up&amp;utm_content=20131110_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a>, <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131110_meet_up&amp;utm_content=20131110_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>, positive reinforcements (now with cashew as an option, not just chocolate!) and pizza.</p>\n\n<p>We intend to broadcast this meetup on ustream or justin.tv, but considering the amount of chaos and the fact we never did it before, it's unlikely to turn out well. So join us in person if you can.</p>\n\n<p><em>If you are going for the first time:</em></p>\n\n<p>We gather in the Yandex office. Look the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance</a>. You need to pass the first entrance and the bicycle parking on your way. Here is an additional guide on how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian) to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Now_2_Sigma_More_Awesome1\">Discussion article for the meetup : <a href=\"/meetups/t6\">Moscow, Now 2 Sigma More Awesome</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Now 2 Sigma More Awesome", "anchor": "Discussion_article_for_the_meetup___Moscow__Now_2_Sigma_More_Awesome", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Now 2 Sigma More Awesome", "anchor": "Discussion_article_for_the_meetup___Moscow__Now_2_Sigma_More_Awesome1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T18:27:04.851Z", "modifiedAt": null, "url": null, "title": "[Link] \"A Long-run Perspective on Strategic Cause Selection and Philanthropy\" by Nick Beckstead and Carl Shulman", "slug": "link-a-long-run-perspective-on-strategic-cause-selection-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rsnz5gjLB9acLRrEN/link-a-long-run-perspective-on-strategic-cause-selection-and", "pageUrlRelative": "/posts/Rsnz5gjLB9acLRrEN/link-a-long-run-perspective-on-strategic-cause-selection-and", "linkUrl": "https://www.lesswrong.com/posts/Rsnz5gjLB9acLRrEN/link-a-long-run-perspective-on-strategic-cause-selection-and", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20%22A%20Long-run%20Perspective%20on%20Strategic%20Cause%20Selection%20and%20Philanthropy%22%20by%20Nick%20Beckstead%20and%20Carl%20Shulman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20%22A%20Long-run%20Perspective%20on%20Strategic%20Cause%20Selection%20and%20Philanthropy%22%20by%20Nick%20Beckstead%20and%20Carl%20Shulman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsnz5gjLB9acLRrEN%2Flink-a-long-run-perspective-on-strategic-cause-selection-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20%22A%20Long-run%20Perspective%20on%20Strategic%20Cause%20Selection%20and%20Philanthropy%22%20by%20Nick%20Beckstead%20and%20Carl%20Shulman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsnz5gjLB9acLRrEN%2Flink-a-long-run-perspective-on-strategic-cause-selection-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRsnz5gjLB9acLRrEN%2Flink-a-long-run-perspective-on-strategic-cause-selection-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<blockquote>\n<p>A philanthropist who will remain anonymous recently asked us about what we would do if we didn&rsquo;t face financial constraints. We gave a detailed answer that we thought we might as well share with others, who may also find our perspective interesting. We gave the answer largely in hope of creating some interest in our way of thinking about philanthropy and some of the causes that we find interesting for further investigation, and because we thought the answer would be fruitful for conversation.</p>\n</blockquote>\n<p><a href=\"http://www.effective-altruism.com/a-long-run-perspective-on-strategic-cause-selection-and-philanthropy/\">Link</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rsnz5gjLB9acLRrEN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.4100623363927448e-06, "legacy": true, "legacyId": "24605", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T20:47:52.979Z", "modifiedAt": null, "url": null, "title": "Is the orthogonality thesis at odds with moral realism?", "slug": "is-the-orthogonality-thesis-at-odds-with-moral-realism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:07.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wYZKtAhqawoj9PKtb/is-the-orthogonality-thesis-at-odds-with-moral-realism", "pageUrlRelative": "/posts/wYZKtAhqawoj9PKtb/is-the-orthogonality-thesis-at-odds-with-moral-realism", "linkUrl": "https://www.lesswrong.com/posts/wYZKtAhqawoj9PKtb/is-the-orthogonality-thesis-at-odds-with-moral-realism", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20the%20orthogonality%20thesis%20at%20odds%20with%20moral%20realism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20the%20orthogonality%20thesis%20at%20odds%20with%20moral%20realism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYZKtAhqawoj9PKtb%2Fis-the-orthogonality-thesis-at-odds-with-moral-realism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20the%20orthogonality%20thesis%20at%20odds%20with%20moral%20realism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYZKtAhqawoj9PKtb%2Fis-the-orthogonality-thesis-at-odds-with-moral-realism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwYZKtAhqawoj9PKtb%2Fis-the-orthogonality-thesis-at-odds-with-moral-realism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 441, "htmlBody": "<p>Continuing <a href=\"/r/discussion/lw/iwy/why_didnt_people_apparently_understand_the/\">my</a> <a href=\"/lw/iza/no_universally_compelling_arguments_in_math_or/\">quest</a> to untangle people's confusions about Eliezer's metaethics... I've started to wonder if maybe some people have the intuition that the <a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\">orthogonality thesis</a> is at odds with moral realism.</p>\n<p>I personally have a very hard time seeing why anyone would think that, perhaps in part because of my experience in philosophy of religion. Theistic apologists would love to be able to say, \"moral realism, therefore a sufficiently intelligent being would also be good.\" It would help patch some obvious holes in their arguments and help them respond to things like Stephen Law's <a href=\"http://stephenlaw.blogspot.com/2010/02/evil-god-challenge.html\">Evil God Challenge</a>. But they mostly don't even <em>try </em>to argue that, for whatever reason.</p>\n<p>You did see philosophers claiming things like that back in the bad old days before Kant, which raises the question of what's changed. I suspect the reason is fairly mundane, though: before Kant (roughly), it was not only dangerous to be an atheist, it was dangerous to question that the existence of God could be proven through reason (because it would get you suspected of being an atheist). It was even dangerous to advocated philosophical views that might possibly undermine the standard arguments for the existence of God. That guaranteed that philosophers could used whatever half-baked premises they wanted in constructing arguments for the existence of God, and have little fear of being contradicted.</p>\n<p>Besides, even if you think an all-knowing would also necessarily be perfectly good, it still seems perfectly possible to have an otherwise all-knowing being with a horrible blind spot regarding morality.</p>\n<p>On the other hand, in the comments of <a href=\"/lw/cej/general_purpose_intelligence_arguing_the/\">a post on the orthogonality thesis</a>, Stuart Armstrong mentions that:</p>\n<blockquote>\n<p>I've read the various papers [by people who reject the orthogonality thesis], and they all orbit around an implicit and often unstated moral realism. I've also debated philosophers on this, and the same issue rears its head - I can counter their arguments, but their opinions don't shift. There is an implicit moral realism that does not make any sense to me, and the more I analyse it, the less sense it makes, and the less convincing it becomes. Every time a philosopher has encouraged me to read a particular work, it's made me find their moral realism less likely, because the arguments are always weak.</p>\n</blockquote>\n<p>This is not super-enlightening, partly because Stuart is talking about people whose views he admits he doesn't understand... but on the other hand, maybe Stuart agrees that there is some kind of conflict there, since he seems to imply that he himself rejects moral realism.</p>\n<p>I realize I'm struggling a bit to guess what people could be thinking here, but I suspect some people are thinking it, so... anyone?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BXL4riEJvJJHoydjG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wYZKtAhqawoj9PKtb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "24606", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 118, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KZfoF7MdqJPcrhXaX", "FpupDqv4vbHSiawER", "nvKZchuTW8zY6wvAj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-05T21:16:12.717Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt", "slug": "meetup-frankfurt-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uiM6wSuGSBypYF7i9/meetup-frankfurt-0", "pageUrlRelative": "/posts/uiM6wSuGSBypYF7i9/meetup-frankfurt-0", "linkUrl": "https://www.lesswrong.com/posts/uiM6wSuGSBypYF7i9/meetup-frankfurt-0", "postedAtFormatted": "Tuesday, November 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuiM6wSuGSBypYF7i9%2Fmeetup-frankfurt-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuiM6wSuGSBypYF7i9%2Fmeetup-frankfurt-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuiM6wSuGSBypYF7i9%2Fmeetup-frankfurt-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t7'>Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our next meetup will take place on the 24th of November, 2pm. The usual location, contact me if you don't know where it is and want to attend. If you face any difficulties that might make it hard for you attend (social anxiety, etc.) please tell me in advance, I'll try to accomodate your needs as best as I can.\nLast topic of our meetup was Effective Altruism, we want to focus on many different things in the future. (You are welcome to bring your ideas!)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t7'>Frankfurt</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uiM6wSuGSBypYF7i9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.410225533444213e-06, "legacy": true, "legacyId": "24607", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt\">Discussion article for the meetup : <a href=\"/meetups/t7\">Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our next meetup will take place on the 24th of November, 2pm. The usual location, contact me if you don't know where it is and want to attend. If you face any difficulties that might make it hard for you attend (social anxiety, etc.) please tell me in advance, I'll try to accomodate your needs as best as I can.\nLast topic of our meetup was Effective Altruism, we want to focus on many different things in the future. (You are welcome to bring your ideas!)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt1\">Discussion article for the meetup : <a href=\"/meetups/t7\">Frankfurt</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-06T21:02:10.395Z", "modifiedAt": null, "url": null, "title": "Meetup : Comfort Zone Expansion at Citadel, Boston", "slug": "meetup-comfort-zone-expansion-at-citadel-boston", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ARmEm3aHv5DKSko2f/meetup-comfort-zone-expansion-at-citadel-boston", "pageUrlRelative": "/posts/ARmEm3aHv5DKSko2f/meetup-comfort-zone-expansion-at-citadel-boston", "linkUrl": "https://www.lesswrong.com/posts/ARmEm3aHv5DKSko2f/meetup-comfort-zone-expansion-at-citadel-boston", "postedAtFormatted": "Wednesday, November 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Comfort%20Zone%20Expansion%20at%20Citadel%2C%20Boston&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Comfort%20Zone%20Expansion%20at%20Citadel%2C%20Boston%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARmEm3aHv5DKSko2f%2Fmeetup-comfort-zone-expansion-at-citadel-boston%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Comfort%20Zone%20Expansion%20at%20Citadel%2C%20Boston%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARmEm3aHv5DKSko2f%2Fmeetup-comfort-zone-expansion-at-citadel-boston", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARmEm3aHv5DKSko2f%2Fmeetup-comfort-zone-expansion-at-citadel-boston", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t8'>Comfort Zone Expansion at Citadel, Boston</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Comfort Zone Expansion (CoZE) is the practice of exposing yourself to uncomfortable or unusual situations with strangers in order to develop social skills, persuasion, and confidence. (as taught at CFAR)</p>\n\n<p>A wide variety of activities fall under the umbrella of CoZE - here are some examples:</p>\n\n<ul>\n<li><p>starting conversations and asking personal questions</p></li>\n<li><p>asking people for favors</p></li>\n<li><p>singing or dancing in public</p></li>\n<li><p>going to stores and asking for free samples</p></li>\n<li><p>trading objects with people</p></li>\n</ul>\n\n<p>After a brief intro to CoZE, we will warm up by playing improv games, and then take a trip to a mall in downtown Boston for the CoZE practice (the location is chosen to minimize the chances of encountering someone you know). We will disperse around the mall, and then reconvene for dinner and an exchange of stories!</p>\n\n<p>Our schedule for this meetup is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: Intro to CoZE - 3:30-4pm.</p>\n\n<p>\u2014Phase 3: Improv games (warmup) - 4-5pm.</p>\n\n<p>\u2014Phase 4: CoZE outing in downtown Boston - 5-7pm.</p>\n\n<p>\u2014Phase 5: Dinner and exchanging CoZE stories - 7pm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t8'>Comfort Zone Expansion at Citadel, Boston</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ARmEm3aHv5DKSko2f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.4116027940962724e-06, "legacy": true, "legacyId": "24614", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Comfort_Zone_Expansion_at_Citadel__Boston\">Discussion article for the meetup : <a href=\"/meetups/t8\">Comfort Zone Expansion at Citadel, Boston</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Comfort Zone Expansion (CoZE) is the practice of exposing yourself to uncomfortable or unusual situations with strangers in order to develop social skills, persuasion, and confidence. (as taught at CFAR)</p>\n\n<p>A wide variety of activities fall under the umbrella of CoZE - here are some examples:</p>\n\n<ul>\n<li><p>starting conversations and asking personal questions</p></li>\n<li><p>asking people for favors</p></li>\n<li><p>singing or dancing in public</p></li>\n<li><p>going to stores and asking for free samples</p></li>\n<li><p>trading objects with people</p></li>\n</ul>\n\n<p>After a brief intro to CoZE, we will warm up by playing improv games, and then take a trip to a mall in downtown Boston for the CoZE practice (the location is chosen to minimize the chances of encountering someone you know). We will disperse around the mall, and then reconvene for dinner and an exchange of stories!</p>\n\n<p>Our schedule for this meetup is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: Intro to CoZE - 3:30-4pm.</p>\n\n<p>\u2014Phase 3: Improv games (warmup) - 4-5pm.</p>\n\n<p>\u2014Phase 4: CoZE outing in downtown Boston - 5-7pm.</p>\n\n<p>\u2014Phase 5: Dinner and exchanging CoZE stories - 7pm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Comfort_Zone_Expansion_at_Citadel__Boston1\">Discussion article for the meetup : <a href=\"/meetups/t8\">Comfort Zone Expansion at Citadel, Boston</a></h2>", "sections": [{"title": "Discussion article for the meetup : Comfort Zone Expansion at Citadel, Boston", "anchor": "Discussion_article_for_the_meetup___Comfort_Zone_Expansion_at_Citadel__Boston", "level": 1}, {"title": "Discussion article for the meetup : Comfort Zone Expansion at Citadel, Boston", "anchor": "Discussion_article_for_the_meetup___Comfort_Zone_Expansion_at_Citadel__Boston1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-07T07:45:07.565Z", "modifiedAt": null, "url": null, "title": "Yes, Virginia, You Can Be 99.99% (Or More!) Certain That 53 Is Prime", "slug": "yes-virginia-you-can-be-99-99-or-more-certain-that-53-is", "viewCount": null, "lastCommentedAt": "2020-10-18T17:38:14.278Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CFYBTbLciMmXdyE2f/yes-virginia-you-can-be-99-99-or-more-certain-that-53-is", "pageUrlRelative": "/posts/CFYBTbLciMmXdyE2f/yes-virginia-you-can-be-99-99-or-more-certain-that-53-is", "linkUrl": "https://www.lesswrong.com/posts/CFYBTbLciMmXdyE2f/yes-virginia-you-can-be-99-99-or-more-certain-that-53-is", "postedAtFormatted": "Thursday, November 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yes%2C%20Virginia%2C%20You%20Can%20Be%2099.99%25%20(Or%20More!)%20Certain%20That%2053%20Is%20Prime&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYes%2C%20Virginia%2C%20You%20Can%20Be%2099.99%25%20(Or%20More!)%20Certain%20That%2053%20Is%20Prime%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFYBTbLciMmXdyE2f%2Fyes-virginia-you-can-be-99-99-or-more-certain-that-53-is%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yes%2C%20Virginia%2C%20You%20Can%20Be%2099.99%25%20(Or%20More!)%20Certain%20That%2053%20Is%20Prime%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFYBTbLciMmXdyE2f%2Fyes-virginia-you-can-be-99-99-or-more-certain-that-53-is", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFYBTbLciMmXdyE2f%2Fyes-virginia-you-can-be-99-99-or-more-certain-that-53-is", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1830, "htmlBody": "<p><strong>TLDR;</strong> though you can't be 100% certain of anything, a lot of the people who go around talking about how you can't be 100% certain of anything would be surprised at how often you can be 99.99% certain. Indeed, we're often justified in assigning odds ratios well in excess of a million to one to certain claims. Realizing this is important for avoiding certain rookie Bayesian's mistakes, as well as for thinking about existential risk.</p>\n<hr />\n<p>53 is prime. I'm <em>very </em>confident of this. 99.99% confident, at the very least. How can I be so confident? Because of the following argument:</p>\n<p>If a number is composite, it must have a prime factor <a href=\"http://stackoverflow.com/questions/5811151/why-do-we-check-upto-the-square-root-of-a-prime-number-to-determine-if-it-is-pri\">no greater than its square root</a>. Because 53 is less than 64, sqrt(53) is less than 8. So, to find out if 53 is prime or not, we only need to check if it can be divided by primes less than 8 (i.e. 2, 3, 5, and 7). 53's last digit is odd, so it's not divisible by 2. 53's last digit is neither 0 nor 5, so it's not divisible by 5. The nearest multiples of 3 are 51 (=17x3) and 54, so 53 is not divisible by 3. The nearest multiples of 7 are 49 (=7^2) and 56, so 53 is not divisible by 7. Therefore, 53 is prime.</p>\n<p>(My confidence in this argument is helped by the fact that I was good at math in high school. Your confidence in your math abilities may vary.)</p>\n<p>I mention this because in his post <a href=\"/lw/mo/infinite_certainty/\">Infinite Certainty</a>, Eliezer writes:</p>\n<blockquote>\n<p>Suppose you say that you're 99.99% confident that 2 + 2 = 4. &nbsp;Then you have just asserted that you could make 10,000 <em>independent</em> statements, in which you repose equal confidence, and be wrong, on average, around once. &nbsp;Maybe for 2 + 2 = 4 this extraordinary degree of confidence would be possible: \"2 + 2 = 4\" extremely simple, and mathematical as well as empirical, and widely believed socially (not with passionate affirmation but just quietly taken for granted). &nbsp;So maybe you really could get up to 99.99% confidence on this one.</p>\n<p>I don't think you could get up to 99.99% confidence for assertions like \"53 is a prime number\". &nbsp;Yes, it seems likely, but by the time you tried to set up protocols that would let you assert 10,000 <em>independent</em> statements of this sort&mdash;that is, not just a set of statements about prime numbers, but a new protocol each time&mdash;you would fail more than once. &nbsp;Peter de Blanc has an amusing anecdote on this point, which he is welcome to retell in the comments.</p>\n</blockquote>\n<p>I think this argument that you can't be 99.99% certain that 53 is prime is fallacious. Stuart Armstrong <a href=\"/lw/mo/infinite_certainty/hj2\">explains why</a> in the comments:<a id=\"more\"></a></p>\n<blockquote>\n<p><em>If you say 99.9999% confidence, you're implying that you could make one million equally fraught statements, one after the other, and be wrong, on average, about once.</em></p>\n<p>Excellent post overall, but that part seems weakest - we suffer from an unavailability problem, in that we can't just think up random statements with those properties. When I said I agreed 99.9999% with \"P(P is never equal to 1)\" it doesn't mean that I feel I could produce such a list - just that I have a very high belief that such a list could exist.</p>\n</blockquote>\n<p>In other words, it's true that:</p>\n<ul>\n<li>If a well-calibrated person claims to be 99.99% certain of 10,000 independent statements, on average one of those statements should be false.</li>\n</ul>\n<p>But it doesn't follow that:</p>\n<ul>\n<li>If a well-calibrated person claims to be 99.99% certain of one statement, they should be able to produce 9,999 other independent statements of equal certainty and be wrong on average once.</li>\n</ul>\n<p>If it's not clear why this doesn't follow consider the <a href=\"http://www.spaceandgames.com/?p=27\">anecdote</a> Eliezer references in the quote above, which runs as follows: A gets B to agree that if 7 is not prime, B will give A $100. B then makes the same agreement for 11, 13, 17, 19, and 23. Then A asks about 27. B refuses. What about 29? Sure. 31? Yes. 33? No. 37? Yes. 39? No. 41? Yes. 43? Yes. 47? Yes. 49? No. 51? Yes. And suddenly B is $100 poorer.</p>\n<p>Now, B claimed to be 100% sure about 7 being prime, which I don't agree with. But that's not what lost him his $100. What lost him his $100 is that, as the game went on, he got careless. If he'd taken the time to ask himself, \"am I really as sure about 51 as I am about 7?\" he'd probably have realized the answer was \"no.\" He probably didn't check &nbsp;he primality of 51 as carefully as I checked the primality of 53 at the beginning of this post. (From the provided chat transcript, sleep deprivation may have also had something to do with it.)</p>\n<p>If you tried to make 10,000 statements with 99.99% certainty, sooner or later you would get careless. Heck, before I started writing this post, I tried typing up a list of statements I was sure of, and it wasn't long before I'd typed 1 + 0 = 10 (I'd <em>meant </em>to type 1 + 9 = 10. Oops.) But the fact that, as the exercise went on, you'd start including statements that weren't really as certain as the first statement doesn't mean you couldn't be justified in being 99.99% certain of that first statement.</p>\n<p>I almost feel like I should apologize for nitpicking this, because I agree with the main point of the \"Infinite Certainty\" post, that you should never assign a proposition probability 1. Assigning a proposition a probability of 1 implies that no evidence could ever convince you otherwise, and I agree that that's bad. But I think it's important to say that you're often justified in putting <em>a lot </em>of 9s after the decimal point in your probability assignments, for a few reasons.</p>\n<p>One reason is arguments in the style of Eliezer's \"10,000 independent statements\" argument lead to inconsistencies. From <a href=\"/lw/sg/when_not_to_use_probabilities/\">another post</a> of Eliezer's:</p>\n<blockquote>\n<p>I would be substantially more alarmed about a lottery device with a well-defined chance of 1 in 1,000,000 of destroying the world, than I am about the Large Hadron Collider being switched on.</p>\n<p>On the other hand, if you asked me whether I could make one million statements of authority equal to \"The Large Hadron Collider will not destroy the world\", and be wrong, on average, around once, then I would have to say no.</p>\n<p>What should I do about this inconsistency? &nbsp;I'm not sure, but I'm certainly not going to wave a magic wand to make it go away. &nbsp;That's like finding an inconsistency in a pair of maps you own, and quickly scribbling some alterations to make sure they're consistent.</p>\n<p>I would also, by the way, be substantially more worried about a lottery device with a 1 in 1,000,000,000 chance of destroying the world, than a device which destroyed the world if the Judeo-Christian God existed. &nbsp;But I would not suppose that I could make one billion statements, one after the other, fully independent and equally fraught as \"There is no God\", and be wrong on average around once.</p>\n</blockquote>\n<p>Okay, so that's just Eliezer. But in a way, it's just a sophisticated version of a mistake a lot of novice students of probability make. Many people, when you tell them they can never be 100% certain of anything, respond switching to saying 99% or 99.9% whenever they previously would have said 100%.</p>\n<p>In a sense they have the right idea&mdash;there are lots of situations where, while the appropriate probability is not 0, it's still negligible. But 1% or even 0.1% isn't negligible enough in many contexts. Generally, you should not be in the habit of doing things that have a 0.1% chance of killing you. Do so on a daily basis, and on average you will be dead in less than three years. Conversely, if you mistakenly assign a 0.1% chance that you will die each time you leave the house, you may never leave the house.</p>\n<p>Furthermore, the ways this can trip people up aren't just hypothetical. Christian apologist William Lane Craig <a href=\"http://www.infidels.org/library/modern/paul_doland/strobel.html#obj2\">claims</a> the skeptical slogan \"extraordinary claims require extraordinary evidence\" is contradicted by probability theory, because it actually wouldn't take all that much evidence to convince us that, for example, \"the numbers chosen in last night's lottery were 4, 2, 9, 7, 8 and 3.\" The correct response to this argument is to say that the prior probability of a miracle occurring is orders of magnitude smaller than mere one in a million odds.</p>\n<p>I suspect many novice students of probability will be uncomfortable with that response. They shouldn't be, though. After all, if you tried to convince the average Christian of Joseph Smith's story with the golden plates, they'd require <em>much </em>more evidence than they'd need to be convinced that last night's lottery numbers were 4, 2, 9, 7, 8 and 3. That suggests their prior for Mormonism is much less than one in a million.</p>\n<p>This also matters a lot for thinking about futurism and existential risk. If someone is in the habit of using \"99%\" as shorthand for \"basically 100%,\" they will have trouble grasping the thought \"I am 99% certain this futuristic scenario will not happen, but the stakes are high enough that I need to take the 1% chance into account in my decision making.\" Actually, I suspect that problems in this vicinity explain much of the problems ordinary people (read: including average scientists) have thinking about existential risk.</p>\n<p>I agree with what Eliezer has said about <a href=\"/lw/sg/when_not_to_use_probabilities/\">being ware of picking numbers out of thin air and trying to do math with them</a>. (Or if you are going to pick numbers out of thin air, at least be ready&nbsp;<a href=\"/lw/sg/when_not_to_use_probabilities/lxt\">to abandon your numbers at the drop of a hat</a>.) Such advice goes double for dealing with very small probabilities, which humans seem to be <em>especially </em>bad at thinking about.</p>\n<p>But it's worth trying to internalize a sense that there are several very different categories of improbable claims, along the lines of:</p>\n<ul>\n<li>Things that have a probability of something like 1%. These are things you really don't want to bet your life on if you can help it.</li>\n<li>Things that have a probability of something like one in a million. Includes many common ways to die that don't involve doing anything most people would regard as especially risky. For example,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Transportation_safety_in_the_United_States\">these stats</a>&nbsp;suggest&nbsp;the odds of a 100 mile car trip killing you are somewhere on the order of one in a million.</li>\n<li>Things whose probability is truly negligible outside alternate universes where your evidence is radically different than what it actually is. For example, the risk of the Earth being overrun by demons.</li>\n</ul>\n<p>Furthermore, it's worth trying to learn to think coherently about which claims belong in which category. That includes not being afraid to assign claims to the third category when necessary.</p>\n<p><strong>Added: </strong>I also recommend the links in <a href=\"/lw/izs/yes_virginia_you_can_be_9999_or_more_certain_that/a0vx\">this comment by komponisto</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 2, "Ng8Gice9KNkncxqcj": 2, "LhX3F2SvGDarZCuh6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CFYBTbLciMmXdyE2f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 68, "extendedScore": null, "score": 0.000204, "legacy": true, "legacyId": "24616", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ooypcn7qFzsMcy53R", "AJ9dX59QXokZb35fk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-07T11:17:21.889Z", "modifiedAt": null, "url": null, "title": "Meetup: Somewhere you do not live even close to", "slug": "meetup-somewhere-you-do-not-live-even-close-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.064Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/27xMpjx9MzTtuQgx8/meetup-somewhere-you-do-not-live-even-close-to", "pageUrlRelative": "/posts/27xMpjx9MzTtuQgx8/meetup-somewhere-you-do-not-live-even-close-to", "linkUrl": "https://www.lesswrong.com/posts/27xMpjx9MzTtuQgx8/meetup-somewhere-you-do-not-live-even-close-to", "postedAtFormatted": "Thursday, November 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Somewhere%20you%20do%20not%20live%20even%20close%20to&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Somewhere%20you%20do%20not%20live%20even%20close%20to%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27xMpjx9MzTtuQgx8%2Fmeetup-somewhere-you-do-not-live-even-close-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Somewhere%20you%20do%20not%20live%20even%20close%20to%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27xMpjx9MzTtuQgx8%2Fmeetup-somewhere-you-do-not-live-even-close-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27xMpjx9MzTtuQgx8%2Fmeetup-somewhere-you-do-not-live-even-close-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>Six out of the last 10 posts are about single meetups. And I care about none of them.</p>\n<p>I will look into whipping together some kind of bot that periodically posts threads like the open thread or a meetup thread. Voice your opinion in the comments if you do not want me to do that.</p>\n<p>That is all.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "27xMpjx9MzTtuQgx8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 23, "extendedScore": null, "score": 1.4124299203461054e-06, "legacy": true, "legacyId": "24623", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-08T04:20:52.149Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014The Merits of Specificity", "slug": "meetup-west-la-the-merits-of-specificity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KZsWeHaBLSbPkHW7i/meetup-west-la-the-merits-of-specificity", "pageUrlRelative": "/posts/KZsWeHaBLSbPkHW7i/meetup-west-la-the-merits-of-specificity", "linkUrl": "https://www.lesswrong.com/posts/KZsWeHaBLSbPkHW7i/meetup-west-la-the-merits-of-specificity", "postedAtFormatted": "Friday, November 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94The%20Merits%20of%20Specificity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94The%20Merits%20of%20Specificity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZsWeHaBLSbPkHW7i%2Fmeetup-west-la-the-merits-of-specificity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94The%20Merits%20of%20Specificity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZsWeHaBLSbPkHW7i%2Fmeetup-west-la-the-merits-of-specificity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZsWeHaBLSbPkHW7i%2Fmeetup-west-la-the-merits-of-specificity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 225, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/t9'>West LA\u2014The Merits of Specificity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 November 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you know the eight secrets of the coven.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<p>Can you give a specific example of abstract argument being more useful than a specific example? How about an abstract argument that specific examples are more useful than abstract arguments? The ladder of abstraction, from abstract to concrete, superficially seems unrelated to but is quite entangled with the other well-known dichotomy, that between the general and the specific. This mess is our focus. It is obvious that both \"<a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">sides</a>\" have their place, but what modes are best for what purposes?</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://worrydream.com/LadderOfAbstraction/\" rel=\"nofollow\">Up and Down The Ladder of Abstraction</a></li>\n<li><a href=\"http://lesswrong.com/r/lesswrong/lw/bc3/sotw_be_specific/\">Skill of the Week: Be Specific</a></li>\n<li><a href=\"http://lesswrong.com/lw/ic/the_virtue_of_narrowness/\">The Virtue of Narrowness</a></li>\n<li><a href=\"http://lesswrong.com/lw/ly/hug_the_query/\">Hug the Query</a></li>\n<li><a href=\"http://lesswrong.com/lw/lc/leaky_generalizations/\">Leaky Generalizations</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Metabooks/G%C3%B6del%2C%20Escher%2C%20Bach.pdf\" rel=\"nofollow\">G\u00f6del, Escher, Bach: An Eternal Golden Braid</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/t9'>West LA\u2014The Merits of Specificity</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KZsWeHaBLSbPkHW7i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.41342096366669e-06, "legacy": true, "legacyId": "24632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_The_Merits_of_Specificity\">Discussion article for the meetup : <a href=\"/meetups/t9\">West LA\u2014The Merits of Specificity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 November 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you know the eight secrets of the coven.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<p>Can you give a specific example of abstract argument being more useful than a specific example? How about an abstract argument that specific examples are more useful than abstract arguments? The ladder of abstraction, from abstract to concrete, superficially seems unrelated to but is quite entangled with the other well-known dichotomy, that between the general and the specific. This mess is our focus. It is obvious that both \"<a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">sides</a>\" have their place, but what modes are best for what purposes?</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://worrydream.com/LadderOfAbstraction/\" rel=\"nofollow\">Up and Down The Ladder of Abstraction</a></li>\n<li><a href=\"http://lesswrong.com/r/lesswrong/lw/bc3/sotw_be_specific/\">Skill of the Week: Be Specific</a></li>\n<li><a href=\"http://lesswrong.com/lw/ic/the_virtue_of_narrowness/\">The Virtue of Narrowness</a></li>\n<li><a href=\"http://lesswrong.com/lw/ly/hug_the_query/\">Hug the Query</a></li>\n<li><a href=\"http://lesswrong.com/lw/lc/leaky_generalizations/\">Leaky Generalizations</a></li>\n<li><a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/Metabooks/G%C3%B6del%2C%20Escher%2C%20Bach.pdf\" rel=\"nofollow\">G\u00f6del, Escher, Bach: An Eternal Golden Braid</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_The_Merits_of_Specificity1\">Discussion article for the meetup : <a href=\"/meetups/t9\">West LA\u2014The Merits of Specificity</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014The Merits of Specificity", "anchor": "Discussion_article_for_the_meetup___West_LA_The_Merits_of_Specificity", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014The Merits of Specificity", "anchor": "Discussion_article_for_the_meetup___West_LA_The_Merits_of_Specificity1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NgtYDP3ZtLJaM248W", "yDfxTj9TKYsYiWH5o", "2jp98zdLo898qExrr", "Tc2H9KbKRjuDJ3WSS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-08T04:27:43.075Z", "modifiedAt": null, "url": null, "title": "Academic Cliques", "slug": "academic-cliques", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:04.968Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t9oXxwss8p6oXGg3W/academic-cliques", "pageUrlRelative": "/posts/t9oXxwss8p6oXGg3W/academic-cliques", "linkUrl": "https://www.lesswrong.com/posts/t9oXxwss8p6oXGg3W/academic-cliques", "postedAtFormatted": "Friday, November 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Academic%20Cliques&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAcademic%20Cliques%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9oXxwss8p6oXGg3W%2Facademic-cliques%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Academic%20Cliques%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9oXxwss8p6oXGg3W%2Facademic-cliques", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9oXxwss8p6oXGg3W%2Facademic-cliques", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p>In my article on <a href=\"/lw/iu0/trusting_expert_consensus/\">trusting expert consensus</a>, I talked about the value of having hard data on the opinions of experts in a given field. The unspoken subtext was that you should be careful of claims of expert consensus that don't have hard data to back them up. I've joked that when a philosopher says there's a philosophical consensus, what he really means is \"I talked to a few of my friends about this and they agreed with me.\"</p>\n<p>What's often really happening, though (at least in philosophy) is that the \"consensus\" really reflects the opinions of a particular academic clique. A sub-group of experts in the field spend a disproportionate amount of time talking to each other, and end up convincing themselves they represent the consensus of the entire profession. A rather conspicuous example of this is what I've called <a href=\"http://www.patheos.com/blogs/hallq/2013/11/the-plantinga-clique/\">the Plantinga clique</a>&nbsp;on my own blog&mdash;theistic philosophers who've convinced themselves that the opinions of Alvin Plantinga represent the consensus of philosophy.</p>\n<p>But it isn't just theistic philosophers who do this. When I was in school, it was still possible to hear fans of Quine claim that everyone knew Quine had refuted the analytic synthetic distinction. Post <a href=\"http://philpapers.org/surveys/\">PhilPapers survey</a>, hopefully people have stopped claiming this. And one time, I heard a philosophy blogger berating scientists for being ignorant of the findings in philosophy that all philosophers agree on. I asked him for examples of claims that all philosophers agree on, I responded with examples of philosophers who rejected some of those claims, \"Ah,\" he said, \"but they don't count. Let me tell you who's opinions matter...\" (I'm paraphrasing, but that was what it amounted to.)</p>\n<p>I <em>strongly </em>suspect this happens in other disciplines: supposed \"consensuses of experts\" are really just the opinions of one clique within a discipline. Thus, I tend to approach claims of consensus in any discipline with skepticism when they're not backed up by hard data. But I don't actually know of verifiable examples of this problem outside of philosophy. Has other people with backgrounds in other disciplines noticed things like this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x3zyEPFaJANB2BHmP": 1, "ALwRRZqvhaop8gxkT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t9oXxwss8p6oXGg3W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 34, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "24633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R8YpYTq8LoD3k948L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-08T16:54:22.325Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-107", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6D49FhNsvSLh5Tgcb/weekly-lw-meetups-107", "pageUrlRelative": "/posts/6D49FhNsvSLh5Tgcb/weekly-lw-meetups-107", "linkUrl": "https://www.lesswrong.com/posts/6D49FhNsvSLh5Tgcb/weekly-lw-meetups-107", "postedAtFormatted": "Friday, November 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6D49FhNsvSLh5Tgcb%2Fweekly-lw-meetups-107%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6D49FhNsvSLh5Tgcb%2Fweekly-lw-meetups-107", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6D49FhNsvSLh5Tgcb%2Fweekly-lw-meetups-107", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 490, "htmlBody": "<p><strong>This summary was posted to LW Main on November 1st. The following week's summary is <a href=\"/lw/j0b/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/st\">Atlanta: November Meetup (First of Two):&nbsp;<span class=\"date\">03 November 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/sa\">Cologne (K&ouml;ln):&nbsp;<span class=\"date\">10 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/sm\">Princeton NJ Meetup:&nbsp;<span class=\"date\">16 November 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/su\">Urbana-Champaign: Thinking Fast and Slow Discussion:&nbsp;<span class=\"date\">03 November 2013 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">02 November 2019 01:30PM</span></a> </li>\n<li><a href=\"/meetups/sw\">Durham/RTLW HPMoR discussion, chapters 94-96:&nbsp;<span class=\"date\">02 November 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/sx\">Washington DC/VA Games meetup:&nbsp;<span class=\"date\">03 November 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6D49FhNsvSLh5Tgcb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4141513554468378e-06, "legacy": true, "legacyId": "24558", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XbzWTpvGtpv2Dh8c7", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-08T19:55:27.747Z", "modifiedAt": null, "url": null, "title": "[Prize] Essay Contest: Cryonics and Effective Altruism", "slug": "prize-essay-contest-cryonics-and-effective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bb7FiySyRgjXptAww/prize-essay-contest-cryonics-and-effective-altruism", "pageUrlRelative": "/posts/bb7FiySyRgjXptAww/prize-essay-contest-cryonics-and-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/bb7FiySyRgjXptAww/prize-essay-contest-cryonics-and-effective-altruism", "postedAtFormatted": "Friday, November 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPrize%5D%20Essay%20Contest%3A%20Cryonics%20and%20Effective%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPrize%5D%20Essay%20Contest%3A%20Cryonics%20and%20Effective%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbb7FiySyRgjXptAww%2Fprize-essay-contest-cryonics-and-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPrize%5D%20Essay%20Contest%3A%20Cryonics%20and%20Effective%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbb7FiySyRgjXptAww%2Fprize-essay-contest-cryonics-and-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbb7FiySyRgjXptAww%2Fprize-essay-contest-cryonics-and-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 410, "htmlBody": "<p>I'm starting a contest for the best essay describing why a rational person of a not particularly selfish nature might consider cryonics an exceptionally worthwhile place to allocate resources. There are three distinct questions relating to this, and you can pick any one of them to focus on, or answer all three.</p>\n<p><strong>Contest Summary</strong>:</p>\n<ul>\n<li>Essay Topic: Cryonics and Effective Altruism</li>\n<li>Answers at least one of the following questions: <br /><ol>\n<li>Why might a utilitarian seeking to do the most good consider contributing time and/or money towards cryonics (as opposed to other causes)?</li>\n<li>What is the most optimal way (or at least, some highly optimal, perhaps counterintuitive way) to contribute to cryonics?</li>\n<li>What reasons might a utilitarian have for actually signing up for cryonics services, as opposed to just making a charitable donation towards cryonics (or vice versa)?</li>\n</ol></li>\n<li>Length: 800-1200 words</li>\n<li>Target audience: Utilitarians, Consequentialists, Effective Altruists, etc.</li>\n<li>Prize: 1 BTC (around $350, at the moment)</li>\n<li>Deadline: Sunday 11/17/2013, at 8:00PM PST</li>\n</ul>\n<p>To enter, post your essay as a comment in this thread. Feel free to edit your submission up until the deadline. If it is a repost of something old, a link to the original would be appreciated. I will judge the essays partly based on upvotes/downvotes, but also based on how well it meets the criteria and makes its points. Essays that do not directly answer any of the three questions will not be considered for the prize. If there are multiple entries that are too close to call, I will flip a coin to determine the winner.</p>\n<p>Terminology clarification: I realise that for some individuals there is confusion about the term 'utilitarian' because historically it has been represented using very simple, humanly unrealistic utility functions such as pure hedonism. For the purposes of this contest, I mean to include anyone whose utility function is well defined and self-consistent -- it is not meant to imply a particular utility function. You may wish to clarify in your essay the kind of utilitarian you are describing.</p>\n<p>Regarding the prize: If you win the contest and prefer to receive cash equivalent via paypal, this wll be an option, although I consider bitcoin to be more convenient (and there is no guarantee how many dollars it will come out to due to the volatility of bitcoin).</p>\n<p>\n<hr />\n<a href=\"/r/discussion/lw/j3s/effective_altruism_and_cryonics_contest_results/\">Contest results</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bb7FiySyRgjXptAww", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "24625", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q7PFyobNPwqBsma9g"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-08T20:13:00.805Z", "modifiedAt": null, "url": null, "title": "Open Thread, November 8 - 14, 2013", "slug": "open-thread-november-8-14-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "witzvo", "createdAt": "2012-05-10T07:45:38.575Z", "isAdmin": false, "displayName": "witzvo"}, "userId": "efsbsXkkuRESNruDe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KAg8P97pRR2NZYaNF/open-thread-november-8-14-2013", "pageUrlRelative": "/posts/KAg8P97pRR2NZYaNF/open-thread-november-8-14-2013", "linkUrl": "https://www.lesswrong.com/posts/KAg8P97pRR2NZYaNF/open-thread-november-8-14-2013", "postedAtFormatted": "Friday, November 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20November%208%20-%2014%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20November%208%20-%2014%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAg8P97pRR2NZYaNF%2Fopen-thread-november-8-14-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20November%208%20-%2014%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAg8P97pRR2NZYaNF%2Fopen-thread-november-8-14-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAg8P97pRR2NZYaNF%2Fopen-thread-november-8-14-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_iyj\" class=\"content clear\"><span style=\"line-height: 12.660714149475098px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KAg8P97pRR2NZYaNF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.414344015318033e-06, "legacy": true, "legacyId": "24638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 141, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-08T21:12:44.652Z", "modifiedAt": null, "url": null, "title": "[Link] Bet Your Friends to Be More Right", "slug": "link-bet-your-friends-to-be-more-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:45.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uvvYSGC7vnGRRzZZ2/link-bet-your-friends-to-be-more-right", "pageUrlRelative": "/posts/uvvYSGC7vnGRRzZZ2/link-bet-your-friends-to-be-more-right", "linkUrl": "https://www.lesswrong.com/posts/uvvYSGC7vnGRRzZZ2/link-bet-your-friends-to-be-more-right", "postedAtFormatted": "Friday, November 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Bet%20Your%20Friends%20to%20Be%20More%20Right&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Bet%20Your%20Friends%20to%20Be%20More%20Right%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuvvYSGC7vnGRRzZZ2%2Flink-bet-your-friends-to-be-more-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Bet%20Your%20Friends%20to%20Be%20More%20Right%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuvvYSGC7vnGRRzZZ2%2Flink-bet-your-friends-to-be-more-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuvvYSGC7vnGRRzZZ2%2Flink-bet-your-friends-to-be-more-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<p><a href=\"http://tynan.com/bet\">This article</a> does a good job of explaining how betting can be a useful rationality practice. An excerpt:</p>\n<blockquote>\n<p>The interesting thing about this practice was that it made us both think very carefully about the accuracy of all of our statements. The most embarrassing thing ever was to say, \"I bet you anything that I'll be on time...\" and then be unwilling to back up the assertion with a bet. Failing to bet was an admission that you'd just said something that you had no real confidence in.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E8PHMuf7tsr8teXAe": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uvvYSGC7vnGRRzZZ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.4144019565643265e-06, "legacy": true, "legacyId": "24639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-09T06:50:10.579Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories", "slug": "meetup-washington-dc-goals-political-advocacy-as-effective", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dGZrvSudgY4umdPyB/meetup-washington-dc-goals-political-advocacy-as-effective", "pageUrlRelative": "/posts/dGZrvSudgY4umdPyB/meetup-washington-dc-goals-political-advocacy-as-effective", "linkUrl": "https://www.lesswrong.com/posts/dGZrvSudgY4umdPyB/meetup-washington-dc-goals-political-advocacy-as-effective", "postedAtFormatted": "Saturday, November 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Goals%2C%20Political%20Advocacy%20as%20Effective%20Altruism%2C%20and%20Stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Goals%2C%20Political%20Advocacy%20as%20Effective%20Altruism%2C%20and%20Stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdGZrvSudgY4umdPyB%2Fmeetup-washington-dc-goals-political-advocacy-as-effective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Goals%2C%20Political%20Advocacy%20as%20Effective%20Altruism%2C%20and%20Stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdGZrvSudgY4umdPyB%2Fmeetup-washington-dc-goals-political-advocacy-as-effective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdGZrvSudgY4umdPyB%2Fmeetup-washington-dc-goals-political-advocacy-as-effective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ta'>Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 03:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So, we came up with three ideas, none of which will necessarily take a long time. So we're doing them together. We will be:\nTalking about goals and trying to revive the goals spreadsheet\nDiscussing several posts on <a href=\"http://blog.givewell.org/\" rel=\"nofollow\">Givewell&#39;s blog</a> about political advocacy as effective altruism\nAnd (if people have them) telling Haloween stories, as per <a href=\"http://lesswrong.com/lw/iyb/halloween_thread_rationalists_horrors/\">this thread</a> (a bit late, I know, but why not?)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ta'>Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dGZrvSudgY4umdPyB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.4149623055954444e-06, "legacy": true, "legacyId": "24643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories\">Discussion article for the meetup : <a href=\"/meetups/ta\">Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 03:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So, we came up with three ideas, none of which will necessarily take a long time. So we're doing them together. We will be:\nTalking about goals and trying to revive the goals spreadsheet\nDiscussing several posts on <a href=\"http://blog.givewell.org/\" rel=\"nofollow\">Givewell's blog</a> about political advocacy as effective altruism\nAnd (if people have them) telling Haloween stories, as per <a href=\"http://lesswrong.com/lw/iyb/halloween_thread_rationalists_horrors/\">this thread</a> (a bit late, I know, but why not?)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories1\">Discussion article for the meetup : <a href=\"/meetups/ta\">Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NPqBcdqdh9rauZd8W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-09T08:00:42.977Z", "modifiedAt": null, "url": null, "title": "Good movies for rationalists?", "slug": "good-movies-for-rationalists", "viewCount": null, "lastCommentedAt": "2013-11-25T14:31:32.306Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/borfjzJTksCgMsjmC/good-movies-for-rationalists", "pageUrlRelative": "/posts/borfjzJTksCgMsjmC/good-movies-for-rationalists", "linkUrl": "https://www.lesswrong.com/posts/borfjzJTksCgMsjmC/good-movies-for-rationalists", "postedAtFormatted": "Saturday, November 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20movies%20for%20rationalists%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20movies%20for%20rationalists%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FborfjzJTksCgMsjmC%2Fgood-movies-for-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20movies%20for%20rationalists%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FborfjzJTksCgMsjmC%2Fgood-movies-for-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FborfjzJTksCgMsjmC%2Fgood-movies-for-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 313, "htmlBody": "<p>Hi,</p>\n<p>what good movies can you suggest that give ideas or inspirations on how to be more rational?</p>\n<p>I just watched [Memento](https://en.wikipedia.org/wiki/Memento_%28film%29) last night and I was very impressed.</p>\n<p>(No spoilers in this post)</p>\n<p>The main character is a guy who suffers from amnesia, he forgets everything after a couple minutes so he has developed a system to cope with it. He takes pictures and writes notes. E.g. when staying at a hotel he takes a picture of it and put it in his pocket. So later when he doesnt know where he is staying he searches his pockets, finds the picture of the hotel and then he knows.</p>\n<h2>What I learned<br /></h2>\n<p>I identified with the character in the movie because in spite of not having amnesia my memory as everyone elses isn't perfect either and I have all the quirks(biases) of a normal human brain. I cant exactly remember what I did last Thursday at 3 PM. Do I actually know why I am doing what Im doing or why I believe what I believe? I may have good rationalizations for both, of course, but that doesnt mean they are the real reasons.</p>\n<p>I like to read LW but I havent developed much of a system to actually be more rational. If anyone has, I would be eager to read about it.</p>\n<h2>Practical Advice<br /></h2>\n<p>What system could I develop to be more rational? One thing that a lot of management experts(e.g. Peter Drucker) have already pointed out is to write down how we actually spend our time because often how we spend it is not how we think we spend it and we end up spending much more time on unproductive activities than we are aware of. How much time went into random internet browsing last week?</p>\n<p>I will start an activity log during work: how much time Im spending on what. This will be a first step.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "borfjzJTksCgMsjmC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "24644", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Hi,</p>\n<p>what good movies can you suggest that give ideas or inspirations on how to be more rational?</p>\n<p>I just watched [Memento](https://en.wikipedia.org/wiki/Memento_%28film%29) last night and I was very impressed.</p>\n<p>(No spoilers in this post)</p>\n<p>The main character is a guy who suffers from amnesia, he forgets everything after a couple minutes so he has developed a system to cope with it. He takes pictures and writes notes. E.g. when staying at a hotel he takes a picture of it and put it in his pocket. So later when he doesnt know where he is staying he searches his pockets, finds the picture of the hotel and then he knows.</p>\n<h2 id=\"What_I_learned\">What I learned<br></h2>\n<p>I identified with the character in the movie because in spite of not having amnesia my memory as everyone elses isn't perfect either and I have all the quirks(biases) of a normal human brain. I cant exactly remember what I did last Thursday at 3 PM. Do I actually know why I am doing what Im doing or why I believe what I believe? I may have good rationalizations for both, of course, but that doesnt mean they are the real reasons.</p>\n<p>I like to read LW but I havent developed much of a system to actually be more rational. If anyone has, I would be eager to read about it.</p>\n<h2 id=\"Practical_Advice\">Practical Advice<br></h2>\n<p>What system could I develop to be more rational? One thing that a lot of management experts(e.g. Peter Drucker) have already pointed out is to write down how we actually spend our time because often how we spend it is not how we think we spend it and we end up spending much more time on unproductive activities than we are aware of. How much time went into random internet browsing last week?</p>\n<p>I will start an activity log during work: how much time Im spending on what. This will be a first step.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "What I learned", "anchor": "What_I_learned", "level": 1}, {"title": "Practical Advice", "anchor": "Practical_Advice", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "66 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-09T17:42:27.313Z", "modifiedAt": null, "url": null, "title": "Non-standard cryo ideas", "slug": "non-standard-cryo-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.826Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FGtptGyRtHTLjG3s4/non-standard-cryo-ideas", "pageUrlRelative": "/posts/FGtptGyRtHTLjG3s4/non-standard-cryo-ideas", "linkUrl": "https://www.lesswrong.com/posts/FGtptGyRtHTLjG3s4/non-standard-cryo-ideas", "postedAtFormatted": "Saturday, November 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Non-standard%20cryo%20ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANon-standard%20cryo%20ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGtptGyRtHTLjG3s4%2Fnon-standard-cryo-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Non-standard%20cryo%20ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGtptGyRtHTLjG3s4%2Fnon-standard-cryo-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGtptGyRtHTLjG3s4%2Fnon-standard-cryo-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 431, "htmlBody": "<p>What plans could a prospective cryonicist try out, beyond simply signing up, that could increase the odds of eventually having a pleasant re-animation experience?<br /><br />To show what I mean, here are the main ideas I've managed to come up with so far. None of these particular ideas are a standard part of a cryonics preservation package. Some are easier to implement than others, some are more likely to have an effect than others, some have potentially greater effect than others.<br /><br />* Arranging for as much information about oneself (photo albums, emails, grade school report cards, etc) as possible to be placed on archival media and stored along with one's body. Reasoning: If the cryo-preservation procedure causes brain damage, and technology advances sufficiently before re-animation, then this information potentially allows for that damage to be at least partially reconstructed.<br /><br />* Requesting additional data about the cryo-preservation procedure used on oneself be archived. Eg, requesting that, to whatever degree doesn't interfere with the procedure, it be videoed.<br /><br />* Making arrangements for an animal body to be cryo-preserved with the same procedure one's own body was preserved with. A lab chimp would be ideal, but difficult to arrange for a number of reasons; more likely, a more common animal of around human mass would be feasible, such as a dog or goat. Even a few lab-rats might help. Reasoning: It gives future re-animators an additional opportunity to experiment with re-animation techniques, before attempting to re-animate a person.<br /><br />* Noting down one's preferences and requests for future re-animators. Eg, from \"I'd appreciate having a cat nearby to pet and calm down as I wake up\" to \"If you have to rebuild my body from scratch anyway, and it's within cultural norms, I would appreciate being gender _____\" to \"If you create a digital/electronic/computer/data copy of my mind, I would like a copy of that to be placed in offline, air-gapped storage, so that if every active copy of my mind is destroyed, there will always be that original backup available to re-instantiate myself.\" Or just more general ideas, such as, \"My goal is to live forever, and I would prefer whatever means most likely lead to that happening to be tried.\"<br /><br /><br />I'm not nearly as creative as I wish I could be; so I'm hoping that the local group-mind here might be able to offer further ideas, or improvements or refinements to the above ones.<br /><br />So: What extras can you think of?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FGtptGyRtHTLjG3s4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 1.415595759768993e-06, "legacy": true, "legacyId": "24646", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-09T20:51:34.599Z", "modifiedAt": null, "url": null, "title": "Cryonics Presentation [help request]", "slug": "cryonics-presentation-help-request", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MathieuRoy", "createdAt": "2013-06-16T02:41:27.071Z", "isAdmin": false, "displayName": "Mati_Roy"}, "userId": "Tw9etd8rMnHLeSQ9q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cbGhSJ3Zpg5umfXk3/cryonics-presentation-help-request", "pageUrlRelative": "/posts/cbGhSJ3Zpg5umfXk3/cryonics-presentation-help-request", "linkUrl": "https://www.lesswrong.com/posts/cbGhSJ3Zpg5umfXk3/cryonics-presentation-help-request", "postedAtFormatted": "Saturday, November 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20Presentation%20%5Bhelp%20request%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20Presentation%20%5Bhelp%20request%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcbGhSJ3Zpg5umfXk3%2Fcryonics-presentation-help-request%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20Presentation%20%5Bhelp%20request%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcbGhSJ3Zpg5umfXk3%2Fcryonics-presentation-help-request", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcbGhSJ3Zpg5umfXk3%2Fcryonics-presentation-help-request", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<p>This Monday (November 11th 2013 EDIT: it has been postponed to November 18th 2013) I will participate in a 'scientific communication' competition Laval University. If I win, I will go to the Quebec Engineering Competition (<a href=\"http://cqi-qec.qc.ca/\">http://cqi-qec.qc.ca/</a>). If I win again, I will go to the Canadian Engineering Competition (<a href=\"http://cec.cfes.ca/\">http://cec.cfes.ca/</a>).</p>\n<p>I need to do a presentation of 15 to 20 minutes. Then the judges can ask me questions during 10 minutes. I will do my presentation on cryonics. I want to invest approximately 12 to 15 hours for preparing it. I will not have time to read everything there is on Internet about cryonics in that time period, so if some of you are familiar with the subject, I would appreciate if you could link me to the best resources on the scientific and ethic aspects of cryonics.</p>\n<p>I will do my presentation with Google Drive Presentation. It will be in French. I will put the link here later on if someone wants to review the presentation (EDIT: the presentation is done; you can see it and comment it on <a href=\"https://docs.google.com/presentation/d/1W6MSvH7iQ_vHHnc3hSDPRHt_EiC5EtjUVZhqYrHH8PY/edit?usp=sharing\">Google Drive</a>). Moreover, I would like to practice my presentation tomorrow in a Google+ hangout if some people want to watch and comment it.</p>\n<p>Thank you.</p>\n<p>P.S.: If there are any Canadian engineering students reading this, check out the competition: there's 7 categories and it's a really interesting competition in my opinion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cbGhSJ3Zpg5umfXk3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 1.4157795168377358e-06, "legacy": true, "legacyId": "24649", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-10T00:03:55.103Z", "modifiedAt": null, "url": null, "title": "Meetup : First Meetup in Jacksonville, FL", "slug": "meetup-first-meetup-in-jacksonville-fl", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rule_and_line", "createdAt": "2012-12-28T15:40:39.942Z", "isAdmin": false, "displayName": "rule_and_line"}, "userId": "FEtpkTTXzu5n6xs65", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kq5gki2wACiq7dRRu/meetup-first-meetup-in-jacksonville-fl", "pageUrlRelative": "/posts/Kq5gki2wACiq7dRRu/meetup-first-meetup-in-jacksonville-fl", "linkUrl": "https://www.lesswrong.com/posts/Kq5gki2wACiq7dRRu/meetup-first-meetup-in-jacksonville-fl", "postedAtFormatted": "Sunday, November 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Meetup%20in%20Jacksonville%2C%20FL&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Meetup%20in%20Jacksonville%2C%20FL%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq5gki2wACiq7dRRu%2Fmeetup-first-meetup-in-jacksonville-fl%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Meetup%20in%20Jacksonville%2C%20FL%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq5gki2wACiq7dRRu%2Fmeetup-first-meetup-in-jacksonville-fl", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq5gki2wACiq7dRRu%2Fmeetup-first-meetup-in-jacksonville-fl", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tb'>First Meetup in Jacksonville, FL</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">869 Stockton St, Jacksonville, FL 32204 (Bold Bean Coffee Roasters)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Jacksonville's a pretty big town, but not much represented in LessWrong land. I'm looking to meet folks who also live here and are similarly interested in LW / CFAR / MIRI / etc.\nThis first meetup will likely be a socializing event - folks getting to know other folks and gauge interest.\nPlease leave a comment if you're interested in a LW meetup in Jacksonville, even if you can't attend one in the next weeks/months.\nAnd remember, (almost) everyone is welcome, especially newbies!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tb'>First Meetup in Jacksonville, FL</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kq5gki2wACiq7dRRu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.41596644630306e-06, "legacy": true, "legacyId": "24650", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Meetup_in_Jacksonville__FL\">Discussion article for the meetup : <a href=\"/meetups/tb\">First Meetup in Jacksonville, FL</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">869 Stockton St, Jacksonville, FL 32204 (Bold Bean Coffee Roasters)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Jacksonville's a pretty big town, but not much represented in LessWrong land. I'm looking to meet folks who also live here and are similarly interested in LW / CFAR / MIRI / etc.\nThis first meetup will likely be a socializing event - folks getting to know other folks and gauge interest.\nPlease leave a comment if you're interested in a LW meetup in Jacksonville, even if you can't attend one in the next weeks/months.\nAnd remember, (almost) everyone is welcome, especially newbies!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Meetup_in_Jacksonville__FL1\">Discussion article for the meetup : <a href=\"/meetups/tb\">First Meetup in Jacksonville, FL</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Meetup in Jacksonville, FL", "anchor": "Discussion_article_for_the_meetup___First_Meetup_in_Jacksonville__FL", "level": 1}, {"title": "Discussion article for the meetup : First Meetup in Jacksonville, FL", "anchor": "Discussion_article_for_the_meetup___First_Meetup_in_Jacksonville__FL1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-10T07:30:54.279Z", "modifiedAt": null, "url": null, "title": "The Evolutionary Heuristic and Rationality Techniques", "slug": "the-evolutionary-heuristic-and-rationality-techniques", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.267Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xQQvmtPTD85Fv72ux/the-evolutionary-heuristic-and-rationality-techniques", "pageUrlRelative": "/posts/xQQvmtPTD85Fv72ux/the-evolutionary-heuristic-and-rationality-techniques", "linkUrl": "https://www.lesswrong.com/posts/xQQvmtPTD85Fv72ux/the-evolutionary-heuristic-and-rationality-techniques", "postedAtFormatted": "Sunday, November 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Evolutionary%20Heuristic%20and%20Rationality%20Techniques&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Evolutionary%20Heuristic%20and%20Rationality%20Techniques%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxQQvmtPTD85Fv72ux%2Fthe-evolutionary-heuristic-and-rationality-techniques%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Evolutionary%20Heuristic%20and%20Rationality%20Techniques%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxQQvmtPTD85Fv72ux%2Fthe-evolutionary-heuristic-and-rationality-techniques", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxQQvmtPTD85Fv72ux%2Fthe-evolutionary-heuristic-and-rationality-techniques", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1261, "htmlBody": "<p>Nick Bostrom and Anders Sandberg (<a href=\"http://www.nickbostrom.com/evolution.pdf\">2008</a>) have proposed what they call the \"evolutionary heuristic\" for evaluating possible ways to enhance humans. It begins with posing a challenge, the \"evolutionary optimality challenge\" or EOC: \"if the proposed intervention would result in an enhancement, why have we not already evolved to be that way?\"</p>\n<p>They write that there seem to be three main categories of answers to this challenge (what follows are abbreviated quotes, see original paper for full explanation):</p>\n<ul>\n<li>Changed tradeoffs:&nbsp;\"Evolution 'designed' the system for operation in one type of environment, but now we wish to deploy it in a very different type of environment...\"</li>\n<li>Value discordance: \"There is a discrepancy between the standards by which evolution measured the quality of her work, and the standards that we wish to apply...\"</li>\n<li>Evolutionary restrictions: \"We have access to various tools, materials, and techniques that were unavailable to evolution...\"</li>\n</ul>\n<p>In their original paper, Bostrom and Sandberg are interested in biological interventions like drugs and embryo selection, but it seems that their heuristic could also tell us a lot about \"rationality techniques,\" i.e. methods of trying to become more rational expressible in the form of how-to advice, like what you often find advocated here at LessWrong or by <a href=\"http://rationality.org\">CFAR</a>.</p>\n<p>Applying the evolutionary heuristic to rationality techniques supports the value of things like statistics, science, and prediction markets. However, it also gives us reason to doubt that a rationality technique is likely to be effective when it does't have any good answer to the EOC.<a id=\"more\"></a></p>\n<p>Let's start with values dissonance. I've <a href=\"/lw/9e7/two_kinds_of_irrationality_and_how_to_avoid_one/\">previously noted</a> that much human irrationality seems to be evolutionarily adaptive: \"We have evolved to have an irrationally inflated view of ourselves, so as to better sell others on that view.\" That suggests that if you value truth more than inclusive fitness, you might want to take steps to counteract that tendency, say by actively trying to force yourself to ignore how having various beliefs will affect your self-image or others' opinions of you. (I spell this idea out a little more carefully at the previous link).</p>\n<p>But it seems like the kind of rationality techniques discussed at LessWrong generally don't fall into the \"values dissonance\" category. Rather, if they make any sense at all, they're going to make sense because of differences between the modern environment environment and the ancestral environment. That is, they fall under the category of \"changed tradeoffs.\" (Note: it's unclear to me how the category of \"evolutionary restrictions\" could apply to rationality techniques. Suggestions?)</p>\n<p>For example, consider the <a href=\"http://en.wikipedia.org/wiki/Availability_heuristic\">availability heuristic</a>. This is the thing that makes people wrongly assume the risk of getting attacked by a shark when you go swimming is really high because of one memorable news story they saw about a shark attack. But if you think about it, the availability heuristic probably wasn't that much of a problem 100,000 years ago on the African savannah. Back then, if you heard a story about someone getting eaten by a lion, it was probably because someone in your band or a neighboring band had gotten eaten by a lion. That probably meant the chance of getting eaten by a lion was non-trivial in the area where you were and you needed to watch out for lions.</p>\n<p>On the other hand, if you're a modern American and you hear a story about someone getting eaten by a shark, it was probably because you heard about it on the news. Maybe it happened hundreds of miles away in Florida. News outlets selectively report sensational stories, so for all you know that was the only person to get eaten by a shark out of 300 million Americans that year, maybe even in the past few years. Thus it is written: don't try to judge the frequency of events based on how often you hear about them on the news; use Google to find the actual statistics.</p>\n<p>The value&mdash;and counter-intuitiveness&mdash;of a lot of scientific techniques seems similarly explicable. Randomized, double-blind, placebo-controlled studies with 1000 subjects are hard to do if you're a hunter-gatherer band with 50 members. Even when primitive hunter-gatherers could have theoretically done a particular experiment, rigorous scientific experiments are a lot of work. They may not pass cost-benefit analysis if it's just your band that will be using the results. In order for science to be worthwhile, it helps to have a printing press that you can use to share your findings all over the world.</p>\n<p>A third example is prediction markets, indeed markets in general. In a <a href=\"http://www.overcomingbias.com/2013/10/finance-celebrated-ignored.html\">post</a> last month, Robin Hanson writes (emphasis mine):</p>\n<blockquote>\n<p>Speculative markets generally do an excellent job of aggregating information...</p>\n<p>Yet even though this simple fact seems too obvious for finance experts to mention, the vast majority of the rest of news coverage and commentary on all other subjects today, and pretty much every day, will act as if they disagreed. Folks will discuss and debate and disagree on other subjects, and talk as if the best way for most of us to form accurate opinions on such subjects is to listen to arguments and commentary offered by various pundits and experts and then decide who and what we each believe. <strong>Yes this is the way our ancestors did it, and yes this is how we deal with disagreements in our personal lives, and yes this was usually the best method.</strong></p>\n<p>But by now we should know very well that we would get more accurate estimates more cheaply on most widely discussed issues of fact by creating (and legalizing), and if need be subsidizing, speculative betting markets on such topics. This isn&rsquo;t just vague speculation, this is based on very well established results in finance, results too obvious to seem worth mentioning when experts discuss finance. Yet somehow the world of media continues to act is if it doesn&rsquo;t know. Or perhaps it doesn&rsquo;t care; punditry just isn&rsquo;t about accuracy.</p>\n</blockquote>\n<p>The evolutionary heuristic suggests a different explanation for reluctance to use prediction markets: the fact that \"listen to arguments and form your own opinion\" was the best method we had on the African savannah meant we evolved to use it.&nbsp;Thus, other methods, like prediction markets, feel deeply counter-intuitive, even for people who can appreciate their merits in the abstract.</p>\n<p>In short, the evolutionary heuristic supports what many people have already concluded for other reasons: most people could do better at forming their view of the world by relying more on statistics, science, and (when available) prediction markets. People likely fail to rely on them as much as they should, because those were not available in the ancestral environment, and therefore relying on them does not come naturally to us.</p>\n<p>There's a flip side to this, though, that the evolutionary heuristic might suggest certain rationality techniques are unlikely to work. In an earlier version of this post, I suggested a <a href=\"http://rationality.org/2013/09/27/surprise-as-a-cue-to-probability-experiment-1/\">CFAR experiment</a> in trying to improve people's probability estimates as an example of such a rationality technique. But as <a href=\"/lw/itk/the_evolutionary_heuristic_and_rationality/a17v\">Benja</a> pointed out in the comments, our ancestors faced little selection pressure for accurate <em>verbal </em>probability estimates, which suggests there might be a lot of room to improve people's verbal probability estimates.</p>\n<p>On the other hand, given that our ancestors managed to be successful without being good at making verbal probability estimates might suggest that rationality techniques based on improving that skill would be unlikely to result in increased performance in areas where the skill isn't obviously relevant. (Yvain's post&nbsp;<a href=\"/lw/9p/extreme_rationality_its_not_that_great/\">Extreme Rationality: It's Not That Great</a>&nbsp;is relevant here.) On the <em>other </em>other hand, maybe abstract reasoning skills like \"making verbal probability estimates\" is generally useful for dealing with evolutionarily novel problems.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xQQvmtPTD85Fv72ux", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "24392", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B3RGZg4KNdsaDC2J4", "LgavAYtzFQZKg95WC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-10T08:59:35.069Z", "modifiedAt": null, "url": null, "title": "A diagram for a simple two-player game", "slug": "a-diagram-for-a-simple-two-player-game", "viewCount": null, "lastCommentedAt": "2020-09-28T01:05:23.439Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xe9KyzvcD3fnAogA7/a-diagram-for-a-simple-two-player-game", "pageUrlRelative": "/posts/xe9KyzvcD3fnAogA7/a-diagram-for-a-simple-two-player-game", "linkUrl": "https://www.lesswrong.com/posts/xe9KyzvcD3fnAogA7/a-diagram-for-a-simple-two-player-game", "postedAtFormatted": "Sunday, November 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20diagram%20for%20a%20simple%20two-player%20game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20diagram%20for%20a%20simple%20two-player%20game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxe9KyzvcD3fnAogA7%2Fa-diagram-for-a-simple-two-player-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20diagram%20for%20a%20simple%20two-player%20game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxe9KyzvcD3fnAogA7%2Fa-diagram-for-a-simple-two-player-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxe9KyzvcD3fnAogA7%2Fa-diagram-for-a-simple-two-player-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p>(Copied from <a href=\"http://mindsarentmagic.wordpress.com/2013/11/09/diagrams-for-preferences-matrices/\">my blog</a>)</p>\n<p>I always have a hard time making sense of preference matrices in two-player games. Here are some diagrams I drew to make it easier. This is a two-player game:</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; line-height: 1.5em; text-align: justify;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/2.png\"><img class=\"size-full wp-image-112 aligncenter\" style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent; max-width: 100%; height: auto; clear: both; display: block;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/1.png?w=500\" alt=\"1\" /></a></p>\n<p>North wants to end up on the northernmost point, and East on the eastmost. North goes first, and chooses which of the two bars will be used; East then goes second and chooses which point on the bar will be used.</p>\n<p>North knows that East will always choose the easternmost point on the bar picked, so one of these two:</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: justify; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/2.png\"><img class=\"size-full wp-image-112 aligncenter\" style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent; max-width: 100%; height: auto; clear: both; display: block;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/2.png?w=500\" alt=\"2\" /></a></p>\n<p>North checks which of the two points is further north, and so chooses the leftmost bar, and they both end up on this point:</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: justify; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/3.png\"><img class=\"size-full wp-image-111 aligncenter\" style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent; max-width: 100%; height: auto; clear: both; display: block;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/3.png?w=500\" alt=\"3\" /></a></p>\n<p>Which is sad, because there&rsquo;s a point north-east of this that they&rsquo;d both prefer. Unfortunately, North knows that if they choose the rightmost bar, they&rsquo;ll end up on the easternmost, southernmost point.</p>\n<p>Unless East can somehow precommit to not choosing this point:</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: justify; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/4.png\"><img class=\"size-full wp-image-110 aligncenter\" style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent; max-width: 100%; height: auto; clear: both; display: block;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/4.png?w=500\" alt=\"4\" /></a></p>\n<p>Now East is going to end up choosing one of these two points:</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: justify; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/5.png\"><img class=\"size-full wp-image-109 aligncenter\" style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent; max-width: 100%; height: auto; clear: both; display: block;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/5.png?w=500\" alt=\"5\" /></a></p>\n<p>So North can choose the rightmost bar, and the two players end up here, a result both prefer:</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: justify; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/6.png\"><img class=\"size-full wp-image-108 aligncenter\" style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent; max-width: 100%; height: auto; clear: both; display: block;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/6.png?w=500\" alt=\"6\" /></a></p>\n<p>I won&rsquo;t be surprised if this has been invented before, and it may even be superceded &ndash; please do comment if so :)</p>\n<p>Here&rsquo;s a game where East has to both promise and threaten to get a better outcome:</p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: center; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/01-13_22-30.png\"><img style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/01-13_22-30.png?w=256&amp;h=256\" alt=\"0,1-1,3_2,2-3,0\" width=\"256\" height=\"256\" /></a></p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: justify; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/01-13_22-30-x.png\"><img class=\"alignnone size-full wp-image-118 aligncenter\" style=\"margin: 5px auto; padding: 0px; border: 1px solid #cccccc; outline: 0px; vertical-align: baseline; background-color: transparent; max-width: 100%; height: auto; clear: both; display: block;\" src=\"http://mindsarentmagic.files.wordpress.com/2013/11/01-13_22-30-x.png?w=500\" alt=\"0,1-1,3_2,2-3,0-x\" /></a></p>\n<p style=\"margin: 0px 0px 1.7em; padding: 0px; border: 0px; outline: 0px; font-size: 14px; vertical-align: baseline; line-height: 1.5em; color: #333333; text-align: justify; font-family: Palatino, 'Times New Roman', serif;\"><a style=\"margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background-color: transparent; color: #9c8a6a;\" href=\"http://mindsarentmagic.files.wordpress.com/2013/11/01-13_22-30-x.png\"></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xe9KyzvcD3fnAogA7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 56, "extendedScore": null, "score": 0.00014, "legacy": true, "legacyId": "24651", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-10T13:59:31.709Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:33.624Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eBbJhA4q3PBHD2wcB/meetup-melbourne-social-meetup-12", "pageUrlRelative": "/posts/eBbJhA4q3PBHD2wcB/meetup-melbourne-social-meetup-12", "linkUrl": "https://www.lesswrong.com/posts/eBbJhA4q3PBHD2wcB/meetup-melbourne-social-meetup-12", "postedAtFormatted": "Sunday, November 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBbJhA4q3PBHD2wcB%2Fmeetup-melbourne-social-meetup-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBbJhA4q3PBHD2wcB%2Fmeetup-melbourne-social-meetup-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBbJhA4q3PBHD2wcB%2Fmeetup-melbourne-social-meetup-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tc'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 November 2013 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's Social Meetup will be running as usual on the third Friday of the month. All are welcome from 6:30pm.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Just ring number 5 on the buzzer when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tc'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eBbJhA4q3PBHD2wcB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4167790505853046e-06, "legacy": true, "legacyId": "24652", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/tc\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 November 2013 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's Social Meetup will be running as usual on the third Friday of the month. All are welcome from 6:30pm.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Just ring number 5 on the buzzer when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/tc\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-10T14:26:53.257Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Meetup: Goals spreadsheet, political advocacy as effective altruism, scary rationalist stories", "slug": "meetup-dc-meetup-goals-spreadsheet-political-advocacy-as", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QkLB96h2MouCLsKZ5/meetup-dc-meetup-goals-spreadsheet-political-advocacy-as", "pageUrlRelative": "/posts/QkLB96h2MouCLsKZ5/meetup-dc-meetup-goals-spreadsheet-political-advocacy-as", "linkUrl": "https://www.lesswrong.com/posts/QkLB96h2MouCLsKZ5/meetup-dc-meetup-goals-spreadsheet-political-advocacy-as", "postedAtFormatted": "Sunday, November 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Meetup%3A%20Goals%20spreadsheet%2C%20political%20advocacy%20as%20effective%20altruism%2C%20scary%20rationalist%20stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Meetup%3A%20Goals%20spreadsheet%2C%20political%20advocacy%20as%20effective%20altruism%2C%20scary%20rationalist%20stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkLB96h2MouCLsKZ5%2Fmeetup-dc-meetup-goals-spreadsheet-political-advocacy-as%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Meetup%3A%20Goals%20spreadsheet%2C%20political%20advocacy%20as%20effective%20altruism%2C%20scary%20rationalist%20stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkLB96h2MouCLsKZ5%2Fmeetup-dc-meetup-goals-spreadsheet-political-advocacy-as", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkLB96h2MouCLsKZ5%2Fmeetup-dc-meetup-goals-spreadsheet-political-advocacy-as", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/td'>DC Meetup: Goals spreadsheet, political advocacy as effective altruism, scary rationalist stories</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Roger's description:</p>\n\n<p>\"We will be: Talking about goals and trying to revive the goals spreadsheet Discussing several posts on Givewell's blog about political advocacy as effective altruism And (if people have them) telling Haloween stories, as per this thread (a bit late, I know, but why not?):\"\n<a href=\"http://lesswrong.com/lw/iyb/halloween_thread_rationalists_horrors/\" rel=\"nofollow\">http://lesswrong.com/lw/iyb/halloween_thread_rationalists_horrors/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/td'>DC Meetup: Goals spreadsheet, political advocacy as effective altruism, scary rationalist stories</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QkLB96h2MouCLsKZ5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.4168056703915005e-06, "legacy": true, "legacyId": "24653", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Goals_spreadsheet__political_advocacy_as_effective_altruism__scary_rationalist_stories\">Discussion article for the meetup : <a href=\"/meetups/td\">DC Meetup: Goals spreadsheet, political advocacy as effective altruism, scary rationalist stories</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Roger's description:</p>\n\n<p>\"We will be: Talking about goals and trying to revive the goals spreadsheet Discussing several posts on Givewell's blog about political advocacy as effective altruism And (if people have them) telling Haloween stories, as per this thread (a bit late, I know, but why not?):\"\n<a href=\"http://lesswrong.com/lw/iyb/halloween_thread_rationalists_horrors/\" rel=\"nofollow\">http://lesswrong.com/lw/iyb/halloween_thread_rationalists_horrors/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Goals_spreadsheet__political_advocacy_as_effective_altruism__scary_rationalist_stories1\">Discussion article for the meetup : <a href=\"/meetups/td\">DC Meetup: Goals spreadsheet, political advocacy as effective altruism, scary rationalist stories</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Meetup: Goals spreadsheet, political advocacy as effective altruism, scary rationalist stories", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Goals_spreadsheet__political_advocacy_as_effective_altruism__scary_rationalist_stories", "level": 1}, {"title": "Discussion article for the meetup : DC Meetup: Goals spreadsheet, political advocacy as effective altruism, scary rationalist stories", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Goals_spreadsheet__political_advocacy_as_effective_altruism__scary_rationalist_stories1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NPqBcdqdh9rauZd8W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-10T17:32:00.753Z", "modifiedAt": null, "url": null, "title": "Design-space traps: mapping the utility-design trajectory space", "slug": "design-space-traps-mapping-the-utility-design-trajectory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "joaolkf", "createdAt": "2010-02-24T18:52:27.966Z", "isAdmin": false, "displayName": "joaolkf"}, "userId": "woC2b5rav5sGrAo3E", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kPuMGajcEdbkzj4HS/design-space-traps-mapping-the-utility-design-trajectory", "pageUrlRelative": "/posts/kPuMGajcEdbkzj4HS/design-space-traps-mapping-the-utility-design-trajectory", "linkUrl": "https://www.lesswrong.com/posts/kPuMGajcEdbkzj4HS/design-space-traps-mapping-the-utility-design-trajectory", "postedAtFormatted": "Sunday, November 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Design-space%20traps%3A%20mapping%20the%20utility-design%20trajectory%20space&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADesign-space%20traps%3A%20mapping%20the%20utility-design%20trajectory%20space%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkPuMGajcEdbkzj4HS%2Fdesign-space-traps-mapping-the-utility-design-trajectory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Design-space%20traps%3A%20mapping%20the%20utility-design%20trajectory%20space%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkPuMGajcEdbkzj4HS%2Fdesign-space-traps-mapping-the-utility-design-trajectory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkPuMGajcEdbkzj4HS%2Fdesign-space-traps-mapping-the-utility-design-trajectory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1006, "htmlBody": "<p style=\"margin-top: 0cm; margin-right: 0cm; margin-bottom: 10.0pt; margin-left: 0cm; text-align: justify;\"><span style=\"font-family: Arial, sans-serif;\">This is a small section on a paper I'm writing on moral enhancement. I'm trying to briefly summarize some of the points which were already made concerning local optima in evolutionary process and safety regarding taking humanity out of those local optima. You might find the text helpful in that it summarizes a very important concept. I don't think there's nothing new here, but I hope the way I tried to more properly phrase the utility-design trajectory space topology at the end can be fruitful. I would appreciate any insights you might have about that formulation in the end, how to better develop it more rigorously and some consequences. I do have some ideas, but I would want to hear what you have to say first. &nbsp;Any other kind of general feedback on the text is also welcomed. But keep in mind this is just a section of a larger paper and I'm mainly interested in how to develop and what are the consequences of the framework at the end, rather than in properly developing any points in the middle.</span></p>\n<p style=\"margin-top: 0cm; margin-right: 0cm; margin-bottom: 10.0pt; margin-left: 0cm; text-align: justify;\"><span style=\"font-family: Arial, sans-serif;\">Local optima are points where every nearby reachable positions are worse off, but there is at least one far away position which is vastly better. A strong case has been made that evolution often gets stuck on such local optima. In evolutionary processes, fitness is a monotonic function, i.e., it will necessarily increase or be maintained, any decrease in fitness will always be selected against. If there are vastly better solutions (for, e.g., solving cooperation problems) but in order to achieve those solutions organisms would have to pass through a lesser fit step, evolution will never reach that vastly better configuration. Evolutionary processes are limited by the topology of the fitness-design trajectory space, it can only go from design <em>x</em> to design <em>y</em> if there is at least one trajectory from <em>x</em> to <em>y</em> which is flat or ascendant, any trajectory momentarily descendent cannot be taken by the evolutionary processes. Say one is on the cyan ring ridge of the colored graphic.<img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_j0u_0.png?v=4bbc3db5a3688d03c0c126b0a90dfce4\" alt=\"\" width=\"460\" height=\"370\" />&nbsp;Although there is a vastly better configuration on the red peak, one would have to travel through the blue moat in order to get there. Unless one is a process who could pass through a sharp decrease in fitness, there would be no way of improving towards the red peak. Evolution is particularly prone to local optima due to fitness monotonicity. Enhancing human beings with the use of technology does not fall prey to the fitness monotonicity or any sort of utility monotonicity in general, we could initially make changes which would be harmful in order to latter achieve a vastly better configuration. Therefore, it seems plausible there would be a technological path out of evolution&rsquo;s local optimum whereby we could rescue our species from these evolutionary imprisonments. Moreover, it is considered evolutionary local optima can be easily identifiable provided a careful, evolutionary and technical informed analysis is made. Hence these would be low-hanging fruits in the task for improving evolutionary products such as humans, easily accessible and able to produce great advances to humanity with little effort.</span></p>\n<p style=\"margin-top: 0cm; margin-right: 0cm; margin-bottom: 10.0pt; margin-left: 0cm; text-align: justify;\"><span style=\"font-family: Arial, sans-serif;\">Nevertheless, it should be noted getting out of evolutionary local optima might not always be easy or even possible. Fitness does have a <em>relatively</em> strong correlation with overall human utility. And although human intelligence is not so dull as evolutionary process and does accept a decrease in utility in order to achieve a better design in the end, if the downward moat is deep enough, the risk of catastrophe - or much worse, extinction -, might not be worth taking. At least by being monotonic on a dimension correlated with utility, evolution was able to rightly avoid extreme losses. Perform widespread willy-nilly human enhancement, and we might fall on the moat guarding utility-design space garden&rsquo;s delicious low-hanging fruits and not come back up. Particularly so in the case of moral enhancement, there is a self-reinforcing aspect of changing morality, motivations, values and desires. It might be the case tampering with deep and fundamental human morality is irreversible, because once we fundamentally value something else, we would not have any compelling reason for wanting to come back to our old values, desires or aspirations. Thus, it seems there are indeed cases where a small step past the edge of the moat will lead us to an irreversible path. To correctly map how each technology shapes utility-design trajectory space topology is a task deeply needed in order to carefully avoid falling on moats while attempting to reach local optima low-hanging fruits, or on even more dangerous existential holes. We ought to better get stuck at local optima than absolute minima.&nbsp;</span></p>\n<p style=\"margin-top: 0cm; margin-right: 0cm; margin-bottom: 10.0pt; margin-left: 0cm; text-align: justify;\"><span style=\"font-family: Arial, sans-serif;\">Utility-design trajectory space could be more properly defined as a space on R<sup>n+u</sup> , a point there would use n-coordinates to locate all physically possible designs in all relevant dimensions n, it is defined by the laws of physics and by an utility function on <em>u</em>. A point will correspond to a design <em>a</em> iff all its neighbouring points <em>x</em> correspond to designs one physical step away from design <em>a</em>. Emergent designer processes such as evolution, human enhancement and AIs draw shapes on R<sup>n+u</sup> by connecting points that are linked by one possible step under that process. Evolution&rsquo;s hand is monotonic on dimension <em>f</em>, fitness, which makes for a pretty clumsy drawing. Biochemical human enhancement can more freely vary on <em>f</em>, but might contain other constraints elsewhere, that, e.g., uploaded minds would not. Extinctions correspond to singularities on <em>u</em>, once reached no other point is reachable, it designates lack of design. These points that can be reached but cannot reach need to be correctly mapped. It would also be relevant to investigate how each technology draws its specific shape on design space. Using <em>u</em> as some height analogue, some technologies might be inherently prone to shape moats with peaks on the middle, extinctions holes, effortless utility maximizing curves and so on. I believe moral enhancement draws a particularly bumpy hole-prone shape. FAI an ever utility-ascending shape, with all mishaps being existential holes.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kPuMGajcEdbkzj4HS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -3, "extendedScore": null, "score": 1.4169858161422543e-06, "legacy": true, "legacyId": "24654", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-10T20:23:51.037Z", "modifiedAt": null, "url": null, "title": "Meetup : Saint-Petersburg: Game Event", "slug": "meetup-saint-petersburg-game-event", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.952Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "efim", "createdAt": "2013-04-14T00:57:28.743Z", "isAdmin": false, "displayName": "efim"}, "userId": "Y8azdhZD6fvWdGwaB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fY4tAayKjtaydq9f9/meetup-saint-petersburg-game-event", "pageUrlRelative": "/posts/fY4tAayKjtaydq9f9/meetup-saint-petersburg-game-event", "linkUrl": "https://www.lesswrong.com/posts/fY4tAayKjtaydq9f9/meetup-saint-petersburg-game-event", "postedAtFormatted": "Sunday, November 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saint-Petersburg%3A%20Game%20Event&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saint-Petersburg%3A%20Game%20Event%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfY4tAayKjtaydq9f9%2Fmeetup-saint-petersburg-game-event%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saint-Petersburg%3A%20Game%20Event%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfY4tAayKjtaydq9f9%2Fmeetup-saint-petersburg-game-event", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfY4tAayKjtaydq9f9%2Fmeetup-saint-petersburg-game-event", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/te'>Saint-Petersburg: Game Event</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us for evening of games from lesswrong! For our third meetup we will try out new format - maximizing games and exercises!\nWe will be meeting in PMG cafe, to find it consult our mailing list or contact me (see below)</p>\n\n<p>16:00 - 17:00 - first part\nsort of unregulated discussion on lesswrong related topics and\\or selected posts that will be determined later and announced via mailing list. (see below)\nSuggestions are welcomed and can be made in [<a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a>]</p>\n\n<p>17:00 - 19:00 - second part\nGame of Rationalist Taboo.\n[<a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Rationalist_taboo</a>]</p>\n\n<p>19:00 - 21:00 - last part\nGames and excersises on calibration:\nConfidence intervals - solo, one-round Aumann for warm-up and as main course - Paranoid Debating!\n[<a href=\"http://wiki.lesswrong.com/wiki/The_Aumann_Game\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/The_Aumann_Game</a>] [<a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Paranoid_debating</a>]</p>\n\n<p>This is our mailing list - <a href=\"https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg</a>, subscribe for announcements, updates and other meetup-related information.\nWe also have group in <a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a>\nYou can contact me on 8-(911)-843-56-44</p>\n\n<p>You can come to beginning of any announced part of this meetup, though exact timing is not guaranteed each game will not start earlier than specified.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/te'>Saint-Petersburg: Game Event</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fY4tAayKjtaydq9f9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -2, "extendedScore": null, "score": 1.4171530686629322e-06, "legacy": true, "legacyId": "24655", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Game_Event\">Discussion article for the meetup : <a href=\"/meetups/te\">Saint-Petersburg: Game Event</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us for evening of games from lesswrong! For our third meetup we will try out new format - maximizing games and exercises!\nWe will be meeting in PMG cafe, to find it consult our mailing list or contact me (see below)</p>\n\n<p>16:00 - 17:00 - first part\nsort of unregulated discussion on lesswrong related topics and\\or selected posts that will be determined later and announced via mailing list. (see below)\nSuggestions are welcomed and can be made in [<a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a>]</p>\n\n<p>17:00 - 19:00 - second part\nGame of Rationalist Taboo.\n[<a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Rationalist_taboo</a>]</p>\n\n<p>19:00 - 21:00 - last part\nGames and excersises on calibration:\nConfidence intervals - solo, one-round Aumann for warm-up and as main course - Paranoid Debating!\n[<a href=\"http://wiki.lesswrong.com/wiki/The_Aumann_Game\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/The_Aumann_Game</a>] [<a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Paranoid_debating</a>]</p>\n\n<p>This is our mailing list - <a href=\"https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg</a>, subscribe for announcements, updates and other meetup-related information.\nWe also have group in <a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a>\nYou can contact me on 8-(911)-843-56-44</p>\n\n<p>You can come to beginning of any announced part of this meetup, though exact timing is not guaranteed each game will not start earlier than specified.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Game_Event1\">Discussion article for the meetup : <a href=\"/meetups/te\">Saint-Petersburg: Game Event</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saint-Petersburg: Game Event", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Game_Event", "level": 1}, {"title": "Discussion article for the meetup : Saint-Petersburg: Game Event", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Game_Event1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-11T12:35:40.100Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin", "slug": "meetup-berlin-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:33.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yzKcJc3SCEQkjY3vZ/meetup-berlin-0", "pageUrlRelative": "/posts/yzKcJc3SCEQkjY3vZ/meetup-berlin-0", "linkUrl": "https://www.lesswrong.com/posts/yzKcJc3SCEQkjY3vZ/meetup-berlin-0", "postedAtFormatted": "Monday, November 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzKcJc3SCEQkjY3vZ%2Fmeetup-berlin-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzKcJc3SCEQkjY3vZ%2Fmeetup-berlin-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzKcJc3SCEQkjY3vZ%2Fmeetup-berlin-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tf'>Berlin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 January 2019 01:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berlin, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is a long term meetup announcement for Berlin. We're not sure we'll actually meet on the date given here, but there'll definitely be meetups (usually at least monthly) in the meantime.</p>\n\n<p>Please join our <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tf'>Berlin</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yzKcJc3SCEQkjY3vZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.418099610721508e-06, "legacy": true, "legacyId": "24664", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin\">Discussion article for the meetup : <a href=\"/meetups/tf\">Berlin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 January 2019 01:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berlin, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is a long term meetup announcement for Berlin. We're not sure we'll actually meet on the date given here, but there'll definitely be meetups (usually at least monthly) in the meantime.</p>\n\n<p>Please join our <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin1\">Discussion article for the meetup : <a href=\"/meetups/tf\">Berlin</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin", "anchor": "Discussion_article_for_the_meetup___Berlin", "level": 1}, {"title": "Discussion article for the meetup : Berlin", "anchor": "Discussion_article_for_the_meetup___Berlin1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-11T14:55:42.135Z", "modifiedAt": null, "url": null, "title": "Reduced impact AI: no back channels", "slug": "reduced-impact-ai-no-back-channels", "viewCount": null, "lastCommentedAt": "2021-08-30T20:26:35.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gzQT5AAw8oQdzuwBG/reduced-impact-ai-no-back-channels", "pageUrlRelative": "/posts/gzQT5AAw8oQdzuwBG/reduced-impact-ai-no-back-channels", "linkUrl": "https://www.lesswrong.com/posts/gzQT5AAw8oQdzuwBG/reduced-impact-ai-no-back-channels", "postedAtFormatted": "Monday, November 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reduced%20impact%20AI%3A%20no%20back%20channels&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReduced%20impact%20AI%3A%20no%20back%20channels%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgzQT5AAw8oQdzuwBG%2Freduced-impact-ai-no-back-channels%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reduced%20impact%20AI%3A%20no%20back%20channels%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgzQT5AAw8oQdzuwBG%2Freduced-impact-ai-no-back-channels", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgzQT5AAw8oQdzuwBG%2Freduced-impact-ai-no-back-channels", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2969, "htmlBody": "<p><em>A putative new idea for AI control; index&nbsp;<a href=\"/lw/lt6/newish_ai_control_ideas/\">here</a>.</em></p>\n<p>This post presents a further development of the <a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">reduced impact AI</a> approach, bringing in some novel ideas and setups that allow us to accomplish more. It still isn't a complete approach - further development is needed, which I will do when I return to the concept - but may already allow certain types of otherwise dangerous AIs to be made safe. And this time, without needing to <a href=\"/r/all/lw/gmx/domesticating_reduced_impact_ais/\">encase them</a> in clouds of chaotic anti-matter!</p>\n<p>Specifically, consider the following scenario. A comet is heading towards Earth, and it is generally agreed that a collision is suboptimal for everyone involved. Human governments have come together in peace and harmony to build a <a href=\"http://www.youtube.com/watch?v=nw3dWOssOhs\">giant laser on the moon</a>&nbsp;- this could be used to vaporise the approaching comet, except there isn't enough data to aim it precisely. A superintelligent AI programmed with a naive \"save all humans\" utility function is asked to furnish the coordinates to aim the laser. The AI is mobile and not contained in any serious way. Yet the AI furnishes the coordinates - and nothing else - and then turns itself off completely, not optimising anything else.</p>\n<p>The rest of this post details an approach that could might make that scenario possible. It is slightly complex: I haven't found a way of making it simpler. Most of the complication comes from attempts to precisely define the needed counterfactuals. We're trying to bring rigour to inherently un-sharp ideas, so some complexity is, alas, needed. I will try to lay out the ideas with as much clarity as possible - first the ideas to constrain the AI, then ideas as to how to get some useful work out of it anyway. Classical mechanics (general relativity) will be assumed throughout. As in a <a href=\"/r/all/lw/gmx/domesticating_reduced_impact_ais/\">previous post</a>, the approach will be illustrated by a drawing of unsurpassable elegance; the rest of the post will aim to clarify everything in the picture:<a id=\"more\"></a></p>\n<p><img src=\"http://images.lesswrong.com/t3_iyx_0.png?v=7088f9d971d26d10a9c5af8967cfd0b2\" alt=\"\" width=\"672\" height=\"1058\" /></p>\n<p>&nbsp;</p>\n<h2>Can Super-God detect whether God existed?</h2>\n<p>If we say that Mr. Y has had a reduced impact upon the world, what do we mean by that? It's hard to define, but here is a sufficient condition: if we come back in 50 years time and cannot find any evidence that Mr. Y ever existed, then he probably had a minute impact. If a very thorough analysis of the whole world and surrounding solar system, fed into the most sophisticated computer systems, fails to detect any evidence for or against the existence of Mr. Y, then he certainly had a reduced impact.</p>\n<p>This is the heart of the approach. The idea is to examine a slice of the future, and if any effect of the AI's existence is undetectable in that slice, then the AI had a (very) reduced impact. But who would do this \"examining\"? The AI might be able to hide its impact from any human examiner, and there might not even be any humans left alive to examine anything. No, the examination has to be a counterfactual examination, carried out by the AI itself p or better yet, a smarter AI. A (hypothetical) smarter version of the AI will be helping to constrain the AI's current behaviour.</p>\n<p>&nbsp;</p>\n<h2>Scouring the future for traces of the AI</h2>\n<p>We will assume that the AI has an understanding of physics, and that the heart of the AI is a probability module, capable of probabilistic reasoning about physical systems and other things. This module comes in two versions: the module P, which is an idealised version, which has almost unlimited storage space and time with which to answer the question. And a bounded module P', the realistic, constrained one that the AI uses (these modules give rise to expectation operators E and E'). Crucially, the module P' can give sensible answers to what the output of the module P would be. This is a <a href=\"/lw/h1k/reflection_in_probabilistic_logic/\">reflection principle</a>, allowing the AI to estimate what kind of answers it would give if it had (much) more time to think about it.</p>\n<p>This reflection principle isn't magic: P' can't mysteriously compute the answer to a question by peering into P. But P' could, for instance, estimate whether P will have lower uncertainty than P'. It could estimate how much better P would be at particular task, and other similar estimates.</p>\n<p>This P will be the strong version of the AI, looking at the future data. And P' will allow the AI to estimate, now, what the answer of P might be. How exactly will it do this?</p>\n<p>Define s to be a slice of the future. We could, for instance, say that s consists of any point 50 years into the future (on any time-like geodesic), thickened to one metre, say. There are some subtleties with this description - we might encounter black holes and other singularities - but we'll lay them aside for the moment. The crucial aspect is that s is a barrier: we cannot get from now to a point 60 years in the future, without crossing s at some point.</p>\n<p>Then once we've defined s, define S to be the total of all information present in that slice: the position and velocities of every particle. This S is a noisy estimate, not a perfect one: the positions and velocities are somewhat uncertain. This noise is quite important, as in a deterministic general relativistic universe, it is the only thing preventing P from knowing the past perfectly. But there need not be much noise for the past to become blurred enough.</p>\n<p>From the perspective of today, S is a random variable who's values are unknown: we can estimate some of its properties, but it will depend on many things, not least of all the decisions that we (and the AI) will make.</p>\n<p>What would be a bad S, from the reduced impact perspective? One where it would be extremely obvious, upon examination, that the AI was present in the past. This bad S would lead to a large likelihood ratio:</p>\n<p style=\"padding-left: 60px;\">P(S|\"The AI existed\") / P(S|\"The AI never exited).</p>\n<ul>\n</ul>\n<p>If that ratio is large, there is a lot of evidence in slice s that the AI existed. Conversely, if the ratio is close to 1, there is little evidence of the AI's existence.</p>\n<p>&nbsp;</p>\n<h2>A very complicated way of doing nothing</h2>\n<p>We're now going to add more details to make that approach rigorous. First of all, how can we rigorise \"The AI existed\" and its negation? My favourite method for defining such a counterfactual is to have an event X - maybe an electron travelling down a wire - that will turn on the AI initially. The process is slightly noisy, so there is a tiny chance that X will fail; if it does fail, then the AI is not turned on at all. If we designate by B a collection of background knowledge about the world before the AI was turned on, we can define \"The AI existed\" as \"X and B\" and \"The AI never existed\" as \"&not;X and B\". Note that we are banning anthropic reasoning, allowing the AI to consider hypotheticals in which it didn't exist. If P and P' can do physics simulations, this should be fine: &not;X is a perfectly acceptable physical situation, with its own future consequences.</p>\n<p>But if the turning on of the AI is done in full view of everyone, with the press recording everything and people twittering excitedly, then it will be blatant that the turning on was successful (or not). Hence we should put the AI in a room - not to constrain it, like a boxed oracle, but to give it some privacy. Thus it could be plausible that the future - the data S - would not have any trace of the AI's existence. After an hour's time, the AI's mind will be thoroughly overwritten, and the room will be given a vigorous shake. This is not to constrain the AI: we'll assume the AI is mobile and has access to enough tools inside the room to do whatever it wants. The setup is only to <em>give the AI the option of pretending it never existed</em> - of arranging events so that there is no evidence of it being turned on. Thus it becomes possible that the AI can act to minimise the future ratio P(S|X,B)/P(S|&not;X,B), especially if we make the data in S somewhat noisy.</p>\n<p>But that involves P, not P', and involves the data S which the AI doesn't know. So the AI needs to estimate the expected value of the ratio, using its expectation estimator E' (powered by P', which can estimate P and S):</p>\n<p style=\"padding-left: 60px;\">E'(P(S|X,B)/P(S|&not;X,B))</p>\n<p>That expression means the expectation, as best the AI can currently calculate it, of the ratios of probabilities that would be calculated by a (hypothetical) super version of itself, with access to all the data S about the future slice s. That's worth digesting for a second. The contents of S will be determined by the AI's actions now, so the AI is in effect trying to pick the action that would make it hard for a super version of itself to tell whether the AI had ever been turned on. Thus it will try to minimise its impact on the world, by perfectly mimicking non-existence: by choosing the action a that minimises the above expected ration. This is</p>\n<p style=\"padding-left: 60px;\">argmax<sub>a</sub>&nbsp;-E'(P(S<sup>a</sup>|X,B)/P(S<sup>a</sup>|&not;X,B) | a)</p>\n<p>Here we've added a minus sign so we're maximising the negative of the ratio (entirely equivalent with minimising the ratio), and added superscript to S to signify that the contents of S depend (probabilistically) on the action a.</p>\n<p>As it stands, this is a complicated way of making the AI do absolutely nothing: it will seek to make the world indistinguishable from one in which it wasn't turned on. We haven't even added a utility function yet! So let's do that, adding a U representing the AI's goals, and keeping the above expression as a penalty function to ensure it accomplishes these goals with a minimum of fuss. Let's require that U be bounded between 0 and 1, and then fix a ratio&nbsp;&lambda; such that the AI picks the action following:</p>\n<p style=\"padding-left: 60px;\">argmax<sub>a</sub>&nbsp;E'(U|a) -&lambda;E'(P(S<sup>a</sup>|X,B)/P(S<sup>a</sup>|&not;X,B) | a).</p>\n<p>For most values of &lambda;, this will either be an expected utility maximiser (for &lambda; too small: the utility dominates) or remain a complicated way of doing nothing (for &lambda; to big: the penalty function dominates). It's not too clear what happens in the transition: trading off utility versus penalty is not obvious. We have to find a better way of extracting useful work from the system.</p>\n<p>&nbsp;</p>\n<h2>The output channel: a very specific loophole</h2>\n<p>Part of the problem is that the AI can have a massive effect just by existing and answering a single question correctly - people will be motivated to run out and build more AIs, write philosophical papers about AIs, and so on. In the example that we started the paper off with, the AIs will have a massive impact: it alone will be the difference between the planet getting rendered uninhabitable by the comet or not. So reduced impact seems impossible in these cases.</p>\n<p>For these we devise an output channel. The idea is simple: information along this output channel, and only along this channel, is allowed to affect the future. This means that the content of the output channel is potentially very dangerous and other precautions need to be taken. Still, it can be useful: for instance, we could ask the AI to help us aim our laser, and restrict it's output so that we only get coordinates, no secret info. Then our reduced impact approach would preclude the AI from having any <em>other</em> impacts on the world (i.e. no optimising the future), and we'd get our world saved for free.</p>\n<p>How can we encode this idea? If O is the output information (which will be a function of the AI's actions), we can include it inside the probability ratio, writing P(S<sup>a</sup>|X,O<sup>a</sup>,B)/P(S<sup>a</sup>|&not;X,O<sup>a</sup>,B) (with the little superscript a to remind us that O depends on the AI's actions). This has the right formal structure: if the ratio is close to one, then it means that if you know the background information B <em>and</em> the output information O<sup>a</sup>, then observing S<sup>a</sup> gives you no extra information about whether the AI was on or not. So the AI is allowed to affect the future through the output channel, but not through any alternative means.</p>\n<p>But what exactly is P(S<sup>a</sup>|&not;X,O<sup>a</sup>,B)? How can the AI have sent that output, if it wasn't even turned on? We can imagine that if &not;X happens - if the AI is not turned on - then the output wire is connected to an exited, noisy gas, and random fluctuations in pressure or something will type out the output message. Now, it's extremely improbable that such a process would actually type out a coherent useful message. But that doesn't matter: a coherent message is extreme evidence of X over &not;X, but as long as there is no <em>additional</em> information detectable in slice s (i.e. in the future of the universe), we're fine: the AI has not broken out and started optimising things.</p>\n<p>At least... that's the theory, and it would be true if we knew for a fact that the only way of the AI being turned on was through the X process. In practice, if we observe a coherent output O and know &not;X happened, we wouldn't say \"the AI was never turned on and this message arrived by chance\" but \"the AI was turned on in some other way (maybe a passing cosmic ray) and typed out that message\". So &not;X and O together no longer imply the AI was not turned on, undermining our assumptions. So in the denominator, we need to replace O with &Otilde;. &Otilde; corresponds to the same output as O, but specifically describes the output being created by the gas (and not any other process). This does not seem infeasible to require, as P and P' can comprehend physics concepts. Given this, we can take &not;X, &Otilde; and B together to mean that the AI was (almost certainly) never turned on (and the output message arose by chance). Thanks to Daniel Dewey for pointing out this issue.</p>\n<p>So the final equation is that that AI must take the action:</p>\n<p style=\"padding-left: 60px;\">argmax<sub>a</sub>&nbsp;E'(U|a) -&lambda;E'(P(S<sup>a</sup>|X,O<sup>a</sup>,B)/P(S<sup>a</sup>|&not;X,&Otilde;<sup>a</sup>,B) | a).</p>\n<p>This seems to be enough to accomplish what I claimed at the beginning: to get an uncontained AI that will give us the coordinates to point the laser at to kill the comet, then quietly turn itself off and leave no further trace of its existence. I'll repost the image again here, detailing the penalty function; it should now be more comprehensible:</p>\n<p><img src=\"http://images.lesswrong.com/t3_iyx_0.png?v=7088f9d971d26d10a9c5af8967cfd0b2\" alt=\"\" width=\"672\" height=\"1058\" /></p>\n<div><br /></div>\n<p>&nbsp;</p>\n<h2>More (or less) on the output channel</h2>\n<p>The output channel is worth pondering in more detail. There are in reality two components to the output: any message the AI may be transmitting, and the indirect effect of confirmation of the AI's existence (and its performance). Most of the time, the first component includes the second: if we ask the AI to aim a laser to save the planet, the observation \"hey the planet isn't dead - the AI saved us!\" derives simply from the AI's output of coordinates.</p>\n<p>But we can make some of the output dedicated to the second effect without having the first at all. If we had established it was safe to video the AI while it went about its business (note to everyone: it isn't safe), then we could add that camera feed to the output channel - the AI would not be supposed to output anything through that. If we had some really good physics models, we might have someone observe the AI doing its stuff, and having the optic nerve of that person on the output channel. So then the penalty function equation would encode \"given that you had the camera feed, the background knowledge, and the AI's answer, <em>then</em> if observing the future slice s gives you no extra info as to whether the AI existed or not, the AI is of reduced impact\".</p>\n<p>Or at least, of reduced impact outside the output channels: we've mainly constructed a \"no back channels for action\" AI, rather than a reduced impact AI per se.</p>\n<p>But the idea feels as if it could be developed to get a true reduced impact AI, by carefully parsing what we put on or removed from the output channel. Some AI actions do not need the output channel. Suppose the AI was a paperclip maximiser, and that a constant stream of paperclips poured into and out of the room it was veiled in. If the AI added a few more paperclips to the multitude, this would probably be impossible to notice, especially if we'd been sloppy about keeping records about the quantity of paperclips in the first place. In this situation, a reduced impact AI could build a few paperclips, affecting the world (slightly) in a away that wouldn't be detectable in 50 years time. And all this without needing to put the impact on an output channel (a similar, more gory idea, would allow the AI to make people's deaths more pleasant, in undetectable ways).</p>\n<p>We could slice up outputs like that, removing things that would genuinely be of reduced impact from the output channel. We might also put a dampener on (some parts of) the output channel. Currently the AI has no penalty for affecting the universe through O; maybe instead we could impose a reduced penalty, rather than no penalty at all. So it's easier to affect the future through O, but not free. I'll return to these ideas once I'm convinced this current setup works, both in theory and for a plausible AI design (for instance, I think the above formula would have to be massaged to allow reasonable self-improvement).</p>\n<p>This is the point in the post where the author normally asks for thoughts and impressions. I will show my rebellious and independent nature by asking instead asking for impressions and thoughts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gzQT5AAw8oQdzuwBG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "24585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>A putative new idea for AI control; index&nbsp;<a href=\"/lw/lt6/newish_ai_control_ideas/\">here</a>.</em></p>\n<p>This post presents a further development of the <a href=\"/lw/a39/the_mathematics_of_reduced_impact_help_needed/\">reduced impact AI</a> approach, bringing in some novel ideas and setups that allow us to accomplish more. It still isn't a complete approach - further development is needed, which I will do when I return to the concept - but may already allow certain types of otherwise dangerous AIs to be made safe. And this time, without needing to <a href=\"/r/all/lw/gmx/domesticating_reduced_impact_ais/\">encase them</a> in clouds of chaotic anti-matter!</p>\n<p>Specifically, consider the following scenario. A comet is heading towards Earth, and it is generally agreed that a collision is suboptimal for everyone involved. Human governments have come together in peace and harmony to build a <a href=\"http://www.youtube.com/watch?v=nw3dWOssOhs\">giant laser on the moon</a>&nbsp;- this could be used to vaporise the approaching comet, except there isn't enough data to aim it precisely. A superintelligent AI programmed with a naive \"save all humans\" utility function is asked to furnish the coordinates to aim the laser. The AI is mobile and not contained in any serious way. Yet the AI furnishes the coordinates - and nothing else - and then turns itself off completely, not optimising anything else.</p>\n<p>The rest of this post details an approach that could might make that scenario possible. It is slightly complex: I haven't found a way of making it simpler. Most of the complication comes from attempts to precisely define the needed counterfactuals. We're trying to bring rigour to inherently un-sharp ideas, so some complexity is, alas, needed. I will try to lay out the ideas with as much clarity as possible - first the ideas to constrain the AI, then ideas as to how to get some useful work out of it anyway. Classical mechanics (general relativity) will be assumed throughout. As in a <a href=\"/r/all/lw/gmx/domesticating_reduced_impact_ais/\">previous post</a>, the approach will be illustrated by a drawing of unsurpassable elegance; the rest of the post will aim to clarify everything in the picture:<a id=\"more\"></a></p>\n<p><img src=\"http://images.lesswrong.com/t3_iyx_0.png?v=7088f9d971d26d10a9c5af8967cfd0b2\" alt=\"\" width=\"672\" height=\"1058\"></p>\n<p>&nbsp;</p>\n<h2 id=\"Can_Super_God_detect_whether_God_existed_\">Can Super-God detect whether God existed?</h2>\n<p>If we say that Mr. Y has had a reduced impact upon the world, what do we mean by that? It's hard to define, but here is a sufficient condition: if we come back in 50 years time and cannot find any evidence that Mr. Y ever existed, then he probably had a minute impact. If a very thorough analysis of the whole world and surrounding solar system, fed into the most sophisticated computer systems, fails to detect any evidence for or against the existence of Mr. Y, then he certainly had a reduced impact.</p>\n<p>This is the heart of the approach. The idea is to examine a slice of the future, and if any effect of the AI's existence is undetectable in that slice, then the AI had a (very) reduced impact. But who would do this \"examining\"? The AI might be able to hide its impact from any human examiner, and there might not even be any humans left alive to examine anything. No, the examination has to be a counterfactual examination, carried out by the AI itself p or better yet, a smarter AI. A (hypothetical) smarter version of the AI will be helping to constrain the AI's current behaviour.</p>\n<p>&nbsp;</p>\n<h2 id=\"Scouring_the_future_for_traces_of_the_AI\">Scouring the future for traces of the AI</h2>\n<p>We will assume that the AI has an understanding of physics, and that the heart of the AI is a probability module, capable of probabilistic reasoning about physical systems and other things. This module comes in two versions: the module P, which is an idealised version, which has almost unlimited storage space and time with which to answer the question. And a bounded module P', the realistic, constrained one that the AI uses (these modules give rise to expectation operators E and E'). Crucially, the module P' can give sensible answers to what the output of the module P would be. This is a <a href=\"/lw/h1k/reflection_in_probabilistic_logic/\">reflection principle</a>, allowing the AI to estimate what kind of answers it would give if it had (much) more time to think about it.</p>\n<p>This reflection principle isn't magic: P' can't mysteriously compute the answer to a question by peering into P. But P' could, for instance, estimate whether P will have lower uncertainty than P'. It could estimate how much better P would be at particular task, and other similar estimates.</p>\n<p>This P will be the strong version of the AI, looking at the future data. And P' will allow the AI to estimate, now, what the answer of P might be. How exactly will it do this?</p>\n<p>Define s to be a slice of the future. We could, for instance, say that s consists of any point 50 years into the future (on any time-like geodesic), thickened to one metre, say. There are some subtleties with this description - we might encounter black holes and other singularities - but we'll lay them aside for the moment. The crucial aspect is that s is a barrier: we cannot get from now to a point 60 years in the future, without crossing s at some point.</p>\n<p>Then once we've defined s, define S to be the total of all information present in that slice: the position and velocities of every particle. This S is a noisy estimate, not a perfect one: the positions and velocities are somewhat uncertain. This noise is quite important, as in a deterministic general relativistic universe, it is the only thing preventing P from knowing the past perfectly. But there need not be much noise for the past to become blurred enough.</p>\n<p>From the perspective of today, S is a random variable who's values are unknown: we can estimate some of its properties, but it will depend on many things, not least of all the decisions that we (and the AI) will make.</p>\n<p>What would be a bad S, from the reduced impact perspective? One where it would be extremely obvious, upon examination, that the AI was present in the past. This bad S would lead to a large likelihood ratio:</p>\n<p style=\"padding-left: 60px;\">P(S|\"The AI existed\") / P(S|\"The AI never exited).</p>\n<ul>\n</ul>\n<p>If that ratio is large, there is a lot of evidence in slice s that the AI existed. Conversely, if the ratio is close to 1, there is little evidence of the AI's existence.</p>\n<p>&nbsp;</p>\n<h2 id=\"A_very_complicated_way_of_doing_nothing\">A very complicated way of doing nothing</h2>\n<p>We're now going to add more details to make that approach rigorous. First of all, how can we rigorise \"The AI existed\" and its negation? My favourite method for defining such a counterfactual is to have an event X - maybe an electron travelling down a wire - that will turn on the AI initially. The process is slightly noisy, so there is a tiny chance that X will fail; if it does fail, then the AI is not turned on at all. If we designate by B a collection of background knowledge about the world before the AI was turned on, we can define \"The AI existed\" as \"X and B\" and \"The AI never existed\" as \"\u00acX and B\". Note that we are banning anthropic reasoning, allowing the AI to consider hypotheticals in which it didn't exist. If P and P' can do physics simulations, this should be fine: \u00acX is a perfectly acceptable physical situation, with its own future consequences.</p>\n<p>But if the turning on of the AI is done in full view of everyone, with the press recording everything and people twittering excitedly, then it will be blatant that the turning on was successful (or not). Hence we should put the AI in a room - not to constrain it, like a boxed oracle, but to give it some privacy. Thus it could be plausible that the future - the data S - would not have any trace of the AI's existence. After an hour's time, the AI's mind will be thoroughly overwritten, and the room will be given a vigorous shake. This is not to constrain the AI: we'll assume the AI is mobile and has access to enough tools inside the room to do whatever it wants. The setup is only to <em>give the AI the option of pretending it never existed</em> - of arranging events so that there is no evidence of it being turned on. Thus it becomes possible that the AI can act to minimise the future ratio P(S|X,B)/P(S|\u00acX,B), especially if we make the data in S somewhat noisy.</p>\n<p>But that involves P, not P', and involves the data S which the AI doesn't know. So the AI needs to estimate the expected value of the ratio, using its expectation estimator E' (powered by P', which can estimate P and S):</p>\n<p style=\"padding-left: 60px;\">E'(P(S|X,B)/P(S|\u00acX,B))</p>\n<p>That expression means the expectation, as best the AI can currently calculate it, of the ratios of probabilities that would be calculated by a (hypothetical) super version of itself, with access to all the data S about the future slice s. That's worth digesting for a second. The contents of S will be determined by the AI's actions now, so the AI is in effect trying to pick the action that would make it hard for a super version of itself to tell whether the AI had ever been turned on. Thus it will try to minimise its impact on the world, by perfectly mimicking non-existence: by choosing the action a that minimises the above expected ration. This is</p>\n<p style=\"padding-left: 60px;\">argmax<sub>a</sub>&nbsp;-E'(P(S<sup>a</sup>|X,B)/P(S<sup>a</sup>|\u00acX,B) | a)</p>\n<p>Here we've added a minus sign so we're maximising the negative of the ratio (entirely equivalent with minimising the ratio), and added superscript to S to signify that the contents of S depend (probabilistically) on the action a.</p>\n<p>As it stands, this is a complicated way of making the AI do absolutely nothing: it will seek to make the world indistinguishable from one in which it wasn't turned on. We haven't even added a utility function yet! So let's do that, adding a U representing the AI's goals, and keeping the above expression as a penalty function to ensure it accomplishes these goals with a minimum of fuss. Let's require that U be bounded between 0 and 1, and then fix a ratio&nbsp;\u03bb such that the AI picks the action following:</p>\n<p style=\"padding-left: 60px;\">argmax<sub>a</sub>&nbsp;E'(U|a) -\u03bbE'(P(S<sup>a</sup>|X,B)/P(S<sup>a</sup>|\u00acX,B) | a).</p>\n<p>For most values of \u03bb, this will either be an expected utility maximiser (for \u03bb too small: the utility dominates) or remain a complicated way of doing nothing (for \u03bb to big: the penalty function dominates). It's not too clear what happens in the transition: trading off utility versus penalty is not obvious. We have to find a better way of extracting useful work from the system.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_output_channel__a_very_specific_loophole\">The output channel: a very specific loophole</h2>\n<p>Part of the problem is that the AI can have a massive effect just by existing and answering a single question correctly - people will be motivated to run out and build more AIs, write philosophical papers about AIs, and so on. In the example that we started the paper off with, the AIs will have a massive impact: it alone will be the difference between the planet getting rendered uninhabitable by the comet or not. So reduced impact seems impossible in these cases.</p>\n<p>For these we devise an output channel. The idea is simple: information along this output channel, and only along this channel, is allowed to affect the future. This means that the content of the output channel is potentially very dangerous and other precautions need to be taken. Still, it can be useful: for instance, we could ask the AI to help us aim our laser, and restrict it's output so that we only get coordinates, no secret info. Then our reduced impact approach would preclude the AI from having any <em>other</em> impacts on the world (i.e. no optimising the future), and we'd get our world saved for free.</p>\n<p>How can we encode this idea? If O is the output information (which will be a function of the AI's actions), we can include it inside the probability ratio, writing P(S<sup>a</sup>|X,O<sup>a</sup>,B)/P(S<sup>a</sup>|\u00acX,O<sup>a</sup>,B) (with the little superscript a to remind us that O depends on the AI's actions). This has the right formal structure: if the ratio is close to one, then it means that if you know the background information B <em>and</em> the output information O<sup>a</sup>, then observing S<sup>a</sup> gives you no extra information about whether the AI was on or not. So the AI is allowed to affect the future through the output channel, but not through any alternative means.</p>\n<p>But what exactly is P(S<sup>a</sup>|\u00acX,O<sup>a</sup>,B)? How can the AI have sent that output, if it wasn't even turned on? We can imagine that if \u00acX happens - if the AI is not turned on - then the output wire is connected to an exited, noisy gas, and random fluctuations in pressure or something will type out the output message. Now, it's extremely improbable that such a process would actually type out a coherent useful message. But that doesn't matter: a coherent message is extreme evidence of X over \u00acX, but as long as there is no <em>additional</em> information detectable in slice s (i.e. in the future of the universe), we're fine: the AI has not broken out and started optimising things.</p>\n<p>At least... that's the theory, and it would be true if we knew for a fact that the only way of the AI being turned on was through the X process. In practice, if we observe a coherent output O and know \u00acX happened, we wouldn't say \"the AI was never turned on and this message arrived by chance\" but \"the AI was turned on in some other way (maybe a passing cosmic ray) and typed out that message\". So \u00acX and O together no longer imply the AI was not turned on, undermining our assumptions. So in the denominator, we need to replace O with \u00d5. \u00d5 corresponds to the same output as O, but specifically describes the output being created by the gas (and not any other process). This does not seem infeasible to require, as P and P' can comprehend physics concepts. Given this, we can take \u00acX, \u00d5 and B together to mean that the AI was (almost certainly) never turned on (and the output message arose by chance). Thanks to Daniel Dewey for pointing out this issue.</p>\n<p>So the final equation is that that AI must take the action:</p>\n<p style=\"padding-left: 60px;\">argmax<sub>a</sub>&nbsp;E'(U|a) -\u03bbE'(P(S<sup>a</sup>|X,O<sup>a</sup>,B)/P(S<sup>a</sup>|\u00acX,\u00d5<sup>a</sup>,B) | a).</p>\n<p>This seems to be enough to accomplish what I claimed at the beginning: to get an uncontained AI that will give us the coordinates to point the laser at to kill the comet, then quietly turn itself off and leave no further trace of its existence. I'll repost the image again here, detailing the penalty function; it should now be more comprehensible:</p>\n<p><img src=\"http://images.lesswrong.com/t3_iyx_0.png?v=7088f9d971d26d10a9c5af8967cfd0b2\" alt=\"\" width=\"672\" height=\"1058\"></p>\n<div><br></div>\n<p>&nbsp;</p>\n<h2 id=\"More__or_less__on_the_output_channel\">More (or less) on the output channel</h2>\n<p>The output channel is worth pondering in more detail. There are in reality two components to the output: any message the AI may be transmitting, and the indirect effect of confirmation of the AI's existence (and its performance). Most of the time, the first component includes the second: if we ask the AI to aim a laser to save the planet, the observation \"hey the planet isn't dead - the AI saved us!\" derives simply from the AI's output of coordinates.</p>\n<p>But we can make some of the output dedicated to the second effect without having the first at all. If we had established it was safe to video the AI while it went about its business (note to everyone: it isn't safe), then we could add that camera feed to the output channel - the AI would not be supposed to output anything through that. If we had some really good physics models, we might have someone observe the AI doing its stuff, and having the optic nerve of that person on the output channel. So then the penalty function equation would encode \"given that you had the camera feed, the background knowledge, and the AI's answer, <em>then</em> if observing the future slice s gives you no extra info as to whether the AI existed or not, the AI is of reduced impact\".</p>\n<p>Or at least, of reduced impact outside the output channels: we've mainly constructed a \"no back channels for action\" AI, rather than a reduced impact AI per se.</p>\n<p>But the idea feels as if it could be developed to get a true reduced impact AI, by carefully parsing what we put on or removed from the output channel. Some AI actions do not need the output channel. Suppose the AI was a paperclip maximiser, and that a constant stream of paperclips poured into and out of the room it was veiled in. If the AI added a few more paperclips to the multitude, this would probably be impossible to notice, especially if we'd been sloppy about keeping records about the quantity of paperclips in the first place. In this situation, a reduced impact AI could build a few paperclips, affecting the world (slightly) in a away that wouldn't be detectable in 50 years time. And all this without needing to put the impact on an output channel (a similar, more gory idea, would allow the AI to make people's deaths more pleasant, in undetectable ways).</p>\n<p>We could slice up outputs like that, removing things that would genuinely be of reduced impact from the output channel. We might also put a dampener on (some parts of) the output channel. Currently the AI has no penalty for affecting the universe through O; maybe instead we could impose a reduced penalty, rather than no penalty at all. So it's easier to affect the future through O, but not free. I'll return to these ideas once I'm convinced this current setup works, both in theory and for a plausible AI design (for instance, I think the above formula would have to be massaged to allow reasonable self-improvement).</p>\n<p>This is the point in the post where the author normally asks for thoughts and impressions. I will show my rebellious and independent nature by asking instead asking for impressions and thoughts.</p>", "sections": [{"title": "Can Super-God detect whether God existed?", "anchor": "Can_Super_God_detect_whether_God_existed_", "level": 1}, {"title": "Scouring the future for traces of the AI", "anchor": "Scouring_the_future_for_traces_of_the_AI", "level": 1}, {"title": "A very complicated way of doing nothing", "anchor": "A_very_complicated_way_of_doing_nothing", "level": 1}, {"title": "The output channel: a very specific loophole", "anchor": "The_output_channel__a_very_specific_loophole", "level": 1}, {"title": "More (or less) on the output channel", "anchor": "More__or_less__on_the_output_channel", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "42 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BEJ4PRQGXuz6PRYwB", "8Nwg7kqAfCM46tuHq", "FdcxknHjeNH2MzrTj", "duAkuSqJhGDcfMaTA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-11T19:40:07.830Z", "modifiedAt": null, "url": null, "title": "AI Policy?", "slug": "ai-policy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.587Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A6oN8tCg24prgKb9f/ai-policy", "pageUrlRelative": "/posts/A6oN8tCg24prgKb9f/ai-policy", "linkUrl": "https://www.lesswrong.com/posts/A6oN8tCg24prgKb9f/ai-policy", "postedAtFormatted": "Monday, November 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Policy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Policy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA6oN8tCg24prgKb9f%2Fai-policy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Policy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA6oN8tCg24prgKb9f%2Fai-policy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA6oN8tCg24prgKb9f%2Fai-policy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 543, "htmlBody": "<p>Here's a question: are there any policies that could be worth lobbying for to improve humanity's chances re: AI risk?</p>\n<p>In the near term, it's possible that not much can be done. Human-level AI still seems a long ways off (and it probably is), making it both hard to craft effective policy on, and hard to convince people it's worth doing something about. The US government currently funds work on what it calls \"AI\" and \"nanotechnology,\" but that mostly means stuff that might be realizable in the near-term, not human-level AI or molecular assemblers. Still, if anyone has ideas on what can be done in the near term, they'd be worth discussing.</p>\n<p>Furthermore, I suspect that as human-level AI gets closer, there will be a lot the US government will be able to do to affect the outcome. For example, there's been talk of secret AI projects, but if the US gov got worried about those, I suspect they'd be hard to keep secret from a determined US gov, especially if you believe (as I do) that larger organizations will have a much better shot at building AI than smaller ones.</p>\n<p>The lesson of Snowden's NSA revelations seems to be that, while in theory there are procedures humans can use to keep secrets, in practice humans are so bad at implementing those procedures that secrecy will fail against a determined attacker. Ironically, this applies both to the government and everyone the government has spied on. However, the ability of people outside the US gov to find out about hypothetical secret government AI projects seems less predictable, dependent on decisions of individual would-be leakers.</p>\n<p>And it seems like, as long as the US government is aware of an AI project, there will be a lot it will be able to do to shut the project down if desired. For foreign projects, there will be the possibility of a Stuxnet-style attack, though the government might be reluctant to do that against a nuclear power like China or Russia (or would it?) However, I expect the US to lead the world in innovation for a long time to come, so I don't expect foreign AI projects to be much of an issue in the early stages of the game.</p>\n<p>The real issue is US gov vs. private US groups working on AI. And there, given the current status quo for how these things work in the US, my guess is that if the government ever became convinced that an AI project was dangerous, they would find some way to shut it down citing \"national security\" &nbsp;and basically that would work. However, I can see big companies with an interest in AI lobbying the government to make that not happen. I can also see them deciding to pack their AI operations off to Europe or South Korea or something.</p>\n<p>And on top of all this is simply the fact that, if it becomes convinced that AI is important, the US government has a lot of money to throw at AI research.</p>\n<p>These are just some very hastily sketched thoughts, don't take them too seriously, and there's probably a lot more that can be said. I do strongly suspect, however, that people who are concerned about risks from AI ignore the government at our peril.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A6oN8tCg24prgKb9f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 4, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "24666", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-11T23:35:31.327Z", "modifiedAt": "2022-05-28T21:57:55.833Z", "url": null, "title": "On learning difficult things", "slug": "on-learning-difficult-things", "viewCount": null, "lastCommentedAt": "2021-01-01T18:42:48.669Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things", "pageUrlRelative": "/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things", "linkUrl": "https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things", "postedAtFormatted": "Monday, November 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20learning%20difficult%20things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20learning%20difficult%20things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw5F4w8tNZc6LcBKRP%2Fon-learning-difficult-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20learning%20difficult%20things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw5F4w8tNZc6LcBKRP%2Fon-learning-difficult-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw5F4w8tNZc6LcBKRP%2Fon-learning-difficult-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1908, "htmlBody": "<p>I have been autodidacting quite a bit lately. You may have seen my <a href=\"/lw/ixn/very_basic_model_theory/\">reviews</a> of books on the <a href=\"http://intelligence.org/courses\">MIRI course list</a>. I've been going for about ten weeks now. This post contains my notes about the experience thus far.</p>\n<p>Much of this may seem obvious, and would have seemed obvious if somebody had told me in advance. But nobody told me in advance. As such, this is a collection of things that were somewhat surprising at the time.</p>\n<p>Part of the reason I'm posting this is because I don't know a lot of autodidacts, and I'm not sure how normal any of my experiences are. (Though on average, I'd guess they're about average.) As always, keep in mind that I am only one person and that your mileage may vary.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"pairup\">Pair up</h2>\n<p>When I began my quest for more knowledge, I figured that in this modern era, a well-written textbook and an account on <a href=\"/math.stackexchange.com\">math.stackexchange</a> would be enough to get me through anything. And I was right&hellip; sort of.</p>\n<p>But not really.</p>\n<p>The problem is, most of the time that I get stuck, I get stuck on something incredibly stupid. I've either misread something somewhere or misremembered a concept from earlier in the book. Usually, someone looking over my shoulder could correct me in ten seconds with three words.</p>\n<p>\"Dude. Disjunction. <em>Dis</em>junction<em>.</em>\"</p>\n<p>These are the things that eat my days.</p>\n<p>In principle, places like stackexchange can get me unstuck, but they're an awkward tool for the job. First of all, my stupid mistakes are heavily contextualized. A full context dump is necessary before I can even ask my question, and this takes time. Furthermore, I feel dumb asking stupid questions on stackexchange-type sites. My questions are usually things that I can figure out with a close re-read (except, I'm not sure which part needs a re-read). I usually opt for a close re-read of everything rather than asking for help. This is even more time consuming.</p>\n<p>The infuriating thing is that answering these questions usually doesn't require someone who already knows the answers: it just requires someone who didn't make exactly the same mistakes as me. I lose hours on little mistakes that could have been fixed within seconds if I was doing this with someone else.</p>\n<p>That's why my number one piece of advice for other people attempting to learn on their own is <em>do it with a friend</em>. They don't need to be more knowledgeable than you to answer most of the questions that come up. They just need to make <em>different</em> misunderstandings, and you'll be able to correct each other as you go along.</p>\n<p>The thing I miss most about college is tight feedback loops while learning. When autodidacting, the feedback loop can be long.</p>\n<p>I still haven't managed to follow my own advice here. I'm writing this advice in part because it should motivate me to actually pair up. Unfortunately, there is nobody in my immediate circle who has the time or patience to read along with me, but there are a number of resources I have not yet explored (the LessWrong study hall, for example, or soliciting to actual mathematicians). It's on my list of things to do.</p>\n<h2 id=\"readrereadrereread\">Read, reread, rereread</h2>\n<p>Reading <em>Model Theory</em> was one of the hardest things I've done. Not necessarily because the content was hard, but because it was the first time I actually learned something that was way outside my comfort zone.</p>\n<p>The short version is that <em>Basic Category Theory</em> and <em>Na&iuml;ve Set Theory</em> left me somewhat overconfident, and that I should have read a formal logic textbook before diving in. I had basic familiarity with logic, but no practice. Turns out practice is important.</p>\n<p>Anyway, it's not like <em>Model Theory</em> was impossible just because I skipped my logic exercises. It was just <em>hard</em>. There are a number of little misconceptions you have when you're familiar with something but you've never applied it, and I found myself having to clean those out just to understand what <em>Model Theory</em> was trying to say to me.</p>\n<p>In retrospect, this was an efficient way to strengthen my understanding of mathematical logic and learn <em>Model Theory</em> at the same time. (I've moved on to a logic textbook, and it's been a cakewalk.) That said, I wouldn't wish the experience on others.</p>\n<p>In the process, I learned how to learn things that are way outside my comfort zone. In the past, all the stuff I've learned has been either easy, or an extension of things that I was already interested in and experienced with. Reading <em>Model Theory</em> was the first time in my life where I read a chapter of a textbook and it made <em>absolutely no sense</em>. In fact, it took about three passes per chapter before they made sense.</p>\n<ol>\n<li>The first pass was barely sufficient to understand all the words and symbols. I constantly had to go research a topic. I followed proofs one step at a time, able to verify the validity of each step but not really understand what was going on. I came out the other end believing the results, but not knowing them.</li>\n<li>Another pass was required to figure out what the book was actually trying to say to me. Once all the words made sense and I was comfortable with their usage, the second pass allowed me to see what the theorems and proofs were actually saying. This was nice, but it still wasn't sufficient: I understood the theorems, but they seemed like a random walk through theorem-space. I couldn't yet understand why anyone would say those particular things on purpose.</li>\n<li>The third pass was necessary to understand the greater theory. I've never been particularly good at memorizing things, and it's not sufficient for me to believe and memorize a theorem. If it's going to stick, I have to understand why it's important. I have to understand why this theorem in particular is being stated, rather than another. I have to understand the problem that's being solved. A third pass was necessary to figure out the context in which the text made sense.</li>\n</ol>\n<p>After a third pass of any given chapter, the next chapter didn't seem quite so random. When the upcoming content started feeling like a natural progression instead of a random walk, I knew I was making progress.</p>\n<p>I note this because this is the first time that I had to read a math text more than once to understand what was going on. I'm not talking about individual sentences or paragraphs, I'm talking about finishing a chapter, feeling like \"wat\", and then starting the whole chapter over. Twice.</p>\n<p>I'm not sure if I'm being na&iuml;ve (for never having needed to do this before) or slow (for having to do this for <em>Model Theory</em>), but I did not anticipate requiring three passes. Mostly, I didn't anticipate gaining as much as I did from a re-read; I would have guessed that something opaque on the first pass would remain opaque on a second pass.</p>\n<p>This, I'm pretty sure, was na&iuml;vety.</p>\n<p>So take note: if you stumble upon something that feels very hard, it might be more useful than anticipated to re-read it.</p>\n<h2 id=\"cognitiveexchangerates\">Cognitive exchange rates</h2>\n<p>When reading Model Theory, I was only able to convert 30-50% of my allotted \"study time\" into actual study.</p>\n<p>This is somewhat surprising, as I had no such troubles with <em>Basic Category Theory</em> or <em>Na&iuml;ve Set Theory</em>.</p>\n<p>(I often have the <em>opposite</em> problem when writing code; this is probably due to the different reward structure.)</p>\n<p>I was somewhat frustrated with my inability to study as much as I would have liked. My usual time-into-studying conversion rate is much higher (I'd guess 80%ish, though I haven't been measuring).</p>\n<p>I'm not sure what factor made it harder for me to study model theory. I don't think it was the difficulty directly, as I often tend to work harder in the face of a challenge. I'd guess that it was either the slower rate of rewards (caused by a slower pace of learning) or actual cognitive exhaustion.</p>\n<p>In the vein of cognitive exhaustion, there were a few times while reading <em>Model Theory</em> where I seem to have become cognitively exhausted before becoming physically exhausted. This was a first for me. I'm not referring to those times when you've done a lot of mental work and you shy away from doing anything difficult, that's happened to me plenty. Rather, in this case, I felt fully awake and ready to keep reading. And I did keep reading. It just&hellip; didn't work. I'd have trouble following simple proofs. I'd fail at parsing sentences that were quite clear after resting.</p>\n<p>I'm still not sure what to make of this, and I don't have sufficient data to draw conclusions. However, it seems like there are mental states where my I feel awake and able to continue, but my mind is just not capable of doing the heavy lifting.</p>\n<p>Again, the fact that I'm only just realizing this now is probably na&iuml;vety, but it's something to remember before getting frustrated with yourself.</p>\n<h2 id=\"explainittosomeone\">Explain it to someone</h2>\n<p>As I've said before, one of the best ways to learn something is to do the problem sets. For <em>Model Theory</em>, though, there were times when I finished reading through a chapter and was not capable of doing the problems.</p>\n<p>Re-reading helped, as mentioned above. Another thing that helped was explaining the concepts.</p>\n<p>I explained model theory pretty extensively to a text file on my computer. I sketched the proofs in my own words and stated their significance. I explained the syntax being used. I tried to motivate each idea. (The notes are still lying around somewhere; I haven't posted them because they're pretty much a derivative work at this point.)</p>\n<p>I found that this went a <em>long</em> way towards helping me track down places where I'd thought I learned something, but actually hadn't. If you're having trouble, go explain the concept to somebody (or to a text file). This can bridge the gap between \"I read it\" and \"I can do the problems\" quite well. For me, this technique often took problems from \"unapproachable\" to \"easy\" in one fell swoop.</p>\n<h2 id=\"dontbookyourselfsolid\">Don't book yourself solid</h2>\n<p>I'm pretty good at avoiding stress. I have the (apparently rare) ability to drop all work-related concerns at the door when I leave. I don't even know <em>how</em> to get stressed by bad luck, especially if I made good choices given the information I had at the time. I get normally tense in stressful situations with time constraints, but I'm adept at avoiding the permastress that I've seen plague friends and family&nbsp;<span style=\"font-size: 10px;\">&mdash;</span>&nbsp;unless I've booked myself solid.</p>\n<p>I've had a packed schedule these past few weeks. I try to move the needle on at least two projects a day (more on weekends). Even if it's entirely reasonable to fit all these things into my schedule, I have not yet found a way to avoid the stress.</p>\n<p>Even when I know that, if I push myself, I can read this much and write that much and code this feature all in one day, I haven't found a good way to push myself without pressure-stress.</p>\n<p>I'm still hoping that I'll learn how to move quickly without stress as I learn my capabilities, but I'm not sure I've been adequately accounting for the <a href=\"http://scholar.google.com/scholar?q=adverse+effects+of+stress&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart&amp;sa=X&amp;ei=KE6AUuGTEqerigLmyoHADw&amp;ved=0CCwQgQMwAA\">cost of stress</a>.</p>\n<p>It's worth remembering that doing less than you're capable of <em>on purpose&nbsp;</em>might be a good strategy for maximizing long-term output.</p>\n<hr />\n<p>There you go. Those are my notes gathered from trying to learn lots of things very quickly (and trying to learn one hard thing in particular). Comments are encouraged; I am by no means an expert.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 8, "fkABsGCJZ6y9qConW": 12}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w5F4w8tNZc6LcBKRP", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 116, "baseScore": 175, "extendedScore": null, "score": 0.000439, "legacy": true, "legacyId": "24660", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 175, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I have been autodidacting quite a bit lately. You may have seen my <a href=\"/lw/ixn/very_basic_model_theory/\">reviews</a> of books on the <a href=\"http://intelligence.org/courses\">MIRI course list</a>. I've been going for about ten weeks now. This post contains my notes about the experience thus far.</p>\n<p>Much of this may seem obvious, and would have seemed obvious if somebody had told me in advance. But nobody told me in advance. As such, this is a collection of things that were somewhat surprising at the time.</p>\n<p>Part of the reason I'm posting this is because I don't know a lot of autodidacts, and I'm not sure how normal any of my experiences are. (Though on average, I'd guess they're about average.) As always, keep in mind that I am only one person and that your mileage may vary.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Pair_up\">Pair up</h2>\n<p>When I began my quest for more knowledge, I figured that in this modern era, a well-written textbook and an account on <a href=\"/math.stackexchange.com\">math.stackexchange</a> would be enough to get me through anything. And I was right\u2026 sort of.</p>\n<p>But not really.</p>\n<p>The problem is, most of the time that I get stuck, I get stuck on something incredibly stupid. I've either misread something somewhere or misremembered a concept from earlier in the book. Usually, someone looking over my shoulder could correct me in ten seconds with three words.</p>\n<p>\"Dude. Disjunction. <em>Dis</em>junction<em>.</em>\"</p>\n<p>These are the things that eat my days.</p>\n<p>In principle, places like stackexchange can get me unstuck, but they're an awkward tool for the job. First of all, my stupid mistakes are heavily contextualized. A full context dump is necessary before I can even ask my question, and this takes time. Furthermore, I feel dumb asking stupid questions on stackexchange-type sites. My questions are usually things that I can figure out with a close re-read (except, I'm not sure which part needs a re-read). I usually opt for a close re-read of everything rather than asking for help. This is even more time consuming.</p>\n<p>The infuriating thing is that answering these questions usually doesn't require someone who already knows the answers: it just requires someone who didn't make exactly the same mistakes as me. I lose hours on little mistakes that could have been fixed within seconds if I was doing this with someone else.</p>\n<p>That's why my number one piece of advice for other people attempting to learn on their own is <em>do it with a friend</em>. They don't need to be more knowledgeable than you to answer most of the questions that come up. They just need to make <em>different</em> misunderstandings, and you'll be able to correct each other as you go along.</p>\n<p>The thing I miss most about college is tight feedback loops while learning. When autodidacting, the feedback loop can be long.</p>\n<p>I still haven't managed to follow my own advice here. I'm writing this advice in part because it should motivate me to actually pair up. Unfortunately, there is nobody in my immediate circle who has the time or patience to read along with me, but there are a number of resources I have not yet explored (the LessWrong study hall, for example, or soliciting to actual mathematicians). It's on my list of things to do.</p>\n<h2 id=\"Read__reread__rereread\">Read, reread, rereread</h2>\n<p>Reading <em>Model Theory</em> was one of the hardest things I've done. Not necessarily because the content was hard, but because it was the first time I actually learned something that was way outside my comfort zone.</p>\n<p>The short version is that <em>Basic Category Theory</em> and <em>Na\u00efve Set Theory</em> left me somewhat overconfident, and that I should have read a formal logic textbook before diving in. I had basic familiarity with logic, but no practice. Turns out practice is important.</p>\n<p>Anyway, it's not like <em>Model Theory</em> was impossible just because I skipped my logic exercises. It was just <em>hard</em>. There are a number of little misconceptions you have when you're familiar with something but you've never applied it, and I found myself having to clean those out just to understand what <em>Model Theory</em> was trying to say to me.</p>\n<p>In retrospect, this was an efficient way to strengthen my understanding of mathematical logic and learn <em>Model Theory</em> at the same time. (I've moved on to a logic textbook, and it's been a cakewalk.) That said, I wouldn't wish the experience on others.</p>\n<p>In the process, I learned how to learn things that are way outside my comfort zone. In the past, all the stuff I've learned has been either easy, or an extension of things that I was already interested in and experienced with. Reading <em>Model Theory</em> was the first time in my life where I read a chapter of a textbook and it made <em>absolutely no sense</em>. In fact, it took about three passes per chapter before they made sense.</p>\n<ol>\n<li>The first pass was barely sufficient to understand all the words and symbols. I constantly had to go research a topic. I followed proofs one step at a time, able to verify the validity of each step but not really understand what was going on. I came out the other end believing the results, but not knowing them.</li>\n<li>Another pass was required to figure out what the book was actually trying to say to me. Once all the words made sense and I was comfortable with their usage, the second pass allowed me to see what the theorems and proofs were actually saying. This was nice, but it still wasn't sufficient: I understood the theorems, but they seemed like a random walk through theorem-space. I couldn't yet understand why anyone would say those particular things on purpose.</li>\n<li>The third pass was necessary to understand the greater theory. I've never been particularly good at memorizing things, and it's not sufficient for me to believe and memorize a theorem. If it's going to stick, I have to understand why it's important. I have to understand why this theorem in particular is being stated, rather than another. I have to understand the problem that's being solved. A third pass was necessary to figure out the context in which the text made sense.</li>\n</ol>\n<p>After a third pass of any given chapter, the next chapter didn't seem quite so random. When the upcoming content started feeling like a natural progression instead of a random walk, I knew I was making progress.</p>\n<p>I note this because this is the first time that I had to read a math text more than once to understand what was going on. I'm not talking about individual sentences or paragraphs, I'm talking about finishing a chapter, feeling like \"wat\", and then starting the whole chapter over. Twice.</p>\n<p>I'm not sure if I'm being na\u00efve (for never having needed to do this before) or slow (for having to do this for <em>Model Theory</em>), but I did not anticipate requiring three passes. Mostly, I didn't anticipate gaining as much as I did from a re-read; I would have guessed that something opaque on the first pass would remain opaque on a second pass.</p>\n<p>This, I'm pretty sure, was na\u00efvety.</p>\n<p>So take note: if you stumble upon something that feels very hard, it might be more useful than anticipated to re-read it.</p>\n<h2 id=\"Cognitive_exchange_rates\">Cognitive exchange rates</h2>\n<p>When reading Model Theory, I was only able to convert 30-50% of my allotted \"study time\" into actual study.</p>\n<p>This is somewhat surprising, as I had no such troubles with <em>Basic Category Theory</em> or <em>Na\u00efve Set Theory</em>.</p>\n<p>(I often have the <em>opposite</em> problem when writing code; this is probably due to the different reward structure.)</p>\n<p>I was somewhat frustrated with my inability to study as much as I would have liked. My usual time-into-studying conversion rate is much higher (I'd guess 80%ish, though I haven't been measuring).</p>\n<p>I'm not sure what factor made it harder for me to study model theory. I don't think it was the difficulty directly, as I often tend to work harder in the face of a challenge. I'd guess that it was either the slower rate of rewards (caused by a slower pace of learning) or actual cognitive exhaustion.</p>\n<p>In the vein of cognitive exhaustion, there were a few times while reading <em>Model Theory</em> where I seem to have become cognitively exhausted before becoming physically exhausted. This was a first for me. I'm not referring to those times when you've done a lot of mental work and you shy away from doing anything difficult, that's happened to me plenty. Rather, in this case, I felt fully awake and ready to keep reading. And I did keep reading. It just\u2026 didn't work. I'd have trouble following simple proofs. I'd fail at parsing sentences that were quite clear after resting.</p>\n<p>I'm still not sure what to make of this, and I don't have sufficient data to draw conclusions. However, it seems like there are mental states where my I feel awake and able to continue, but my mind is just not capable of doing the heavy lifting.</p>\n<p>Again, the fact that I'm only just realizing this now is probably na\u00efvety, but it's something to remember before getting frustrated with yourself.</p>\n<h2 id=\"Explain_it_to_someone\">Explain it to someone</h2>\n<p>As I've said before, one of the best ways to learn something is to do the problem sets. For <em>Model Theory</em>, though, there were times when I finished reading through a chapter and was not capable of doing the problems.</p>\n<p>Re-reading helped, as mentioned above. Another thing that helped was explaining the concepts.</p>\n<p>I explained model theory pretty extensively to a text file on my computer. I sketched the proofs in my own words and stated their significance. I explained the syntax being used. I tried to motivate each idea. (The notes are still lying around somewhere; I haven't posted them because they're pretty much a derivative work at this point.)</p>\n<p>I found that this went a <em>long</em> way towards helping me track down places where I'd thought I learned something, but actually hadn't. If you're having trouble, go explain the concept to somebody (or to a text file). This can bridge the gap between \"I read it\" and \"I can do the problems\" quite well. For me, this technique often took problems from \"unapproachable\" to \"easy\" in one fell swoop.</p>\n<h2 id=\"Don_t_book_yourself_solid\">Don't book yourself solid</h2>\n<p>I'm pretty good at avoiding stress. I have the (apparently rare) ability to drop all work-related concerns at the door when I leave. I don't even know <em>how</em> to get stressed by bad luck, especially if I made good choices given the information I had at the time. I get normally tense in stressful situations with time constraints, but I'm adept at avoiding the permastress that I've seen plague friends and family&nbsp;<span style=\"font-size: 10px;\">\u2014</span>&nbsp;unless I've booked myself solid.</p>\n<p>I've had a packed schedule these past few weeks. I try to move the needle on at least two projects a day (more on weekends). Even if it's entirely reasonable to fit all these things into my schedule, I have not yet found a way to avoid the stress.</p>\n<p>Even when I know that, if I push myself, I can read this much and write that much and code this feature all in one day, I haven't found a good way to push myself without pressure-stress.</p>\n<p>I'm still hoping that I'll learn how to move quickly without stress as I learn my capabilities, but I'm not sure I've been adequately accounting for the <a href=\"http://scholar.google.com/scholar?q=adverse+effects+of+stress&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart&amp;sa=X&amp;ei=KE6AUuGTEqerigLmyoHADw&amp;ved=0CCwQgQMwAA\">cost of stress</a>.</p>\n<p>It's worth remembering that doing less than you're capable of <em>on purpose&nbsp;</em>might be a good strategy for maximizing long-term output.</p>\n<hr>\n<p>There you go. Those are my notes gathered from trying to learn lots of things very quickly (and trying to learn one hard thing in particular). Comments are encouraged; I am by no means an expert.</p>", "sections": [{"title": "Pair up", "anchor": "Pair_up", "level": 1}, {"title": "Read, reread, rereread", "anchor": "Read__reread__rereread", "level": 1}, {"title": "Cognitive exchange rates", "anchor": "Cognitive_exchange_rates", "level": 1}, {"title": "Explain it to someone", "anchor": "Explain_it_to_someone", "level": 1}, {"title": "Don't book yourself solid", "anchor": "Don_t_book_yourself_solid", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "45 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["F6BrJFkqEhh22rFsZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 17, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-12T05:44:57.723Z", "modifiedAt": "2021-01-24T20:33:21.841Z", "url": null, "title": "MIRI course list study pairs", "slug": "miri-course-list-study-pairs", "viewCount": null, "lastCommentedAt": "2017-11-23T16:17:22.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Adele_L", "createdAt": "2012-05-25T06:52:13.187Z", "isAdmin": false, "displayName": "Adele_L"}, "userId": "5cAXqfacg2fkQPK8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zfvng53hsYtrPjX5d/miri-course-list-study-pairs", "pageUrlRelative": "/posts/Zfvng53hsYtrPjX5d/miri-course-list-study-pairs", "linkUrl": "https://www.lesswrong.com/posts/Zfvng53hsYtrPjX5d/miri-course-list-study-pairs", "postedAtFormatted": "Tuesday, November 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIRI%20course%20list%20study%20pairs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIRI%20course%20list%20study%20pairs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfvng53hsYtrPjX5d%2Fmiri-course-list-study-pairs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIRI%20course%20list%20study%20pairs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfvng53hsYtrPjX5d%2Fmiri-course-list-study-pairs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfvng53hsYtrPjX5d%2Fmiri-course-list-study-pairs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 363, "htmlBody": "<p>Inspired by: <a href=\"/lw/j10/on_learning_difficult_things/\">On learning difficult things</a></p>\n<p>In his recent post, user So8res says his number one piece of advice for learning something difficult is to have study partner to learn with you.&nbsp;</p>\n<p>Since there is a decent amount of interest here in going through the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>, it might be worth finding other people here to learn and study this with, and to form pairs or groups.&nbsp;</p>\n<p>So here is a space for finding and organizing such partnerships!</p>\n<hr />\n<p>Of course, part of the reason I wrote this is because I am interested in learning these books with people. My background: I'm currently a second year Ph.D. student in mathematics (number theory). I'm still pretty new to the type of math emphasized here. I have <em>Probabilistic Graphical Models</em>,&nbsp;<em>Category Theory for Computer Scientists</em>&nbsp;and <em>The Logic of Provability</em>&nbsp;(by George Boolos -- not on the course list, but good to get background for the Robust Cooperation paper and for understanding Loeb's theorem) all lying around. I'm also taking a class on numerical analysis. Part of my problem is that I start lots of projects and then end up fizzling out on them, and I hope having a partner will help with this.&nbsp;</p>\n<p>I've already been going through MIRI's publications with a friend from the local LW community, which has been really nice. I'm still interested in finding more partners &lt;insert poly joke here&gt; for going through books on the course list specifically. I'm also willing to explain things I understand, or let someone explain things to me (I've found that explaining things to someone else is a very good way of solidifying your understanding of something) when I have time.&nbsp;</p>\n<hr />\n<p>Some things to consider:</p>\n<ul>\n<li>What is the best online format for doing this? I've been doing this sort of thing with <a href=\"https://workflowy.com\">Workflowy</a>&nbsp;+ <a href=\"https://chrome.google.com/webstore/detail/mathflowy/pbeenjljklomfnijhecnomdppbhknjed\">Mathflowy</a>&nbsp;but there is probably something better.&nbsp;</li>\n<li>Does a pair dynamic, or a group dynamic seem more likely to work? I'm hoping that there can be a collection of pairs all centered in a common community, or something like that.</li>\n<li>If a central community seems like a good idea, how should it be centralized?</li>\n<li>Probably some other issues/meta stuff.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zfvng53hsYtrPjX5d", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 26, "extendedScore": null, "score": 1.4191033543357215e-06, "legacy": true, "legacyId": "24671", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w5F4w8tNZc6LcBKRP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-11-12T05:44:57.723Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-12T09:12:31.710Z", "modifiedAt": null, "url": null, "title": "Meetup : Amsterdam", "slug": "meetup-amsterdam", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:38.590Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AoLAJcSBQH2FmWXWG/meetup-amsterdam", "pageUrlRelative": "/posts/AoLAJcSBQH2FmWXWG/meetup-amsterdam", "linkUrl": "https://www.lesswrong.com/posts/AoLAJcSBQH2FmWXWG/meetup-amsterdam", "postedAtFormatted": "Tuesday, November 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Amsterdam&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Amsterdam%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAoLAJcSBQH2FmWXWG%2Fmeetup-amsterdam%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Amsterdam%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAoLAJcSBQH2FmWXWG%2Fmeetup-amsterdam", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAoLAJcSBQH2FmWXWG%2Fmeetup-amsterdam", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tg'>Amsterdam</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 November 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Oosterdokseiland 4, Amsterdam, The Netherlands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a Netherlands LessWrong meetup on Saturday the 23rd. We're meeting in the Starbucks / \"East Dock Lounge\" on the Oosterdokseiland next to Amsterdam's Central Station. (The building is new and not yet pictured on Google Maps, but I've verified that it exists in the territory. Note that there's also a Starbucks in the station itself; that isn't where we're meeting.)</p>\n\n<p>I'll bring a sign that says \"LW\". You can reach me at 0611431304.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tg'>Amsterdam</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AoLAJcSBQH2FmWXWG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.4193059206859633e-06, "legacy": true, "legacyId": "24672", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Amsterdam\">Discussion article for the meetup : <a href=\"/meetups/tg\">Amsterdam</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 November 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Oosterdokseiland 4, Amsterdam, The Netherlands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a Netherlands LessWrong meetup on Saturday the 23rd. We're meeting in the Starbucks / \"East Dock Lounge\" on the Oosterdokseiland next to Amsterdam's Central Station. (The building is new and not yet pictured on Google Maps, but I've verified that it exists in the territory. Note that there's also a Starbucks in the station itself; that isn't where we're meeting.)</p>\n\n<p>I'll bring a sign that says \"LW\". You can reach me at 0611431304.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Amsterdam1\">Discussion article for the meetup : <a href=\"/meetups/tg\">Amsterdam</a></h2>", "sections": [{"title": "Discussion article for the meetup : Amsterdam", "anchor": "Discussion_article_for_the_meetup___Amsterdam", "level": 1}, {"title": "Discussion article for the meetup : Amsterdam", "anchor": "Discussion_article_for_the_meetup___Amsterdam1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-12T14:39:13.441Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup - New venue", "slug": "meetup-london-social-meetup-new-venue", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xqmWBmq7An7vfvane/meetup-london-social-meetup-new-venue", "pageUrlRelative": "/posts/xqmWBmq7An7vfvane/meetup-london-social-meetup-new-venue", "linkUrl": "https://www.lesswrong.com/posts/xqmWBmq7An7vfvane/meetup-london-social-meetup-new-venue", "postedAtFormatted": "Tuesday, November 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup%20-%20New%20venue&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%20-%20New%20venue%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqmWBmq7An7vfvane%2Fmeetup-london-social-meetup-new-venue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20-%20New%20venue%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqmWBmq7An7vfvane%2Fmeetup-london-social-meetup-new-venue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqmWBmq7An7vfvane%2Fmeetup-london-social-meetup-new-venue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/th'>London social meetup - New venue</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">London W6 8BS</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LW London is experimenting with switching to weekly meetups, and <em>also</em> experimenting with a new venue! You should come along and complain about poor experimental design, over discussion of whatever else seems interesting.</p>\n\n<p>I can't get the map to point at the right place, so: the venue is <a href=\"http://goo.gl/maps/qQ0Cp\" rel=\"nofollow\">The Latymers, on Hammersmith Road</a>. It's about five minutes' walk from the Hammersmith tube station, served by the District, Piccadilly, Circle, and Hammersmith &amp; City lines. We'll meet at 2pm, and we'll have the two tables on the right as you enter, and we'll have a sign identifying us as us.</p>\n\n<p>Feel free to contact me through email (philip.hazelden@gmail.com) or phone (07792009646) if you have questions or difficulty finding us.</p>\n\n<p>Also check out our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/th'>London social meetup - New venue</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xqmWBmq7An7vfvane", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "24673", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___New_venue\">Discussion article for the meetup : <a href=\"/meetups/th\">London social meetup - New venue</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">London W6 8BS</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LW London is experimenting with switching to weekly meetups, and <em>also</em> experimenting with a new venue! You should come along and complain about poor experimental design, over discussion of whatever else seems interesting.</p>\n\n<p>I can't get the map to point at the right place, so: the venue is <a href=\"http://goo.gl/maps/qQ0Cp\" rel=\"nofollow\">The Latymers, on Hammersmith Road</a>. It's about five minutes' walk from the Hammersmith tube station, served by the District, Piccadilly, Circle, and Hammersmith &amp; City lines. We'll meet at 2pm, and we'll have the two tables on the right as you enter, and we'll have a sign identifying us as us.</p>\n\n<p>Feel free to contact me through email (philip.hazelden@gmail.com) or phone (07792009646) if you have questions or difficulty finding us.</p>\n\n<p>Also check out our <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup___New_venue1\">Discussion article for the meetup : <a href=\"/meetups/th\">London social meetup - New venue</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup - New venue", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___New_venue", "level": 1}, {"title": "Discussion article for the meetup : London social meetup - New venue", "anchor": "Discussion_article_for_the_meetup___London_social_meetup___New_venue1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-12T18:44:13.952Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels monthly meetup: time!", "slug": "meetup-brussels-monthly-meetup-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.706Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EuL2c24gptxvHBNPM/meetup-brussels-monthly-meetup-time", "pageUrlRelative": "/posts/EuL2c24gptxvHBNPM/meetup-brussels-monthly-meetup-time", "linkUrl": "https://www.lesswrong.com/posts/EuL2c24gptxvHBNPM/meetup-brussels-monthly-meetup-time", "postedAtFormatted": "Tuesday, November 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20time!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20monthly%20meetup%3A%20time!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEuL2c24gptxvHBNPM%2Fmeetup-brussels-monthly-meetup-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20time!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEuL2c24gptxvHBNPM%2Fmeetup-brussels-monthly-meetup-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEuL2c24gptxvHBNPM%2Fmeetup-brussels-monthly-meetup-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ti'>Brussels monthly meetup: time!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 December 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's topic: time! Are you struggling with the planning fallacy? Do you have scientific knowledge and/or science-fiction recommendations to share? Have you been stuck in the 14th of December 2013 for the past thousand years? If you have, there are worse places to spend that afternoon than with LW Brussels.</p>\n\n<p>I'll try to find some kind of relevant experiment or game we can do, to continue the streak. Suggestions welcome.</p>\n\n<p>From a non-linear, non-subjective viewpoint, we will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members. Time travelers, please come incognito.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</p>\n\n<p>The Brussels meetups use a Google Group: https://groups.google.com/forum/#!forum/lesswrong-brussels</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ti'>Brussels monthly meetup: time!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EuL2c24gptxvHBNPM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4198641174114775e-06, "legacy": true, "legacyId": "24676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__time_\">Discussion article for the meetup : <a href=\"/meetups/ti\">Brussels monthly meetup: time!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 December 2013 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's topic: time! Are you struggling with the planning fallacy? Do you have scientific knowledge and/or science-fiction recommendations to share? Have you been stuck in the 14th of December 2013 for the past thousand years? If you have, there are worse places to spend that afternoon than with LW Brussels.</p>\n\n<p>I'll try to find some kind of relevant experiment or game we can do, to continue the streak. Suggestions welcome.</p>\n\n<p>From a non-linear, non-subjective viewpoint, we will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members. Time travelers, please come incognito.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</p>\n\n<p>The Brussels meetups use a Google Group: https://groups.google.com/forum/#!forum/lesswrong-brussels</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__time_1\">Discussion article for the meetup : <a href=\"/meetups/ti\">Brussels monthly meetup: time!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels monthly meetup: time!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__time_", "level": 1}, {"title": "Discussion article for the meetup : Brussels monthly meetup: time!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__time_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-12T19:01:50.622Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal Meetup: Long-term decision-making", "slug": "meetup-montreal-meetup-long-term-decision-making", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oriane", "createdAt": "2013-06-15T06:22:47.851Z", "isAdmin": false, "displayName": "Oriane"}, "userId": "aKb3Xf7wYLxw7mQjC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YTa3GDeLJh8toDGoC/meetup-montreal-meetup-long-term-decision-making", "pageUrlRelative": "/posts/YTa3GDeLJh8toDGoC/meetup-montreal-meetup-long-term-decision-making", "linkUrl": "https://www.lesswrong.com/posts/YTa3GDeLJh8toDGoC/meetup-montreal-meetup-long-term-decision-making", "postedAtFormatted": "Tuesday, November 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20Meetup%3A%20Long-term%20decision-making&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20Meetup%3A%20Long-term%20decision-making%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTa3GDeLJh8toDGoC%2Fmeetup-montreal-meetup-long-term-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20Meetup%3A%20Long-term%20decision-making%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTa3GDeLJh8toDGoC%2Fmeetup-montreal-meetup-long-term-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTa3GDeLJh8toDGoC%2Fmeetup-montreal-meetup-long-term-decision-making", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tj'>Montreal Meetup: Long-term decision-making</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 November 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">McGill University</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The meetup will take place in the Cyberth\u00e8que Pod 1 in Redpath Library Building.</p>\n\n<p>The idea would be to find common patterns in our ways to make important long-term decisions, for example when choosing a university degree or a job, which is a problem that concerns a majority of us. \nWe will try to find tools to help us break or reinforce those patterns depending on what we think the major problems for us are (ex: aversion, sense of being overwhelmed, procrastination, difficulty to create subgoals, difficulty to narrow down choices, etc.)</p>\n\n<p>If you cannot find the library pod, feel free to call/text at: 514 582 1052.</p>\n\n<p>Another Montreal meetup will take place on Tuesday November 19th in Pod 3 at 6pm</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tj'>Montreal Meetup: Long-term decision-making</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YTa3GDeLJh8toDGoC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4198813186271237e-06, "legacy": true, "legacyId": "24677", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup__Long_term_decision_making\">Discussion article for the meetup : <a href=\"/meetups/tj\">Montreal Meetup: Long-term decision-making</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 November 2013 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">McGill University</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The meetup will take place in the Cyberth\u00e8que Pod 1 in Redpath Library Building.</p>\n\n<p>The idea would be to find common patterns in our ways to make important long-term decisions, for example when choosing a university degree or a job, which is a problem that concerns a majority of us. \nWe will try to find tools to help us break or reinforce those patterns depending on what we think the major problems for us are (ex: aversion, sense of being overwhelmed, procrastination, difficulty to create subgoals, difficulty to narrow down choices, etc.)</p>\n\n<p>If you cannot find the library pod, feel free to call/text at: 514 582 1052.</p>\n\n<p>Another Montreal meetup will take place on Tuesday November 19th in Pod 3 at 6pm</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup__Long_term_decision_making1\">Discussion article for the meetup : <a href=\"/meetups/tj\">Montreal Meetup: Long-term decision-making</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal Meetup: Long-term decision-making", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup__Long_term_decision_making", "level": 1}, {"title": "Discussion article for the meetup : Montreal Meetup: Long-term decision-making", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup__Long_term_decision_making1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T03:39:47.861Z", "modifiedAt": null, "url": null, "title": "How to have high-value conversations", "slug": "how-to-have-high-value-conversations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:38.757Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3nMNa9cKYkYdoSf3D/how-to-have-high-value-conversations", "pageUrlRelative": "/posts/3nMNa9cKYkYdoSf3D/how-to-have-high-value-conversations", "linkUrl": "https://www.lesswrong.com/posts/3nMNa9cKYkYdoSf3D/how-to-have-high-value-conversations", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20have%20high-value%20conversations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20have%20high-value%20conversations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3nMNa9cKYkYdoSf3D%2Fhow-to-have-high-value-conversations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20have%20high-value%20conversations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3nMNa9cKYkYdoSf3D%2Fhow-to-have-high-value-conversations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3nMNa9cKYkYdoSf3D%2Fhow-to-have-high-value-conversations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 651, "htmlBody": "<p>Since I moved into the Boston rationalist house, I've found myself having an overwhelming amount of conversation compared to my previous baseline. The conversations at Citadel tend to be fairly intellectual and interesting, but there is a lot of topic drift and tendency for entertainment over depth, which seems to be a fairly common pitfall. How can we optimize conversations and direct them towards areas of usefulness and insight?</p>\n<p>There have been some previous discussions on this topic on LW, e.g. on <a href=\"/lw/6m4/having_useful_conversations/\">useful ways</a> to avoid low-value conversations or steer out of them. I would like to focus on the complementary skill of stimulating high-value directions in a conversation.</p>\n<p>First of all, what makes a conversation high-value? There are several possible metrics:</p>\n<ul>\n<li>people learning from each other&rsquo;s expertise and experience</li>\n<li>people getting to know each other better</li>\n<li>exchange of advice and feedback</li>\n<li>generating ideas and insights</li>\n</ul>\n<p>All of these involve increasing the total amount of information available to the participants, either through revealing information that is already there, or through creating new information. This is more likely to happen in a topic area where someone has strong opinions or expertise, or, on the other hand, an area that someone finds challenging where they stand to learn a lot.</p>\n<p>One effective way to steer a conversation is through asking purposeful questions. The questions should have sufficient depth to lead to interesting answers, but not be vague or put the other person on the spot. In that sense, a question like &ldquo;What have you been thinking about lately?&rdquo; is better than &ldquo;What do you care about?&rdquo; or &ldquo;What are you terminal goals?&rdquo;. It is better if the question leaves a line of retreat and doesn&rsquo;t make the person feel low status if they don&rsquo;t have an answer.</p>\n<p>The types of questions that are productive and comfortable are generally different for group and one-on-one conversations. Two-person conversations are more conducive to openness, so one would be able to ask personal questions like</p>\n<ul>\n<li>what memes have affected you strongly in the past or shaped your beliefs?</li>\n<li>what has been important to you lately?</li>\n<li>what has been difficult for you lately?</li>\n<li>what eccentric things have you done?</li>\n</ul>\n<p>Some questions are likely to lead to interesting topics in an N-person conversation for any N:</p>\n<ul>\n<li>what have you learned recently?</li>\n<li>what surprised you about experience X?</li>\n<li>what have you been reading?</li>\n<li>who are your role models?</li>\n<li>I have been confused about X, does anyone have advice?</li>\n</ul>\n<p>It is generally harder to steer a group conversation in productive directions than a two-person conversation, but the payoff is higher as well, since more people&rsquo;s time is at stake. Since a single person has less influence in a group conversation, it&rsquo;s important to use it well. Sometimes the most useful thing to do in a group conversation is to split it into smaller conversations. Asking someone about a subject that only they are likely to be interested in might be considered impolite to the others, but often leads to better separate conversations for everyone involved.</p>\n<p>Questions do have <a href=\"/lw/2co/how_to_always_have_interesting_conversations/25df\">limitations</a> as a conversation tactic, and can sometimes result in awkward silence or a string of brief uninformative replies. If this happens, it&rsquo;s handy to be prepared to answer your own question, which might inspire others to answer it as well. It is generally a good idea to have something that you&rsquo;d like to talk about, perhaps something you've been working on or a concept that puzzles you, that you can bring up independently of whether and how people respond to your questions. Thinking in advance of topics to discuss with specific people is especially useful, e.g. relating to their past experiences or skill areas.</p>\n<p>Do people have advice or good examples of directing conversations? Recalling the best conversations you've ever had, what made them happen?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3nMNa9cKYkYdoSf3D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 21, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "24684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CCriujRF39gWF5xAw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T06:09:53.715Z", "modifiedAt": null, "url": null, "title": "Buying Debt as Effective Altruism?", "slug": "buying-debt-as-effective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:35.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aarongertler", "createdAt": "2017-06-17T00:54:32.818Z", "isAdmin": false, "displayName": "aarongertler"}, "userId": "8mnQfaaqwi2mevNtq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o8c9vkhpq9APphvxW/buying-debt-as-effective-altruism", "pageUrlRelative": "/posts/o8c9vkhpq9APphvxW/buying-debt-as-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/o8c9vkhpq9APphvxW/buying-debt-as-effective-altruism", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Buying%20Debt%20as%20Effective%20Altruism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuying%20Debt%20as%20Effective%20Altruism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8c9vkhpq9APphvxW%2Fbuying-debt-as-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Buying%20Debt%20as%20Effective%20Altruism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8c9vkhpq9APphvxW%2Fbuying-debt-as-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8c9vkhpq9APphvxW%2Fbuying-debt-as-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 382, "htmlBody": "<p><strong><a href=\"http://www.theguardian.com/world/2013/nov/12/occupy-wall-street-activists-15m-personal-debt\">http://www.theguardian.com/world/2013/nov/12/occupy-wall-street-activists-15m-personal-debt</a></strong></p>\n<p><strong>A collection of Occupy activists recently bought over $14,000,000 in personal debt for $400,000.</strong></p>\n<p>Normally, debt-buying companies do this with the intention of collecting the money from the debtors--Occupy did not, and I was struck by the lopsidedness of the figures.</p>\n<p>A number I see often in the high-impact philanthropy world is $2300 to save a life (with plenty of caveats). At Occupy's rates, that would buy roughly $80,000 in debt--enough to get two or three families out of a hole that would otherwise render them bankrupt.</p>\n<p>By itself, this isn't enough to be better than mosquito nets or deworming. But the thing about personal debt is that, thanks to interest payments and stress, it prevents people with high earning potential (compared to an average African) from making decisions that would optimal were they debt-free--like finishing college or buying a used car so they can take on a higher-paying job.</p>\n<p><strong>My idea, though it's a tentative, spur-of-the-moment thing:</strong></p>\n<p>Why not found a charity that acts like a combination of <strong><a href=\"http://www.vittana.org/\">Vittana</a></strong> and <a href=\"http://www.givingwhatwecan.org/\"><strong>Giving What We Can</strong></a>, freeing people with good prospects from debt in exchange for their signing a contract to donate a small portion of their future salary to charity?</p>\n<p><strong><br />A few issues that come to mind:</strong></p>\n<p>1) Occupy bought a lot of medical debt, which this company wouldn't, and other types of debt might be harder to buy.</p>\n<p>2) People who have decent earning potential have more valuable debt, since they're more likely to pay it off later. (On the other hand, freeing them of interest payments might help them get into a better position for repayment.)</p>\n<p>3) The idea is a lot like micro-lending, and organizations that offer that service don't have a great track record (though some have been successful).</p>\n<p>4) People just freed from debt might not be in a position to donate much salary/might be unreliable. (Deferred payments until college is finished/the new job is had could be helpful here.)</p>\n<p>5) There might be (well, almost certainly are) difficult legal issues with finding information on people in debt before you actually own their debt.</p>\n<p>Are there any other obstacles you all can think of? Other features of the charity that might make it more effective? How does it sound as an intervention that increases the world's productivity in the long run, stacked up against other such interventions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o8c9vkhpq9APphvxW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 17, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "24687", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"http___www_theguardian_com_world_2013_nov_12_occupy_wall_street_activists_15m_personal_debt\"><a href=\"http://www.theguardian.com/world/2013/nov/12/occupy-wall-street-activists-15m-personal-debt\">http://www.theguardian.com/world/2013/nov/12/occupy-wall-street-activists-15m-personal-debt</a></strong></p>\n<p><strong id=\"A_collection_of_Occupy_activists_recently_bought_over__14_000_000_in_personal_debt_for__400_000_\">A collection of Occupy activists recently bought over $14,000,000 in personal debt for $400,000.</strong></p>\n<p>Normally, debt-buying companies do this with the intention of collecting the money from the debtors--Occupy did not, and I was struck by the lopsidedness of the figures.</p>\n<p>A number I see often in the high-impact philanthropy world is $2300 to save a life (with plenty of caveats). At Occupy's rates, that would buy roughly $80,000 in debt--enough to get two or three families out of a hole that would otherwise render them bankrupt.</p>\n<p>By itself, this isn't enough to be better than mosquito nets or deworming. But the thing about personal debt is that, thanks to interest payments and stress, it prevents people with high earning potential (compared to an average African) from making decisions that would optimal were they debt-free--like finishing college or buying a used car so they can take on a higher-paying job.</p>\n<p><strong id=\"My_idea__though_it_s_a_tentative__spur_of_the_moment_thing_\">My idea, though it's a tentative, spur-of-the-moment thing:</strong></p>\n<p>Why not found a charity that acts like a combination of <strong><a href=\"http://www.vittana.org/\">Vittana</a></strong> and <a href=\"http://www.givingwhatwecan.org/\"><strong>Giving What We Can</strong></a>, freeing people with good prospects from debt in exchange for their signing a contract to donate a small portion of their future salary to charity?</p>\n<p><strong id=\"A_few_issues_that_come_to_mind_\"><br>A few issues that come to mind:</strong></p>\n<p>1) Occupy bought a lot of medical debt, which this company wouldn't, and other types of debt might be harder to buy.</p>\n<p>2) People who have decent earning potential have more valuable debt, since they're more likely to pay it off later. (On the other hand, freeing them of interest payments might help them get into a better position for repayment.)</p>\n<p>3) The idea is a lot like micro-lending, and organizations that offer that service don't have a great track record (though some have been successful).</p>\n<p>4) People just freed from debt might not be in a position to donate much salary/might be unreliable. (Deferred payments until college is finished/the new job is had could be helpful here.)</p>\n<p>5) There might be (well, almost certainly are) difficult legal issues with finding information on people in debt before you actually own their debt.</p>\n<p>Are there any other obstacles you all can think of? Other features of the charity that might make it more effective? How does it sound as an intervention that increases the world's productivity in the long run, stacked up against other such interventions?</p>", "sections": [{"title": "http://www.theguardian.com/world/2013/nov/12/occupy-wall-street-activists-15m-personal-debt", "anchor": "http___www_theguardian_com_world_2013_nov_12_occupy_wall_street_activists_15m_personal_debt", "level": 1}, {"title": "A collection of Occupy activists recently bought over $14,000,000 in personal debt for $400,000.", "anchor": "A_collection_of_Occupy_activists_recently_bought_over__14_000_000_in_personal_debt_for__400_000_", "level": 1}, {"title": "My idea, though it's a tentative, spur-of-the-moment thing:", "anchor": "My_idea__though_it_s_a_tentative__spur_of_the_moment_thing_", "level": 1}, {"title": "A few issues that come to mind:", "anchor": "A_few_issues_that_come_to_mind_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T18:02:39.641Z", "modifiedAt": null, "url": null, "title": "Wolfram's new \"Cloud\" initiative", "slug": "wolfram-s-new-cloud-initiative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:33.417Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gvaerg", "createdAt": "2013-06-09T12:59:57.465Z", "isAdmin": false, "displayName": "Gvaerg"}, "userId": "cJWZEv5sCrY2MShPT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MysJBfpFWoueyL9ao/wolfram-s-new-cloud-initiative", "pageUrlRelative": "/posts/MysJBfpFWoueyL9ao/wolfram-s-new-cloud-initiative", "linkUrl": "https://www.lesswrong.com/posts/MysJBfpFWoueyL9ao/wolfram-s-new-cloud-initiative", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wolfram's%20new%20%22Cloud%22%20initiative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWolfram's%20new%20%22Cloud%22%20initiative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMysJBfpFWoueyL9ao%2Fwolfram-s-new-cloud-initiative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wolfram's%20new%20%22Cloud%22%20initiative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMysJBfpFWoueyL9ao%2Fwolfram-s-new-cloud-initiative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMysJBfpFWoueyL9ao%2Fwolfram-s-new-cloud-initiative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p>I was reading through this:</p>\n<p><a href=\"http://blog.stephenwolfram.com/2013/11/something-very-big-is-coming-our-most-important-technology-project-yet/\">http://blog.stephenwolfram.com/2013/11/something-very-big-is-coming-our-most-important-technology-project-yet/</a></p>\n<p>which announces the new <a href=\"http://www.wolframcloud.com/\">\"Wolfram Cloud\"</a> project of Stephen Wolfram and I noticed how much this sounds like \"Flare\" and EY's old <a href=\"https://web.archive.org/web/20130508135328/http://yudkowsky.net/obsolete/plan.html\">\"Plan to Singularity\"</a> document (linking from archive.org because the site seems to not show that particular page right now).</p>\n<p>Only this time it's for real.</p>\n<p>What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MysJBfpFWoueyL9ao", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 1.4212311439503526e-06, "legacy": true, "legacyId": "24691", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T19:36:40.270Z", "modifiedAt": null, "url": null, "title": "Meetup : New Meetup: Milwaukee", "slug": "meetup-new-meetup-milwaukee", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CyrilDan", "createdAt": "2013-11-12T20:08:23.896Z", "isAdmin": false, "displayName": "CyrilDan"}, "userId": "mvuZ5zS5ovQCzsWZ7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kFxxh5ygA9gGB2p5K/meetup-new-meetup-milwaukee", "pageUrlRelative": "/posts/kFxxh5ygA9gGB2p5K/meetup-new-meetup-milwaukee", "linkUrl": "https://www.lesswrong.com/posts/kFxxh5ygA9gGB2p5K/meetup-new-meetup-milwaukee", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20New%20Meetup%3A%20Milwaukee&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20New%20Meetup%3A%20Milwaukee%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFxxh5ygA9gGB2p5K%2Fmeetup-new-meetup-milwaukee%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20New%20Meetup%3A%20Milwaukee%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFxxh5ygA9gGB2p5K%2Fmeetup-new-meetup-milwaukee", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFxxh5ygA9gGB2p5K%2Fmeetup-new-meetup-milwaukee", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tk'>New Meetup: Milwaukee</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 November 2013 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Starbucks, 920 N Water St, Milwaukee, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hoping to start a Milwaukee LW chapter. Come on in and meet some new people!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tk'>New Meetup: Milwaukee</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kFxxh5ygA9gGB2p5K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4213231269718626e-06, "legacy": true, "legacyId": "24692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___New_Meetup__Milwaukee\">Discussion article for the meetup : <a href=\"/meetups/tk\">New Meetup: Milwaukee</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 November 2013 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Starbucks, 920 N Water St, Milwaukee, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hoping to start a Milwaukee LW chapter. Come on in and meet some new people!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___New_Meetup__Milwaukee1\">Discussion article for the meetup : <a href=\"/meetups/tk\">New Meetup: Milwaukee</a></h2>", "sections": [{"title": "Discussion article for the meetup : New Meetup: Milwaukee", "anchor": "Discussion_article_for_the_meetup___New_Meetup__Milwaukee", "level": 1}, {"title": "Discussion article for the meetup : New Meetup: Milwaukee", "anchor": "Discussion_article_for_the_meetup___New_Meetup__Milwaukee1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T20:38:54.068Z", "modifiedAt": null, "url": null, "title": "LessWrong Study Hall will be password-protected", "slug": "lesswrong-study-hall-will-be-password-protected", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:35.409Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tkadlubo", "createdAt": "2017-06-17T00:52:52.525Z", "isAdmin": false, "displayName": "tkadlubo"}, "userId": "r5sMqghv4tZrZWgdt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BCvED5FkJ9GBAwQ9M/lesswrong-study-hall-will-be-password-protected", "pageUrlRelative": "/posts/BCvED5FkJ9GBAwQ9M/lesswrong-study-hall-will-be-password-protected", "linkUrl": "https://www.lesswrong.com/posts/BCvED5FkJ9GBAwQ9M/lesswrong-study-hall-will-be-password-protected", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Study%20Hall%20will%20be%20password-protected&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Study%20Hall%20will%20be%20password-protected%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCvED5FkJ9GBAwQ9M%2Flesswrong-study-hall-will-be-password-protected%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Study%20Hall%20will%20be%20password-protected%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCvED5FkJ9GBAwQ9M%2Flesswrong-study-hall-will-be-password-protected", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCvED5FkJ9GBAwQ9M%2Flesswrong-study-hall-will-be-password-protected", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>The community of the <a href=\"http://tinychat.com/lesswrong\">LessWrong Study Hall</a> decided to protect access to the chatroom with a password. The password is: \"<strong>lw</strong>\".</p>\n<p>Rationale: the LessWrong Study Hall started to have a serious trolling problem recently. As it grew in popularity it started to be automatically advertised on the tinychat.com main page, which attracts a demographics that has no idea about the goals of the Study Hall. We have no business dealing with those who seek online company to smoke weed and/or masturbate.</p>\n<p>The password should be publicly available, and shown in plain text whenever the URL to the Study Hall is mentioned. The only users we want to exclude are the internet passers-by, who have literally no idea what that chatroom is about.</p>\n<p>The password will be introduced in a day or two, after we inform all the regulars about the change.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BCvED5FkJ9GBAwQ9M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 30, "extendedScore": null, "score": 1.421384020637978e-06, "legacy": true, "legacyId": "24693", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T20:40:43.402Z", "modifiedAt": null, "url": null, "title": "Am I Understanding Bayes Right?", "slug": "am-i-understanding-bayes-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CyrilDan", "createdAt": "2013-11-12T20:08:23.896Z", "isAdmin": false, "displayName": "CyrilDan"}, "userId": "mvuZ5zS5ovQCzsWZ7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K9j6jnFk7Pi7JpdaG/am-i-understanding-bayes-right", "pageUrlRelative": "/posts/K9j6jnFk7Pi7JpdaG/am-i-understanding-bayes-right", "linkUrl": "https://www.lesswrong.com/posts/K9j6jnFk7Pi7JpdaG/am-i-understanding-bayes-right", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Am%20I%20Understanding%20Bayes%20Right%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAm%20I%20Understanding%20Bayes%20Right%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9j6jnFk7Pi7JpdaG%2Fam-i-understanding-bayes-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Am%20I%20Understanding%20Bayes%20Right%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9j6jnFk7Pi7JpdaG%2Fam-i-understanding-bayes-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9j6jnFk7Pi7JpdaG%2Fam-i-understanding-bayes-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1091, "htmlBody": "<p>Hello, everyone.</p>\n<p>&nbsp;</p>\n<p>I'm relatively new here as a user rather than as a lurker, but even after trying to read ever tutorial on Bayes' Theorem I could get my hands on, I'm still not sure I understand it. So I was hoping that I could explain Bayesianism as I understand it, and some more experienced Bayesians could tell me where I'm going wrong (or maybe if I'm not going wrong and it's a confidence issue rather than an actual knowledge issue). If this doesn't interest you at all, then feel free to tap out now, because here we go!</p>\n<p>&nbsp;</p>\n<p>Abstraction</p>\n<p>Bayes' Theorem is an application of probability. Probability is an abstraction based on logic, which is in turn based on possible worlds. By this I mean that they are both maps that refer to multiple territories: whereas a map of Cincinatti (or a \"map\" of what my brother is like, for instance), abstractions are good for more than one thing. Trigonometry is a map of not just this triangle here, but of all triangles everywhere, to the extent that they are triangular. Because of this it is useful even for triangular objects that one has never encountered before, but only tells you about it partially (e.g. it won't tell you the lengths of the sides, because that wouldn't be part of the definition of a triangle; also, it only works at scales at which the object in question approximates a triangle (i.e. the \"triangle\" map is probably useful at macroscopic scales, but breaks down as you get smaller).</p>\n<p>&nbsp;</p>\n<p>Logic and Possible Worlds</p>\n<p>Logic is an attempt to construct a map that covers as much territory as possible, ideally all of it. Thus when people say that logic is true at all times, at all places, and with all things, they aren't really telling you about the territory, they're telling you about the purpose of logic (in the same way that the \"triangle\" map is ideally useful for triangles at all times, at all places).</p>\n<p>&nbsp;</p>\n<p>One form of logic is Propositional Logic. In propositional logic, all the possible worlds are imagined as points. Each point is exactly one possible world: a logically-possible arrangement that gives a value to all the different variables in the universe. Ergo no two possible universes are exactly the same (though they will share elements).</p>\n<p>&nbsp;</p>\n<p>These possible universes are then joined together in sets called \"propositions\". These \"sets\" are Venn diagrams, or what George Lakoff refers to as \"container schemas\"). Thus, for any given set, every possible universe is either inside or outside of it, with no middle ground (see \"questions\" below). Thus if the set I'm referring to is the proposition \"The Snow is White\", that set would include all possible universes in which the snow is white. The rules of propositional logic follow from the container schema.</p>\n<p>&nbsp;</p>\n<p>Bayesian Probability</p>\n<p>If propositional logic is about what's inside a set or outside of a set, probability is about the size of the sets themselves. Probability is a measurement of how many possible worlds are inside a set, and conditional probability is about the size of the intersections of sets.</p>\n<p>&nbsp;</p>\n<p>Take the example of the dragon in your garage. To start with, there either is or isn't a dragon in your garage. Both sets of possible worlds have elements in them. But if we look in your garage and don't see a dragon, then that eliminates all the possibilities of there being a *visible* dragon in your garage, and thus eliminates those possible universes from the 'there is a dragon in your garage' set. In other words, the probability of that being true goes down. And because not seeing a dragon in your garage would be what you would expect if there in fact isn't a dragon in your garage, that set remains intact. Then if we look at the ratio of the remaining possible worlds, we see that the probability of the no-dragon-in-your-garage set has gone up, not because in absolute terms (because the set of all possible worlds is what we started with; there isn't any more!) but relative to the alternate hypothesis (in the same way that if the denominator of a fraction goes down, the size of the fraction goes up.)</p>\n<p>&nbsp;</p>\n<p>This is what Bayes' Theorem is about: the use of process of elimination to eliminate *part* of the set of a proposition, thus providing evidence against it without it being a full refutation.</p>\n<p>&nbsp;</p>\n<p>Naturally, this all takes place in ones mind: the world doesn't shift around you just because you've encountered new information. Probability is in this way subjective (it has to do with maps, not territories per se), but it's not arbitrary: as long as you accept that possible worlds/logic metaphor, it necessarily follows</p>\n<p>&nbsp;</p>\n<p>Questions/trouble points that I'm not sure of:</p>\n<p>*I keep seeing probability referred to as an estimation of how certain you are in a belief. And while I guess it could be argued that you should be certain of a belief relative to the number of possible worlds left or whatever, that doesn't necessarily follow. Does the above explanation differ from how other people use probability?</p>\n<p>*Also, if probability is defined as an arbitrary estimation of how sure you are, why should those estimations follow the laws of probability? I've heard the Dutch book argument, so I get why there might be practical reasons for obeying them, but unless you accept a pragmatist epistemology, that doesn't provide reasons why your beliefs are more likely to be true if you follow them. (I've also heard of Cox's rules, but I haven't been able to find a copy. And if I understand right, they says that Bayes' theorem follows from Boolean logic, which is similar to what I've said above, yes?)</p>\n<p>*Another question: above I used propositional logic, which is okay, but it's not exactly the creme de la creme of logics. I understand that fuzzy logics work better for a lot of things, and I'm familiar with predicate logics as well, but I'm not sure what the interaction of any of them is with probability or the use of it, although I know that technically probability doesn't have to be binary (sets just need to be exhaustive and mutually exclusive for the Kolmogorov axioms to work, right?). I don't know, maybe it's just something that I haven't learned yet, but the answer really is out there?</p>\n<p>&nbsp;</p>\n<p>Those are the only questions that are coming to mind right now (if I think of any more, I can probably ask them in comments). So anyone? Am I doing something wrong? Or do I feel more confused than I really am?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K9j6jnFk7Pi7JpdaG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.4213858037793204e-06, "legacy": true, "legacyId": "24694", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T20:47:20.587Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Thinking Fast and Slow Discussion November 17", "slug": "meetup-urbana-champaign-thinking-fast-and-slow-discussion-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ymnas6qD7fReSqqYR/meetup-urbana-champaign-thinking-fast-and-slow-discussion-0", "pageUrlRelative": "/posts/Ymnas6qD7fReSqqYR/meetup-urbana-champaign-thinking-fast-and-slow-discussion-0", "linkUrl": "https://www.lesswrong.com/posts/Ymnas6qD7fReSqqYR/meetup-urbana-champaign-thinking-fast-and-slow-discussion-0", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Discussion%20November%2017&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Discussion%20November%2017%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmnas6qD7fReSqqYR%2Fmeetup-urbana-champaign-thinking-fast-and-slow-discussion-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Thinking%20Fast%20and%20Slow%20Discussion%20November%2017%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmnas6qD7fReSqqYR%2Fmeetup-urbana-champaign-thinking-fast-and-slow-discussion-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmnas6qD7fReSqqYR%2Fmeetup-urbana-champaign-thinking-fast-and-slow-discussion-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tl'>Urbana-Champaign: Thinking Fast and Slow Discussion November 17</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coordinates are: 40.109545,-88.227318 Meetup will be held in the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Discussion will be on pages 140-210 of Thinking Fast and Slow.</p>\n\n<p>Cross posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/VDSdxVkU4QU\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tl'>Urbana-Champaign: Thinking Fast and Slow Discussion November 17</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ymnas6qD7fReSqqYR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.4213922816363902e-06, "legacy": true, "legacyId": "24695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion_November_17\">Discussion article for the meetup : <a href=\"/meetups/tl\">Urbana-Champaign: Thinking Fast and Slow Discussion November 17</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Coordinates are: 40.109545,-88.227318 Meetup will be held in the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Discussion will be on pages 140-210 of Thinking Fast and Slow.</p>\n\n<p>Cross posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/VDSdxVkU4QU\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion_November_171\">Discussion article for the meetup : <a href=\"/meetups/tl\">Urbana-Champaign: Thinking Fast and Slow Discussion November 17</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Thinking Fast and Slow Discussion November 17", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion_November_17", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Thinking Fast and Slow Discussion November 17", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Thinking_Fast_and_Slow_Discussion_November_171", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-13T21:18:32.474Z", "modifiedAt": null, "url": null, "title": "Help the Brain Preservation Foundation", "slug": "help-the-brain-preservation-foundation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:53.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aurellem", "createdAt": "2013-10-23T21:45:39.406Z", "isAdmin": false, "displayName": "aurellem"}, "userId": "7vWyB9qh5yx4f7sj7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C2E8Qt68PJmFnMN28/help-the-brain-preservation-foundation", "pageUrlRelative": "/posts/C2E8Qt68PJmFnMN28/help-the-brain-preservation-foundation", "linkUrl": "https://www.lesswrong.com/posts/C2E8Qt68PJmFnMN28/help-the-brain-preservation-foundation", "postedAtFormatted": "Wednesday, November 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20the%20Brain%20Preservation%20Foundation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20the%20Brain%20Preservation%20Foundation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC2E8Qt68PJmFnMN28%2Fhelp-the-brain-preservation-foundation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20the%20Brain%20Preservation%20Foundation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC2E8Qt68PJmFnMN28%2Fhelp-the-brain-preservation-foundation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC2E8Qt68PJmFnMN28%2Fhelp-the-brain-preservation-foundation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 343, "htmlBody": "<p>(First time poster, long time reader)<br /><br />I'm currently volunteering for the Brain Preservation Foundation (http://www.brainpreservation.org/), and I'd like to ask for your&nbsp; help.<br /><br />The purpose of the BPF is to incentivize and evaluate the development of technology which can preserve a human brain in such intricate detail that all of the brain's cells and connections are preserved. It's the only prize of its kind for a relatively endangered, yet essential type of research.<br /><br />We run a cash prize ($100,000 USD) called the \"Brain Preservation Technology Prize\" for the first team that can preserve a large mammal's brain to our high standards. The first $25,000 of that prize goes to the first team that can preserve the ultrastructure of a mouse brain.<br /><br />Steve Aoki (http://steveaoki.com/), a musician that you might have heard of, is currently planning to give around $50,000 to one of four brain-related charities. One of these charities is the Brain Preservation Foundation! Whichever charity gets the most votes will win all the money.<br /><br />This money is critically important to us to get the necessary supplies and lab time to administer the brain preservation technology prize. Evaluating brains that people send us involves electron microscopy, which is quite expensive (around $8,000 to evaluate a brain!) We are currently getting submissions and this extra money will give us the funds we need to run the prize.<br /><br />To vote, just visit http://on.fb.me/15XFdTG, and click the \"like\" button by the \"Brain Preservation Foundation\" comment. You can see a graph of the votes at http://aurellem.org/bpf/votes.png (updates every 15 minutes). Thanks for taking the time to read<br />and vote!<br /><br />More about the Brain Preservation Foundation :<br />http://www.brainpreservation.org/<br /><br />More about the charity:<br />https://www.facebook.com/photo.php?fbid=10151608608587461 <br /><br />Votes graph:<br />http://aurellem.org/bpf/votes.png<br /><br /><br />I'd also love to hear your own opinions on the BPF and your assessment of its effectiveness, as well as your thoughts on&nbsp; chemopreservation vs cryopreservation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jZF2jwLnPKBv6m3Ag": 1, "vmvTYnmaKA73fYDe5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C2E8Qt68PJmFnMN28", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 34, "extendedScore": null, "score": 1.4214228118686631e-06, "legacy": true, "legacyId": "24679", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-14T02:26:59.072Z", "modifiedAt": null, "url": null, "title": "To like, or not to like?", "slug": "to-like-or-not-to-like", "viewCount": null, "lastCommentedAt": "2021-06-05T07:34:59.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tiTk2tuCF27pswB6r/to-like-or-not-to-like", "pageUrlRelative": "/posts/tiTk2tuCF27pswB6r/to-like-or-not-to-like", "linkUrl": "https://www.lesswrong.com/posts/tiTk2tuCF27pswB6r/to-like-or-not-to-like", "postedAtFormatted": "Thursday, November 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20like%2C%20or%20not%20to%20like%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20like%2C%20or%20not%20to%20like%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtiTk2tuCF27pswB6r%2Fto-like-or-not-to-like%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20like%2C%20or%20not%20to%20like%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtiTk2tuCF27pswB6r%2Fto-like-or-not-to-like", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtiTk2tuCF27pswB6r%2Fto-like-or-not-to-like", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1116, "htmlBody": "<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">Do you like Shakespeare?</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">I've&nbsp;</span><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">been reading the </span><a style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\" href=\"http://www.theparisreview.org/interviews\">Paris Review interviews</a><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\"> with famous authors of the 20th century. Famous authors don't always like other famous authors. Hemingway, Faulkner, Joyce, Fitzgerald &mdash; for all of them, you could find some famous author who found them unreadable. (Especially Joyce and Faulkner.)</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">Except Shakespeare. Everyone loved Shakespeare. In fact, those who mentioned Shakespeare sometimes said he was the best author who has ever lived.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">How likely is this?</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\"><a id=\"more\"></a></span></p>\n<p><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">I have a divergent opinion. I realized this during a performance of </span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">A Midsummer Night's Dream</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">. I've seen the play three or four times. Every year, people perform it at Renaissance festivals, in Central Park, and in at least one high school within 5 miles of my house. I was sitting in the audience as they got into the part where Bottom acts like an ass and this is supposed to be funny. I was just waiting for them to get it over with, and then remembered that there was nothing after it in the play that I looked forward to anyway. I suddenly realized, \"This... is a </span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">bad play</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">.\" Up until that moment, I had somehow believed that it was one of my favorite plays without actually liking almost anything in it.</span></p>\n<p><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">A Midsummer Night's Dream</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\"> is supposed to be a magical romantic comedy. It contains nothing of the magic one finds in a Peter Beagle or Charles de Lint fantasy, less-stirring romances than the average fan-fiction, and less humor than one would find in a randomly-chosen paragraph of Terry Pratchett. It has never made me laugh or cry once. Yet even having read it, and having watched it at least twice, I somehow voluntarily paid to sit and suffer through it again when I still had unread stories by Chekov, Borges, Katherine Anne Porter, and a hundred other worthies whose work seldom failed to move me at least as much as Shakespeare's best.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">I have two competing hypotheses. Hypothesis #1 is that Shakespeare was the greatest author who ever lived, or at least in the top 10, whatever that means. You would be hard-pressed to find more than a handful of literary critics who would dispute this. Hypothesis #2 is that something about the time that Shakespeare wrote in made it very likely that we would elevate some writer from that time period to \"Greatest Writer Ever\". For instance:</span></p>\n<ul>\n<li><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">It was at the start of commercial English literature and of English military, economic, and cultural dominance, and someone had to be chosen.</span></li>\n<li><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">It was the one point in time (and this is true) when florid speech, as over-ornamented as the embroidery and ruffled sleeves of Elizabethan men's clothing, was in fashion.</span></li>\n<li><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">It was the only time since Chaucer (and this may also be true) when writers had contact with and immediate feedback from their audiences, and attempted to please both the opera-box and the pit at the same time.</span></li>\n<li><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">Shakespeare's world is so foreign to us, with its strange speech and clothing and worldview, that to a modern audience, Shakespeare is simply a fantasist with a colorful and meticulously-constructed fantasy world, richer and more consistent than Tolkien's, that we love to visit.</span></li>\n</ul>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">I can easily compute how likely it is that one of the Elizabethan authors was the greatest author of all time given that hypothesis 2 is false: It is the number of Elizabethan authors divided by number of authors of all time.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">So how many Elizabethan authors were there? This is probably the sort of thing that shouldn't be attempted using Google, but I don't have a university library at hand. Using Google, it appears that we have about 600 plays from that time period. Most of the writing from that time seems to have been by amateur poets, mostly members of the nobility. The number of serious authors during the Elizabethan period &mdash; and I'm really guessing here; the number of distinct professional author names I've come across is about a dozen &mdash; might be around 100.</span></p>\n<p><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">How many people write novels in English today? Hard to say, but </span><a style=\"line-height: 1.15; text-decoration: none;\" href=\"http://mattwilkens.com/2009/10/14/how-many-novels-are-published-each-year/\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">this web page</span></a><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\"> makes a reasonable case that about 100,000 novels in English are published each year. Publishers accept about one out of every thousand books submitted; it is not unusual for a book to be submitted to 10 different publishers. I will therefore estimate that 10 million novelists write 10 million novels in English every year today. Our first approximation for the prior odds for some Elizabethan author of being the greatest English writer of all time are therefore about one in 100,000. I'm going to multiply this by a factor of 10 to account for the fact that authors in Elizabethan times had no libraries, and few good writings to take as models even if they'd been able to acquire copies. I'm going to multiply by another factor of 10 to account for the strange fact that almost everyone </span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">agrees</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\"> that Shakespeare is the greatest writer of all time, when this is not how appraisals of artistic merit ever work. It is almost never the case that a blinded evaluation of the works of different experts in any kind of art results in a unanimous opinion on which one is the greatest. I suppose Beethoven or Aristotle might be such cases, but I do not find the degree of unanimity regarding their merits versus Bach and Newton that I find on the merits of Shakespeare versus everyone else. This gives prior odds of one in 10 million.</span></p>\n<p><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">(Yes, I am actually arguing that unanimity of expert opinion in this case makes that expert opinion <em>less</em> likely, because non-merit-based mechanisms produce unanimity much more often than objective evaluations of artistic merit.)</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">At this point, is there even any need to consider the proposition that Shakespeare was the greatest author of all time? For myself, I think not. There's nothing left to explain away. Sure, there are people claiming that <em>Hamlet</em> or <em>King Lear</em> are masterpieces. But I already know that some weird mechanism is at work that convinces people <em>every day</em> to actually pay money to watch <em>A Comedy of Errors</em>. Whatever that mechanism is, it can also explain our attachment to <em>Hamlet.</em></span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">Given that I <em>know</em> there's a powerful reality-distortion field around Shakespeare, isn't it more rational to assume that whatever fondness I have for any Shakespeare play is a result of that field, than to try to evaluate the play and trust in my superhuman ability to resist that field's force?</span></p>\n<p><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\">And what do you do if you still feel that you </span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">like</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline; white-space: pre-wrap;\"> Shakespeare? If you logically conclude that you've been deceived into over-valuing his work, do you will yourself by force of intellect to stop liking it so much?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tiTk2tuCF27pswB6r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 1, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "24700", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-14T03:54:55.400Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 97-98", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-97-98", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x4ZdqjGKkdWPc3kyR/meetup-durham-rtlw-hpmor-discussion-ch-97-98", "pageUrlRelative": "/posts/x4ZdqjGKkdWPc3kyR/meetup-durham-rtlw-hpmor-discussion-ch-97-98", "linkUrl": "https://www.lesswrong.com/posts/x4ZdqjGKkdWPc3kyR/meetup-durham-rtlw-hpmor-discussion-ch-97-98", "postedAtFormatted": "Thursday, November 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2097-98&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2097-98%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4ZdqjGKkdWPc3kyR%2Fmeetup-durham-rtlw-hpmor-discussion-ch-97-98%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2097-98%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4ZdqjGKkdWPc3kyR%2Fmeetup-durham-rtlw-hpmor-discussion-ch-97-98", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4ZdqjGKkdWPc3kyR%2Fmeetup-durham-rtlw-hpmor-discussion-ch-97-98", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tm'>Durham/RTLW HPMoR discussion, ch. 97-98</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 November 2013 12:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham, NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It might be our last HPMoR meeting for a while!  Bring a question, a reflection, or an epic tale of HPMoR-enabled conversions to rationality.</p>\n\n<p>12:00 gather at Fullsteam <br />\n12:30 discuss <br />\n2:00 wrap up formal discussion</p>\n\n<p>Tentative plans exist to hang out awhile at Fullsteam after formal discussion concludes.  There might be games and there will definitely be beers.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tm'>Durham/RTLW HPMoR discussion, ch. 97-98</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x4ZdqjGKkdWPc3kyR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__97_98\">Discussion article for the meetup : <a href=\"/meetups/tm\">Durham/RTLW HPMoR discussion, ch. 97-98</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 November 2013 12:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham, NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It might be our last HPMoR meeting for a while!  Bring a question, a reflection, or an epic tale of HPMoR-enabled conversions to rationality.</p>\n\n<p>12:00 gather at Fullsteam <br>\n12:30 discuss <br>\n2:00 wrap up formal discussion</p>\n\n<p>Tentative plans exist to hang out awhile at Fullsteam after formal discussion concludes.  There might be games and there will definitely be beers.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__97_981\">Discussion article for the meetup : <a href=\"/meetups/tm\">Durham/RTLW HPMoR discussion, ch. 97-98</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 97-98", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__97_98", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 97-98", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__97_981", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-14T16:17:21.558Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014In Apprehending Hard Stuff", "slug": "meetup-west-la-in-apprehending-hard-stuff", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iMYo5DF78SD3nmQ2q/meetup-west-la-in-apprehending-hard-stuff", "pageUrlRelative": "/posts/iMYo5DF78SD3nmQ2q/meetup-west-la-in-apprehending-hard-stuff", "linkUrl": "https://www.lesswrong.com/posts/iMYo5DF78SD3nmQ2q/meetup-west-la-in-apprehending-hard-stuff", "postedAtFormatted": "Thursday, November 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94In%20Apprehending%20Hard%20Stuff&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94In%20Apprehending%20Hard%20Stuff%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMYo5DF78SD3nmQ2q%2Fmeetup-west-la-in-apprehending-hard-stuff%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94In%20Apprehending%20Hard%20Stuff%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMYo5DF78SD3nmQ2q%2Fmeetup-west-la-in-apprehending-hard-stuff", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiMYo5DF78SD3nmQ2q%2Fmeetup-west-la-in-apprehending-hard-stuff", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 369, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tn'>West LA\u2014In Apprehending Hard Stuff</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 November 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you worked long and hard to figure out how to get it for free for longer.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<p>So8res's excellent post <a href=\"http://lesswrong.com/lw/j10/on_learning_difficult_things/\">On Learning Difficult Things</a> has inspired this meetup. Less Wrong is about learning brutal and unforgiving topics like <em>what your brain is actually doing</em> and <em>what the objectively correct way to interpret evidence is</em> and <em>how to reason about reason</em></p>\n\n<p>Since this is post is so great, you are all going to read it, and since not all of you are going to read it, I'm going to summarize it for you, and then we'll talk about maybe actually doing something involving the learning.</p>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/j10/on_learning_difficult_things/\">On Learning Difficult Things</a></li>\n<li><a href=\"http://lesswrong.com/lw/ita/how_habits_work_and_how_you_may_control_them/\">How habits work and how you may control them</a></li>\n<li><a href=\"http://lesswrong.com/lw/hqg/learning_programming_so_ive_learned_the_basics_of/96t1\">shev&#39;s comment on learning to program</a></li>\n<li><a href=\"http://www.cogtech.usc.edu/publications/kirschner_Sweller_Clark.pdf\" rel=\"nofollow\">Kirschner, P. A., Sweller, J., and Clark, R. E. (2006) Why minimal guidance during instruction does not work: an analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-based teaching.</a></li>\n</ul>\n\n<p><strong>Non-recommended reading</strong>:</p>\n\n<ul>\n<li>I don't know, you can read <a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/self-help/Josh_Waitzkin__The_Art_of_Learning__2007.pdf\" rel=\"nofollow\">The Art of Learning</a> if you want, but be warned it's <em>much</em> more a book about how great it is to be Josh Waitzkin than it is a book designed to teach you how to learn.</li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. Do note that if you show up without exposure to Less Wrong, you will leave with exposure to Less Wrong. For this reason, do not show up if you do not wish to be exposed to Less Wrong. However, if you are reading this, you probably have been exposed to Less Wrong, and as such, it is too late for you, so you might as well show up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tn'>West LA\u2014In Apprehending Hard Stuff</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iMYo5DF78SD3nmQ2q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.422538045350103e-06, "legacy": true, "legacyId": "24703", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_In_Apprehending_Hard_Stuff\">Discussion article for the meetup : <a href=\"/meetups/tn\">West LA\u2014In Apprehending Hard Stuff</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 November 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you worked long and hard to figure out how to get it for free for longer.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<p>So8res's excellent post <a href=\"http://lesswrong.com/lw/j10/on_learning_difficult_things/\">On Learning Difficult Things</a> has inspired this meetup. Less Wrong is about learning brutal and unforgiving topics like <em>what your brain is actually doing</em> and <em>what the objectively correct way to interpret evidence is</em> and <em>how to reason about reason</em></p>\n\n<p>Since this is post is so great, you are all going to read it, and since not all of you are going to read it, I'm going to summarize it for you, and then we'll talk about maybe actually doing something involving the learning.</p>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/j10/on_learning_difficult_things/\">On Learning Difficult Things</a></li>\n<li><a href=\"http://lesswrong.com/lw/ita/how_habits_work_and_how_you_may_control_them/\">How habits work and how you may control them</a></li>\n<li><a href=\"http://lesswrong.com/lw/hqg/learning_programming_so_ive_learned_the_basics_of/96t1\">shev's comment on learning to program</a></li>\n<li><a href=\"http://www.cogtech.usc.edu/publications/kirschner_Sweller_Clark.pdf\" rel=\"nofollow\">Kirschner, P. A., Sweller, J., and Clark, R. E. (2006) Why minimal guidance during instruction does not work: an analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-based teaching.</a></li>\n</ul>\n\n<p><strong>Non-recommended reading</strong>:</p>\n\n<ul>\n<li>I don't know, you can read <a href=\"https://dl.dropboxusercontent.com/u/33627365/Scholarship/self-help/Josh_Waitzkin__The_Art_of_Learning__2007.pdf\" rel=\"nofollow\">The Art of Learning</a> if you want, but be warned it's <em>much</em> more a book about how great it is to be Josh Waitzkin than it is a book designed to teach you how to learn.</li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. Do note that if you show up without exposure to Less Wrong, you will leave with exposure to Less Wrong. For this reason, do not show up if you do not wish to be exposed to Less Wrong. However, if you are reading this, you probably have been exposed to Less Wrong, and as such, it is too late for you, so you might as well show up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_In_Apprehending_Hard_Stuff1\">Discussion article for the meetup : <a href=\"/meetups/tn\">West LA\u2014In Apprehending Hard Stuff</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014In Apprehending Hard Stuff", "anchor": "Discussion_article_for_the_meetup___West_LA_In_Apprehending_Hard_Stuff", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014In Apprehending Hard Stuff", "anchor": "Discussion_article_for_the_meetup___West_LA_In_Apprehending_Hard_Stuff1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w5F4w8tNZc6LcBKRP", "5wMTZLZZmZEbXdoMD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-14T20:38:22.845Z", "modifiedAt": null, "url": null, "title": "I notice that I am confused about Identity and Resurrection", "slug": "i-notice-that-i-am-confused-about-identity-and-resurrection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:39.923Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sMNHzx7BJBvgbCaKT/i-notice-that-i-am-confused-about-identity-and-resurrection", "pageUrlRelative": "/posts/sMNHzx7BJBvgbCaKT/i-notice-that-i-am-confused-about-identity-and-resurrection", "linkUrl": "https://www.lesswrong.com/posts/sMNHzx7BJBvgbCaKT/i-notice-that-i-am-confused-about-identity-and-resurrection", "postedAtFormatted": "Thursday, November 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20notice%20that%20I%20am%20confused%20about%20Identity%20and%20Resurrection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20notice%20that%20I%20am%20confused%20about%20Identity%20and%20Resurrection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMNHzx7BJBvgbCaKT%2Fi-notice-that-i-am-confused-about-identity-and-resurrection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20notice%20that%20I%20am%20confused%20about%20Identity%20and%20Resurrection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMNHzx7BJBvgbCaKT%2Fi-notice-that-i-am-confused-about-identity-and-resurrection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMNHzx7BJBvgbCaKT%2Fi-notice-that-i-am-confused-about-identity-and-resurrection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1849, "htmlBody": "<p>I've spent quite a bit of time trying to work out how to explain the roots of my confusion. I think, in the great LW tradition, I'll start with a story.</p>\n<p>[Editor's note: The original story was in 16th century Mandarin, and used peculiar and esoteric terms for concepts that are just now being re-discovered. Where possible, I have translated these terms into their modern mathematical and philosophical equivalents. Such terms are denoted with curly braces, {like so}.]</p>\n<p>Once upon a time there was a man by the name of Shen Chun-lieh, and he had a beautiful young daughter named Ah-Chen. She died.</p>\n<p>Shen Chun-lieh was heartbroken, moreso he thought than any man who had lost a daughter, and so he struggled and scraped and misered until he had amassed a great fortune, and brought that fortune before me - for he had heard it told that I was could resurrect the dead.</p>\n<p>I frowned when he told me his story, for many things are true after a fashion, but wisdom is in understanding the nature of that truth - and he did not bear the face of a wise man.</p>\n<p>\"Tell me about your daughter, Ah-Chen.\", I commanded.</p>\n<p><a href=\"http://thewhipple.livejournal.com/10610.html\">And so he told me</a>.</p>\n<p>I frowned, for my suspicions were confirmed.</p>\n<p>\"You wish for me to give you this back?\", I asked.</p>\n<p>He nodded and dried his tears. \"More than anything in the world.\"</p>\n<p>\"Then come back tomorrow, and I will have for you a beautiful daughter who will do all the things you described.\"</p>\n<p>His face showed a sudden flash of understanding. Perhaps, I thought, this one might see after all.</p>\n<p>\"But\", he said, \"will it be Ah-Chen?\"</p>\n<p>I smiled sagely. \"What do you mean by that, Shen Chun-lieh?\"</p>\n<p>\"I mean, you said that you would give me 'a' daughter. I wish for MY daughter.\"</p>\n<p>I bowed to his small wisdom. \"Indeed I did. If you wish for YOUR daughter, then you must be much, much more precise with me.\"</p>\n<p>He frowned, and I saw in his face that he did not have the words.</p>\n<p>\"You are wise in the way of the Tao\", he said, \"surely you can find the words in my heart, so that even such as me could say them?\"</p>\n<p>I nodded. \"I can. But it will take a great amount of time, and much courage from you. Shall we proceed?\"</p>\n<p>He nodded.</p>\n<p>&nbsp;</p>\n<p>I am wise enough in the way of the Tao. The Tao whispers things that have been discovered and forgotten, and things that have yet to be discovered, and things that may never be discovered. And while Shen Chun-lieh was neither wise nor particularly courageous, his overwhelming desire to see his daughter again propelled him with an intensity seldom seen in my students. And so it was, many years later, that I judged him finally ready to discuss his daughter with me, in earnest.</p>\n<p>\"Shen\", I said, \"it is time to talk about your Ah-Chen.\"</p>\n<p>His eyes brightened and he nodded eagerly. \"Yes, Teacher.\"</p>\n<p>\"Do you understand why I said on that first day, that you must be much, much more precise with me?\"</p>\n<p>\"Yes, Teacher. I had come to you believing that the soul was a thing that could be conjured back to the living, rather than a {computational process}.\"</p>\n<p>\"Even now, you are not quite correct. The soul is not a {computational process}, but a {specification of a search space} which describes any number of similar {computational processes}. For example, Shen Chun-lieh, would you still be Shen Chun-lieh if I were to cut off your left arm?\"</p>\n<p>\"Of course, Teacher. My left arm does not define who I am.\"</p>\n<p>\"Indeed. And are you still the same Shen Chun-lieh who came to me all those years ago, begging me to give him back his daughter Ah-Chen?\"</p>\n<p>\"I am, Teacher, although I understand much more now than I did then.\"</p>\n<p>\"That you do. But tell me - would you be the same Shen Chun-lieh if you had not come to me? If you had continued to save and to save your money, and craft more desperate and eager schemes for amassing more money, until finally you forgot the purpose of your misering altogether, and abandoned your Ah-Chen to the pursuit of gold and jade for its own sake?\"</p>\n<p>\"Teacher, my love for Ah-Chen is all-consuming; such a fate could never befall me.\"</p>\n<p>\"Do not be so sure, my student. Remember the tale of the butterfly's wings, and the storm that sank an armada. Ever-shifting is the Tao, and so ever-shifting is our place in it.\"</p>\n<p>Shen Chun-lieh understood, and in a brief moment he glimpsed his life as it could have been, as an old Miser Shen hoarding gold and jade in a great walled city. He shuddered and prostrated himself.</p>\n<p>\"Teacher, you are correct. And even such a wretch as Miser Shen, that wretch would still be me. But I thank the Buddha and the Eight Immortal Sages that I was spared that fate.\"</p>\n<p>I smiled benevolently and helped him to his feet. \"Then suppose that you had died and not your daughter, and one day a young woman named Ah-Chen had burst into my door, flinging gold and jade upon my table, and described the caring and wonderful father that she wished returned to her? What could she say about Shen Chun-lieh that would allow me to find his soul amongst the infinite chaos of the Nine Hells?\"</p>\n<p>\"I...\" He looked utterly lost.</p>\n<p>\"Tell me, Shen Chun-lieh, what is the meaning of the parable of the {Ship of Theseus}?\"</p>\n<p>\"That personal identity cannot be contained within the body, for the flow of the Tao slowly strips away and the flow of the Tao slowly restores, such that no single piece of my body is the same from one year to the next; and within the Tao, <a href=\"/lw/pm/identity_isnt_in_specific_atoms/\">even the distinction of 'sameness' is meaningless.</a>\"</p>\n<p>\"And what is the relevance of the parable of the {Shroedinger's Cat} to this discussion?\"</p>\n<p>\"Umm... that... let me think. I suppose, that personal identity cannot be contained within the history of choices that have been made, because for every choice that has been made, if it was truly a 'choice' at all, it was also made the other way in some other tributary of the Great Tao.\"</p>\n<p>\"And the parable of the tiny {Paramecium}?\"</p>\n<p>\"That neither is the copy; <a href=\"/lw/qx/timeless_identity/\">there are<em> two </em>originals.</a>\"</p>\n<p>\"So, Shen. Can you yet articulate the dilemma that you present to me?\"</p>\n<p>\"No, Teacher. I fear that yet again, you must point it out to your humble student.\"</p>\n<p>\"You ask for Ah-Chen, my student. But <em>which one</em>? Of all the Ah-Chens that could be brought before you, which would satisfy you? Because there is no hard line, between {configurations} that you would recognize as your daughter and {configurations} which you would not. So why did my original offer, to construct you a daughter that would do all the things you described Ah-Chen as doing, not appeal to you?\"</p>\n<p>Shen looked horrified. \"Because she would not BE Ah-Chen! Even if you made her respond perfectly, it would not be HER! I do not simply miss my six-year-old girl; I miss what she could have become! I regret that she never got to see the world, never got to grow up, never got to...\"</p>\n<p>\"In what sense did she never do these things? She died, yes; but even a dead Ah-Chen is still an Ah-Chen. She has since experienced being worms beneath the earth, and flowers, and then bees and birds and foxes and deer and even peasants and noblemen. All these are Ah-Chen, so why is it so important that she appear before you as YOU remember her?\"</p>\n<p>\"Because I miss her, and because she has no conscious awareness of those things.\"</p>\n<p>\"Ah, but then which conscious awareness do you wish her to have? There is no copy; all possible tributaries of the Great Tao contain an original. And each of those originals experience in their own way. You wish me to pluck out a {configuration} and present it to you, and declare \"This one! This one is Ah-Chen!\". But which one? Or do you leave that choice to me?\"</p>\n<p>\"No, Teacher. I know better than to leave that choice to you. But... you have shown me many great wonders, in alchemy and in other works of the Tao. If her brain had been preserved, perhaps frozen as you showed me the frozen koi, I could present that to you and you could reconstruct her {configuration} from that?\"</p>\n<p>I smiled sadly. \"To certain degrees of precision, yes, I could. But the question still remains - you have only narrowed down the possible {configurations}. And what makes you say that the boundary of {configurations} that are achievable from a frozen brain are correct? If I smash that brain with a hammer, melt it, and paint a portrait of Ah-Chen with it, is that not a {configuration} that is achievable from that brain?\"</p>\n<p>Shen looked disgusted. \"You... how can you be so wise and yet not understand such simple things? We are talking about people! Not paintings!\"</p>\n<p>I continued to smile sadly. \"Because these things are not so simple. 'People' are not things, as you said before. 'People' are {sets of configurations}; they are {specifications of search spaces}. And those boundaries are so indistinct that anything that claims to capture them is in error.\"</p>\n<p>Now it was Shen's turn to look animated. \"<a href=\"/lw/mm/the_fallacy_of_gray/\">Just because the boundary cannot be drawn perfectly, does not make the boundary meaningless!</a>\"</p>\n<p>I nodded. \"You have indeed learned much. But you still have not described the purpose of your boundary-drawing. Do you wish for Ah-Chen's resurrection for yourself, so that you may feel less lonely and grieved, or do you wish it for Ah-Chen's sake, so that she may see the world anew? For these two purposes will give us very different boundaries for what is an acceptable Ah-Chen.\"</p>\n<p>Shen grimaced, as war raged within his heart. \"You are so wise in the Tao; stop these games and <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">do what I mean</a>!\"</p>\n<p>And so it was that Miser Shen came to live in the walled city of Ch'in, and hoarded gold and jade, and lost all memory and desire for his daughter Ah-Chen, until it was that the Tao swept him up into another tale.</p>\n<p>&nbsp;</p>\n<p>So, there we are. My confusion is in two parts:</p>\n<p>1. When I imagine resurrecting loved ones, what makes me believe that even a perfectly preserved brain state is any more 'resurrection' than an overly sophisticated wind-up toy that happens to behave in ways that fulfill my desire for that loved one's company? In a certain sense, avoiding true 'resurrection' should be PREFERABLE - since it is possible that a \"wind-up toy\" could be constructed that provides a superstimulus version of that loved one's company, while an actual 'resurrection' will only be as good as the real thing.</p>\n<p>2. When I imagine being resurrected \"myself\", how different from this 'me' can it be and still count? How is this fundamentally different from \"I will for the future to contain a being like myself\", which is really just \"I will for the future to contain a being like I imagine myself to be\" - in which case, we're back to the superstimulus option (which is perhaps a little weird in this case, since I'm not there to receive the stimulus).</p>\n<p>I'd really like to discuss this.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 1, "PbShukhzpLsWpGXkM": 1, "etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sMNHzx7BJBvgbCaKT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 65, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "24704", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RLScTpwc5W2gGGrL9", "924arDrTu3QRHFA5r", "dLJv2CoRCgeC2mPgj", "4ARaTpNX62uaL86j6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-14T23:26:19.693Z", "modifiedAt": null, "url": null, "title": "Game theory and expected opponents", "slug": "game-theory-and-expected-opponents", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tsY7HBrRaPvN2Sdm9/game-theory-and-expected-opponents", "pageUrlRelative": "/posts/tsY7HBrRaPvN2Sdm9/game-theory-and-expected-opponents", "linkUrl": "https://www.lesswrong.com/posts/tsY7HBrRaPvN2Sdm9/game-theory-and-expected-opponents", "postedAtFormatted": "Thursday, November 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Game%20theory%20and%20expected%20opponents&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGame%20theory%20and%20expected%20opponents%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtsY7HBrRaPvN2Sdm9%2Fgame-theory-and-expected-opponents%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Game%20theory%20and%20expected%20opponents%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtsY7HBrRaPvN2Sdm9%2Fgame-theory-and-expected-opponents", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtsY7HBrRaPvN2Sdm9%2Fgame-theory-and-expected-opponents", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 890, "htmlBody": "<p>Thanks to V_V and Emile for some great discussion. Since writing up a post seems to reliably spark interesting comments, that's what I'll do!</p>\n<p>&nbsp;</p>\n<p><strong>Summary</strong></p>\n<p>If I wanted to write down a decision theory that gets the correct answer to game-theoretic problems (like playing the middle Nash equilibrium in a blind <a href=\"/r/discussion/lw/ixl/kidnapping_and_the_game_of_chicken/\">chicken-like game</a>), it would have to, in a sense, implement all of game theory. This is hard because human-generated solutions to games use a lot of assumptions about what the other players will do, and putting those assumptions into our algorithm is a confusing problem. In order to tell what's really going on, we need to make that information more explicit. Once we do that, maybe we can get a UDT-like algorithm to make good moves in tricky games.</p>\n<p>&nbsp;</p>\n<p><strong>Newcomb's Problem</strong></p>\n<p>For an example of a game with unusually good information about our opponent, how about Newcomb's problem. Is it really a game, you ask? Sure, I say!</p>\n<p><img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_j1t_0.png\" alt=\"\" width=\"381\" height=\"275\" />In the payoff matrix to the right, you play red and Omega plays blue. The numbers for Omega just indicate that Omega only wants to put in the million dollars if you will 1-box. If this was a normal game-theory situation, you wouldn't easily know what to do - your best move depends on Omega's move. This is where typical game theory procedure would be to say \"well, that's silly, let's specify some extra nice properties the choice of both players should have so that we get a unique solution.\"</p>\n<p>But the route taken in Newcomb's problem is different - we pick out a unique solution by increasing how much information the players have about each other. Omega knows what you will play, and you know that Omega knows what you will play. Now all we need to figure out what to do is some information like \"If Omega has an available strategy that will definitely get it the highest possible payoff, it will take it.\" The best strategy, of course, is to one-box so that Omega puts in the million dollars.</p>\n<p>&nbsp;</p>\n<p><strong>Newcomb's Game vs. an Ignorant opponent</strong></p>\n<p>Consider another possible opponent in this game - one who has no information about what your move will be. Whereas Omega always knows your move, an ignorant opponent has no idea what you will play - they have no basis to think you're more likely to 1-box than 2-box, or vice versa.</p>\n<p>Interestingly, for this particular payoff matrix this makes you ignorant too - you have no basis to think the ignorant opponent would rather put the money in than not, or vice versa. So you assign a 50% chance to each (probability being quantified ignorance) and find that two-boxing has the highest rewards. This didn't even require the sophistication of taking into account your own action, like the game against Omega did, since an ignorant opponent can't respond to your action.</p>\n<p>&nbsp;</p>\n<p><strong>Human opponents</strong></p>\n<p>Ok, so we've looked at a super-knowledgeable opponent, and a super-ignorant opponent, what does a more typical game theory situation look like? Well, it's when our opponent is more like us - someone trying to pick the strategy that gets them the best reward, with similar information to what we have. In typical games between humans, both know that the other is a human player - and they know that it's known, etc.</p>\n<p>In terms of what we know, we know that our opponent is drawn from some distribution of opponents that are about as good as we are at games, and that they have the same information about us that we have about them.</p>\n<p>What information do we mean we have when we say our opponent is \"good at games\"? I don't know. I can lay out some possibilities, but this is the crux of the post. I'll frame our possible knowledge in terms of past games, like how one could say of Newcomb's problem \"you observe a thousand games, and Omega always predicts right.\"</p>\n<p>Possibility 1: We know our opponent has played a lot of games against completely unknown opponents in the past, and has a good record, where \"good\" means \"as good or better than the average opponent.\"</p>\n<p>Possibility 2: We know our opponent played some games against a closed group of players who played each other, and that group collectively had a good record.</p>\n<p>Possibility 3: We know our opponent is a neural net that's been trained in some standard way to be good at playing a variety of games, or some sort of hacked-together implementation of game theory, or a UDT agent if that's a good idea. (Seems more complicated than necessary, but on the other hand opponents are totally allowed to be complicated)</p>\n<p>&nbsp;</p>\n<p>Suppose we know information set #2. I think it's the most straightforward. All we have to do to turn this information into a distribution over opponents is to figure out what mixtures of players get above-average group results, then average those together. Once we know who our opponent is on average, we just follow the strategy that on average gets the best average payoff.</p>\n<p>Does the strategy picked this way look like what game theory would say? Not quite - it assumes that the opponent has a medium chance of being stupid. &nbsp;And in some games, like the prisoner's dilemma, the best-payoff groups are actually the ones you can exploit the most. &nbsp;So on closer examination, someone in a successful group isn't the game-theory opponent we're looking for.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tsY7HBrRaPvN2Sdm9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.4229585319388363e-06, "legacy": true, "legacyId": "24689", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Thanks to V_V and Emile for some great discussion. Since writing up a post seems to reliably spark interesting comments, that's what I'll do!</p>\n<p>&nbsp;</p>\n<p><strong id=\"Summary\">Summary</strong></p>\n<p>If I wanted to write down a decision theory that gets the correct answer to game-theoretic problems (like playing the middle Nash equilibrium in a blind <a href=\"/r/discussion/lw/ixl/kidnapping_and_the_game_of_chicken/\">chicken-like game</a>), it would have to, in a sense, implement all of game theory. This is hard because human-generated solutions to games use a lot of assumptions about what the other players will do, and putting those assumptions into our algorithm is a confusing problem. In order to tell what's really going on, we need to make that information more explicit. Once we do that, maybe we can get a UDT-like algorithm to make good moves in tricky games.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Newcomb_s_Problem\">Newcomb's Problem</strong></p>\n<p>For an example of a game with unusually good information about our opponent, how about Newcomb's problem. Is it really a game, you ask? Sure, I say!</p>\n<p><img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_j1t_0.png\" alt=\"\" width=\"381\" height=\"275\">In the payoff matrix to the right, you play red and Omega plays blue. The numbers for Omega just indicate that Omega only wants to put in the million dollars if you will 1-box. If this was a normal game-theory situation, you wouldn't easily know what to do - your best move depends on Omega's move. This is where typical game theory procedure would be to say \"well, that's silly, let's specify some extra nice properties the choice of both players should have so that we get a unique solution.\"</p>\n<p>But the route taken in Newcomb's problem is different - we pick out a unique solution by increasing how much information the players have about each other. Omega knows what you will play, and you know that Omega knows what you will play. Now all we need to figure out what to do is some information like \"If Omega has an available strategy that will definitely get it the highest possible payoff, it will take it.\" The best strategy, of course, is to one-box so that Omega puts in the million dollars.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Newcomb_s_Game_vs__an_Ignorant_opponent\">Newcomb's Game vs. an Ignorant opponent</strong></p>\n<p>Consider another possible opponent in this game - one who has no information about what your move will be. Whereas Omega always knows your move, an ignorant opponent has no idea what you will play - they have no basis to think you're more likely to 1-box than 2-box, or vice versa.</p>\n<p>Interestingly, for this particular payoff matrix this makes you ignorant too - you have no basis to think the ignorant opponent would rather put the money in than not, or vice versa. So you assign a 50% chance to each (probability being quantified ignorance) and find that two-boxing has the highest rewards. This didn't even require the sophistication of taking into account your own action, like the game against Omega did, since an ignorant opponent can't respond to your action.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Human_opponents\">Human opponents</strong></p>\n<p>Ok, so we've looked at a super-knowledgeable opponent, and a super-ignorant opponent, what does a more typical game theory situation look like? Well, it's when our opponent is more like us - someone trying to pick the strategy that gets them the best reward, with similar information to what we have. In typical games between humans, both know that the other is a human player - and they know that it's known, etc.</p>\n<p>In terms of what we know, we know that our opponent is drawn from some distribution of opponents that are about as good as we are at games, and that they have the same information about us that we have about them.</p>\n<p>What information do we mean we have when we say our opponent is \"good at games\"? I don't know. I can lay out some possibilities, but this is the crux of the post. I'll frame our possible knowledge in terms of past games, like how one could say of Newcomb's problem \"you observe a thousand games, and Omega always predicts right.\"</p>\n<p>Possibility 1: We know our opponent has played a lot of games against completely unknown opponents in the past, and has a good record, where \"good\" means \"as good or better than the average opponent.\"</p>\n<p>Possibility 2: We know our opponent played some games against a closed group of players who played each other, and that group collectively had a good record.</p>\n<p>Possibility 3: We know our opponent is a neural net that's been trained in some standard way to be good at playing a variety of games, or some sort of hacked-together implementation of game theory, or a UDT agent if that's a good idea. (Seems more complicated than necessary, but on the other hand opponents are totally allowed to be complicated)</p>\n<p>&nbsp;</p>\n<p>Suppose we know information set #2. I think it's the most straightforward. All we have to do to turn this information into a distribution over opponents is to figure out what mixtures of players get above-average group results, then average those together. Once we know who our opponent is on average, we just follow the strategy that on average gets the best average payoff.</p>\n<p>Does the strategy picked this way look like what game theory would say? Not quite - it assumes that the opponent has a medium chance of being stupid. &nbsp;And in some games, like the prisoner's dilemma, the best-payoff groups are actually the ones you can exploit the most. &nbsp;So on closer examination, someone in a successful group isn't the game-theory opponent we're looking for.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Newcomb's Problem", "anchor": "Newcomb_s_Problem", "level": 1}, {"title": "Newcomb's Game vs. an Ignorant opponent", "anchor": "Newcomb_s_Game_vs__an_Ignorant_opponent", "level": 1}, {"title": "Human opponents", "anchor": "Human_opponents", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GQy2BSQG9Dd6vPhs8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-15T02:04:56.294Z", "modifiedAt": null, "url": null, "title": "[Video Link] PostHuman: An Introduction to Transhumanism", "slug": "video-link-posthuman-an-introduction-to-transhumanism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:58.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Joshua_Blaine", "createdAt": "2013-07-15T18:37:17.985Z", "isAdmin": false, "displayName": "Joshua_Blaine"}, "userId": "jzvkfAuoy9X7dp9Ma", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EMSW3sGdNvySvEBrJ/video-link-posthuman-an-introduction-to-transhumanism", "pageUrlRelative": "/posts/EMSW3sGdNvySvEBrJ/video-link-posthuman-an-introduction-to-transhumanism", "linkUrl": "https://www.lesswrong.com/posts/EMSW3sGdNvySvEBrJ/video-link-posthuman-an-introduction-to-transhumanism", "postedAtFormatted": "Friday, November 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BVideo%20Link%5D%20PostHuman%3A%20An%20Introduction%20to%20Transhumanism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BVideo%20Link%5D%20PostHuman%3A%20An%20Introduction%20to%20Transhumanism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMSW3sGdNvySvEBrJ%2Fvideo-link-posthuman-an-introduction-to-transhumanism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BVideo%20Link%5D%20PostHuman%3A%20An%20Introduction%20to%20Transhumanism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMSW3sGdNvySvEBrJ%2Fvideo-link-posthuman-an-introduction-to-transhumanism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMSW3sGdNvySvEBrJ%2Fvideo-link-posthuman-an-introduction-to-transhumanism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>http://youtu.be/bTMS9y8OVuY</p>\n<p>I think this is a nice introduction to Transhumanism, inspired by the style of many well known Youtube educators. Given how much LessWrong likes these ideas, I thought it was worth sharing.</p>\n<p>The group also has a Kickstarter <a href=\"http://www.kickstarter.com/projects/1264778197/posthuman-a-video-series-on-transhumanism\">here</a>&nbsp;to fund an entire series of videos of this kind. I think they deserve to be backed, and LW can probably influence the video creators in a useful/helpful way.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jiuackr7B5JAetbF6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EMSW3sGdNvySvEBrJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 1.4231140612974918e-06, "legacy": true, "legacyId": "24706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-15T17:05:56.848Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-74", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XbzWTpvGtpv2Dh8c7/weekly-lw-meetups-74", "pageUrlRelative": "/posts/XbzWTpvGtpv2Dh8c7/weekly-lw-meetups-74", "linkUrl": "https://www.lesswrong.com/posts/XbzWTpvGtpv2Dh8c7/weekly-lw-meetups-74", "postedAtFormatted": "Friday, November 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbzWTpvGtpv2Dh8c7%2Fweekly-lw-meetups-74%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbzWTpvGtpv2Dh8c7%2Fweekly-lw-meetups-74", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbzWTpvGtpv2Dh8c7%2Fweekly-lw-meetups-74", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 546, "htmlBody": "<p><strong>This summary was posted to LW main on November 8th. The following week's summary is <a href=\"/lw/j2k/new_lw_meetups_amsterdam_jacksonville_milwaukee/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/t5\">[Atlanta GA] November Meetup (Second of Two):&nbsp;<span class=\"date\">23 November 2013 06:00PM</span></a><a href=\"/meetups/sa\"></a></li>\n<li><a href=\"/meetups/sa\">First Meetup in Cologne (K&ouml;ln):&nbsp;<span class=\"date\">10 November 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/t7\">Frankfurt:&nbsp;<span class=\"date\">24 November 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/t6\">Moscow, Now 2 Sigma More Awesome:&nbsp;<span class=\"date\">10 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/sm\"></a><a href=\"/meetups/sm\">Princeton NJ Meetup:&nbsp;<span class=\"date\">16 November 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/sz\">Saint Petersburg. Why rationality?:&nbsp;<span class=\"date\">10 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/t2\">Urbana-Champaign: Thinking Fast and Slow Pages 70-140 Discussion Meetup:&nbsp;<span class=\"date\">10 November 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\"></a><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">09 November 2019 02:30PM</span></a> </li>\n<li><a href=\"/meetups/t1\">Brussels monthly meetup: memory!:&nbsp;<span class=\"date\">09 November 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/t8\">[Cambridge MA] Comfort Zone Expansion at Citadel, Boston:&nbsp;<span class=\"date\">10 November 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/t0\">London social meetup, 10/11/13:&nbsp;<span class=\"date\">10 November 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/t9\">West LA&mdash;The Merits of Specificity:&nbsp;<span class=\"date\">13 November 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XbzWTpvGtpv2Dh8c7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.4239981413914084e-06, "legacy": true, "legacyId": "24635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["krLeqganGKCG8XSE4", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-15T17:59:25.789Z", "modifiedAt": null, "url": null, "title": "The Restoration of William: the skeleton of a short story about resurrection and identity", "slug": "the-restoration-of-william-the-skeleton-of-a-short-story", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:39.887Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlanCrowe", "createdAt": "2009-03-27T20:27:44.704Z", "isAdmin": false, "displayName": "AlanCrowe"}, "userId": "ntpQeHnTHt4QCgEQF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SzNkE3kRzZbzikAKZ/the-restoration-of-william-the-skeleton-of-a-short-story", "pageUrlRelative": "/posts/SzNkE3kRzZbzikAKZ/the-restoration-of-william-the-skeleton-of-a-short-story", "linkUrl": "https://www.lesswrong.com/posts/SzNkE3kRzZbzikAKZ/the-restoration-of-william-the-skeleton-of-a-short-story", "postedAtFormatted": "Friday, November 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Restoration%20of%20William%3A%20the%20skeleton%20of%20a%20short%20story%20about%20resurrection%20and%20identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Restoration%20of%20William%3A%20the%20skeleton%20of%20a%20short%20story%20about%20resurrection%20and%20identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzNkE3kRzZbzikAKZ%2Fthe-restoration-of-william-the-skeleton-of-a-short-story%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Restoration%20of%20William%3A%20the%20skeleton%20of%20a%20short%20story%20about%20resurrection%20and%20identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzNkE3kRzZbzikAKZ%2Fthe-restoration-of-william-the-skeleton-of-a-short-story", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzNkE3kRzZbzikAKZ%2Fthe-restoration-of-william-the-skeleton-of-a-short-story", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 869, "htmlBody": "<p>Bill died. He never liked having dumps done. Each year he would make excuses, put it off. \"Next year.\" he would say. Only after Bill's death do people realise just how long this has been going on for: thirty years. They will have to restore Bill from a 30 year old tape. Is \"restore\" even the right word? How about \"roll-back\"?<br /><br />Worse still, there was a big change in Bill's life 25 years ago when he had a mid-life crisis. He joined a personal growth cult, dropped old friends, made new ones. Some of his new friends can remember encounters with the old Bill of 30 years ago. They didn't like him and avoided him. There was a lot of friction when he joined the personal growth cult 5 years later. Some members wanted to black ball him. You cannot teach an old dog new tricks. It might be true, but the personal growth cult could hardly admit it.<br /><br />Those who dreaded Bill's return had a week of respite when it seemed that Bill's tape had been lost. Lost? Bill really dead and gone for ever? That was unthinkable. Losing some-ones only back up tape would be a huge scandal. Who would stake their life with a careless archiving company?<br /><br />After an increasingly panicky search it was found. Found! And still readable, after all those years, with a bit of manual fixing of uncorrectable errors.<br /><br />Restored Bill woke to find 30 years had gone by. When we think back to what we were like 30 years ago, we do so as a process of diffs. What changed last year. What changed the year before that. What changed between two and three years ago. So when we think back to what changed between year 29 and year 30 and find we cannot remember, what are we to do? No doubt there were a whole years worth of changes, but not knowing what they were, we are seduced by the lazy assumption that they didn't amount to much. Restored Bill did not have the option of making lazy assumptions. He had 30 years of change dumped on him. The genuine article, the whole ka-boodle, with little relation to the convenient fictions that human memory embroiders over 30 years of telling, forgetting, patching and re-telling.<br /><br />People who remembered disliking Bill 30 years ago were never-the-less sympathetic to the bewildered and pathetic figure, uncertain who and when he was.&nbsp; Phoning close friends to continue yesterday's conversation only to suffer them denying having know him was distressing. It wasn't people deny knowing him in retaliation for a falling out 25 years previously. It was worse than that. How many of your old friends from 30 years ago have you completely forgotten about? You'll soon find that you cannot remember any-one who you have completely forgotten about. The difference between tautology and fact is about a dozen dear old friends.<br /><br />Restored Bill was struggling to cope with a huge disruption to the natural order of things. Was he acting out of character? Some of deceased Bill's new friends and some of his old friends tried the trick of getting a temporary hologram made from their own 30 year old dump tapes so that they could ask about Restored Bill. As usual this was a distressing experience as the hologram of ones old self turns out to be incompatible with ones own self image and personal narrative. People seeking an explanation for why Restored Bill was different from how they remembered him found instead a question: why were they so different from how they remembered themselves?<br /><br />One reason was that \"hologram\" is a rather nasty euphemism, coined to disguise the harsh reality of the law that says \"There can be only one.\" A \"hologram\" is actually a freshly down loaded flesh and blood person who must be euthanised after the consultation to ensure that there is only ever one copy of a person. The \"hologram\" is the origin of two genres of fiction. In the hologram-horror one is invited to share the chill of waking up and realising that one is only temporary with but an hour to live. In the hologram-thriller a copy of you has escaped and must be hunted down and killed before he can infiltrate society and impersonate you. There can be only one. If he succeeds you will die in his place, but he knows all about you, he is you!<br /><br />So the hologram hasn't revolutionised the study of history in the way that you might at first imagine. A history student might try asking a hologram about the past, but pretty soon the hologram realises his predicament and lapses into sullen despair.</p>\n<p>No such problem for Restored Bill. Previous Bill was dead and Restored Bill was the one. It all worked out right in the end. Restored Bill learned to rub along with most of deceased Bill's social circle, and the \"clerical error\" that had actually restored Fred-minus30 never came to light. Current Fred never learned against whom his deep loathing of Restored Bill was truly directed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SzNkE3kRzZbzikAKZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 15, "extendedScore": null, "score": 1.424050649405433e-06, "legacy": true, "legacyId": "24719", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-15T22:54:55.781Z", "modifiedAt": null, "url": null, "title": "Research on unconscious visual processing", "slug": "research-on-unconscious-visual-processing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "passive_fist", "createdAt": "2012-12-28T22:14:43.333Z", "isAdmin": false, "displayName": "passive_fist"}, "userId": "BiX9vSWsWJEJEEjZG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X9ZhCnvpSiLhkr2Ss/research-on-unconscious-visual-processing", "pageUrlRelative": "/posts/X9ZhCnvpSiLhkr2Ss/research-on-unconscious-visual-processing", "linkUrl": "https://www.lesswrong.com/posts/X9ZhCnvpSiLhkr2Ss/research-on-unconscious-visual-processing", "postedAtFormatted": "Friday, November 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Research%20on%20unconscious%20visual%20processing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResearch%20on%20unconscious%20visual%20processing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9ZhCnvpSiLhkr2Ss%2Fresearch-on-unconscious-visual-processing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Research%20on%20unconscious%20visual%20processing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9ZhCnvpSiLhkr2Ss%2Fresearch-on-unconscious-visual-processing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9ZhCnvpSiLhkr2Ss%2Fresearch-on-unconscious-visual-processing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 832, "htmlBody": "<p>There is a new paper out by Sanguinetti, Allen, and Peterson, <a href=\"http://pss.sagepub.com/content/early/2013/11/08/0956797613502814.abstract?rss=1\" target=\"_self\">The Ground Side of an Object: Perceived as Shapeless yet Processed for Semantics</a>. In it, the authors conduct a series of experiments to try to answer the question of how the brain separates background from foreground in visual processing. I found it interesting, so I thought I'd share. The human visual system is incredibly complex and we still have no clear idea how it does a lot of the things it does.<br /><br />The experimental protocol was as follows:</p>\n<blockquote>\n<p>The stimuli were 120 small, mirror-symmetric, enclosed white silhouettes (Trujillo, Allen, Schnyer, &amp; Peterson, 2010). Of these, 40 portrayed meaningful name-able objects (animals, plants, symbols) inside their borders and suggested only meaningless novel objects on the ground side of their borders. The remaining 80 silhouettes depicted meaningless novel objects (objects not encountered previously) inside their borders. Of these, 40 suggested portions of nameable meaningful objects on the ground side of their borders. Note, however, that participants were not aware of the meaningful objects suggested on the ground side of these silhouettes. The remaining 40 novel silhouettes suggested novel shapes on both sides of their borders.\"<br /><img style=\"float: right; margin: 16px;\" src=\"http://uanews.org/sites/default/files/styles/story_body_aspect_ratio_switcher/public/story-images/4seahors_CleanEdges.jpg?itok=ggyZPKsU\" alt=\"Credit: Jay Sanguinetti\" width=\"220\" height=\"234\" /><br />Stimuli were presented on a 20-in. CRT monitor 90 cm from the participants using DMDX software. Participants&rsquo; heads were unrestrained. Their task was to classify the silhouettes as depicting real-world or novel objects. Responses were made via button press; assignment of the responses to the two response buttons was random.</p>\n</blockquote>\n<p>They then recorded the EEG signals from the participants and found something surprising: When the background was meaningful, the subject's brain waves produced the same signatures as would be expected when conscious awareness had taken place (called 'N300' and 'N400' signatures because they occur 300 and 400 ms after presentation of the stimulus), <em>even if the subjects did not report percieving anything meaningful in the images</em>. However, if the background really was meaningless, this signature was absent.<br /><br />This is consistent with the \"Brains aren't magic\" doctrine. The brain can't magically separate objects from non-objects in the simplest levels of processing. No, just like a computer it has to tediously go through the image, pixel by pixel, identifying patterns and gradually building up into complex representations. It is only after this process is over that it can reliably separate background from foreground. It's just that this process happens with a huge degree of parallelism and we are not consciously aware of what is happening. Of course, none of this should be of any surprise to those here at LessWrong. However, as the authors note, the thinking that there is some mysterious mechanism in the brain that does complex visual processing in the very first visual areas seems to be subconsciously prevalent in psychological studies, and this has also confused many machine learning researchers.<br /><br />There is a question about the study, though. How far did the processing go? Was it just the brain recognizing some abstract features of the background, not identifying it as a particular object, or did the brain actually subconsciously figure out what the object was? To determine this, they designed another experiment:</p>\n<blockquote>\n<p>To ascertain that semantic access underlies the N300 and N400 effects observed in Experiment 1, we preceded each silhouette with a word in Experiment 2. For a critical novel-object/meaningful-ground silhouette, the word named either the object suggested on the silhouette&rsquo;s ground side (match condition) or a different object (mis-match condition). If semantic access occurs for grounds, N300 and N400 responses to the novel-object/meaningful-ground silhouettes would be reduced in the match condition compared with the mismatch condition even though the semantic repetition involved stimuli from different modalities.</p>\n</blockquote>\n<p><br /> What they found was that indeed, semantic access underlies the responses - the brain knows what it's seeing, even when the interpretation doesn't rise up to the level of full conscious awareness (the full statistical analysis is in the paper, which is sadly not open-access):</p>\n<blockquote>\n<p>Experiment 2 replicated these results when semantics were accessed cross-modally. Our interpretation of the neurophysiological evidence is buttressed by a recent behavioral experiment showing semantic priming from objects suggested on the ground side of the silhouettes (Peterson et al., 2012). These results are contrary to the traditional serial-processing assumption that semantic representations are accessed only after object segregation and instead support a dynamic view of perception according to which more objects are evaluated by the visual system than are ultimately perceived.</p>\n</blockquote>\n<p>If you regard brains as Bayesian networks and spikes as messages, neurons in the visual cortex send <em>trillions</em> of messages amongst each other before arriving on a 'consensus' probability distribution on what is being seen. Throughout this whole process, many complex hypotheses are generated but discarded before reaching conscious perception. The brain does not magically side-step through the search space. It must trudge through it just like a computer. This is consistent with what we know about existing bayesian vision systems and the limitations of computer hardware.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X9ZhCnvpSiLhkr2Ss", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 15, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "24721", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-16T01:36:54.422Z", "modifiedAt": null, "url": null, "title": "Open Thread, November 15-22, 2013", "slug": "open-thread-november-15-22-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:30.449Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drethelin", "createdAt": "2011-03-02T17:38:08.607Z", "isAdmin": false, "displayName": "drethelin"}, "userId": "ZwawHK4dwF53ZDvMM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3qcaWd6GGuKEoWQcA/open-thread-november-15-22-2013", "pageUrlRelative": "/posts/3qcaWd6GGuKEoWQcA/open-thread-november-15-22-2013", "linkUrl": "https://www.lesswrong.com/posts/3qcaWd6GGuKEoWQcA/open-thread-november-15-22-2013", "postedAtFormatted": "Saturday, November 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20November%2015-22%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20November%2015-22%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3qcaWd6GGuKEoWQcA%2Fopen-thread-november-15-22-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20November%2015-22%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3qcaWd6GGuKEoWQcA%2Fopen-thread-november-15-22-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3qcaWd6GGuKEoWQcA%2Fopen-thread-november-15-22-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>&nbsp;</p>\n<div id=\"entry_t3_j0e\" class=\"content clear\" style=\"color: #000000; font-family: Arial, Helvetica, sans-serif; font-size: 11.818181991577148px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 16.363636016845703px; orphans: auto; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: #ffffff;\">\n<div class=\"md\" style=\"font-size: small;\">\n<div>\n<div>\n<div id=\"entry_t3_iyj\" class=\"content clear\"><span style=\"line-height: 12.660714149475098px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3qcaWd6GGuKEoWQcA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.4244999324293613e-06, "legacy": true, "legacyId": "24723", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 261, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-16T03:42:36.885Z", "modifiedAt": null, "url": null, "title": "I am switching to biomedical engineering and am looking for feedback on my strategy and assumptions", "slug": "i-am-switching-to-biomedical-engineering-and-am-looking-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:57.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dn9Dvwr6DWracJ7zW/i-am-switching-to-biomedical-engineering-and-am-looking-for", "pageUrlRelative": "/posts/Dn9Dvwr6DWracJ7zW/i-am-switching-to-biomedical-engineering-and-am-looking-for", "linkUrl": "https://www.lesswrong.com/posts/Dn9Dvwr6DWracJ7zW/i-am-switching-to-biomedical-engineering-and-am-looking-for", "postedAtFormatted": "Saturday, November 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20am%20switching%20to%20biomedical%20engineering%20and%20am%20looking%20for%20feedback%20on%20my%20strategy%20and%20assumptions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20am%20switching%20to%20biomedical%20engineering%20and%20am%20looking%20for%20feedback%20on%20my%20strategy%20and%20assumptions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDn9Dvwr6DWracJ7zW%2Fi-am-switching-to-biomedical-engineering-and-am-looking-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20am%20switching%20to%20biomedical%20engineering%20and%20am%20looking%20for%20feedback%20on%20my%20strategy%20and%20assumptions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDn9Dvwr6DWracJ7zW%2Fi-am-switching-to-biomedical-engineering-and-am-looking-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDn9Dvwr6DWracJ7zW%2Fi-am-switching-to-biomedical-engineering-and-am-looking-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2242, "htmlBody": "<p>I wrote this post up and circulated it among my rationalist friends. I've copied it verbatim. I figure the more rationally inclined people that can critique my plan the better.</p>\n<p>--</p>\n<p><strong>TL;DR:<br /><br /></strong>* I'm going to commit to biomedical engineering for  a very specific set of reasons related to career flexibility and  intrinsic interest.<br />* I still want to have computer science and  design arts skills, but biomedical engineering seems like a better  university investment.<br />* I would like to have my cake and eat it too  by doing biomedical engineering, while practicing computer science and  design on the side.<br />* There are potential tradeoffs, weaknesses and  assumptions in this decision that are relevant and possibly critical.  This includes time management, ease of learning, development of problem  solving solving abilities and working conditions. <br /><br />I am posting  this here because everyone is pretty clever and likes decisions. I am  looking for feedback on my reasoning and the facts in my assumptions so  that I can do what's best. This was me mostly thinking out loud, and  given the timeframe I'm on I couldn't learn and apply any real formal  method other than just thinking it through. So it's long, but I hope  that everyone can benefit by me putting this here.<br /><br />--<br />So  currently I'm weighing going into biomedical engineering as my major  over a major in computer science, or the [human-computer  interaction/media studies/gaming/ industrial design grab bag] major, at  Simon Fraser University. Other than the fact that engineering biology <em>is so damn cool</em>, the relevant decision factors include reasons like:</p>\n<ol>\n<li>medical  science is booming with opportunities at all levels in the system,  meaning that there might be a lot of financial opportunity in more  exploratory economies like in SV;</li>\n<li>the interdisciplinary nature  of biomedical engineering means that I have skills with greater  transferability as well as insight into a wide range of technologies and  processes instead of a narrow few;</li>\n<li>aside from molecular  biology, biomedical engineering is the field that appears closest to  cognitive enhancement and making cyborgs for a living;</li>\n<li>compared  to most kinds of engineering, it is more easy to self-teach computer  science and other forms of digital value-making (web design or graphical  modelling) due to the availability of educational resources; the  approaching-free cost of computing power; established communities based  around development; and clear measurements of feedback. By contrast,  biomedical engineering may require labs to be educated on biological  principles, which are increasingly available but scarce for hobbyists;  basic science textbooks are strongly variant in quality; and there isn't  the equivalent of a Github for biology making non-school collaborative  learning difficult.</li>\n</ol>\n<p>The two implications here are that even  if I am still interested in computer science, which I am, and although  biomedical engineering is less upwind than programming and math, it  makes more sense to blow a lot of money on a more specialized education  to get domain knowledge while doing computer science on the side, than  to spend money on an option whose potential cost is so low because of  self study. This conjecture, and the assumptions therein, is critical to  my strategy.<br /><br />So the best option combination that I figure that I should take is this:</p>\n<ol>\n<li>To  get the value from Biomedical Engineering, I will do the biomedical  engineering curriculum formally at SFU for the rest of my time there as  my main focus.</li>\n<li>To get the value from computer science, I  will make like a hacker and educate myself with available textbooks and  look for working gigs in my spare time.</li>\n<li>To get the value  from the media and design major, I will talk to the faculty directly  about what I can do to take their courses on human computer interaction  and industrial design, and otherwise be mentored. As a result I could  seize all the real interesting knowledge while ignoring the crap.</li>\n</ol>\n<p>Tradeoffs exist, of course. These are a few that I can think of:</p>\n<ul>\n<li>I  don't expect to be making as much as an entry level biomedical engineer  as I would as a programmer in Silicon Valley, if that was ever  possible; nor do I believe that my income would grow at the same rate.  As a counterpoint, my range of potential competencies will be greater  than the typical programmer, due to an exposure to physical, chemical,  and biological systems, their experimentation, and product development. I  feel that this greater flexibility could help with companies or  startups that are oriented towards health or technological forecasting,  but this is just a guess. In any case that makes me feel more  comfortable, having that broader knowledge, but one could argue that  programming being so popular and upwind makes it the more stable choice  anyway. Don't know.</li>\n<li>It's difficult to make money as an  undergraduate with any of the skills I would pick up in biomedical  engineering for at least a few years. This is important to me because I  want to have more-than-minimum wages jobs as a way of completing my  education on a debit. While web and graphic designers can start forming  their own employment almost immediately, and while programmers can walk  into a business or a bank and hustle; doing so with physics, chemistry  or biology seems a bit more difficult. This is somewhat countered by  co-op and work placement, and the fact that it doesn't seem to take too  much programming or web design theory and practice before being able to  start selling your skills (i.e. on the order of months).</li>\n<li>Biomedical  Engineering has few aesthetic and artistic aspects, the two of which I  value. This is what attracted me to the media and design program in the  first place. Instead I get to work with technologies which I know will  have measurable and practical use, improving the quality of life for the  sick and dying. Expressing myself with art and more free-wheeling  design is not <em>super </em>urgent, so I'm willing to make this trade. I  still hope to be able to orient myself for developing beautiful and  useful data visualizations in practical applications, <a href=\"http://mkweb.bcgsc.ca/\" target=\"_blank\">like this guy</a>, and to experiment with maker hacking.</li>\n</ul>\n<p>There  is still the issue of assuring more-than-dilettante expertise in  computer science and design stuff (see Expert Beginner syndrome: <a href=\"http://www.daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner\" target=\"_blank\">http://www.daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner</a>).  I am semi-confident in my ability to network myself into mentorships  with members of faculty [at SFU] that are not my own, and if I'm not  good at it now I still believe that it's possible. In addition, my dad  has recently become a software consultant and is willing to apprentice  me, giving a direct education about software engineering (although not  necessarily a good one, at least it's somewhat real).<br /><br />There are potential weaknesses in my analysis and strategy.</p>\n<ul>\n<li>The  time investment in the biomedical engineering faculty as SFU is very  high. The requirements are similar to those of being a grad student,  complete with a 3.00 minimum GPA and research project. The faculty does  everything in its power to allay the burden while still maintaining the  standard. However, this crowding out of time reduces the amount of  potential time spent learning computer science. This makes the  probability of efficient self-teaching go down. (that GPA standard might  lead to scholarship access which is good, but more of an externality in  this case.)</li>\n<li>While we're on the conscientiousness load:  conscientiousness is considered to be an invariant personality trait,  but I'm not buying it. The typical person may experience on average no  change in their conscientiousness, but typical people don't commit to  interventions that affect the workload they can take on either by  strengthening willpower, increasing energy, changing thought patterns  (see \"The Motivation Hacker\") or improving organization through external  aids. Still, my baseline level of conscientiousness has historically  been quite low. This raises the up front cost of learning novel material  I'm not familiar with, unlike computing, of which I have a stronger  familiarity due to lifelong exposure; this lets me cruise by in  computing courses but not necessarily ace them. Nevertheless, that's a  lower downside risk.</li>\n<li>Although medical problems are interesting  and I have a lot of intrinsic interest in the domain knowledge, there  are components of research that interest me while others that I don't  currently enjoy as much as evidenced from my current exposure. I can  seem myself getting into the data processing and visualization, drafting  ergonomic wearable tech, and circuit design especially wrt EEGs. Brute  force labwork would be less engaging and takes more out of me, despite  systems biology principles being tough but engaging. So there's the  possibility that I would only enjoy a limited scope of biomedical  engineering work, making the major not worth it or unpleasant.</li>\n<li>Due  to the less steep learning curve and more coherent structure of the  computer science field, it seems easier to approach the \"career  satisfaction\" or \"work passion\" threshold with CS than for BME. Feeling  satisfied with your career depends on many factors, but Cal Newport  argues that the largest factor is essentially mastery, which leads to  involvement. Mastery seems more difficult to guage with the noisy and  prolonged feedback of the engineering sciences, so the motivations with  the greatest relative importance might be the satisfaction of turning  out product, satisfying factual curiosity or curiosity about  established/canon models (as opposed to curiosity which is more local to  your own circumstances or you figuring things out), and in the case of  biomed, saving lives by design. With mathematics and programming the  problem space is such that you can do math and programming for their own  sakes.</li>\n<li>Most instances of biomedical engineering majors around  the world are mainly graduate studies. The most often reported  experience is that when you have someone getting a PhD in biomedical  engineering, it's in addition to their undergraduate experience as a  mechanical engineer, an electrical engineer or a computer scientist. The  story goes that these problem solving skills are applied to the biology  after being developed - once again a case of some fields being more  upwind than others. By contrast, an undergradute in bioengineering would  be taking courses where they are not developing these skills, as our  current understanding of biology is not strongly predictive. After  talking to one of the faculty heads, the person who designed the  program, he is very much aware of problems such as these in engineers as  they are currently educated. This includes overdoing specialization and  under-emphasizing the entire product development process, or a  principle of \"first, do no harm\". He has been working on the curriculum  for thirty years as opposed to the seven years of cases like MIT - I  consider this moderate evidence that I will not be missing out on the  necessary mental toolkit over other engineers.</li>\n<li>In the case  where biomedical engineering is less flexible than I believed, I would  essentially have a \"jack of all trades\" education meaning engineering  firms in general would pass over me in favor of a more specialized  candidate. This is partially hedged against by learning the computer  science as an \"out\", but in the end it points to the possibility that  the way I'm perceiving this major's value is incorrect.</li>\n</ul>\n<p>So  for this \"have cake and eat it to\" plan to work there are a larger  string of case exceptions in the biomedical option than the computing  options, and definitely the media and design option. The reward would be  that the larger amount of domain specific knowledge in a field that has  held my curiosity for several years now, while hitting on. I would also  be playing to one of SFU's comparative advantages: the quality of the  biomedical faculty here is high relative to other institutions if the  exceptions hold, and potentially the relative quality of the computer  science and design faculties as well. (This could be an argument for  switching institutions if those two skillsets are a \"better fit\".  However, my intuition is that the cost for such is very high and  probably wouldn't be worth it.)<br /><br />Possible points of investigation:</p>\n<ul>\n<li>What  is hooking me most strongly to biomedical engineering were the  potentials of cognitive enhancement research and molecular design (like  what they have going on at the bio-nano group at Autodesk: <a href=\"http://www.autodeskresearch.com/groups/nano\" target=\"_blank\">http://www.autodeskresearch.com/groups/nano</a>).  If these were the careers I was optimizing towards as an ends, it might  make more sense to actual model what skills and people will actually be  needed to develop these technologies and take advantage of them. After  writing this I feel less strongly about these <em>exact</em> fields or careers. Industry research still seems like a good exercise.</li>\n<li>I  will have to be honest that after my experience doing lab work for  chemistry at school, I was frustrated by how exhausted I am at the end  of each session, physically and mentally. This doesn't necessarily  reflect on how all lab work will be, especially if it's more intimately  tied with something else I want to achieve. And granted, the labs are  three hours long of standing. It does make me question how I would be  like in this work environment, however, and that is worth collecting  more information for.</li>\n<li>To get actual evidence of flexibility  in skillset it would be worth polling actual alumni from the program, to  see if any of the convictions about the program are true.</li>\n</ul>\n<p>--</p>\n<p>Thoughts, anyone?<span class=\"HOEnZb\"><span style=\"color: #888888;\"><br /></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dn9Dvwr6DWracJ7zW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "24725", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-16T04:35:46.013Z", "modifiedAt": null, "url": null, "title": "Self-serving meta: Whoever keeps block-downvoting me, is there some way to negotiate peace?", "slug": "self-serving-meta-whoever-keeps-block-downvoting-me-is-there", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:07.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3hpeXsCXPKJLL25df/self-serving-meta-whoever-keeps-block-downvoting-me-is-there", "pageUrlRelative": "/posts/3hpeXsCXPKJLL25df/self-serving-meta-whoever-keeps-block-downvoting-me-is-there", "linkUrl": "https://www.lesswrong.com/posts/3hpeXsCXPKJLL25df/self-serving-meta-whoever-keeps-block-downvoting-me-is-there", "postedAtFormatted": "Saturday, November 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-serving%20meta%3A%20Whoever%20keeps%20block-downvoting%20me%2C%20is%20there%20some%20way%20to%20negotiate%20peace%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-serving%20meta%3A%20Whoever%20keeps%20block-downvoting%20me%2C%20is%20there%20some%20way%20to%20negotiate%20peace%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3hpeXsCXPKJLL25df%2Fself-serving-meta-whoever-keeps-block-downvoting-me-is-there%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-serving%20meta%3A%20Whoever%20keeps%20block-downvoting%20me%2C%20is%20there%20some%20way%20to%20negotiate%20peace%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3hpeXsCXPKJLL25df%2Fself-serving-meta-whoever-keeps-block-downvoting-me-is-there", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3hpeXsCXPKJLL25df%2Fself-serving-meta-whoever-keeps-block-downvoting-me-is-there", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<p>I'm just tired of the signal pollution, and would like to be able to use karma to honestly appraise the worth of my articles and posts, without seeing 80% of my downvotes come in chunks that correspond precisely to how many posts I've made since the last massive downvote spree.</p>\n<p>&nbsp;</p>\n<p>EDIT to add data points:</p>\n<p>Spurious downvoting stopped soon after I named a particular individual (not ALL downvoting stopped, but the downvotes I got all seemed on-the-level.)&nbsp;</p>\n<p>One block of potentially spurious downvoting occurred approximately one week ago, but then karma patterns returned to expected levels. I consider this block dubious, because it reasonably matches what I'd expect to see if someone noticed several of my posts together and disagreed with all of them, and did not match the usual pattern of starting with the earliest or latest post that I had made and downvoting everything (it downvoted all posts in a few threads, but not in other threads), so I'm just adding for completeness.</p>\n<p>Spurious, indiscriminate downvoting started up again approximately half an hour ago on Sunday (12/1/2013), around noon MDT.</p>\n<p>Edit: And now on Tuesday, 12/3/2013, at 10 AM, I'm watching my karma go down again... about 30 points so far.</p>\n<p>Edit: And now on Saturday, 12/14/2013, at 2 PM, I'm watching my karma go down again... about 15 points so far, at a rate of about 1-2 points per second.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "hGzywXvWhSdJi5F2a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3hpeXsCXPKJLL25df", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 28, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "24726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 283, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-16T16:00:41.400Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories ", "slug": "meetup-washington-dc-goals-political-advocacy-as-effective-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f4sfj6Lz8KYMFsJ2u/meetup-washington-dc-goals-political-advocacy-as-effective-0", "pageUrlRelative": "/posts/f4sfj6Lz8KYMFsJ2u/meetup-washington-dc-goals-political-advocacy-as-effective-0", "linkUrl": "https://www.lesswrong.com/posts/f4sfj6Lz8KYMFsJ2u/meetup-washington-dc-goals-political-advocacy-as-effective-0", "postedAtFormatted": "Saturday, November 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Goals%2C%20Political%20Advocacy%20as%20Effective%20Altruism%2C%20and%20Stories%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Goals%2C%20Political%20Advocacy%20as%20Effective%20Altruism%2C%20and%20Stories%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff4sfj6Lz8KYMFsJ2u%2Fmeetup-washington-dc-goals-political-advocacy-as-effective-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Goals%2C%20Political%20Advocacy%20as%20Effective%20Altruism%2C%20and%20Stories%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff4sfj6Lz8KYMFsJ2u%2Fmeetup-washington-dc-goals-political-advocacy-as-effective-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff4sfj6Lz8KYMFsJ2u%2Fmeetup-washington-dc-goals-political-advocacy-as-effective-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/to'>Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So, we came up with three ideas, none of which will necessarily take a long time. So we're doing them together. We will be: Talking about goals and trying to revive the goals spreadsheet Discussing several posts on Givewell's blog about political advocacy as effective altruism And (if people have them) telling Haloween stories, as per this thread (a bit late, I know, but why not?)</p>\n\n<p>(This is a duplicate: last meetup didn't get to these topics).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/to'>Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f4sfj6Lz8KYMFsJ2u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.425348927689768e-06, "legacy": true, "legacyId": "24730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories_\">Discussion article for the meetup : <a href=\"/meetups/to\">Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So, we came up with three ideas, none of which will necessarily take a long time. So we're doing them together. We will be: Talking about goals and trying to revive the goals spreadsheet Discussing several posts on Givewell's blog about political advocacy as effective altruism And (if people have them) telling Haloween stories, as per this thread (a bit late, I know, but why not?)</p>\n\n<p>(This is a duplicate: last meetup didn't get to these topics).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories_1\">Discussion article for the meetup : <a href=\"/meetups/to\">Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories </a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories ", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories_", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Goals, Political Advocacy as Effective Altruism, and Stories ", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Goals__Political_Advocacy_as_Effective_Altruism__and_Stories_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-16T16:16:21.749Z", "modifiedAt": null, "url": null, "title": "Mainstream Epistemology for LessWrong, Part 1: Feldman on Evidentialism", "slug": "mainstream-epistemology-for-lesswrong-part-1-feldman-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oWaZqTHLKdB6pSmTr/mainstream-epistemology-for-lesswrong-part-1-feldman-on", "pageUrlRelative": "/posts/oWaZqTHLKdB6pSmTr/mainstream-epistemology-for-lesswrong-part-1-feldman-on", "linkUrl": "https://www.lesswrong.com/posts/oWaZqTHLKdB6pSmTr/mainstream-epistemology-for-lesswrong-part-1-feldman-on", "postedAtFormatted": "Saturday, November 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mainstream%20Epistemology%20for%20LessWrong%2C%20Part%201%3A%20Feldman%20on%20Evidentialism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMainstream%20Epistemology%20for%20LessWrong%2C%20Part%201%3A%20Feldman%20on%20Evidentialism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoWaZqTHLKdB6pSmTr%2Fmainstream-epistemology-for-lesswrong-part-1-feldman-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mainstream%20Epistemology%20for%20LessWrong%2C%20Part%201%3A%20Feldman%20on%20Evidentialism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoWaZqTHLKdB6pSmTr%2Fmainstream-epistemology-for-lesswrong-part-1-feldman-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoWaZqTHLKdB6pSmTr%2Fmainstream-epistemology-for-lesswrong-part-1-feldman-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1854, "htmlBody": "<p>Richard Feldman's&nbsp;<em><a href=\"http://www.amazon.com/Epistemology-Richard-Feldman/dp/0133416453\" target=\"_blank\">Epistemology</a> </em>is a widely-used philosophy textbook published in 2003. I've decided to write a series of posts summarizing its contents, because it contains some surprisingly reasonable views (given what you may have heard about mainstream philosophy), and it also counters some common myths about what all philosophers supposedly know about evidence, the problem of induction, and so on. This installment briefly covers the first three chapters before moving on to Feldman's discussion of the philosophical view he calls evidentialism.<a id=\"more\"></a></p>\n<hr />\n<p>Chapter 1 is an introductory chapter, which explains that \"The theory of knowledge, or epistemology, is the branch of philosophy that addresses philosophical questions about knowledge and rationality.\" Feldman says he will use the term \"The Standard View\" to refer to the collection of things we ordinarily think about knowledge and rationality.</p>\n<p>On the Standard View, we know a large variety of things about a variety of topics, from science and mathematics to other people's mental states to the past and the future. Furthermore, the Standard View says that our primary sources of knowledge consist of perception, memory, testimony, introspection, reasoning, and rational insight. Those are just the sources of knowledge most people would agree on, though&mdash;Feldman acknowledges that some people might want to add to that list.</p>\n<p>It's worth mentioning here that because <a href=\"/lw/iu0/trusting_expert_consensus/\" target=\"_blank\">philosophers agree on so little</a>, <strong>when you read a philosophy textbook you should assume it will say some things that are just the author's personal opinion, rather than representing any professional consensus</strong>. And in describing the \"Standard View,\" Feldman is mostly just trying to describe what ordinary people commonsensically believe, rather than claiming that it's the standard view within philosophy. But in fact, most philosophers probably would agree with what Feldman calls the \"Standard View.\"</p>\n<p>Chapter 1 also sketches the structure of the rest of the book: chapters 2-5 will develop the Standard View, while chapters 6-9 will consider various challenges and objections to the Standard View.</p>\n<p>I'm going to mostly skip over chapters 2 and 3 because they cover a debate that was pretty accurately summarized by Luke Muelhlauser and Louie Helm in <a href=\"https://intelligence.org/files/IE-ME.pdf\" target=\"_blank\">\"Intelligence Explosion: Machine Ethics\"</a>:</p>\n<blockquote>\n<p>Since Plato, many have believed that knowledge is justified true belief. Gettier (<a href=\"http://www.ditext.com/gettier/gettier.html\" target=\"_blank\">1963</a>) argued that knowledge cannot be justified true belief because there are hypothetical cases of justified true belief that we intuitively would not count as knowledge. Since then, each newly proposed conceptual analysis of knowledge has been met with novel counterexamples (Shope 1983). Weatherson (2003) called this the &ldquo;analysis of knowledge merry go round.&rdquo;</p>\n</blockquote>\n<p>That said, it's worth briefly explaining Feldman's own solution to the problem, which will be relevant for understanding chapter 4. With rare exceptions, philosophers generally agree that true belief is necessary, but not sufficient, for knowledge. The question is how to replace or supplement the justification condition. Feldman's answer is to keep the justification condition, and furthermore add that knowledge requires that ones justification for a belief \"does not essentially depend on any falsehood.\" (If you click the link to Gettier's paper, above, and read Gettier's examples, hopefully you will understand why this is appealing.)</p>\n<p>Feldman includes a disclaimer saying, \"The idea of essential dependence is admittedly not completely clear. However, it gives us a useful working definition of knowledge with which we can proceed.\" Being willing to invoke a somewhat unclear condition in his analysis of knowledge is probably a wise move on Feldman's part, given the history of attempts at more exact analyses being felled by clever counterexamples. And his analysis gives him a rationale for focusing on the next chapter on what it takes for a belief to be justified.</p>\n<p>Chapter 4 is dedicated to discussing evidentialism, which Feldman defines as the view that a belief is justified for a particular person if and only if their evidence supports that belief. This is a view Feldman has defended in a number of journal articles, many of them co-authored with Earl Conee (see their anthology, <em><a href=\"http://www.amazon.com/Evidentialism-Earl-Conee/dp/0199253730\" target=\"_blank\">Evidentialism</a></em>). That this view has defenders within mainstream philosophy may surprise those who've heard that there's supposed to be a philosophical consensus that requiring beliefs to be based on evidence is self-defeating (no such consensus exists, but <a href=\"http://www.patheos.com/blogs/hallq/2013/11/the-plantinga-clique/\" target=\"_blank\">some</a> <a href=\"/lw/j09/academic_cliques/\" target=\"_blank\">philosophers</a> <a href=\"http://www.patheos.com/blogs/hallq/2013/11/paper-on-plantinga-and-classical-foundationalism/\" target=\"_blank\">claim it does</a>).</p>\n<p>Feldman emphasizes that evidentialism, in the form he defends, is an epistemological claim, not a moral or prudential one. He's not interested in defending the claim, found in William Clifford's famous essay <a href=\"http://www.infidels.org/library/historical/w_k_clifford/ethics_of_belief.html\" target=\"_blank\">\"The Ethics of Belief,\"</a> that \"it is wrong always, everywhere, and for anyone, to believe anything upon insufficient evidence\" (at least, not if the claim is taken to be a moral one).</p>\n<p>This is how Feldman responds to the \"loyalty\" objection to evidentialism, which argues that it is sometimes right to, for example, believe in a friend's innocence even when the evidence does not support that conclusion. Feldman allows that that&nbsp;<em>might</em>&nbsp;be right as a matter of morality (he doesn't say it <em>is </em>right), but says that doesn't change what the rational thing to believe in such a case is.</p>\n<p>The rest of the chapter is a fairly long (given that book as whole is only 200 pages) discussion of the infinite regress argument, foundationalism, and coherentism. It's worth interjecting that, while Feldman presents the discussion not in terms of objections to evidentialism, but in terms of \"ways in which the details of evidentialism might be spelled out,\" some philosophers have claimed the infinite regress argument as an argument for radical skepticism, while others have claimed it shows (as a matter of philosophical consensus!) that evidentialism specifically leads to radical skepticism.</p>\n<p>Feldman's discussion could be read as a gentle rebuttal to these claims, even though he never addresses them directly. His version of the infinite regress argument is formulated as an argument for the existence of justified basic beliefs, which he defines as beliefs that are justified, but \"not justified on the basis of any other beliefs.\" It runs as follows:</p>\n<ol>\n<li>Either there &nbsp;are justified basic beliefs or each justified belief has an evidential chain that either: (a) terminates in an unjustified belief (b) is an infinite regress of beliefs (c) is circular.</li>\n<li>But beliefs based on unjustified beliefs are not themselves justified, so no justified belief could have an evidential chain that terminates in an unjustified belief (that is, not (a)).</li>\n<li>No person could have an infinite series of basic beliefs, so no justified belief could have an evidential chain that is an infinite regress of beliefs (that is, not (b)).</li>\n<li>No belief could be justified by itself, so no justified belief could have an evidential chain that is circular (that is, not (c)).</li>\n<li>Therefore, there are justified basic beliefs.</li>\n</ol>\n<p>Feldman lists three main responses to this argument: foundationalism, which accepts the argument's conclusion; coherentism, which rejects premise 4; and skepticism, which says the argument goes wrong assuming that there <em>are </em>justified beliefs in the first place and in fact no beliefs can be justified. In this chapter, Feldman's focuses on foundationalism and coherentism, leaving skepticism for later chapters.</p>\n<p>First, he considers a view he calls \"Cartesian foundationalism,\" named after Ren&eacute; Descartes, though Feldman admits \"that it is unlikely that Descartes actually would agree to all aspects of the view to be described.\" Cartesian foundationalism, in Feldman's sense, combines foundationalism with three further claims:</p>\n<ol>\n<li>Beliefs about one's own inner states of mind (appearance beliefs) and beliefs about elementary truths of logic are justified basic beliefs.</li>\n<li>Justified basic beliefs are justified because we cannot be mistaken about them. We are \"infallible\" about such matters.</li>\n<li>The rest of our justified beliefs&nbsp;(e.g., our beliefs about the external world) are justified because they can be deducded from our basic beliefs.</li>\n</ol>\n<p>People who know something of Descartes' reputation within contemporary philosophy will not be surprised to find out that Feldman sees lots of problems with this view, including that our beliefs about our own inner states of mind <em>aren't </em>infallible, and that much of what what we know cannot be deduced from beliefs about our own inner states of mind.</p>\n<p>Next, Feldman discusses coherentism. The challenge for coherentism is developing it in a way that doesn't involve obvious circularity. As Feldman explains:</p>\n<blockquote>\n<p>So coherentists reject premise (1-4) of <em>The Infinite Regress Argument, </em>the step of the regress argument that rejects circular evidential chains. This is not because they think that you can justify one belief by another, that second by a third, and then justify the third by appeal to the first. Rather, their idea is that justification is a more systematic and holistic matter, that each belief is justified by the way it fits into one's overall system of beliefs.</p>\n</blockquote>\n<p>Feldman discusses various ways to develop this idea, but doesn't find any \"suitable,\" because among other things of difficulties with giving a coherentist account of which beliefs are and are not justified, and with saying what coherence actually is. He also discusses the \"isolation argument\" against coherentism, which complains that coherentism seems to allow any beliefs to be justified when included in the right set of other beliefs, even if the whole set of beliefs is totally detached from reality. The point is that what matters for beliefs being justified isn't just other beliefs, but also things like our sensory experiences.</p>\n<p>Finally, Feldman discusses his own preferred view, which he calls \"modest foundationalism,\" which claims:</p>\n<ol>\n<li>Basic beliefs are spontaneously formed beliefs. Typically, beliefs about the external world, including beliefs about the kinds of objects experienced or their sensory qualities, are justified and basic. Beliefs about mental states can also be justified and basic.</li>\n<li>A spontaneously formed belief is justified provided it is a proper response to experiences and is not defeated by other evidence the believer has.</li>\n<li>Nonbasic beliefs are justified when they are supported by strong inductive inferences&mdash;including enumerative induction and inferences to the best explanation&mdash;from justified basic beliefs.</li>\n</ol>\n<p>The main objection Feldman discusses to this view comes from a coherentist angle: Laurence BonJour's argument that there are <em>no</em> justified basic beliefs. The details of that discussion, though, are less interesting than this simple fact: the suggestion made by some anti-evidentialist philosophers that evidentialism is indisputably self-defeating is just plain wrong. The idea seems to be based on equating \"basic belief\" with \"belief not based on evidence.\" That would seem to allow the infinite regress argument to be turned against evidentialism.</p>\n<p>However, this ignores the fact that Feldman (or someone with similar views) would argue that basic beliefs are supported by evidence to the extent that they're a proper response to experience and not defeated by other evidence. Furthermore, many people would reject other premises of the \"infinite regress argument against evidentialism,\" particularly coherentists rejecting premise 4. This by itself doesn't prove that evidentialism is true. It could still be wrong for other reasons. But at the very least, the case against it is nowhere near as clear-cut as some anti-evidentialist philosophers would like you to believe.</p>\n<hr />\n<p><strong>Note:</strong> I'm very much open to input on how to handle future installments in the series. In particular, I'm not sure how much information to try to cram into one post (this could've easily been two), and I'm not sure if the next post should cover nonevidentialist epistemologies, or if I should just skp straight to skepticism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "GLykb6NukBeBQtDvQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oWaZqTHLKdB6pSmTr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 29, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "24696", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 84, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R8YpYTq8LoD3k948L", "t9oXxwss8p6oXGg3W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-17T15:14:23.400Z", "modifiedAt": null, "url": null, "title": "Quantum versus logical bombs", "slug": "quantum-versus-logical-bombs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:39.353Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JGHQPybvjLAgimXae/quantum-versus-logical-bombs", "pageUrlRelative": "/posts/JGHQPybvjLAgimXae/quantum-versus-logical-bombs", "linkUrl": "https://www.lesswrong.com/posts/JGHQPybvjLAgimXae/quantum-versus-logical-bombs", "postedAtFormatted": "Sunday, November 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantum%20versus%20logical%20bombs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantum%20versus%20logical%20bombs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGHQPybvjLAgimXae%2Fquantum-versus-logical-bombs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantum%20versus%20logical%20bombs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGHQPybvjLAgimXae%2Fquantum-versus-logical-bombs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGHQPybvjLAgimXae%2Fquantum-versus-logical-bombs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 357, "htmlBody": "<p>Child, I'm sorry to tell you that the world is about to end. Most likely. You see, this <a href=\"http://gutenberg.net.au/ebooks06/0602301.txt\">madwoman</a> has designed a <a href=\"http://www.youtube.com/watch?v=cmCKJi3CKGE\">doomsday machine</a> that will end all life as we know it - painlessly and immediately. It is attached to a supercomputer that will calculate the 10<sup>100th</sup> digit of pi - if that digit is zero, we're safe. If not, we're doomed and dead.</p>\n<p>However, there is one thing you are allowed to do - switchout the logical trigger and replaced it by a quantum trigger, that instead generates a quantum event that will prevent the bomb from triggering with 1/10th measure squared (in the other cases, the bomb goes off). You ok paying &euro;5 to replace the triggers like this?</p>\n<p>If you treat quantum measure squared exactly as probability, then you shouldn't see any reason to replace the trigger. But if you believed in many worlds quantum mechanics (or think that MWI is possibly correct with non-zero probability), you might be tempted to accept the deal - after all, everyone will survive in one branch. But strict&nbsp;<a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">total utilitarians</a>&nbsp;may still reject the deal. Unless they refuse to treat quantum measure as akin to probability in the first place (meaning they would accept all <a href=\"http://en.wikipedia.org/wiki/Quantum_suicide_and_immortality\">quantum suicide</a> arguments), they tend to see a universe with a tenth of measure-squared as exactly equally valued to a 10% chance of a universe with full measure. And they'd even do the reverse, replace a quantum trigger with a logical one, if you paid them &euro;5 to do so.</p>\n<p>Still, most people, in practice, would choose to change the logical bomb for a quantum bomb, if only because they were slightly uncertain about their total utilitarian values. It would seem self evident that risking the total destruction of humanity is much worse than reducing its measure by a factor of 10 - a process that would be undetectable to everyone.</p>\n<p>Of course, once you agree with that, we can start squeezing. What if the quantum trigger only has 1/20 measured-squared \"chance\" of saving us? 1/000? 1/10000? If you don't want to fully accept the quantum immortality arguments, you need to stop - but at what point?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JGHQPybvjLAgimXae", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 27, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "24543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pyTuR4ZbLfqpS2oMh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-17T15:44:04.431Z", "modifiedAt": null, "url": null, "title": "Weak repugnant conclusion need not be so repugnant given fixed resources", "slug": "weak-repugnant-conclusion-need-not-be-so-repugnant-given", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.649Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7mFKwWhfoL3BoB8cR/weak-repugnant-conclusion-need-not-be-so-repugnant-given", "pageUrlRelative": "/posts/7mFKwWhfoL3BoB8cR/weak-repugnant-conclusion-need-not-be-so-repugnant-given", "linkUrl": "https://www.lesswrong.com/posts/7mFKwWhfoL3BoB8cR/weak-repugnant-conclusion-need-not-be-so-repugnant-given", "postedAtFormatted": "Sunday, November 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weak%20repugnant%20conclusion%20need%20not%20be%20so%20repugnant%20given%20fixed%20resources&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeak%20repugnant%20conclusion%20need%20not%20be%20so%20repugnant%20given%20fixed%20resources%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7mFKwWhfoL3BoB8cR%2Fweak-repugnant-conclusion-need-not-be-so-repugnant-given%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weak%20repugnant%20conclusion%20need%20not%20be%20so%20repugnant%20given%20fixed%20resources%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7mFKwWhfoL3BoB8cR%2Fweak-repugnant-conclusion-need-not-be-so-repugnant-given", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7mFKwWhfoL3BoB8cR%2Fweak-repugnant-conclusion-need-not-be-so-repugnant-given", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 535, "htmlBody": "<p><em>I want to thank&nbsp;<a href=\"/user/Irgy/overview/\">Irgy</a>&nbsp;for this idea.</em></p>\n<p>As people generally know, total utilitarianism leads to the <a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">repugnant conclusion</a> - the idea that no matter how great a universe X would be, filled without trillions of ultimately happy people having ultimately meaningful lives filled with adventure and joy, there is another universe Y which is better - and that is filled with nothing but dull, boring people whose quasi-empty and repetitive lives are just one tiny iota above being too miserable to endure. But since the second universe is much bigger than the first, it comes out on top. Not only in that if we had Y it would be immoral to move to X (which is perfectly respectable, as doing so might involve killing a lot of people, or at least allowing a lot of people to die). But in that, if we planned for our future world now, we would desperately want to bring Y into existence rather than X - and could run great costs or great risks to do so. And if we were in world X, we must at all costs move to Y, making all current people much more miserable as we do so.</p>\n<p>The repugnant conclusion is the main reason I <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">reject total utilitarianism</a> (the other one being that total utilitarianism sees no problem with painlessly killing someone by surprise, as long as you also gave birth to someone else of equal happiness). But the repugnant conclusion can emerge from many other population ethics as well. If adding more people of <em>slightly</em> less happiness than the average is always a bonus (\"mere addition\"), and if equalising happiness is never a penalty, then you <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">get</a> the repugnant conclusion (caveat: there are some subtleties to do with infinite series).</p>\n<p>But repugnant conclusions reached in that way may not be so repugnant, in practice. Let S be a system of population ethics that accepts the repugnant conclusion, due to the argument above. S may indeed conclude that the big world Y is better than the super-human world X. But S need not conclude that Y is the best world we can build, given any fixed and finite amount of resources. Total utilitarianism is indifferent to having a world with half the population and twice the happiness. But S need to be indifferent to that - it may much prefer the twice-happiness world. Instead of the world Y, it may prefer to reallocate resources to instead achieve the world X', which has the same average happiness as X but is slightly larger.</p>\n<p>Of course, since it accepts the repugnant conclusion, there will be a barely-worth-living world Y' which it prefers to X'. But then it might prefer reallocating the resources of Y' to the happy world X'', and so on.</p>\n<p>This is not an argument for efficiency of resource allocation: even if it's four times as hard to get people twice as happy, S can still want to do so. You can accept the repugnant conclusion and still want to reallocate any fixed amount of resources towards low population and extreme happiness.</p>\n<p>It's always best to have some examples, so here is one: an S whose value is the product of average agent happiness times the logarithm of population size.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7mFKwWhfoL3BoB8cR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.426749898368256e-06, "legacy": true, "legacyId": "24645", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pyTuR4ZbLfqpS2oMh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T04:01:34.107Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-12", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vLAAFsBTv6bngro5w/meetup-washington-dc-fun-and-games-meetup-12", "pageUrlRelative": "/posts/vLAAFsBTv6bngro5w/meetup-washington-dc-fun-and-games-meetup-12", "linkUrl": "https://www.lesswrong.com/posts/vLAAFsBTv6bngro5w/meetup-washington-dc-fun-and-games-meetup-12", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLAAFsBTv6bngro5w%2Fmeetup-washington-dc-fun-and-games-meetup-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLAAFsBTv6bngro5w%2Fmeetup-washington-dc-fun-and-games-meetup-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLAAFsBTv6bngro5w%2Fmeetup-washington-dc-fun-and-games-meetup-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tp'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tp'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vLAAFsBTv6bngro5w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4274767384172982e-06, "legacy": true, "legacyId": "24740", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/tp\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/tp\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T06:06:34.701Z", "modifiedAt": null, "url": null, "title": "Feedback from Less Wrong Community", "slug": "feedback-from-less-wrong-community", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:38.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "scott_from_castify", "createdAt": "2012-12-03T09:22:52.098Z", "isAdmin": false, "displayName": "scott_from_castify"}, "userId": "Q8YfKQGevZXcRDNDt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KgGGhJLgN3436T4TG/feedback-from-less-wrong-community", "pageUrlRelative": "/posts/KgGGhJLgN3436T4TG/feedback-from-less-wrong-community", "linkUrl": "https://www.lesswrong.com/posts/KgGGhJLgN3436T4TG/feedback-from-less-wrong-community", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Feedback%20from%20Less%20Wrong%20Community&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFeedback%20from%20Less%20Wrong%20Community%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgGGhJLgN3436T4TG%2Ffeedback-from-less-wrong-community%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Feedback%20from%20Less%20Wrong%20Community%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgGGhJLgN3436T4TG%2Ffeedback-from-less-wrong-community", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgGGhJLgN3436T4TG%2Ffeedback-from-less-wrong-community", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>Hi from Castify.</p>\n<p>We're continuing our work to <a href=\"http://castify.co/partners/1-less-wrong\" target=\"_blank\">turn the Less Wrong sequences into audio</a>. Could you spare about 30 seconds to <a href=\"https://www.surveymonkey.com/s/W2M57MK\" target=\"_blank\">help us decide which one to do next</a>?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KgGGhJLgN3436T4TG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.4276000072253205e-06, "legacy": true, "legacyId": "24742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T10:38:17.739Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social Meetup, 24/11/2013 [Back to the Shakespeare's Head]", "slug": "meetup-london-social-meetup-24-11-2013-back-to-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:35.934Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hiybFLRZcnXcwGp7n/meetup-london-social-meetup-24-11-2013-back-to-the", "pageUrlRelative": "/posts/hiybFLRZcnXcwGp7n/meetup-london-social-meetup-24-11-2013-back-to-the", "linkUrl": "https://www.lesswrong.com/posts/hiybFLRZcnXcwGp7n/meetup-london-social-meetup-24-11-2013-back-to-the", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20Meetup%2C%2024%2F11%2F2013%20%5BBack%20to%20the%20Shakespeare's%20Head%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20Meetup%2C%2024%2F11%2F2013%20%5BBack%20to%20the%20Shakespeare's%20Head%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhiybFLRZcnXcwGp7n%2Fmeetup-london-social-meetup-24-11-2013-back-to-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20Meetup%2C%2024%2F11%2F2013%20%5BBack%20to%20the%20Shakespeare's%20Head%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhiybFLRZcnXcwGp7n%2Fmeetup-london-social-meetup-24-11-2013-back-to-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhiybFLRZcnXcwGp7n%2Fmeetup-london-social-meetup-24-11-2013-back-to-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/tq\">London Social Meetup, 24/11/2013 [Back to the Shakespeare's Head]</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 November 2013 02:00:00 PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">64 Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK-68 Kingsway, London WC2B 6BG, UK</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>LessWrong London is having another meetup this Sunday (24/11) at 2:00 PM. We are back to our usual venue - <a rel=\"nofollow\" href=\"https://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\">The Shakespeare's Head</a> by Holborn tube station.</p>\n<p>There is no fixed topic of discussion nor is there anything planned so be prepared for anything. There will be a sign identifying us (hopefully one with a paperclip) and if you have any problems feel free to contact me by e-mail - Tenoke[at]Tenoke.com or by phone - 07425168803.</p>\n<p>&nbsp;</p>\n<p><em>Reminder: The LW London Meetups are currently a weekly event - Every Sunday at 2:00 PM!</em></p>\n<p>If you want more information about the meetup or anything else come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> or alternatively to the <a rel=\"nofollow\" href=\"https://www.facebook.com/groups/380103898766356/\">facebook group</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/tq\">London Social Meetup, 24/11/2013 [Back to the Shakespeare's Head]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hiybFLRZcnXcwGp7n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "24748", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__24_11_2013__Back_to_the_Shakespeare_s_Head_\">Discussion article for the meetup : <a href=\"/meetups/tq\">London Social Meetup, 24/11/2013 [Back to the Shakespeare's Head]</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 November 2013 02:00:00 PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">64 Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK-68 Kingsway, London WC2B 6BG, UK</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>LessWrong London is having another meetup this Sunday (24/11) at 2:00 PM. We are back to our usual venue - <a rel=\"nofollow\" href=\"https://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\">The Shakespeare's Head</a> by Holborn tube station.</p>\n<p>There is no fixed topic of discussion nor is there anything planned so be prepared for anything. There will be a sign identifying us (hopefully one with a paperclip) and if you have any problems feel free to contact me by e-mail - Tenoke[at]Tenoke.com or by phone - 07425168803.</p>\n<p>&nbsp;</p>\n<p><em>Reminder: The LW London Meetups are currently a weekly event - Every Sunday at 2:00 PM!</em></p>\n<p>If you want more information about the meetup or anything else come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> or alternatively to the <a rel=\"nofollow\" href=\"https://www.facebook.com/groups/380103898766356/\">facebook group</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__24_11_2013__Back_to_the_Shakespeare_s_Head_1\">Discussion article for the meetup : <a href=\"/meetups/tq\">London Social Meetup, 24/11/2013 [Back to the Shakespeare's Head]</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social Meetup, 24/11/2013 [Back to the Shakespeare's Head]", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__24_11_2013__Back_to_the_Shakespeare_s_Head_", "level": 1}, {"title": "Discussion article for the meetup : London Social Meetup, 24/11/2013 [Back to the Shakespeare's Head]", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__24_11_2013__Back_to_the_Shakespeare_s_Head_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T10:57:42.591Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Memory Tricks", "slug": "meetup-moscow-memory-tricks", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zh3cjdc4vtbdqHGFj/meetup-moscow-memory-tricks", "pageUrlRelative": "/posts/zh3cjdc4vtbdqHGFj/meetup-moscow-memory-tricks", "linkUrl": "https://www.lesswrong.com/posts/zh3cjdc4vtbdqHGFj/meetup-moscow-memory-tricks", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Memory%20Tricks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Memory%20Tricks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzh3cjdc4vtbdqHGFj%2Fmeetup-moscow-memory-tricks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Memory%20Tricks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzh3cjdc4vtbdqHGFj%2Fmeetup-moscow-memory-tricks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzh3cjdc4vtbdqHGFj%2Fmeetup-moscow-memory-tricks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 182, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tr'>Moscow, Memory Tricks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have a report about how memory works, tips and tricks to remember, spaced repetition and Anki.</p>\n\n<p>There will be a discussion section next, so please think about topics you want to discuss on Sunday.</p>\n\n<p>After that we will have a game session: you can bring some tabletop games and play it with other people.</p>\n\n<p>We will also have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131124_meet_up&amp;utm_content=20131124_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131124_meet_up&amp;utm_content=20131124_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.\nWe start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tr'>Moscow, Memory Tricks</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zh3cjdc4vtbdqHGFj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4278871569168294e-06, "legacy": true, "legacyId": "24749", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Memory_Tricks\">Discussion article for the meetup : <a href=\"/meetups/tr\">Moscow, Memory Tricks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have a report about how memory works, tips and tricks to remember, spaced repetition and Anki.</p>\n\n<p>There will be a discussion section next, so please think about topics you want to discuss on Sunday.</p>\n\n<p>After that we will have a game session: you can bring some tabletop games and play it with other people.</p>\n\n<p>We will also have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131124_meet_up&amp;utm_content=20131124_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131124_meet_up&amp;utm_content=20131124_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.\nWe start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Memory_Tricks1\">Discussion article for the meetup : <a href=\"/meetups/tr\">Moscow, Memory Tricks</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Memory Tricks", "anchor": "Discussion_article_for_the_meetup___Moscow__Memory_Tricks", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Memory Tricks", "anchor": "Discussion_article_for_the_meetup___Moscow__Memory_Tricks1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T11:03:46.703Z", "modifiedAt": null, "url": null, "title": "Another problem with quantum measure", "slug": "another-problem-with-quantum-measure", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:39.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cwkgccwRDtuQFH7EY/another-problem-with-quantum-measure", "pageUrlRelative": "/posts/cwkgccwRDtuQFH7EY/another-problem-with-quantum-measure", "linkUrl": "https://www.lesswrong.com/posts/cwkgccwRDtuQFH7EY/another-problem-with-quantum-measure", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20problem%20with%20quantum%20measure&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20problem%20with%20quantum%20measure%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcwkgccwRDtuQFH7EY%2Fanother-problem-with-quantum-measure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20problem%20with%20quantum%20measure%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcwkgccwRDtuQFH7EY%2Fanother-problem-with-quantum-measure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcwkgccwRDtuQFH7EY%2Fanother-problem-with-quantum-measure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 322, "htmlBody": "<p>Let's <a href=\"/lw/ixr/quantum_versus_logical_bombs/\">play around</a> with the quantum measure some more. Specifically, let's posit a theory T that claims that the quantum measure of our universe is increasing - say by 50% each day. Why could this be happening? Well, here's a quasi-justification for it: imagine there are lots and lots of of universes, most of them in chaotic random states, jumping around to other chaotic random states, in accordance with the usual laws of quantum mechanics. Occasionally, one of them will partially tunnel, by chance, into the same state our universe is in - and then will evolve forwards in time exactly as our universe is. Over time, we'll accumulate an ever-growing measure.</p>\n<p>That theory sounds pretty unlikely, no matter what feeble attempts are made to justify it. But T is observationally indistinguishable from our own universe, and has a non-zero probability of being true. It's the reverse of the (more likely) theory <a href=\"/lw/g9n/false_vacuum_the_universe_playing_quantum_suicide/\">presented here</a>, in which the quantum measure was being constantly diminished. And it's very bad news for theories that treat the quantum measure (squared) as akin to a probability, without ever renormalising. It implies that one must continually sacrifice for the long-term: any pleasure today is wasted, as that pleasure will be weighted so much more tomorrow, next week, next year, next century... A slight fleeting smile on the face of the last human is worth more than all the ecstasy of the previous trillions.</p>\n<p>One solution to the \"quantum measure is continually diminishing\" problem was to note that as the measure of the universe diminished, it would eventually get so low that that any alternative, non-measure diminishing theory, not matter how initially unlikely, would predominate. But that solution is not available here - indeed, that argument runs in reverse, and makes the situation worse. No matter how initially unlikely the \"quantum measure is continually increasing\" theory is, eventually, the measure will become so high that it completely dominates all other theories.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cwkgccwRDtuQFH7EY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 19, "baseScore": 2, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "24750", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JGHQPybvjLAgimXae", "cjK6CTW9DyFAFtKHp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T13:19:13.208Z", "modifiedAt": null, "url": null, "title": "To the folks who met in Princeton Saturday 11/16/2013", "slug": "to-the-folks-who-met-in-princeton-saturday-11-16-2013", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HalMorris", "createdAt": "2012-12-08T02:54:12.946Z", "isAdmin": false, "displayName": "HalMorris"}, "userId": "8cZxp4PS87vNbhmCf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H3r7jkK5gJasLC6cK/to-the-folks-who-met-in-princeton-saturday-11-16-2013", "pageUrlRelative": "/posts/H3r7jkK5gJasLC6cK/to-the-folks-who-met-in-princeton-saturday-11-16-2013", "linkUrl": "https://www.lesswrong.com/posts/H3r7jkK5gJasLC6cK/to-the-folks-who-met-in-princeton-saturday-11-16-2013", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20the%20folks%20who%20met%20in%20Princeton%20Saturday%2011%2F16%2F2013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20the%20folks%20who%20met%20in%20Princeton%20Saturday%2011%2F16%2F2013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH3r7jkK5gJasLC6cK%2Fto-the-folks-who-met-in-princeton-saturday-11-16-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20the%20folks%20who%20met%20in%20Princeton%20Saturday%2011%2F16%2F2013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH3r7jkK5gJasLC6cK%2Fto-the-folks-who-met-in-princeton-saturday-11-16-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH3r7jkK5gJasLC6cK%2Fto-the-folks-who-met-in-princeton-saturday-11-16-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 279, "htmlBody": "<p>It was nice to be with some new people, and you seem like some interesting seekers after truth.&nbsp; I'm Hal, the guy who could be grandfather to at least some of you.</p>\n<p>I have a web site at therealtruthproject.blogspot.com.</p>\n<blockquote class=\"tr_bq\"><strong><a href=\"http://truthology101.blogspot.com/p/blog-page.html\"><span style=\"text-decoration: underline;\">Looking for the best thinking about </span></a></strong><br /> \n<ul>\n<li><strong><a href=\"http://truthology101.blogspot.com/p/blog-page.html\"><span style=\"text-decoration: underline;\"> discovering,</span></a></strong></li>\n<li><strong><a href=\"http://truthology101.blogspot.com/p/blog-page.html\"><span style=\"text-decoration: underline;\">propagating,</span></a></strong></li>\n<li><strong><a href=\"http://truthology101.blogspot.com/p/blog-page.html\"><span style=\"text-decoration: underline;\"> recognizing, and</span></a></strong></li>\n<li><strong><a href=\"http://truthology101.blogspot.com/p/blog-page.html\"><span style=\"text-decoration: underline;\"> <em>living life guided by</em></span></a></strong></li>\n</ul>\n</blockquote>\n<blockquote class=\"tr_bq\"><strong><a href=\"http://truthology101.blogspot.com/p/blog-page.html\"><span style=\"text-decoration: underline;\">the truth, </span></a></strong><strong><a href=\"http://truthology101.blogspot.com/p/blog-page.html\"><span style=\"text-decoration: underline;\"><em>whatever it may turn out to be</em>, and by effective ongoing practices that reveal more of the truth as needed.</span></a></strong></blockquote>\n<blockquote class=\"tr_bq\"><br /></blockquote>\n<blockquote class=\"tr_bq\">That's an attempt to sum up what \"Practical Epistemology\" means to me.&nbsp; I approached LW because I thought I'd find some encouragement among people who seem concerned about rational thinking, albeit I don't think it's enough.</blockquote>\n<blockquote class=\"tr_bq\">It will take changing the way the world looks to the vast majority of people who never think of&nbsp; joining groups to ponder the \"methods of rationality\" to make the world more safe, rational, and happy; that's why the phrase \"Truth Project\" came into my head several years ago, even if the \"project\" is nothing more than one guy reading and reading (and learning about social epistemology, memetics, neuroscience, evolutionary psychology, chimpanzees vs bonobos, Why Nations Fail, The Idea of Justice. positive interventions in the economies of Bangladesh, etc.) and wondering how in the hell to make such a thing work.&nbsp; It may sound arrogantly audacious, or crack-pot-ish just to state it, but other people buy lottery tickets.<br /></blockquote>\n<blockquote class=\"tr_bq\"><br /></blockquote>\n<blockquote class=\"tr_bq\"><strong><span style=\"text-decoration: underline;\">If nothing else, this could be a starting point for more conversation, if not with me, with the other person who sat next to you.<br /></span></strong></blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H3r7jkK5gJasLC6cK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -4, "extendedScore": null, "score": 1.4280267687021448e-06, "legacy": true, "legacyId": "24732", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T17:50:48.269Z", "modifiedAt": null, "url": null, "title": "Skirting the mere addition paradox", "slug": "skirting-the-mere-addition-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:02.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cFHpWgk8WF7ekrMmq/skirting-the-mere-addition-paradox", "pageUrlRelative": "/posts/cFHpWgk8WF7ekrMmq/skirting-the-mere-addition-paradox", "linkUrl": "https://www.lesswrong.com/posts/cFHpWgk8WF7ekrMmq/skirting-the-mere-addition-paradox", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Skirting%20the%20mere%20addition%20paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASkirting%20the%20mere%20addition%20paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFHpWgk8WF7ekrMmq%2Fskirting-the-mere-addition-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Skirting%20the%20mere%20addition%20paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFHpWgk8WF7ekrMmq%2Fskirting-the-mere-addition-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcFHpWgk8WF7ekrMmq%2Fskirting-the-mere-addition-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p>Consider the following facts:</p>\n<ol>\n<li>For any population of people of happiness h, you can add more people of happiness less than h, and still improve things.</li>\n<li>For any population of people, you can spread people's happiness in a more egalitarian way, while keeping the same average happiness, and this makes things no worse.</li>\n</ol>\n<p>This sounds a lot like the <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">mere addition paradox</a>, illustrated by the following diagram:</p>\n<p><img src=\"http://images.lesswrong.com/t3_j3k_0.png?v=f0dfec495785baecaf62f468ef926e94\" alt=\"\" width=\"500\" height=\"200\" /></p>\n<p>This is seems to lead directly to the <a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">repugnant conclusion</a> - that there is a huge population of people who's lives are barely worth living, but that this outcome is better because of the large number of them (<a href=\"/r/discussion/lw/j0l/weak_repugnant_conclusion_need_not_be_so/\">in practice</a> this conclusion may have a little less bite than feared, at least for non-total utilitarians).</p>\n<p>But that conclusion doesn't follow at all! Consider the following aggregation formula, where au is the average utility of the population and n is the total number of people in the population:</p>\n<p style=\"padding-left: 60px;\">au(1-(1/2)<sup>n</sup>)</p>\n<p>This obeys the two properties above, and yet does not lead to a repugnant conclusion. How so? Well, property 2 is immediate - since only the average utility appears, the reallocating utility in a more egalitarian way does not decrease the aggregation. For property 1, define f(n)=1-(1/2)<sup>n</sup>. This function f is strictly increasing, so if we add more members of the population, the product goes up - this allows us to diminish the average utility slightly (by decreasing the utility of the people we've added, say), and still end up with a higher aggregation.</p>\n<p>How do we know that there is no repugnant conclusion? Well, f(n) is bounded above by 1. So let au and n be the average utility and size of a given population, and au' and n' those of a population better than this one. Hence au(f(n)) &lt; au'(f(n')) &lt; au'. So the average utility can never sink below au(f(n)): the average utility is bounded.</p>\n<p>So some weaker versions of the mere addition argument do not imply the repugnant conclusion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cFHpWgk8WF7ekrMmq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 1.4282947772249446e-06, "legacy": true, "legacyId": "24752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7mFKwWhfoL3BoB8cR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-18T22:46:44.102Z", "modifiedAt": null, "url": null, "title": "Lotteries & MWI", "slug": "lotteries-and-mwi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:39.299Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K4vsTCqDfz8JAp6yX/lotteries-and-mwi", "pageUrlRelative": "/posts/K4vsTCqDfz8JAp6yX/lotteries-and-mwi", "linkUrl": "https://www.lesswrong.com/posts/K4vsTCqDfz8JAp6yX/lotteries-and-mwi", "postedAtFormatted": "Monday, November 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lotteries%20%26%20MWI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALotteries%20%26%20MWI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4vsTCqDfz8JAp6yX%2Flotteries-and-mwi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lotteries%20%26%20MWI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4vsTCqDfz8JAp6yX%2Flotteries-and-mwi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4vsTCqDfz8JAp6yX%2Flotteries-and-mwi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<p>I haven't been able to find the source of the idea, but I've recently been reminded of:</p>\n<blockquote>\n<p>Lotteries are a way to funnel some money from many of you to a few of you.</p>\n</blockquote>\n<p>This is, of course, based on the Multiple Worlds Interpretation: if the lottery has one-in-a-million odds, then for every million timelines in which you buy a lottery ticket, in one timeline you'll win it. There's a certain amount of friction - it's not a perfect wealth transfer - based on the lottery's odds. But, looked at from this perspective, the question of \"should I buy a lottery ticket?\" seems like it might be slightly more complicated than \"it's a tax on idiots\".</p>\n<p>But I'm reminded of my current .sig: \"Then again, I could be wrong.\" And even if this is, in fact, a valid viewpoint, it brings up further questions, such as: how can the friction be minimized, and the efficiency of the transfer be maximized? Does deliberately introducing randomness at any point in the process ensure that at least some of your MWI-selves gain a benefit, as opposed to buying a ticket after the numbers have been chosen but before they've been revealed?</p>\n<p>How interesting can this idea be made to be?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K4vsTCqDfz8JAp6yX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 2, "extendedScore": null, "score": 1.4285869127247971e-06, "legacy": true, "legacyId": "24754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-19T00:34:21.950Z", "modifiedAt": null, "url": null, "title": "London LW CoZE exercise report", "slug": "london-lw-coze-exercise-report", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:06.421Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ze5Y5dfh5vmRfMknm/london-lw-coze-exercise-report", "pageUrlRelative": "/posts/ze5Y5dfh5vmRfMknm/london-lw-coze-exercise-report", "linkUrl": "https://www.lesswrong.com/posts/ze5Y5dfh5vmRfMknm/london-lw-coze-exercise-report", "postedAtFormatted": "Tuesday, November 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20LW%20CoZE%20exercise%20report&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20LW%20CoZE%20exercise%20report%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fze5Y5dfh5vmRfMknm%2Flondon-lw-coze-exercise-report%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20LW%20CoZE%20exercise%20report%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fze5Y5dfh5vmRfMknm%2Flondon-lw-coze-exercise-report", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fze5Y5dfh5vmRfMknm%2Flondon-lw-coze-exercise-report", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1239, "htmlBody": "<p>(Cross-posted to <a href=\"http://reasonableapproximation.net/2013/11/18/london-lw-coze-report.html\">my shiny new blog</a>.)</p>\n<p>Human brains are bad at evaluating consequences. Sometimes we want to do something, and logically we're pretty sure we won't die or anything, but our lizard hindbrains are screaming at us to flee. Comfort Zone Expansion (CoZE) is an exercise that <a href=\"http://appliedrationality.org\">CFAR</a> teaches to get our lizard hindbrains to accept that what we're doing is actually pretty safe.</p>\n<p>Roughly it involves two steps. One: do something that makes our lizard hindbrains get pretty antsy. Two: don't get eaten as a result.</p>\n<p>I organised a CoZE exercise for LessWrong London on Septmeber 1st. We had a total of eight participants, I think I was the only one who'd done any structured CoZE before.</p>\n<p>My plan was: we meet at 11am, have a discussion about CoZE, play some improv games to warm up, and then head to the nearby mall for 1.30 pm. In reality, the discussion started closer to 12pm, with some people showing up part way through or not until it was finished.</p>\n<p>After finishing the discussion, we didn't end up doing any improv games. We also became slightly disorganised; we agreed on a meeting time an hour and a half in the future, but then our group didn't really split up until about twenty minutes after that. I could have handled this better. We got distracted by the need for lunch, which I could have made specific plans for. (Ideally I would have started the discussion after lunch, but shops close early on Sunday.)</p>\n<h3>Report</h3>\n<p>My solo activities went considerably less well than I'd expected. My first thing was to ask a vendor in the walkway for some free chocolate, which annoyed her more than I'd expected. Maybe she gets asked that a lot? It was kind of discouraging.</p>\n<p>After that I wanted to go into a perfume shop and ask for help with scent, because I don't know anything about it. I wandered past the front a couple of times, deciding to go when the shop was nearly empty, but then when that happened I still chickened out. That, too, was kind of discouraging.</p>\n<p>Then I decided to get back in state by doing something that seemed easy: making eye contact with people and smiling. It turns out that \"making eye-contact\" is a two-player game, and nobody else was playing. After some minutes of that I just gave up for the day.</p>\n<p>In my defense: I had a cold that day and was feeling a bit shitty (while wandering near the perfume shop I got a nose-bleed, and had to divert to the bathroom temporarily), and that might have drained my energy. I did also get a few minor victories in. The most notable is that I did some pull-ups on some scaffolding outside, and someone walking past said something encouraging like \"awesome\". (I'd like to do these more often, but the first time I tried there was ick on the scaffolding. There wasn't this time, so I should collect more data.)</p>\n<p>[[I spoke with <a href=\"http://rationality.org/critch\">Critch from CFAR</a> a few days afterwards, and he gave me a new perspective: if I go in expecting people to respond well to me, then when they don't, that's going to bother me. If I go in expecting to annoy people, but remembering that annoying people, while bad, doesn't correspond to any serious consequences, then it's going to be easier to handle. For any given interaction, I should be trying to make it go well, but I should choose the interactions such that they won't all go well. (Analogy: in a game of Go, the stronger player might give a handicap to the weaker player, but once play starts they'll do their best to win.)</p>\n<p>He also gave me a potential way to avoid chickening out: if I imagine myself doing something, and then I try to do it and it turns out to be scary, then that feels like new information and a reason to actually not do it. If I imagine myself doing something, *being scared and doing it anyway*, then when it turns out to be scary, that no longer counts as an excuse. I haven't had a chance to try this yet.]]</p>\n<p>Other people had more success. We'd primed ourselves by talking about staring contests a lot previously, so a few people asked strangers for those. I think only one stranger accepted. Trying to get high-fives was also common; one person observed that he sometimes does that anyway, and has a much higher success rate than he did in the mall. One person went into a high-end lingerie store and asked what he could buy on a budget of &pound;20 (answer: nothing). And of course there were several other successes that I've forgotten. I got the impression that most people did better than me.</p>\n<p>There was interest in doing this again. At the time I was hesitant but realised that I would probably become less hesitant with time. I've now reached a point where I, too, would quite like to do it again. We haven't got any specific plans yet.</p>\n<h3>Things to take away:</h3>\n<ul>\n<li>Have some well-defined way to transition from \"about-to-start\" to \"started\".</li>\n</ul>\n<ul>\n<li>Having an audience makes some things much easier. This is potentially a way to escalate difficulty slowly.</li>\n</ul>\n<ul>\n<li>When I did CoZE at the CFAR workshop I was hanging out with someone, who encouraged me to actually do things as well as provided an audience. At the time I wondered whether the fact that we were together meant I did less, but I did far more with her than by myself.</li>\n</ul>\n<ul>\n<li>We didn't do anything beforehand to get in state, like improv games. I can't say whether this would have helped, but it seems like it might have done.</li>\n</ul>\n<ul>\n<li>We had a nonparticipant volunteer to be a meeting point if participants wanted to finish early. If she hadn't been there, I might not have quit twenty minutes early, but given that I did, it was nice to be able to hang out. This might be a good way to enable longer sessions.</li>\n</ul>\n<h3>An objection</h3>\n<p>During the discussion about ethics, someone brought up an objection, which I think cashes out as: there are good places to expand our comfort zones into, there are bad places, and there are lame places. A lot of the stuff on the recommended list of activities is lame (do we really need to be better at asking people for staring contests?), and it's not clear how much it generalises to good stuff. Under the novice-driver line of thought, bothering people is an acceptable cost of CoZE; but if we're using CoZE for lame things, the benefits become small, and maybe it's no longer worth the cost.</p>\n<p>I guess this boils down to a question of how much the lame stuff generalises. I'm optimistic; for example, it seems to me that a lot of the lame stuff is going to be overcoming a feeling of \"I don't want to bother this person\", which is also present in the good stuff, so that particular feature should generalise. (It may also be that the lame stuff is dominated by the good stuff, so there's no reason to ever practice anything lame; but that seems a sufficiently complicated hypothesis to be low prior.)</p>\n<p>(There's also a question of whether or not people are actually bothered by strangers asking for staring contests. My initial assumption was not, but after doing the exercise, I'm not sure.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ze5Y5dfh5vmRfMknm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 21, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "24755", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(Cross-posted to <a href=\"http://reasonableapproximation.net/2013/11/18/london-lw-coze-report.html\">my shiny new blog</a>.)</p>\n<p>Human brains are bad at evaluating consequences. Sometimes we want to do something, and logically we're pretty sure we won't die or anything, but our lizard hindbrains are screaming at us to flee. Comfort Zone Expansion (CoZE) is an exercise that <a href=\"http://appliedrationality.org\">CFAR</a> teaches to get our lizard hindbrains to accept that what we're doing is actually pretty safe.</p>\n<p>Roughly it involves two steps. One: do something that makes our lizard hindbrains get pretty antsy. Two: don't get eaten as a result.</p>\n<p>I organised a CoZE exercise for LessWrong London on Septmeber 1st. We had a total of eight participants, I think I was the only one who'd done any structured CoZE before.</p>\n<p>My plan was: we meet at 11am, have a discussion about CoZE, play some improv games to warm up, and then head to the nearby mall for 1.30 pm. In reality, the discussion started closer to 12pm, with some people showing up part way through or not until it was finished.</p>\n<p>After finishing the discussion, we didn't end up doing any improv games. We also became slightly disorganised; we agreed on a meeting time an hour and a half in the future, but then our group didn't really split up until about twenty minutes after that. I could have handled this better. We got distracted by the need for lunch, which I could have made specific plans for. (Ideally I would have started the discussion after lunch, but shops close early on Sunday.)</p>\n<h3 id=\"Report\">Report</h3>\n<p>My solo activities went considerably less well than I'd expected. My first thing was to ask a vendor in the walkway for some free chocolate, which annoyed her more than I'd expected. Maybe she gets asked that a lot? It was kind of discouraging.</p>\n<p>After that I wanted to go into a perfume shop and ask for help with scent, because I don't know anything about it. I wandered past the front a couple of times, deciding to go when the shop was nearly empty, but then when that happened I still chickened out. That, too, was kind of discouraging.</p>\n<p>Then I decided to get back in state by doing something that seemed easy: making eye contact with people and smiling. It turns out that \"making eye-contact\" is a two-player game, and nobody else was playing. After some minutes of that I just gave up for the day.</p>\n<p>In my defense: I had a cold that day and was feeling a bit shitty (while wandering near the perfume shop I got a nose-bleed, and had to divert to the bathroom temporarily), and that might have drained my energy. I did also get a few minor victories in. The most notable is that I did some pull-ups on some scaffolding outside, and someone walking past said something encouraging like \"awesome\". (I'd like to do these more often, but the first time I tried there was ick on the scaffolding. There wasn't this time, so I should collect more data.)</p>\n<p>[[I spoke with <a href=\"http://rationality.org/critch\">Critch from CFAR</a> a few days afterwards, and he gave me a new perspective: if I go in expecting people to respond well to me, then when they don't, that's going to bother me. If I go in expecting to annoy people, but remembering that annoying people, while bad, doesn't correspond to any serious consequences, then it's going to be easier to handle. For any given interaction, I should be trying to make it go well, but I should choose the interactions such that they won't all go well. (Analogy: in a game of Go, the stronger player might give a handicap to the weaker player, but once play starts they'll do their best to win.)</p>\n<p>He also gave me a potential way to avoid chickening out: if I imagine myself doing something, and then I try to do it and it turns out to be scary, then that feels like new information and a reason to actually not do it. If I imagine myself doing something, *being scared and doing it anyway*, then when it turns out to be scary, that no longer counts as an excuse. I haven't had a chance to try this yet.]]</p>\n<p>Other people had more success. We'd primed ourselves by talking about staring contests a lot previously, so a few people asked strangers for those. I think only one stranger accepted. Trying to get high-fives was also common; one person observed that he sometimes does that anyway, and has a much higher success rate than he did in the mall. One person went into a high-end lingerie store and asked what he could buy on a budget of \u00a320 (answer: nothing). And of course there were several other successes that I've forgotten. I got the impression that most people did better than me.</p>\n<p>There was interest in doing this again. At the time I was hesitant but realised that I would probably become less hesitant with time. I've now reached a point where I, too, would quite like to do it again. We haven't got any specific plans yet.</p>\n<h3 id=\"Things_to_take_away_\">Things to take away:</h3>\n<ul>\n<li>Have some well-defined way to transition from \"about-to-start\" to \"started\".</li>\n</ul>\n<ul>\n<li>Having an audience makes some things much easier. This is potentially a way to escalate difficulty slowly.</li>\n</ul>\n<ul>\n<li>When I did CoZE at the CFAR workshop I was hanging out with someone, who encouraged me to actually do things as well as provided an audience. At the time I wondered whether the fact that we were together meant I did less, but I did far more with her than by myself.</li>\n</ul>\n<ul>\n<li>We didn't do anything beforehand to get in state, like improv games. I can't say whether this would have helped, but it seems like it might have done.</li>\n</ul>\n<ul>\n<li>We had a nonparticipant volunteer to be a meeting point if participants wanted to finish early. If she hadn't been there, I might not have quit twenty minutes early, but given that I did, it was nice to be able to hang out. This might be a good way to enable longer sessions.</li>\n</ul>\n<h3 id=\"An_objection\">An objection</h3>\n<p>During the discussion about ethics, someone brought up an objection, which I think cashes out as: there are good places to expand our comfort zones into, there are bad places, and there are lame places. A lot of the stuff on the recommended list of activities is lame (do we really need to be better at asking people for staring contests?), and it's not clear how much it generalises to good stuff. Under the novice-driver line of thought, bothering people is an acceptable cost of CoZE; but if we're using CoZE for lame things, the benefits become small, and maybe it's no longer worth the cost.</p>\n<p>I guess this boils down to a question of how much the lame stuff generalises. I'm optimistic; for example, it seems to me that a lot of the lame stuff is going to be overcoming a feeling of \"I don't want to bother this person\", which is also present in the good stuff, so that particular feature should generalise. (It may also be that the lame stuff is dominated by the good stuff, so there's no reason to ever practice anything lame; but that seems a sufficiently complicated hypothesis to be low prior.)</p>\n<p>(There's also a question of whether or not people are actually bothered by strangers asking for staring contests. My initial assumption was not, but after doing the exercise, I'm not sure.)</p>", "sections": [{"title": "Report", "anchor": "Report", "level": 1}, {"title": "Things to take away:", "anchor": "Things_to_take_away_", "level": 1}, {"title": "An objection", "anchor": "An_objection", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-19T00:56:42.202Z", "modifiedAt": null, "url": null, "title": "CRISPR opens up new genetic engineering potential", "slug": "crispr-opens-up-new-genetic-engineering-potential", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:45.859Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jordan", "createdAt": "2009-04-01T03:52:25.470Z", "isAdmin": false, "displayName": "Jordan"}, "userId": "Za3R2v3y6Dn27G4ey", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LqHzwQah6FSMLjNzi/crispr-opens-up-new-genetic-engineering-potential", "pageUrlRelative": "/posts/LqHzwQah6FSMLjNzi/crispr-opens-up-new-genetic-engineering-potential", "linkUrl": "https://www.lesswrong.com/posts/LqHzwQah6FSMLjNzi/crispr-opens-up-new-genetic-engineering-potential", "postedAtFormatted": "Tuesday, November 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CRISPR%20opens%20up%20new%20genetic%20engineering%20potential&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACRISPR%20opens%20up%20new%20genetic%20engineering%20potential%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqHzwQah6FSMLjNzi%2Fcrispr-opens-up-new-genetic-engineering-potential%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CRISPR%20opens%20up%20new%20genetic%20engineering%20potential%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqHzwQah6FSMLjNzi%2Fcrispr-opens-up-new-genetic-engineering-potential", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqHzwQah6FSMLjNzi%2Fcrispr-opens-up-new-genetic-engineering-potential", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>I've been hearing around the news about a new genetic engineering method called CRISPR. The method can purportedly edit any gene in a human genome (or other animal or bacterium genome) with very high accuracy. The new method may remove the risks associated with gene therapy, which can introduce undesired mutations by inserting genes into the middle of an existing gene sequence.</p>\n<p>Here's a report:</p>\n<p><a href=\"http://www.independent.co.uk/voices/comment/this-is-a-triumph-of-basic-science-with-huge-implications-crispr-technique-breaks-the-mould-8925323.html\">http://www.independent.co.uk/voices/comment/this-is-a-triumph-of-basic-science-with-huge-implications-crispr-technique-breaks-the-mould-8925323.html</a></p>\n<p>Thoughts? There is already discussion about the use of CRISPR with IVF (in-vitro fertilization) for the purposes of germ-line engineering, but even without this the method may prove very efficacious for gene therapy on non-germ-line cells. What are the ramifications for human engineering? For germ-line intelligence enhancement?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LqHzwQah6FSMLjNzi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.4287152476809644e-06, "legacy": true, "legacyId": "24756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-19T01:35:53.684Z", "modifiedAt": null, "url": null, "title": "Happiness and Productivity. Living Alone. Living with Friends. Living with Family. ", "slug": "happiness-and-productivity-living-alone-living-with-friends", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:58.538Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6jLFTDPF2GEft2jJt/happiness-and-productivity-living-alone-living-with-friends", "pageUrlRelative": "/posts/6jLFTDPF2GEft2jJt/happiness-and-productivity-living-alone-living-with-friends", "linkUrl": "https://www.lesswrong.com/posts/6jLFTDPF2GEft2jJt/happiness-and-productivity-living-alone-living-with-friends", "postedAtFormatted": "Tuesday, November 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Happiness%20and%20Productivity.%20Living%20Alone.%20Living%20with%20Friends.%20Living%20with%20Family.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHappiness%20and%20Productivity.%20Living%20Alone.%20Living%20with%20Friends.%20Living%20with%20Family.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jLFTDPF2GEft2jJt%2Fhappiness-and-productivity-living-alone-living-with-friends%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Happiness%20and%20Productivity.%20Living%20Alone.%20Living%20with%20Friends.%20Living%20with%20Family.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jLFTDPF2GEft2jJt%2Fhappiness-and-productivity-living-alone-living-with-friends", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jLFTDPF2GEft2jJt%2Fhappiness-and-productivity-living-alone-living-with-friends", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p>What I want to get people to discuss here is obvious given the title. What has been their experience regarding who and specially how many people they live with, and how that impacted their motivation and happiness.&nbsp;</p>\n<p>I don't want to peruse papers on happiness and productivity, because I'm particularly interested in anecdotal tales coming from a Lesswrong sample.&nbsp;</p>\n<p>Three pieces of information seem relevant, so if that is okay with whoever comments, I'd ask people to tell us if they consider themselves introverts (recharge batteries by being alone) extroverts, or both. As well as their age and hometown.&nbsp;</p>\n<p>The reason I want to have a fuller understanding of this is that I've slowly come to have a strong belief that the main problem with people I know who are suffering, or failing to achieve their goals, is living with fewer tribal affiliates than they \"need\". And that belief could very well be false or biased.&nbsp;</p>\n<p>I'm equally interested in what people think in general about their friends' living situation: \"Most of my friends who live with friends experience such and such emotion, but the ones who live with family experience such and such problems with motivation\"<br />as I am in personal experiences. &nbsp;<br /><br />Following a suggestion about creating topics like this before, I'll put my own case in the comments.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6jLFTDPF2GEft2jJt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 27, "extendedScore": null, "score": 1.4287539504833003e-06, "legacy": true, "legacyId": "24758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-19T17:06:04.714Z", "modifiedAt": null, "url": null, "title": "Effective Altruism and Cryonics, Contest Results", "slug": "effective-altruism-and-cryonics-contest-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:32.927Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q7PFyobNPwqBsma9g/effective-altruism-and-cryonics-contest-results", "pageUrlRelative": "/posts/Q7PFyobNPwqBsma9g/effective-altruism-and-cryonics-contest-results", "linkUrl": "https://www.lesswrong.com/posts/Q7PFyobNPwqBsma9g/effective-altruism-and-cryonics-contest-results", "postedAtFormatted": "Tuesday, November 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Effective%20Altruism%20and%20Cryonics%2C%20Contest%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEffective%20Altruism%20and%20Cryonics%2C%20Contest%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7PFyobNPwqBsma9g%2Feffective-altruism-and-cryonics-contest-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Effective%20Altruism%20and%20Cryonics%2C%20Contest%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7PFyobNPwqBsma9g%2Feffective-altruism-and-cryonics-contest-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ7PFyobNPwqBsma9g%2Feffective-altruism-and-cryonics-contest-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 524, "htmlBody": "<p>Thank you to each of the five contestants who entered an essay into the <a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/\">contest</a> that was started a little over a week ago to explore cryonics as a prospective target for effective altruism. The five entries (listed chronologically) are:</p>\n<ol>\n<li><a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/a13j\">jkaufman</a></li>\n<li><a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/a1lf\">deleted</a></li>\n<li><a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/a1ms\">RomeoStevens</a><a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/a1ms\"><br /></a></li>\n<li><a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/a25j\">jaime2000</a></li>\n<li><a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/a265\">Ishaan</a></li>\n</ol>\n<p>All five of essays show evidence of much thought and hard work. Based on multiple readings of each, here are some brief impressions from each essay:</p>\n<ul>\n<li>The entry by jkaufman is intelligently written and even-handed, giving math-based arguments why cryonics might or might not be competitive with effective charities such as AMF. The bar set by such charities is very high. However, depending what utilitarian framework you use, cryonics could be competitive if the chances of it working are sufficiently good. On the other hand, when considered in person-neutral terms, advertising cryonics seems to be a better use of a given dollar than signing up, as chances are you could induce multiple others to sign up for the same money.</li>\n<li>The entry by deleted, which is presented in outline form, brings up several good points, although it does not defend all of them at length, and comes up slightly short of the 800 word target. Despite the brevity (and some spelling errors), I had a positive reaction to it personally, particularly the discussion of cryonics as a possible alternative to end-of-life intensive care. It would be interesting to see a more fleshed out version and/or multiple essays exploring the points touched on in this outline.</li>\n<li>The entry by RomeoStevens discusses, among other things, the prospect that money raised for cryonics is likely to be money that could not otherwise be raised for a beneficial purpose. Although similar in some respects to the jkaufman essay, it stresses the usefulness of self-interest in others (e.g. aging wealthy people) as a way to attempt to produce the most good. It also goes into some of the more counterintuitive points that argue for cryonics as potential EA, such as effects on low-funded/high-value research, and the altered time preference of a cryonicist.</li>\n<li>The entry by jaime2000 examines the necessary conditions for someone to be signed up for cryonics to determine which is the most efficient use of additional funding. This essay is well organized and sourced. It recommends building public awareness (for example, advertising) as the most efficient path to promoting cryonics.</li>\n<li>The entry by Ishann is an introspective look at why some utilitarians whose intuitions run contrary to cryonics (and life extension in general), as a person-neutral effective altruism target, might reconsider those intuitions when considering life extension in the absence of cognitive decline (the familiar status quo for extending life past 100).</li>\n</ul>\n<p>Each of these impresses me as incredibly valuable in its own right, for  its own reasons. I would encourage the authors to expand them into  top-level posts now that the contest is over.</p>\n<p><strong>Prize Winner:</strong> The essay that that I think best makes its points is <a href=\"/r/discussion/lw/j01/prize_essay_contest_cryonics_and_effective/a1ms\">the one by RomeoStevens</a>, which encompasses significant breadth and depth on this topic. Well done, RomeoStevens!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q7PFyobNPwqBsma9g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 22, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "24760", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bb7FiySyRgjXptAww"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-19T21:01:00.014Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, November 16-30", "slug": "group-rationality-diary-november-16-30", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:39.455Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T5kWv7rENdFuatqov/group-rationality-diary-november-16-30", "pageUrlRelative": "/posts/T5kWv7rENdFuatqov/group-rationality-diary-november-16-30", "linkUrl": "https://www.lesswrong.com/posts/T5kWv7rENdFuatqov/group-rationality-diary-november-16-30", "postedAtFormatted": "Tuesday, November 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20November%2016-30&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20November%2016-30%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5kWv7rENdFuatqov%2Fgroup-rationality-diary-november-16-30%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20November%2016-30%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5kWv7rENdFuatqov%2Fgroup-rationality-diary-november-16-30", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5kWv7rENdFuatqov%2Fgroup-rationality-diary-november-16-30", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 260, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">This is the public group instrumental rationality diary for November 16-30 (that I've now fished out of my drafts folder, *cough*.)</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">The poll earlier this month seems to be sufficiently in favor of maintaining the current schedule that extra votes are unlikely to change things much, but if you'd really like to register your opinion, you are welcome to do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ir8/group_rationality_diary_october_115_plus/9tol\">here</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Immediate past diary: &nbsp;<a href=\"/lw/iy9/group_rationality_diary_november_115/\">November 1-15</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T5kWv7rENdFuatqov", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "24717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AmfDFXFQguDF42wfm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-20T04:47:47.021Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign fun and games", "slug": "meetup-urbana-champaign-fun-and-games-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HZbygXYT2jwheJ2kT/meetup-urbana-champaign-fun-and-games-0", "pageUrlRelative": "/posts/HZbygXYT2jwheJ2kT/meetup-urbana-champaign-fun-and-games-0", "linkUrl": "https://www.lesswrong.com/posts/HZbygXYT2jwheJ2kT/meetup-urbana-champaign-fun-and-games-0", "postedAtFormatted": "Wednesday, November 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%20fun%20and%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%20fun%20and%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZbygXYT2jwheJ2kT%2Fmeetup-urbana-champaign-fun-and-games-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%20fun%20and%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZbygXYT2jwheJ2kT%2Fmeetup-urbana-champaign-fun-and-games-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZbygXYT2jwheJ2kT%2Fmeetup-urbana-champaign-fun-and-games-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ts'>Urbana-Champaign fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">412 W. Elm St, Urbana, Illinois</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at a house to copy those Washington DC people and play games. If you have never played Wits and Wagers, you should definitely come.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ts'>Urbana-Champaign fun and games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HZbygXYT2jwheJ2kT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.4303671456597214e-06, "legacy": true, "legacyId": "24778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games\">Discussion article for the meetup : <a href=\"/meetups/ts\">Urbana-Champaign fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">412 W. Elm St, Urbana, Illinois</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting at a house to copy those Washington DC people and play games. If you have never played Wits and Wagers, you should definitely come.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games1\">Discussion article for the meetup : <a href=\"/meetups/ts\">Urbana-Champaign fun and games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign fun and games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign fun and games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-20T14:13:30.087Z", "modifiedAt": null, "url": null, "title": "Q: Correlation often does imply Causation, but does not specify which kind?", "slug": "q-correlation-often-does-imply-causation-but-does-not", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:02.734Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5ooS8kCBh64dEESYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tSqtptzC2F9v996NM/q-correlation-often-does-imply-causation-but-does-not", "pageUrlRelative": "/posts/tSqtptzC2F9v996NM/q-correlation-often-does-imply-causation-but-does-not", "linkUrl": "https://www.lesswrong.com/posts/tSqtptzC2F9v996NM/q-correlation-often-does-imply-causation-but-does-not", "postedAtFormatted": "Wednesday, November 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%3A%20Correlation%20often%20does%20imply%20Causation%2C%20but%20does%20not%20specify%20which%20kind%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%3A%20Correlation%20often%20does%20imply%20Causation%2C%20but%20does%20not%20specify%20which%20kind%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSqtptzC2F9v996NM%2Fq-correlation-often-does-imply-causation-but-does-not%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%3A%20Correlation%20often%20does%20imply%20Causation%2C%20but%20does%20not%20specify%20which%20kind%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSqtptzC2F9v996NM%2Fq-correlation-often-does-imply-causation-but-does-not", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtSqtptzC2F9v996NM%2Fq-correlation-often-does-imply-causation-but-does-not", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1319, "htmlBody": "<p><strong>Hello, introduction and embarassments</strong></p>\n<p>I'd like start with by apologizing for making this thread and include an explanation why. This post is very likely to be embarassing, it's about math and I can't do math. I'm a fairly smart person and also reasonably adept at handling abstract matters, rationality and so forth. But this is from an average Joe's perspective. On this topic, probability and mathematics in general, it's hardly a merit at all. Mathematics tends to involve people who all are at least fairly smart and reasonably adept at handling abstract matters. This shouldn't be a topic for me to touch, I know there's plenty of people who are more fit to do the job. Also this website seems to have really skilled thinkers, which of most are smarter, better educated and in many ways better suited, and in my opinion some of them could be perceived as exactly the type of people who do math and science. Sorry if I'm not one of you. I hope this post will not become an embarassment for myself, I also hope it will not cause a sudden spike in shared sense of shame for you guys. If that however does occur, I hope that this will be a learning experience for me and/or others who are not adept with mathematics - When you point out my errors, that is. :)</p>\n<p>&nbsp;</p>\n<p><strong><em>Correlation does not imply causation?</em></strong></p>\n<p>I often run into this phrase.. <em>Correlation does not imply causation</em>. Something about it bugs me, intuitively. Which for this audience is like blasphemy. Intuition that is. Last time I made some comments on the subject in a whim, it was very embarassing. I got my silly comments handed right back to me. But later I thought there was something to it and so I thought.. Well I'm gonna ask you guys, since I can't figure it out.</p>\n<p>&nbsp;</p>\n<p><strong>What correlation is about?</strong></p>\n<p>Correlation is a mathematical method of observing relationships between mathematical objects. Points, statistics which can be converted to points and so forth. There at least a couple of ways of calculating these relationships, the only probability related things I know of are the Spearman correlation and Pearson correlation. Then there's also bayes which is not really the same thing.</p>\n<p>Sometimes it's blatantly obvious that a correlation can not be a coincidence. If it's blatanly obvious intuitively into such an extent that you at least think you can trust your intuition, in my book it means<em> I don't know</em> the method of clinically arriving at a rational explanation, rather than intuition being outright wrong.</p>\n<p><br /><strong>Some observations</strong></p>\n<p>If there exists a statistical relationship between two sets of data, it can be explained by:</p>\n<p>1. Coincidence.</p>\n<p>2. Causal relationship</p>\n<p>2.1 Causal relation ship, but which direction does it flow?</p>\n<p>Causal relationship is a complicated topic, because for startes even if there is a relationship between two things, you can't quite tell which way the causality flows. Does eating apples cause condition X or does condition X cause people to eat apples?</p>\n<p>2.2 Causal relationship through outside factors</p>\n<p>But causality does not end there. There maybe an additional factor A which is connected to both - eating apples and condition X.</p>\n<p>2.3 Causal relationship through distant outside factors.</p>\n<p>In fact there may be any number of additional factors. These factors can also be distant in the sense of Markov's Blanket. Just to make an example you could have factors A, B and C, which of B and C cause condition X and eating apples, but A instead causes B. B and C despite being outside factors are in close proximity to X and eating apples, but A is a distant factor, because it causes X and eating apples in directly.</p>\n<p><strong><br /></strong></p>\n<p><strong>The chief point in this post is that this type of causality is largely different than coincidence. Coincidence does not really involve such close causal relationships.</strong></p>\n<p>I believe there should be a mathematical method for calculating<em> an esimate of the probability that a correlation is coincidential</em>. Which I think can be done with standard probability calculations combined with correct observations. To make a simple example you may have an ascending tendency in the values of some dataset and that can be matched through correlation to another dataset through the same ascending tendency. If these tendencies are really simple, then the correlation is more likely to be a coincidence. But if these tendencies are complex then even with a slightly weaker correlation, following a similar pattern, the likelyhood that it's not a coindicence increases a lot, intuitively speaking.</p>\n<p>For an example we may have several series of objects which each contain 2 variables a and b. The variables can be on and off. If you examine the correlation between the on and off variables <em>in the series</em>, you might find that when variable a increases there's correlation to variable b - It also seems to increase when variable a increases examining the series of objects.</p>\n<p>To make a practical example. Your grandmother brings you a basket filled with candy. The candies may or may not have spots on them (variable a) and they may also be spherical or cylindrical (variable b). Your grandmother visits you once a week. By the end of the year you have collected several basketfuls of candy. You may find a correlation between the candies being cylindrical and having spots on them - for an example if there's a tendency for the ratio of candies with spots on them to rise and there's also an increase in cylindrical candies. This can be explained by coincidence as well as causation, and the probability for coincidence can be rather large. You could think of that in the sense of occam's razor and minimum message length. Even if there's lots of candies in the basket, the series may be very short. The length of the series may not be statistically significant. Even if the series was of stastically significant length, the rule which explains the behavior, maybe reasonably short. For an example even if you have 1000 baskets of candies, it may be that there's an average of 2% decrease in the number of candies without spots on them. And it also happens that spherical canies have an average of 2% decrease in number. If you generate a random tendency to increase or decrease from 0-20% and it's <em>maintained</em>, it can become coindicential that these two variables have the same tendency to increase or decrease.. Despite there being a long series of baskets, lots of candy and so forth.</p>\n<p>&nbsp;</p>\n<p><strong>Sometimes you can tell if Correlation does imply causality - but does not specify which kind of causality.</strong></p>\n<p>However if the candies that are cylindrical are more likely to have spots on them than the candies that are round, and there's a large number of candies in the basket. <em>This can not be explained by mere coincidence.</em> If there's 10000 candies on average in the baskets and there is a correlation between each variable <em>within objects</em>, the candies that is, then it's unlikely that it's a coindicence. It \"has\" to (is very likely to) involve causality, because you can't resort to the minimum rule generating probability or the length of the series probability described in the above example. This however is relative to the number of objects and strength of the correlation - correlation and the number of objects being completementary to each other in terms of probability.</p>\n<p>Even if \"coincidence\" is involved, it this example requires an outside factor. Which also means causality. Therefore in this example correlation does imply causality, but you can't tell which kind causality.</p>\n<p>So you can apply methods to try an estimate if there's a high/low probability for a coincidence. If coincidence is unlikely you should be able to tell that somekind of causal relationship is involved.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Can someone tell me where I went wrong?</p>\n<p><em>(I've hunch: It involves the part where I neglected my ignorance and decided to post anyway) </em></p>\n<p>Also please tell me if it's ok to - if I should - delete this. =)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tSqtptzC2F9v996NM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "24785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong id=\"Hello__introduction_and_embarassments\">Hello, introduction and embarassments</strong></p>\n<p>I'd like start with by apologizing for making this thread and include an explanation why. This post is very likely to be embarassing, it's about math and I can't do math. I'm a fairly smart person and also reasonably adept at handling abstract matters, rationality and so forth. But this is from an average Joe's perspective. On this topic, probability and mathematics in general, it's hardly a merit at all. Mathematics tends to involve people who all are at least fairly smart and reasonably adept at handling abstract matters. This shouldn't be a topic for me to touch, I know there's plenty of people who are more fit to do the job. Also this website seems to have really skilled thinkers, which of most are smarter, better educated and in many ways better suited, and in my opinion some of them could be perceived as exactly the type of people who do math and science. Sorry if I'm not one of you. I hope this post will not become an embarassment for myself, I also hope it will not cause a sudden spike in shared sense of shame for you guys. If that however does occur, I hope that this will be a learning experience for me and/or others who are not adept with mathematics - When you point out my errors, that is. :)</p>\n<p>&nbsp;</p>\n<p><strong id=\"Correlation_does_not_imply_causation_\"><em>Correlation does not imply causation?</em></strong></p>\n<p>I often run into this phrase.. <em>Correlation does not imply causation</em>. Something about it bugs me, intuitively. Which for this audience is like blasphemy. Intuition that is. Last time I made some comments on the subject in a whim, it was very embarassing. I got my silly comments handed right back to me. But later I thought there was something to it and so I thought.. Well I'm gonna ask you guys, since I can't figure it out.</p>\n<p>&nbsp;</p>\n<p><strong id=\"What_correlation_is_about_\">What correlation is about?</strong></p>\n<p>Correlation is a mathematical method of observing relationships between mathematical objects. Points, statistics which can be converted to points and so forth. There at least a couple of ways of calculating these relationships, the only probability related things I know of are the Spearman correlation and Pearson correlation. Then there's also bayes which is not really the same thing.</p>\n<p>Sometimes it's blatantly obvious that a correlation can not be a coincidence. If it's blatanly obvious intuitively into such an extent that you at least think you can trust your intuition, in my book it means<em> I don't know</em> the method of clinically arriving at a rational explanation, rather than intuition being outright wrong.</p>\n<p><br><strong>Some observations</strong></p>\n<p>If there exists a statistical relationship between two sets of data, it can be explained by:</p>\n<p>1. Coincidence.</p>\n<p>2. Causal relationship</p>\n<p>2.1 Causal relation ship, but which direction does it flow?</p>\n<p>Causal relationship is a complicated topic, because for startes even if there is a relationship between two things, you can't quite tell which way the causality flows. Does eating apples cause condition X or does condition X cause people to eat apples?</p>\n<p>2.2 Causal relationship through outside factors</p>\n<p>But causality does not end there. There maybe an additional factor A which is connected to both - eating apples and condition X.</p>\n<p>2.3 Causal relationship through distant outside factors.</p>\n<p>In fact there may be any number of additional factors. These factors can also be distant in the sense of Markov's Blanket. Just to make an example you could have factors A, B and C, which of B and C cause condition X and eating apples, but A instead causes B. B and C despite being outside factors are in close proximity to X and eating apples, but A is a distant factor, because it causes X and eating apples in directly.</p>\n<p><strong><br></strong></p>\n<p><strong id=\"The_chief_point_in_this_post_is_that_this_type_of_causality_is_largely_different_than_coincidence__Coincidence_does_not_really_involve_such_close_causal_relationships_\">The chief point in this post is that this type of causality is largely different than coincidence. Coincidence does not really involve such close causal relationships.</strong></p>\n<p>I believe there should be a mathematical method for calculating<em> an esimate of the probability that a correlation is coincidential</em>. Which I think can be done with standard probability calculations combined with correct observations. To make a simple example you may have an ascending tendency in the values of some dataset and that can be matched through correlation to another dataset through the same ascending tendency. If these tendencies are really simple, then the correlation is more likely to be a coincidence. But if these tendencies are complex then even with a slightly weaker correlation, following a similar pattern, the likelyhood that it's not a coindicence increases a lot, intuitively speaking.</p>\n<p>For an example we may have several series of objects which each contain 2 variables a and b. The variables can be on and off. If you examine the correlation between the on and off variables <em>in the series</em>, you might find that when variable a increases there's correlation to variable b - It also seems to increase when variable a increases examining the series of objects.</p>\n<p>To make a practical example. Your grandmother brings you a basket filled with candy. The candies may or may not have spots on them (variable a) and they may also be spherical or cylindrical (variable b). Your grandmother visits you once a week. By the end of the year you have collected several basketfuls of candy. You may find a correlation between the candies being cylindrical and having spots on them - for an example if there's a tendency for the ratio of candies with spots on them to rise and there's also an increase in cylindrical candies. This can be explained by coincidence as well as causation, and the probability for coincidence can be rather large. You could think of that in the sense of occam's razor and minimum message length. Even if there's lots of candies in the basket, the series may be very short. The length of the series may not be statistically significant. Even if the series was of stastically significant length, the rule which explains the behavior, maybe reasonably short. For an example even if you have 1000 baskets of candies, it may be that there's an average of 2% decrease in the number of candies without spots on them. And it also happens that spherical canies have an average of 2% decrease in number. If you generate a random tendency to increase or decrease from 0-20% and it's <em>maintained</em>, it can become coindicential that these two variables have the same tendency to increase or decrease.. Despite there being a long series of baskets, lots of candy and so forth.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Sometimes_you_can_tell_if_Correlation_does_imply_causality___but_does_not_specify_which_kind_of_causality_\">Sometimes you can tell if Correlation does imply causality - but does not specify which kind of causality.</strong></p>\n<p>However if the candies that are cylindrical are more likely to have spots on them than the candies that are round, and there's a large number of candies in the basket. <em>This can not be explained by mere coincidence.</em> If there's 10000 candies on average in the baskets and there is a correlation between each variable <em>within objects</em>, the candies that is, then it's unlikely that it's a coindicence. It \"has\" to (is very likely to) involve causality, because you can't resort to the minimum rule generating probability or the length of the series probability described in the above example. This however is relative to the number of objects and strength of the correlation - correlation and the number of objects being completementary to each other in terms of probability.</p>\n<p>Even if \"coincidence\" is involved, it this example requires an outside factor. Which also means causality. Therefore in this example correlation does imply causality, but you can't tell which kind causality.</p>\n<p>So you can apply methods to try an estimate if there's a high/low probability for a coincidence. If coincidence is unlikely you should be able to tell that somekind of causal relationship is involved.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Can someone tell me where I went wrong?</p>\n<p><em>(I've hunch: It involves the part where I neglected my ignorance and decided to post anyway) </em></p>\n<p>Also please tell me if it's ok to - if I should - delete this. =)</p>", "sections": [{"title": "Hello, introduction and embarassments", "anchor": "Hello__introduction_and_embarassments", "level": 1}, {"title": "Correlation does not imply causation?", "anchor": "Correlation_does_not_imply_causation_", "level": 1}, {"title": "What correlation is about?", "anchor": "What_correlation_is_about_", "level": 1}, {"title": "The chief point in this post is that this type of causality is largely different than coincidence. Coincidence does not really involve such close causal relationships.", "anchor": "The_chief_point_in_this_post_is_that_this_type_of_causality_is_largely_different_than_coincidence__Coincidence_does_not_really_involve_such_close_causal_relationships_", "level": 1}, {"title": "Sometimes you can tell if Correlation does imply causality - but does not specify which kind of causality.", "anchor": "Sometimes_you_can_tell_if_Correlation_does_imply_causality___but_does_not_specify_which_kind_of_causality_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-20T16:46:15.598Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Israel Meetup (Tel Aviv): Quantum Computing", "slug": "meetup-less-wrong-israel-meetup-tel-aviv-quantum-computing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:37.137Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tbKrLWCMzuTgRPi27/meetup-less-wrong-israel-meetup-tel-aviv-quantum-computing", "pageUrlRelative": "/posts/tbKrLWCMzuTgRPi27/meetup-less-wrong-israel-meetup-tel-aviv-quantum-computing", "linkUrl": "https://www.lesswrong.com/posts/tbKrLWCMzuTgRPi27/meetup-less-wrong-israel-meetup-tel-aviv-quantum-computing", "postedAtFormatted": "Wednesday, November 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Quantum%20Computing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Quantum%20Computing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbKrLWCMzuTgRPi27%2Fmeetup-less-wrong-israel-meetup-tel-aviv-quantum-computing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Quantum%20Computing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbKrLWCMzuTgRPi27%2Fmeetup-less-wrong-israel-meetup-tel-aviv-quantum-computing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbKrLWCMzuTgRPi27%2Fmeetup-less-wrong-israel-meetup-tel-aviv-quantum-computing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tt'>Less Wrong Israel Meetup (Tel Aviv): Quantum Computing</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 November 2013 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">7 begin ramat gan</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, November 28th at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.</p>\n\n<p>Our program is:</p>\n\n<p>20:00-20:15: Assembly</p>\n\n<p>20:15-21:00: Main Talk</p>\n\n<p>21:00-22:00: Dinner &amp; Discussion</p>\n\n<p>22:00-23:00: Rump Session (minitalks)</p>\n\n<p>23:00-: End of official programming</p>\n\n<p>Main Talk: Quantum Computing / Anatoly Vorobey</p>\n\n<p>We're used to living in a world where things happen in order. But in reality of quantum physics, things work a little differently. This lets us do all sorts of cool things which starting with the basics of computing theory - you'll learn about in this talk.</p>\n\n<p>Backup Talk: TBA</p>\n\n<p>Rump Session: Each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes. - Note, you don't have to prepare a talk to come! Speaking at the rump session is completely only if you want to.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tt'>Less Wrong Israel Meetup (Tel Aviv): Quantum Computing</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tbKrLWCMzuTgRPi27", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "24786", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Quantum_Computing\">Discussion article for the meetup : <a href=\"/meetups/tt\">Less Wrong Israel Meetup (Tel Aviv): Quantum Computing</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 November 2013 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">7 begin ramat gan</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, November 28th at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.</p>\n\n<p>Our program is:</p>\n\n<p>20:00-20:15: Assembly</p>\n\n<p>20:15-21:00: Main Talk</p>\n\n<p>21:00-22:00: Dinner &amp; Discussion</p>\n\n<p>22:00-23:00: Rump Session (minitalks)</p>\n\n<p>23:00-: End of official programming</p>\n\n<p>Main Talk: Quantum Computing / Anatoly Vorobey</p>\n\n<p>We're used to living in a world where things happen in order. But in reality of quantum physics, things work a little differently. This lets us do all sorts of cool things which starting with the basics of computing theory - you'll learn about in this talk.</p>\n\n<p>Backup Talk: TBA</p>\n\n<p>Rump Session: Each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes. - Note, you don't have to prepare a talk to come! Speaking at the rump session is completely only if you want to.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Quantum_Computing1\">Discussion article for the meetup : <a href=\"/meetups/tt\">Less Wrong Israel Meetup (Tel Aviv): Quantum Computing</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Israel Meetup (Tel Aviv): Quantum Computing", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Quantum_Computing", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Israel Meetup (Tel Aviv): Quantum Computing", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Quantum_Computing1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-20T19:04:36.532Z", "modifiedAt": null, "url": null, "title": "Please vote for a title for an upcoming book", "slug": "please-vote-for-a-title-for-an-upcoming-book", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "listic", "createdAt": "2009-03-11T10:06:44.719Z", "isAdmin": false, "displayName": "listic"}, "userId": "qTfheLmwMdCtMb5ZT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R2ecEunTrLxrxMjDp/please-vote-for-a-title-for-an-upcoming-book", "pageUrlRelative": "/posts/R2ecEunTrLxrxMjDp/please-vote-for-a-title-for-an-upcoming-book", "linkUrl": "https://www.lesswrong.com/posts/R2ecEunTrLxrxMjDp/please-vote-for-a-title-for-an-upcoming-book", "postedAtFormatted": "Wednesday, November 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20vote%20for%20a%20title%20for%20an%20upcoming%20book&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20vote%20for%20a%20title%20for%20an%20upcoming%20book%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR2ecEunTrLxrxMjDp%2Fplease-vote-for-a-title-for-an-upcoming-book%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20vote%20for%20a%20title%20for%20an%20upcoming%20book%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR2ecEunTrLxrxMjDp%2Fplease-vote-for-a-title-for-an-upcoming-book", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR2ecEunTrLxrxMjDp%2Fplease-vote-for-a-title-for-an-upcoming-book", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>MIRI is conducting a survey to determine the best title for an upcoming book. Please consider voting on it to help us find out which title you would like best.</p>\n<p><a href=\"http://www.surveymonkey.com/s/NBHTYVS\">Click here to take survey</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R2ecEunTrLxrxMjDp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 1.431215446507821e-06, "legacy": true, "legacyId": "24787", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-20T21:24:10.263Z", "modifiedAt": null, "url": null, "title": "Brain Preservation Foundation ask me anything on Reddit 7:00PM EST Thursday Nov 21", "slug": "brain-preservation-foundation-ask-me-anything-on-reddit-7", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:36.367Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aurellem", "createdAt": "2013-10-23T21:45:39.406Z", "isAdmin": false, "displayName": "aurellem"}, "userId": "7vWyB9qh5yx4f7sj7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wuwi98tfTuCjiZedm/brain-preservation-foundation-ask-me-anything-on-reddit-7", "pageUrlRelative": "/posts/wuwi98tfTuCjiZedm/brain-preservation-foundation-ask-me-anything-on-reddit-7", "linkUrl": "https://www.lesswrong.com/posts/wuwi98tfTuCjiZedm/brain-preservation-foundation-ask-me-anything-on-reddit-7", "postedAtFormatted": "Wednesday, November 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain%20Preservation%20Foundation%20ask%20me%20anything%20on%20Reddit%207%3A00PM%20EST%20Thursday%20Nov%2021&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain%20Preservation%20Foundation%20ask%20me%20anything%20on%20Reddit%207%3A00PM%20EST%20Thursday%20Nov%2021%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwuwi98tfTuCjiZedm%2Fbrain-preservation-foundation-ask-me-anything-on-reddit-7%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain%20Preservation%20Foundation%20ask%20me%20anything%20on%20Reddit%207%3A00PM%20EST%20Thursday%20Nov%2021%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwuwi98tfTuCjiZedm%2Fbrain-preservation-foundation-ask-me-anything-on-reddit-7", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwuwi98tfTuCjiZedm%2Fbrain-preservation-foundation-ask-me-anything-on-reddit-7", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 351, "htmlBody": "<p>AMA is here : http://www.reddit.com/r/IAmA/comments/1r6exr/i_am_kenneth_hayworth_a_phd_neuroscientist_and/</p>\n<p>&nbsp;</p>\n<p>The Brain Preservation Foundation's founder Ken Hayworth is going to be available on Reddit (http://www.reddit.com/r/IAmA) this Thursday at 7:00PM EST to answer your questions. We hope to have a very interesting discussion ranging from the technical aspects of plastination and cryopreservation to the social consequences of widespread adoption of brain preservation.<br /><br />Hayworth is a Senior Scientist at the Janelia Farm Research Campus and is an expert in state of the art brain preservation and imaging.<br /><br />From the Brain Preservation Site:</p>\n<p><strong>Kenneth Hayworth</strong>, President and Co-Founder of the Brain Preservation Foundation, is currently a <a href=\"http://www.janelia.org/people/scientist/kenneth-hayworth\">Senior Scientist </a>at the Howard Hughes Medical Institute&rsquo;s Janelia Farm Research Campus (<a href=\"http://www.janelia.org/\">JFRC</a>) in Ashburn, Virginia. JFRC is perhaps the leading research institution in the field of connectomics in the United States. At JFRC, Hayworth is currently researching ways to extend Focused Ion Beam Scanning Electron Microscopy (FIBSEM) imaging of brain tissue to encompass much larger volumes than are currently possible. For an overview of this work see <a href=\"http://dx.doi.org/10.1142/S1793843012400057\">his recent review paper</a>&nbsp;and <a href=\"http://www.youtube.com/watch?v=GFA2q-tS5hk\">online presentation</a>. Prior to moving to JFRC, Hayworth was a postdoctoral researcher at Harvard University. Hayworth is co-inventor of the Tape-to-SEM process for high-throughput volume imaging of neural circuits at the nanometer scale and he designed and built several automated machines to implement this process. Hayworth received a PhD in Neuroscience from the University of Southern California for research into how the human visual system encodes spatial relations among objects. Hayworth is a vocal advocate for brain preservation and mind uploading and a co-founder of the Brain Preservation Foundation which calls for the implementation of an emergency glutaraldehyde perfusion procedure in hospitals, and for the development of a whole brain embedding procedure which can demonstrate perfect ultrastructure preservation across an entire human brain.</p>\n<p>Links:</p>\n<p><br />- <a href=\"http://www.reddit.com/r/IAmA\">http://www.reddit.com/r/IAmA</a><br />&nbsp; Ask me anything page where the discussion will be held <br /><br />- <a href=\"http://chronicle.com/article/article-content/132819/\">http://chronicle.com/article/article-content/132819/</a>&nbsp;&nbsp;&nbsp; &nbsp;<br />&nbsp; Overview article explaining plastination and the Brain Preservation foundation<br /><br />- <a href=\"http://hplusmagazine.com/2013/05/28/neuroscience-and-the-future-of-humanity-interview-with-ken-hayworth/\">http://hplusmagazine.com/2013/05/28/neuroscience-and-the-future-of-humanity-interview-with-ken-hayworth/</a><br />&nbsp; Extensive interview with Hayworth.<br /><br />- <a href=\"http://www.brainpreservation.org/\">http://www.brainpreservation.org/</a><br />- <a href=\"http://www.janelia.org/people/scientist/kenneth-hayworth\">http://www.janelia.org/people/scientist/kenneth-hayworth</a><br />- <a href=\"http://www.brainpreservation.org/content/contact\">http://www.brainpreservation.org/content/contact</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wuwi98tfTuCjiZedm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 1.4313537045338355e-06, "legacy": true, "legacyId": "24788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-20T22:19:28.281Z", "modifiedAt": null, "url": null, "title": "Be Skeptical of Correlational Studies", "slug": "be-skeptical-of-correlational-studies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:07.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gWSRQM4vLJRatdddW/be-skeptical-of-correlational-studies", "pageUrlRelative": "/posts/gWSRQM4vLJRatdddW/be-skeptical-of-correlational-studies", "linkUrl": "https://www.lesswrong.com/posts/gWSRQM4vLJRatdddW/be-skeptical-of-correlational-studies", "postedAtFormatted": "Wednesday, November 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Be%20Skeptical%20of%20Correlational%20Studies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABe%20Skeptical%20of%20Correlational%20Studies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWSRQM4vLJRatdddW%2Fbe-skeptical-of-correlational-studies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Be%20Skeptical%20of%20Correlational%20Studies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWSRQM4vLJRatdddW%2Fbe-skeptical-of-correlational-studies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgWSRQM4vLJRatdddW%2Fbe-skeptical-of-correlational-studies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 743, "htmlBody": "<p>People kept noticing that blood donors were healthier than non-donors. Could giving blood be good for you, perhaps by removing excess iron? Perhaps medieval doctors practicing blood-letting were onto something? Running some studies (<a href=\"http://aje.oxfordjournals.org/content/148/5/445.long\">1998</a>, <a href=\"http://circ.ahajournals.org/content/103/1/52.full\">2001</a>) this does seem to be a real correlation, so you see articles like \"<a href=\"http://www.sciencedaily.com/releases/1997/09/970901072035.htm\">Men Who Donate Blood May Reduce Risk Of Heart Disease</a>.\"</p>\n<p>While this sounds good, and it's nice when helpful things turn out to be healthy, the evidence is not very strong. When you notice A and B happen together it may be that A causes B, B causes A, or some hidden C causes A and B. We may have good reasons to believe A might cause B, but it's very hard to rule out a potential C. Instead if you intentionally manipulate A and observe what happens to B then you can actually see how much of an effect A has on B.</p>\n<p>For example, people observed (<a href=\"http://www.nejm.org/doi/full/10.1056/NEJMoa013536?hits=20&amp;#t=article\">2003</a>) that infants fed soy-based formula were more likely to develop peanut allergies. So they <a href=\"http://www.jfponline.com/index.php?id=22143&amp;tx_ttnews[tt_news]=167160\">recommended</a> that \"limiting soy milk or formula in the first 2 years of life may reduce sensitization.\" Here A is soy formula, B is peanut allergy, and we do see a correlation. When intentionally varying A (<a href=\"http://www.jacionline.org/article/S0091-6749(08)00601-5/fulltext\">2008</a>, n=620), however, B stays constant, which kind of sinks the whole theory. A likely candidate for a third cause, C, was a general predisposition to allergies: those infants were more likely to react to cows-milk formula and so be given soy-based ones, and they were also more likely to react to peanuts.</p>\n<p>To take another example, based on studies (<a href=\"http://stlukesmedcollege.edu.ph/uploads/downloads/HARM-caffeine.pdf\">2000</a>, <a href=\"http://www.mombaby.org/PDF/Li%20Caffeine%20SAB%20AJOG%202008.pdf\">2008</a>, <a href=\"http://www.ejog.org/article/S0301-2115(11)00240-5/abstract\">2010</a>) finding a higher miscarriage rate among coffee drinkers pregnant women are <a href=\"http://news.bbc.co.uk/2/hi/health/7195500.stm\">advised</a> to <a href=\"http://www.nytimes.com/2000/12/20/us/study-links-use-of-caffeine-to-higher-risk-of-miscarriage.html\">cut back</a> their <a href=\"http://www.theguardian.com/society/2011/dec/01/pregnant-women-coffee-risk\">caffeine consumption</a>. But a randomized controlled trial (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1804137/\">2007</a>, n=1207) found people randomly assigned to drink regular or decaf coffee were equally likely to have miscarriages. [EDIT: jimrandomh points out that I misread the study and it didn't actually show this. Instead it was too small a study to detect an effect on miscarriage rates.] A potential third cause (<a href=\"http://rd.springer.com/article/10.1007/s10995-012-1034-7/fulltext.html\">2012</a>) here is that lack of morning sickness is associated with miscarriage (<a href=\"http://humrep.oxfordjournals.org/content/25/11/2907.full\">2010</a>) and when you're nauseated you're less likely to drink a morning coffee. This doesn't tell us the root of the problem (why would feeling less sick go along with miscarriages?) but it does tell us cutting back on caffeine is probably not helpful.</p>\n<p>Which brings us back to blood donation. What if instead of blood donation making you healthy, healthier people are more likely to donate blood? There's substantial screening involved in becoming a blood donor, plus all sorts of cultural and economic factors that could lead to people choosing to donate blood or not, and those might also be associated with health outcomes. This was noted as a potential problem <a href=\"http://www.jefftk.com/healthy-donor-effect--atsma-2011.pdf\">in 2011</a> but it's hard to test this with a full experiment because assigning people to give blood or not is difficult, you have to wait a long time, and the apparent size of the effect is small.</p>\n<p>One approach that can work in places like this is to look for a \"natural experiment,\" some way in which people might already be being divided into appropriate groups. A recent study (<a href=\"http://www.jefftk.com/iron-blod-donation-rct--germain-2013.pdf\">2013</a>, n=12,357+50,889) took advantage of the situation where screening tests sometimes give false positives that disqualify people. These are nearly random, and give us a pool of people who are very similar to blood donors but don't quite make it to giving blood. When comparing the health of these disqualified donors to actual donors the health benefits vanish, supporting the \"healthy donor hypothesis.\"</p>\n<p>This isn't to say you should never pay attention to correlations. If your tongue starts peeling after eating lots of <a href=\"http://www.jefftk.com/p/citric-acid\">citric acid</a> you should probably have less in the future, and the discovery (<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2038856/pdf/brmedj03566-0003.pdf\">1950</a>) that smoking causes lung cancer was based on an observation of correlations. Negative results are also helpful: if we don't find a correlation between hair color and musical ability then it's unlikely that one causes the other. Even in cases where correlational studies only provide weak evidence, however, they're so much easier than randomized controlled trials that we still should do them if only to find problems to look into more deeply with a more reliable method. But if you see a news report that comes down to \"we observed people with bad outcome X had feature Y in common,\" it's probably not worth trying to avoid Y.</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/p/be-skeptical-of-correlational-studies\">on my blog</a>.</em></small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gWSRQM4vLJRatdddW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "24789", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-21T12:21:23.684Z", "modifiedAt": null, "url": null, "title": "The dangers of zero and one", "slug": "the-dangers-of-zero-and-one", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:04.003Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9peaSLgk92RJrGfBP/the-dangers-of-zero-and-one", "pageUrlRelative": "/posts/9peaSLgk92RJrGfBP/the-dangers-of-zero-and-one", "linkUrl": "https://www.lesswrong.com/posts/9peaSLgk92RJrGfBP/the-dangers-of-zero-and-one", "postedAtFormatted": "Thursday, November 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20dangers%20of%20zero%20and%20one&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20dangers%20of%20zero%20and%20one%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9peaSLgk92RJrGfBP%2Fthe-dangers-of-zero-and-one%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20dangers%20of%20zero%20and%20one%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9peaSLgk92RJrGfBP%2Fthe-dangers-of-zero-and-one", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9peaSLgk92RJrGfBP%2Fthe-dangers-of-zero-and-one", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1031, "htmlBody": "<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">Eliezer wrote a <a style=\"text-decoration:none;\" href=\"/lw/mo/infinite_certainty/\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline; background-color: transparent;\">post</span></a> warning against unrealistically confident estimates, in which he argued that you can't be 99.99% sure that 53 is prime. Chris Hallquist replied with a <a style=\"text-decoration:none;\" href=\"/lw/izs/yes_virginia_you_can_be_9999_or_more_certain_that/\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline; background-color: transparent;\">post</span></a> arguing that you can.</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\"><br /></span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; line-height: 1.15; white-space: pre-wrap;\">That particular case is tricky. There have been many independent calculations of the first hundred prime numbers. 53 is a small enough number that I think someone would notice if Wikipedia included it erroneously. But can you be 99.99% confident that 1159 is a prime? You found it in one particular source. Can you trust that source? It's large enough that no one would notice if it were wrong. You could try to verify it, but if I write a Perl or C++ program, I can't even be 99.9% sure that the compiler or interpreter will interpret it correctly, let alone that the program is correct.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">Rather than argue over the number of nines to use for a specific case, I want to emphasize the the importance of not assigning things probability zero or one. Here's a real case where approximating 99.9999% confidence as 100% had disastrous consequences.<a id=\"more\"></a><br /></span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">I developed a new gene-caller for JCVI. Genes are interpreted in units of 3 DNA nucleotides called codons. A bacterial gene starts with a start codon (usually ATG, TTG, or GTG) and ends at the first stop codon (usually TAG, TGA, or TAA). Most such sequences are not genes. A gene-caller is a computer program that takes a DNA sequence and guesses which of them are genes. </span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">The first thing I tried was to create a second-order Markov model on codons, and train it on all of the large possible genes in the genome. (Long sequences without stop codons are unlikely to occur by chance and are probably genes.) That means that you set P = 1 and go down the sequence of each large possible gene, codon by codon, multiplying P by the probability of seeing each of the 64 possible codons in the third position given the codons in the first and second positions. Then I created a second Markov model from the entire genome. This took about one day to write, and plugging these two models into Bayes' law as shown below turned out to work better than all the other single-method gene-prediction algorithms developed over the past 30 years.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">But what probability should you assign to a codon sequence that you've never seen? A bacterial genome might have 4 million base pairs, about half of which are in long possible genes and will be used for training. That means your training data for one genome has about 2 million codon triplets. Surprisingly, a little less than half of all possible codon triplets do not occur at all in that data (DNA sequences are not random). What probability do you assign to an event that occurs zero times out of 2 million?</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">This came up recently in an online argument. Another person said that, if the probability that X is true is below your detection threshold or your digits of accuracy, you should assign P(X) = 0, since any other number is just made up.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">Well, I'd already empirically determined whether that was true for the gene caller. First, due to a coding error, I assigned such events P(X) = 1 / (64^3 * size of training set), which is too small by about 64^3. Next I tried P(X) = 0.5 / (size of training set), which is approximately correct. Finally I tried P(X) = 0. I tested the results on genomes where I had strong evidence for what where and were not genes.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">How well do you think each P(X) worked?</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">The two non-zero probabilities gave nearly the same results, despite differing by 6 orders of magnitude. But using P(X) = 0 caused the gene-caller to miss hundreds of genes per genome, which is a disastrous result. Why?</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">Any particular codon triplet that was never found in the training set would have a prior of less than one in 4 million. But because a large number of triplets are in genes outside the training set, that meant some of those triplets (not most, but about a thousand of them) had true priors of being found somewhere in those genes of nearly one half. (You can work it out in more detail by assuming a Zipf law distribution of priors, but I won't get into that.)</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">So some of them did occur within genes in that genome, and each time one did, its assigned probability of zero annihilated all the hundreds of other pieces of evidence for the existence of that gene, making the gene impossible to detect.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">You can think of this using logarithms. I computed</span></p>\n<blockquote>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">P(gene | sequence) = P(sequence | gene) * P(gene) / P(sequence)</span></p>\n</blockquote>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">where P(sequence) and P(sequence | gene) are computed using the two Markov models. Each of them is the product of a sequence of Markov probabilities. Ignoring P(gene), which is constant, we can compute</span></p>\n<blockquote>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">log(P(gene|sequence)) ~ log(P(sequence | gene)) - log(P(sequence)) =</span></p>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">sum (over all codon triplets in the sequence) [ log(P(codon3 | codon1, codon2, gene)) - log(P(codon3 | codon1, codon2)) ]</span></p>\n</blockquote>\n<p style=\"line-height:1.15;margin-top:0pt;margin-bottom:0pt;\" dir=\"ltr\"><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\"><br /></span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">You can think of this as adding the bits of information it would take to specify that triplet outside of a gene, and subtracting the bits of information it would take to specify that information inside a gene, leaving bits of evidence that it is in a gene.</span></p>\n<p><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">If we assign P(codon3 | codon1, codon2, gene) = 0, the number of bits of information it would take to specify \"codon3 | codon1, codon2\" inside a gene is -log(0) = infinity. Assign P(X) = 0 is claiming to have </span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:italic;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\">infinite</span><span style=\"font-size:15px;font-family:Arial;color:#000000;background-color:transparent;font-weight:normal;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre-wrap;\"> bits of information that X is false.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">Going back to the argument, the accuracy of the probabilities assigned by the Markov model are quite low, probably one to three digits of accuracy in most cases. Yet it was important to assign positive probabilities to events whose probabilities were at least seven orders of magnitude below that.</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">It didn't matter what probability I assigned to them! Given hundreds of other bits scores to add up, changing the number of bits taken away by one highly improbable event by 10 had little impact. It just matters not to make it zero.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9peaSLgk92RJrGfBP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 46, "extendedScore": null, "score": 1.4322431084492425e-06, "legacy": true, "legacyId": "24720", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ooypcn7qFzsMcy53R", "CFYBTbLciMmXdyE2f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-21T13:52:02.094Z", "modifiedAt": null, "url": null, "title": "Book Review: Computability and Logic", "slug": "book-review-computability-and-logic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:37.677Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CvhPTwSMPqNju7hhw/book-review-computability-and-logic", "pageUrlRelative": "/posts/CvhPTwSMPqNju7hhw/book-review-computability-and-logic", "linkUrl": "https://www.lesswrong.com/posts/CvhPTwSMPqNju7hhw/book-review-computability-and-logic", "postedAtFormatted": "Thursday, November 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20Computability%20and%20Logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20Computability%20and%20Logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCvhPTwSMPqNju7hhw%2Fbook-review-computability-and-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20Computability%20and%20Logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCvhPTwSMPqNju7hhw%2Fbook-review-computability-and-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCvhPTwSMPqNju7hhw%2Fbook-review-computability-and-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3850, "htmlBody": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After <a href=\"/lw/ix5/mental_context_for_model_theory/\">putting down Model Theory partway through</a> I picked up a book on logic. <em>Computability and Logic</em>, specifically.</p>\n<h1 id=\"computabilityandlogic\">Computability and Logic</h1>\n<p style=\"text-align:center\"><img src=\"http://assets.cambridge.org/97805217/01464/cover/9780521701464.jpg\" alt=\"\" width=\"180\" height=\"263\" /></p>\n<p>This book is not on the MIRI course list. It was recommended to me by <a href=\"http://wiki.lesswrong.com/wiki/Luke_Muehlhauser\">Luke</a> along with a number of other books as a potential way to learn provability logic.</p>\n<p><em>Computability and Logic</em> is a wonderful book. It's well written. It's formal, but pulls off a conversational tone. It demonstrates many difficult concepts with ease. It even feels nice &mdash; it's got thick pages, large text, and a number of useful diagrams.</p>\n<p>That said, I didn't find it very useful to me personally.</p>\n<p>This book is a <em>wonderful</em> introduction to computability, incompleteness, unsatisfiability, and related concepts. It masterfully motivates the connection between computability and logic (a subject near and dear to my heart). It could be an invaluable resource for anyone in computer science looking to branch out into logic. It starts with the basic concept of enumeration and takes you all the way through L&ouml;b's theorem: quite an impressive feat, for one textbook.</p>\n<p>For me, though, it was on the easy side. I already knew all the computability stuff quite well, and skimmed over much of it. The logic sections were a good refresher, though they were somewhat rudimentary by comparison to <em>Model Theory</em>. (Actually, this book would have been a great precursor to <em>Model Theory</em>: It spent quite a bit of time motivating and fleshing out concepts that <em>Model Theory</em> dumps on your head.)</p>\n<p>Still, while this book was not exactly what I needed, I highly recommend it for other purposes. Its contents are summarized below.</p>\n<h1 id=\"contents\">Contents</h1>\n<ol>\n<li><a href=\"#enumerability\">Enumerability</a></li>\n<li><a href=\"#diagonalization\">Diagonalization</a></li>\n<li><a href=\"#turingcomputability\">Turing Computability</a></li>\n<li><a href=\"#uncomputability\">Uncomputability</a></li>\n<li><a href=\"#abacuscomputability\">Abacus Computability</a></li>\n<li><a href=\"#recursivefunctions\">Recursive Functions</a></li>\n<li><a href=\"#recursivesetsandrelations\">Recursive Sets and Relations</a></li>\n<li><a href=\"#equivalentdefinitionsofcomputability\">Equivalent Definitions of Computability</a></li>\n<li><a href=\"#aprcisoffirstorderlogicsyntax\">A Pr&eacute;cis of First-Order Logic: Syntax</a></li>\n<li><a href=\"#aprcisoffirstorderlogicsemantics\">A Pr&eacute;cis of First-Order Logic: Semantics</a></li>\n<li><a href=\"#theundecidabilityoffirstorderlogic\">The Undecidability of First-Order Logic</a></li>\n<li><a href=\"#models\">Models</a></li>\n<li><a href=\"#theexistenceofmodels\">The Existence of Models</a></li>\n<li><a href=\"#proofsandcompleteness\">Proofs and Completeness</a></li>\n<li><a href=\"#arithmetization\">Arithmetization</a></li>\n<li><a href=\"#representabilityofrecursivefunctions\">Representability of Recursive Functions</a></li>\n<li><a href=\"#indefinabilityundecidabilityandincompleteness\">Indefinability, Undecidability, and Incompleteness</a></li>\n<li><a href=\"#theunprovabilityofconsistency\">The Unprovability of Consistency</a></li>\n<li><a href=\"#normalforms\">Normal Forms</a></li>\n<li><a href=\"#thecraiginterpolationtheorem\">The Craig Interpolation Theorem</a></li>\n<li><a href=\"#monadicanddyadicLogic\">Monadic and Dyadic Logic</a></li>\n<li><a href=\"#secondorderlogic\">Second-Order Logic</a></li>\n<li><a href=\"#arithmeticaldecidability\">Arithmetical Decidability</a></li>\n<li><a href=\"#decidabilityofarithmeticwithoutmultiplication\">Decidability of Arithmetic without Multiplication</a></li>\n<li><a href=\"#nonstandardmodels\">Nonstandard Models</a></li>\n<li><a href=\"#ramseystheorem\">Ramsey's Theorem</a></li>\n<li><a href=\"#modallogicandprovability\">Modal Logic and Provability</a></li>\n</ol>\n<div><a id=\"more\"></a></div>\n<h3 id=\"enumerability\">Enumerability</h3>\n<p>This chapter introduces enumerability, and some of the cooler results. If any of the below sounds surprising to you, you'd be well served by reading this chapter:</p>\n<p>If there's a one-to-one mapping between two sets, those sets are \"the same size\". By this measure, the following sets are the same size:</p>\n<ol>\n<li>The natural numbers 0, 1, 2, 3, &hellip; (We call any one-to-one mapping between a set and the natural numbers an \"enumeration\".)</li>\n<li>All integers: we can enumerate them 0, 1, -1, 2, -2, &hellip;</li>\n<li>All even numbers: we can enumerate them 0, 2, 4, 6, &hellip;</li>\n<li>All fractions. See if you can find your own enumeration of the fractions &mdash; It's a fun exercise, if you've never done it before.</li>\n</ol>\n<p>This leads to a discussion of the \"size\" of infinite sets, which can be somewhat surprising the first time through. Knowledge of enumerability is central to many issues in computer science, and is integral in any study of infinities. I highly recommend you familiarize yourself with these concepts at some point in your life, if only for fun.</p>\n<h3 id=\"diagonalization\">Diagonalization</h3>\n<p>When you realize how much can be enumerated (there's a one-to-one mapping between natural numbers and <em>all finite subsets of the natural numbers!</em>) it's easy to get excited, and feel like you can enumerate <em>anything</em>.</p>\n<p>But you can't.</p>\n<p>Diagonalization stops the buck. It's a tool for finding a set that defies a given encoding. Diagonalization is incredibly important if you want to play with infinities or learn about decidability. If you haven't yet seen diagonalization, you're in for a treat.</p>\n<p>I won't ruin the fun here: The second chapter of <em>Computability and Logic</em> is a clever and easy introduction to diagonalization, and I highly recommend it to newcomers. Again, if this sounds new, I highly recommend picking up <em>Computability and Logic</em>: you're in for a treat.</p>\n<h3 id=\"turingcomputability\">Turing Computability</h3>\n<p>This chapter introduces Turing Computability, which is literally the bedrock of computer science. We focus on the question of what can (in principle) be computed. For those of you who don't know, the Turing Machine is an idealized computation engine which can compute answers to a very large class of problems.</p>\n<p>If you're not familiar with the concept, this chapter is a great introduction. Even if you're vaguely familiar with the concept, but you're not quite sure how a Turing machine works (\"wait, it only has a fixed number of states?\"), then this chapter is a great way to beef up your understanding. It walks you through the implementation of a Turing machine, and shows you a number of clever algorithms by which a Turing machine computes simple algorithms. You'll find yourself actually walking through the actions of the machine in your head, which is a great way to get a feel for the idealized basics of computation.</p>\n<p>Again, if any of this sounds new to you, I highly recommend picking up this book and reading the first few chapters.</p>\n<h3 id=\"uncomputability\">Uncomputability</h3>\n<p>After seeing Turing machines, it is again easy to grow overconfident and feel like you can compute <em>anything</em>.</p>\n<p>As before, you can't.</p>\n<p>As a matter of fact, the proof that you can't compute everything with a Turing machine is <em>exactly the same</em> as the proof that you can't enumerate every set. It turns out Turing programs are enumerable (as is anything you can write out with symbols from a finite alphabet). So we can use the exact same technique &mdash; diagonalization &mdash; to construct problems that a Turing machine <em>cannot solve</em>.</p>\n<p>Again, if any of this sounds foreign (or even if you're just rusty), this chapter is a delightful introduction to uncomputability. For example, it constructs the halting problem <em>from</em> a diagonalization of an encoding of Turing machine instructions. This smoothly unifies diagonalization with the intuitive impossibility of the halting problem. The chapter even touches on the busy beaver problem. I can't do it justice in a few mere paragraphs: if any of this sounds fun, don't hesitate to pick up this book.</p>\n<h3 id=\"abacuscomputability\">Abacus Computability</h3>\n<p>This chapter introduces another formalism that has more machinery available to it than a Turing machine. If you don't already know how this story goes, I won't ruin the surprise. Again, this chapter is clever and fun to read. It has lots of diagrams and gives you a good feel for what sort of power an \"abacus machine\" (something that can do real math) has as a computational engine. If computability is a new field to you, you'll enjoy this chapter.</p>\n<h3 id=\"recursivefunctions\">Recursive Functions</h3>\n<p>We now step to the math side of things. This transition is motivated computationally: we discuss five functions that are trivial to compute, and which turn out to be quite powerful when taken together. Those functions are:</p>\n<ul>\n<li>Zero: The function that always returns 0.</li>\n<li>Successor: The function that, given n, returns n+1.</li>\n<li>Identity: Functions that return something they are given.</li>\n<li>Composition: The function that performs function composition.</li>\n</ul>\n<p>These four functions are called the \"primitive recursive\" functions, and some time is spent exploring what they can do. (Answer: quite a bit. Readers of <em>G&ouml;del, Escher, Bach</em> will recognize BlooP.) If we allow use of a fifth function:</p>\n<ul>\n<li>Minimization: Given f, finds the arguments for which f returns 0.</li>\n</ul>\n<p>we get the \"recursive\" functions. (Readers of <em>G&ouml;del, Escher, Bach</em> will recognize this as unbounded search, and thus FlooP.)</p>\n<p>These are a set of building blocks for some pretty interesting functions, and we are now firmly in math land.</p>\n<h3 id=\"recursivesetsandrelations\">Recursive Sets and Relations</h3>\n<p>The above definitions are extended to define recursive sets and relations. Generally speaking, we can define sets and relations by indicator functions which distinguish between elements that are in a set from elements that are not. Sets and relations are called \"primitive recursive\" if their indicator functions can be constructed from primitive recursive building blocks. They are \"recursive\" if their indicator functions can be constructed from recursive building blocks. They are called \"semirecursive\" if there is a recursive function which at least says \"yes\" to elements that are in the set, even if it cannot say \"no\" to elements that are not in the set.</p>\n<p>Note that the indicator function for recursive sets <em>must be total</em> &mdash; i.e., must terminate &mdash; even though not all recursive functions are total. Also, note that semirecursive sets are more commonly known as \"recursively enumerable\" sets.</p>\n<p>This may all seem a bit abstract, especially following all the computationally-motivated content above. However, these distinctions are good to know. This chapter is perhaps less fun than the others, but no less important. (It's easy to get recursive and semirecursive sets mixed up, when you're starting out.)</p>\n<h3 id=\"equivalentdefinitionsofcomputability\">Equivalent Definitions of Computability</h3>\n<p>This is where the magic happens. This chapter shows that <em>a function is recursive iff it is Turing computable.</em> Turing machines and recursive functions are computational engines of <em>precisely</em> the same power. This is one of my favorite results in mathematics, and it's a beautiful (and the original) bridge between the lands of math and computer science.</p>\n<p>This chapter actually shows you how to build a recursive function that computes the result of a Turing machine, and how to encode recursive functions as Turing-machine instructions. Even if you know the results, you may find it useful (or just <em>fun</em>) to toy with an actual bridge between the two formalisms. That's one thing I really like about this book: it doesn't just say \"and these are equivalent, because [brief proof]\". It actually shows you an encoding. It lets you see the guts of the thing.</p>\n<p>If you're casually interested in math or computer science (or if you need a brush up, or you just want a good time) then I <em>highly</em> recommend reading this book at least through chapter 8. It's good clean fun to see these things play out in front of you, instead of just hearing the results secondhand.</p>\n<h3 id=\"aprcisoffirstorderlogicsyntax\">A Pr&eacute;cis of First-Order Logic: Syntax</h3>\n<p>We now dive in to the logic side of things. This chapter introduces the syntax of first order logic. It's a pretty solid introduction, and a good way to brush up on the precise syntax if you're feeling rusty.</p>\n<h3 id=\"aprcisoffirstorderlogicsemantics\">A Pr&eacute;cis of First-Order Logic: Semantics</h3>\n<p>Here we start getting into a little bit of model theory, binding the syntax of first order logic to semantics. This chapter would have been valuable before I started reading <em>Model Theory</em>. It is a solid introduction to the concepts.</p>\n<h3 id=\"theundecidabilityoffirstorderlogic\">The Undecidability of First-Order Logic</h3>\n<p>This chapter is dedicated to encoding a Turing machine as a logical theory. The fact that this can be done is incredibly cool, to say the least.</p>\n<p>As before, the chapter provides actual examples for how to encode Turing machines as logical theories. You get to play with sentences that describe a specific Turing machine and the state of its tape. You get to see the chain of implications that represent the computation of the Turing machine. As before, this is an <em>awesome</em> way to get a hands-on feel for a theoretical result that is commonly acknowledged but seldom explored.</p>\n<p>The end result, of course, is that you can encode the halting problem as a decision problem, and therefore the decision problem (checking whether a set of sentences &Gamma; implies some sentence D) is uncomputable. However, this book doesn't just tell you that result, it <em>shows</em> it to you. You actually get to <em>build</em> a set of sentences &Gamma; encoding a Turing machine, and construct a sentence D that expresses \"this Turing machine halts\". Even if you're already familiar with the result, reading this chapter is quite a bit of fun. There's something magical about seeing the connections between computation and logic laid bare before you.</p>\n<h3 id=\"models\">Models</h3>\n<p>This chapter goes into basic model theory, discussing concepts like isomorphism, model size, and compactness. (Compactness is explained, but not proven.) This chapter would have been invaluable a month and a half ago, before I started <em>Model Theory</em>. If you're interested in model theory, this is a great place to get a feel for the ideas. It illustrates the basic concepts that I tried to cover in my <a href=\"/lw/ixn/very_basic_model_theory/\">very basic model theory</a> post, but it covers them in a much more complete and approachable manner.</p>\n<h3 id=\"theexistenceofmodels\">The Existence of Models</h3>\n<p>This chapter is entirely devoted to a proof of the compactness theorem. The chapter walks you through the whole thing and makes it easy. Again, this would have been good preliminaries for <em>Model Theory</em>, in which compactness is proved about a paragraph. I personally found this chapter a bit slow, but I imagine it would be useful to someone new to the idea of compactness.</p>\n<h3 id=\"proofsandcompleteness\">Proofs and Completeness</h3>\n<p>This chapter is about formalizing a system of proofs. It also introduces the concepts of soundness and completeness. Readers of <em>G&ouml;del, Escher, Bach</em> and others familiar with the subject matter will see where this is going, and the chapter very much feels like it's setting up all the right pieces and putting things in place.</p>\n<p>If you're new to the ideas of arithmetization and representability, then you don't want to read the next three chapters without reading this. Otherwise, it's skippable.</p>\n<h3 id=\"arithmetization\">Arithmetization</h3>\n<p>This chapter develops an arithmetization of the syntax first-order logic, and of the proof system sketched in the previous chapter. This involves developing a numeric encoding of first-order formulas, and recursive functions that can manipulate formulas so encoded. If you've never experienced such an encoding before, this chapter is a fun read: otherwise, it's skippable.</p>\n<h3 id=\"representabilityofrecursivefunctions\">Representability of Recursive Functions</h3>\n<p>This chapter shows how any recursive function can be expressed as a formula in the language of arithmetic (a theory of first-order logic). This closes our loop: all functions may be encoded as arithmetical formulas, which may themselves be encoded as numbers. This is the mechanism by which we will embed arithmetic in itself.</p>\n<p>The chapter then discusses formal systems of arithmetic. PA is mentioned, but a more minimal (and easier to reason about) arithmetic is the focus of conversation.</p>\n<h3 id=\"indefinabilityundecidabilityandincompleteness\">Indefinability, Undecidability, and Incompleteness</h3>\n<p>This is the climax of the book. The previous three chapters have been leading here. We construct a function that does diagonalization, then we represent it as a formula, and show how, given any formula <code>A(x)</code>, we can construct a sentence <code>G &harr; A('G')</code> via some clever uses of our diagonalization function and the diagonalization formulas. From there, indefinability, undecidability, and incompleteness are just a hop, skip, and jump away.</p>\n<p>Diagonalization in arithemtic is always a pleasure to run your mind over, in my experience. This is a result that's assumed in many other texts (\"and, by diagonalization, we can construct&hellip;\"), but rarely explored in full. It's quite fun to pop the thing open and see the little gears. Even if you're pretty comfortable with diagonalization, you may enjoy this chapter for its crisp treatment of the device.</p>\n<p>This chapter comes highly recommended, even if just for fun.</p>\n<h3 id=\"theunprovabilityofconsistency\">The Unprovability of Consistency</h3>\n<p>This chapter breaks down L&ouml;b's theorem and proves it in detail. If you've made it this far in the book, L&ouml;b's theorem won't even seem difficult. If you already know L&ouml;b's theorem well, this chapter is skippable. But if the theorem has always seemed somewhat confusing to you (even after the <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">cartoon guide</a>) then this chapter may well be enlightening.</p>\n<h3 id=\"normalforms\">Normal Forms</h3>\n<p>We now move to the \"further topics\" section of the book. This section was a bit faster, a bit less motivated, and more prone to dump proofs on you and say \"isn't that neat\".</p>\n<p>First, we discuss a variety of normal forms into which we can put logical sentences, each of which makes particular proofs easier. For example, you can do everything with only &exist;, &not;, and &and;. Alternatively, you can resolve to deal only with sentences where all the quantifiers come first. Or you can decide to work without function symbols, and so on.</p>\n<p>It's good to know your normal forms, this chapter is worth a read.</p>\n<h3 id=\"thecraiginterpolationtheorem\">The Craig Interpolation Theorem</h3>\n<p>Craig's interpolation theorem lets us construct a sentence that is \"between\" an implication and has some nice properties. (If A&rarr;C, then the Craig interpolation theorem lets us construct a B such that A&rarr;B, B&rarr;C, and B contains only logical symbols contained in both A and C.) This is a result used often in model theory, and a good tool to have in the toolbox. Read it if you're curious, skip it if you're not.</p>\n<h3 id=\"monadicanddyadiclogic\">Monadic and Dyadic Logic</h3>\n<p>This chapter is pretty neat. It turns out that \"monadic\" logic (first order logic in a language logic with only one-place relation symbols) is decidable. However, it <em>also</em> turns out that \"dyadic\" logic (first order logic in a language with only two-place relation symbols) is undecidable. (We can actually make that stronger: a language in first order logic with <em>exactly one</em> two-place relation symbol is undecidable.)</p>\n<p>This is interesting, because it shows us precisely where undecidability sets in. First order logic becomes undecidable when you add the first two-place relation symbol. This is a fun chapter to read if you feel like exploring the boundaries of undecidability.</p>\n<p>The latter result is proved by showing that we can put any logic into a \"normal form\" where it has at most one two-place relation symbol (and no relation symbols of higher arity). This result was surprising, and the proof is a fun one.</p>\n<h3 id=\"secondorderlogic\">Second-Order Logic</h3>\n<p>This chapter introduces second order logic, and shows some basic results (it's not compact, etc.). It's not a very in-depth introduction. If you're interested in exploring second order logic, I'd recommend finding a text that focuses on it for longer than a chapter. This chapter left me a bit unsatisfied.</p>\n<h3 id=\"arithmeticaldecidability\">Arithmetical Decidability</h3>\n<p>This discusses some results surrounding the definability of truth in arithmetic: for example, the chapter shows that for each <code>n</code> and any sane measure of complexity, the set of sentences of complexity at most <code>n</code> which are true <em>is</em> arithmetically definable.</p>\n<p>The results here were somewhat boring, in my opinion.</p>\n<h3 id=\"decidabilityofarithmeticwithoutmultiplication\">Decidability of Arithmetic without Multiplication</h3>\n<p>It turns out that arithmetic without multiplication is decidable. This is another interesting exploration of the boundaries of undecidability. The result is shown in this chapter by elimination of quantifiers: this is nice, but I feel like the chapter could have had more impact by exploring the mechanism by which multiplication brings about undecidability to arithmetic. I plan to explore this subject more in my free time.</p>\n<h3 id=\"nonstandardmodels\">Nonstandard Models</h3>\n<p>This section discusses nonstandard models of arithmetic, and the usual wacky results. If you've never played with nonstandard models before, this chapter is a nice introduction.</p>\n<h3 id=\"ramseystheorem\">Ramsey's Theorem</h3>\n<p>This chapter introduces Ramsey's theorem (a result in combinatorics) and uses it to obtain an undecidable sentence that doesn't do any overt diagonalization. This chapter is kind of fun, and it's nice to see an undecidable sentence that doesn't come from direct self-reference trickery, but you won't miss much if you skip it.</p>\n<h3 id=\"modallogicandprovability\">Modal Logic and Provability</h3>\n<p>This chapter introduces provability logic (and is the reason I picked up this textbook, initially). I was somewhat disappointed with it: it introduces modal logic well enough, but it just sort of dumps a lot of proofs and results on you. This is all well and good, but I was expecting much more detail and motivation, given the impressive motivation found elsewhere in the book. Overall, this chapter felt somewhat rushed.</p>\n<hr />\n<p>That concludes the summary. As a minor note, this textbook had way more typos than any other textbook I've read. There was one every few chapters. It was especially bad in the \"Further Topics\" section (ch. 19 and up), where there was a typo every chapter or so. However, the writing quality is otherwise top-notch, so I won't complain too loudly.</p>\n<h2 id=\"whoshouldreadthis\">Who should read this?</h2>\n<p>This book comes highly recommended, especially to people just getting interested in the fields of computability and logic. If you're interested in computation or logic, this book introduces some of the coolest results in both fields in a very approachable way.</p>\n<p>This book excels at showing you the actual mechanisms behind results that are often mentioned but seldom exposed. Even if you know that the deduction problem is equivalent to the halting problem, it's illuminating to play directly with an encoding of Turing machines as logical theories.</p>\n<p>This book is a great way to shore up your understanding of some of the most fun proofs in computability theory and in logic. It would make an excellent companion to a computer science curriculum, and a great follow up to <em>G&ouml;del, Escher, Bach</em> by someone hungry for more formalism.</p>\n<p>That said, it's not a good introduction to (modal) provability logic. Don't pick it up for that reason, no matter what Luke tells you :-)</p>\n<h2 id=\"whatshouldiread\">What should I read?</h2>\n<p>You should definitely read chapters 1-8 if you get the chance, <em>especially</em> if you're not already familiar with the bridge between Turing machines and recursive functions.</p>\n<p>Chapters 9-18 also come highly recommended if you want to really understand incompleteness and undecidability. Readers of <em>G&ouml;del, Escher, Bach</em> will find these chapters to be familiar, albeit more formal. They're a great way to brush up on your understanding of incompleteness, if you think you have to.</p>\n<p>Chapter 18 in particular is quite relevant to some of MIRI's recent work, and is good to know in the LessWrong circles.</p>\n<p>Chapters 19-27 are optional. They're less polished and less motivated, and more likely to just dump a proof on you. Read the ones that seem interesting, but don't be afraid to skip the ones that seem boring.</p>\n<h2 id=\"finalnotes\">Final notes</h2>\n<p>This book wasn't the most useful book I've read in this series. I was already quite comfortable with most of these subjects. In fact, I planned to skim the first two sections (up through chapter 18): I only slowed down and gave the book more time when it turned out to be a lot of fun.</p>\n<p>I highly recommend reading this book before <em>Model Theory</em>, if you're planning to read both. Chapters 9-13 would have made <em>Model Theory</em> quite a bit easier to tackle.</p>\n<p>This book is not on the MIRI course list, but I recommend putting it <em>on</em> the course list. The subject matter may start out a little basic for much of the audience (I expect people who approach the course list to already know enumerability, the halting problem, etc.), but chapters 9-18 are a good introduction to fields that are important to MIRI's current research.</p>\n<p>I mean, this book takes readers all the way through a proof of L&ouml;b's theorem, and it does it at an easy pace. That's no small feat. It also sets up <em>Model Theory</em> nicely and has a brief intro to modal logic. It feels a little easy, but that's not a bad thing. I think this would be a great way to get people familiar with the type of logic problems they'll have to be comfortable with if they're going to tackle the rest of the course list (and eventually MIRI research).</p>\n<p>If anyone else is thinking of reading the course list, <em>Computability and Logic</em> is a great place to start. It introduces you to all the right concepts. It may not be the fastest way to get up to speed, but it's definitely entertaining.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CvhPTwSMPqNju7hhw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 41, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "24795", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After <a href=\"/lw/ix5/mental_context_for_model_theory/\">putting down Model Theory partway through</a> I picked up a book on logic. <em>Computability and Logic</em>, specifically.</p>\n<h1 id=\"Computability_and_Logic\">Computability and Logic</h1>\n<p style=\"text-align:center\"><img src=\"http://assets.cambridge.org/97805217/01464/cover/9780521701464.jpg\" alt=\"\" width=\"180\" height=\"263\"></p>\n<p>This book is not on the MIRI course list. It was recommended to me by <a href=\"http://wiki.lesswrong.com/wiki/Luke_Muehlhauser\">Luke</a> along with a number of other books as a potential way to learn provability logic.</p>\n<p><em>Computability and Logic</em> is a wonderful book. It's well written. It's formal, but pulls off a conversational tone. It demonstrates many difficult concepts with ease. It even feels nice \u2014 it's got thick pages, large text, and a number of useful diagrams.</p>\n<p>That said, I didn't find it very useful to me personally.</p>\n<p>This book is a <em>wonderful</em> introduction to computability, incompleteness, unsatisfiability, and related concepts. It masterfully motivates the connection between computability and logic (a subject near and dear to my heart). It could be an invaluable resource for anyone in computer science looking to branch out into logic. It starts with the basic concept of enumeration and takes you all the way through L\u00f6b's theorem: quite an impressive feat, for one textbook.</p>\n<p>For me, though, it was on the easy side. I already knew all the computability stuff quite well, and skimmed over much of it. The logic sections were a good refresher, though they were somewhat rudimentary by comparison to <em>Model Theory</em>. (Actually, this book would have been a great precursor to <em>Model Theory</em>: It spent quite a bit of time motivating and fleshing out concepts that <em>Model Theory</em> dumps on your head.)</p>\n<p>Still, while this book was not exactly what I needed, I highly recommend it for other purposes. Its contents are summarized below.</p>\n<h1 id=\"Contents\">Contents</h1>\n<ol>\n<li><a href=\"#enumerability\">Enumerability</a></li>\n<li><a href=\"#diagonalization\">Diagonalization</a></li>\n<li><a href=\"#turingcomputability\">Turing Computability</a></li>\n<li><a href=\"#uncomputability\">Uncomputability</a></li>\n<li><a href=\"#abacuscomputability\">Abacus Computability</a></li>\n<li><a href=\"#recursivefunctions\">Recursive Functions</a></li>\n<li><a href=\"#recursivesetsandrelations\">Recursive Sets and Relations</a></li>\n<li><a href=\"#equivalentdefinitionsofcomputability\">Equivalent Definitions of Computability</a></li>\n<li><a href=\"#aprcisoffirstorderlogicsyntax\">A Pr\u00e9cis of First-Order Logic: Syntax</a></li>\n<li><a href=\"#aprcisoffirstorderlogicsemantics\">A Pr\u00e9cis of First-Order Logic: Semantics</a></li>\n<li><a href=\"#theundecidabilityoffirstorderlogic\">The Undecidability of First-Order Logic</a></li>\n<li><a href=\"#models\">Models</a></li>\n<li><a href=\"#theexistenceofmodels\">The Existence of Models</a></li>\n<li><a href=\"#proofsandcompleteness\">Proofs and Completeness</a></li>\n<li><a href=\"#arithmetization\">Arithmetization</a></li>\n<li><a href=\"#representabilityofrecursivefunctions\">Representability of Recursive Functions</a></li>\n<li><a href=\"#indefinabilityundecidabilityandincompleteness\">Indefinability, Undecidability, and Incompleteness</a></li>\n<li><a href=\"#theunprovabilityofconsistency\">The Unprovability of Consistency</a></li>\n<li><a href=\"#normalforms\">Normal Forms</a></li>\n<li><a href=\"#thecraiginterpolationtheorem\">The Craig Interpolation Theorem</a></li>\n<li><a href=\"#monadicanddyadicLogic\">Monadic and Dyadic Logic</a></li>\n<li><a href=\"#secondorderlogic\">Second-Order Logic</a></li>\n<li><a href=\"#arithmeticaldecidability\">Arithmetical Decidability</a></li>\n<li><a href=\"#decidabilityofarithmeticwithoutmultiplication\">Decidability of Arithmetic without Multiplication</a></li>\n<li><a href=\"#nonstandardmodels\">Nonstandard Models</a></li>\n<li><a href=\"#ramseystheorem\">Ramsey's Theorem</a></li>\n<li><a href=\"#modallogicandprovability\">Modal Logic and Provability</a></li>\n</ol>\n<div><a id=\"more\"></a></div>\n<h3 id=\"Enumerability\">Enumerability</h3>\n<p>This chapter introduces enumerability, and some of the cooler results. If any of the below sounds surprising to you, you'd be well served by reading this chapter:</p>\n<p>If there's a one-to-one mapping between two sets, those sets are \"the same size\". By this measure, the following sets are the same size:</p>\n<ol>\n<li>The natural numbers 0, 1, 2, 3, \u2026 (We call any one-to-one mapping between a set and the natural numbers an \"enumeration\".)</li>\n<li>All integers: we can enumerate them 0, 1, -1, 2, -2, \u2026</li>\n<li>All even numbers: we can enumerate them 0, 2, 4, 6, \u2026</li>\n<li>All fractions. See if you can find your own enumeration of the fractions \u2014 It's a fun exercise, if you've never done it before.</li>\n</ol>\n<p>This leads to a discussion of the \"size\" of infinite sets, which can be somewhat surprising the first time through. Knowledge of enumerability is central to many issues in computer science, and is integral in any study of infinities. I highly recommend you familiarize yourself with these concepts at some point in your life, if only for fun.</p>\n<h3 id=\"Diagonalization\">Diagonalization</h3>\n<p>When you realize how much can be enumerated (there's a one-to-one mapping between natural numbers and <em>all finite subsets of the natural numbers!</em>) it's easy to get excited, and feel like you can enumerate <em>anything</em>.</p>\n<p>But you can't.</p>\n<p>Diagonalization stops the buck. It's a tool for finding a set that defies a given encoding. Diagonalization is incredibly important if you want to play with infinities or learn about decidability. If you haven't yet seen diagonalization, you're in for a treat.</p>\n<p>I won't ruin the fun here: The second chapter of <em>Computability and Logic</em> is a clever and easy introduction to diagonalization, and I highly recommend it to newcomers. Again, if this sounds new, I highly recommend picking up <em>Computability and Logic</em>: you're in for a treat.</p>\n<h3 id=\"Turing_Computability\">Turing Computability</h3>\n<p>This chapter introduces Turing Computability, which is literally the bedrock of computer science. We focus on the question of what can (in principle) be computed. For those of you who don't know, the Turing Machine is an idealized computation engine which can compute answers to a very large class of problems.</p>\n<p>If you're not familiar with the concept, this chapter is a great introduction. Even if you're vaguely familiar with the concept, but you're not quite sure how a Turing machine works (\"wait, it only has a fixed number of states?\"), then this chapter is a great way to beef up your understanding. It walks you through the implementation of a Turing machine, and shows you a number of clever algorithms by which a Turing machine computes simple algorithms. You'll find yourself actually walking through the actions of the machine in your head, which is a great way to get a feel for the idealized basics of computation.</p>\n<p>Again, if any of this sounds new to you, I highly recommend picking up this book and reading the first few chapters.</p>\n<h3 id=\"Uncomputability\">Uncomputability</h3>\n<p>After seeing Turing machines, it is again easy to grow overconfident and feel like you can compute <em>anything</em>.</p>\n<p>As before, you can't.</p>\n<p>As a matter of fact, the proof that you can't compute everything with a Turing machine is <em>exactly the same</em> as the proof that you can't enumerate every set. It turns out Turing programs are enumerable (as is anything you can write out with symbols from a finite alphabet). So we can use the exact same technique \u2014 diagonalization \u2014 to construct problems that a Turing machine <em>cannot solve</em>.</p>\n<p>Again, if any of this sounds foreign (or even if you're just rusty), this chapter is a delightful introduction to uncomputability. For example, it constructs the halting problem <em>from</em> a diagonalization of an encoding of Turing machine instructions. This smoothly unifies diagonalization with the intuitive impossibility of the halting problem. The chapter even touches on the busy beaver problem. I can't do it justice in a few mere paragraphs: if any of this sounds fun, don't hesitate to pick up this book.</p>\n<h3 id=\"Abacus_Computability\">Abacus Computability</h3>\n<p>This chapter introduces another formalism that has more machinery available to it than a Turing machine. If you don't already know how this story goes, I won't ruin the surprise. Again, this chapter is clever and fun to read. It has lots of diagrams and gives you a good feel for what sort of power an \"abacus machine\" (something that can do real math) has as a computational engine. If computability is a new field to you, you'll enjoy this chapter.</p>\n<h3 id=\"Recursive_Functions\">Recursive Functions</h3>\n<p>We now step to the math side of things. This transition is motivated computationally: we discuss five functions that are trivial to compute, and which turn out to be quite powerful when taken together. Those functions are:</p>\n<ul>\n<li>Zero: The function that always returns 0.</li>\n<li>Successor: The function that, given n, returns n+1.</li>\n<li>Identity: Functions that return something they are given.</li>\n<li>Composition: The function that performs function composition.</li>\n</ul>\n<p>These four functions are called the \"primitive recursive\" functions, and some time is spent exploring what they can do. (Answer: quite a bit. Readers of <em>G\u00f6del, Escher, Bach</em> will recognize BlooP.) If we allow use of a fifth function:</p>\n<ul>\n<li>Minimization: Given f, finds the arguments for which f returns 0.</li>\n</ul>\n<p>we get the \"recursive\" functions. (Readers of <em>G\u00f6del, Escher, Bach</em> will recognize this as unbounded search, and thus FlooP.)</p>\n<p>These are a set of building blocks for some pretty interesting functions, and we are now firmly in math land.</p>\n<h3 id=\"Recursive_Sets_and_Relations\">Recursive Sets and Relations</h3>\n<p>The above definitions are extended to define recursive sets and relations. Generally speaking, we can define sets and relations by indicator functions which distinguish between elements that are in a set from elements that are not. Sets and relations are called \"primitive recursive\" if their indicator functions can be constructed from primitive recursive building blocks. They are \"recursive\" if their indicator functions can be constructed from recursive building blocks. They are called \"semirecursive\" if there is a recursive function which at least says \"yes\" to elements that are in the set, even if it cannot say \"no\" to elements that are not in the set.</p>\n<p>Note that the indicator function for recursive sets <em>must be total</em> \u2014 i.e., must terminate \u2014 even though not all recursive functions are total. Also, note that semirecursive sets are more commonly known as \"recursively enumerable\" sets.</p>\n<p>This may all seem a bit abstract, especially following all the computationally-motivated content above. However, these distinctions are good to know. This chapter is perhaps less fun than the others, but no less important. (It's easy to get recursive and semirecursive sets mixed up, when you're starting out.)</p>\n<h3 id=\"Equivalent_Definitions_of_Computability\">Equivalent Definitions of Computability</h3>\n<p>This is where the magic happens. This chapter shows that <em>a function is recursive iff it is Turing computable.</em> Turing machines and recursive functions are computational engines of <em>precisely</em> the same power. This is one of my favorite results in mathematics, and it's a beautiful (and the original) bridge between the lands of math and computer science.</p>\n<p>This chapter actually shows you how to build a recursive function that computes the result of a Turing machine, and how to encode recursive functions as Turing-machine instructions. Even if you know the results, you may find it useful (or just <em>fun</em>) to toy with an actual bridge between the two formalisms. That's one thing I really like about this book: it doesn't just say \"and these are equivalent, because [brief proof]\". It actually shows you an encoding. It lets you see the guts of the thing.</p>\n<p>If you're casually interested in math or computer science (or if you need a brush up, or you just want a good time) then I <em>highly</em> recommend reading this book at least through chapter 8. It's good clean fun to see these things play out in front of you, instead of just hearing the results secondhand.</p>\n<h3 id=\"A_Pr_cis_of_First_Order_Logic__Syntax\">A Pr\u00e9cis of First-Order Logic: Syntax</h3>\n<p>We now dive in to the logic side of things. This chapter introduces the syntax of first order logic. It's a pretty solid introduction, and a good way to brush up on the precise syntax if you're feeling rusty.</p>\n<h3 id=\"A_Pr_cis_of_First_Order_Logic__Semantics\">A Pr\u00e9cis of First-Order Logic: Semantics</h3>\n<p>Here we start getting into a little bit of model theory, binding the syntax of first order logic to semantics. This chapter would have been valuable before I started reading <em>Model Theory</em>. It is a solid introduction to the concepts.</p>\n<h3 id=\"The_Undecidability_of_First_Order_Logic\">The Undecidability of First-Order Logic</h3>\n<p>This chapter is dedicated to encoding a Turing machine as a logical theory. The fact that this can be done is incredibly cool, to say the least.</p>\n<p>As before, the chapter provides actual examples for how to encode Turing machines as logical theories. You get to play with sentences that describe a specific Turing machine and the state of its tape. You get to see the chain of implications that represent the computation of the Turing machine. As before, this is an <em>awesome</em> way to get a hands-on feel for a theoretical result that is commonly acknowledged but seldom explored.</p>\n<p>The end result, of course, is that you can encode the halting problem as a decision problem, and therefore the decision problem (checking whether a set of sentences \u0393 implies some sentence D) is uncomputable. However, this book doesn't just tell you that result, it <em>shows</em> it to you. You actually get to <em>build</em> a set of sentences \u0393 encoding a Turing machine, and construct a sentence D that expresses \"this Turing machine halts\". Even if you're already familiar with the result, reading this chapter is quite a bit of fun. There's something magical about seeing the connections between computation and logic laid bare before you.</p>\n<h3 id=\"Models\">Models</h3>\n<p>This chapter goes into basic model theory, discussing concepts like isomorphism, model size, and compactness. (Compactness is explained, but not proven.) This chapter would have been invaluable a month and a half ago, before I started <em>Model Theory</em>. If you're interested in model theory, this is a great place to get a feel for the ideas. It illustrates the basic concepts that I tried to cover in my <a href=\"/lw/ixn/very_basic_model_theory/\">very basic model theory</a> post, but it covers them in a much more complete and approachable manner.</p>\n<h3 id=\"The_Existence_of_Models\">The Existence of Models</h3>\n<p>This chapter is entirely devoted to a proof of the compactness theorem. The chapter walks you through the whole thing and makes it easy. Again, this would have been good preliminaries for <em>Model Theory</em>, in which compactness is proved about a paragraph. I personally found this chapter a bit slow, but I imagine it would be useful to someone new to the idea of compactness.</p>\n<h3 id=\"Proofs_and_Completeness\">Proofs and Completeness</h3>\n<p>This chapter is about formalizing a system of proofs. It also introduces the concepts of soundness and completeness. Readers of <em>G\u00f6del, Escher, Bach</em> and others familiar with the subject matter will see where this is going, and the chapter very much feels like it's setting up all the right pieces and putting things in place.</p>\n<p>If you're new to the ideas of arithmetization and representability, then you don't want to read the next three chapters without reading this. Otherwise, it's skippable.</p>\n<h3 id=\"Arithmetization\">Arithmetization</h3>\n<p>This chapter develops an arithmetization of the syntax first-order logic, and of the proof system sketched in the previous chapter. This involves developing a numeric encoding of first-order formulas, and recursive functions that can manipulate formulas so encoded. If you've never experienced such an encoding before, this chapter is a fun read: otherwise, it's skippable.</p>\n<h3 id=\"Representability_of_Recursive_Functions\">Representability of Recursive Functions</h3>\n<p>This chapter shows how any recursive function can be expressed as a formula in the language of arithmetic (a theory of first-order logic). This closes our loop: all functions may be encoded as arithmetical formulas, which may themselves be encoded as numbers. This is the mechanism by which we will embed arithmetic in itself.</p>\n<p>The chapter then discusses formal systems of arithmetic. PA is mentioned, but a more minimal (and easier to reason about) arithmetic is the focus of conversation.</p>\n<h3 id=\"Indefinability__Undecidability__and_Incompleteness\">Indefinability, Undecidability, and Incompleteness</h3>\n<p>This is the climax of the book. The previous three chapters have been leading here. We construct a function that does diagonalization, then we represent it as a formula, and show how, given any formula <code>A(x)</code>, we can construct a sentence <code>G \u2194 A('G')</code> via some clever uses of our diagonalization function and the diagonalization formulas. From there, indefinability, undecidability, and incompleteness are just a hop, skip, and jump away.</p>\n<p>Diagonalization in arithemtic is always a pleasure to run your mind over, in my experience. This is a result that's assumed in many other texts (\"and, by diagonalization, we can construct\u2026\"), but rarely explored in full. It's quite fun to pop the thing open and see the little gears. Even if you're pretty comfortable with diagonalization, you may enjoy this chapter for its crisp treatment of the device.</p>\n<p>This chapter comes highly recommended, even if just for fun.</p>\n<h3 id=\"The_Unprovability_of_Consistency\">The Unprovability of Consistency</h3>\n<p>This chapter breaks down L\u00f6b's theorem and proves it in detail. If you've made it this far in the book, L\u00f6b's theorem won't even seem difficult. If you already know L\u00f6b's theorem well, this chapter is skippable. But if the theorem has always seemed somewhat confusing to you (even after the <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">cartoon guide</a>) then this chapter may well be enlightening.</p>\n<h3 id=\"Normal_Forms\">Normal Forms</h3>\n<p>We now move to the \"further topics\" section of the book. This section was a bit faster, a bit less motivated, and more prone to dump proofs on you and say \"isn't that neat\".</p>\n<p>First, we discuss a variety of normal forms into which we can put logical sentences, each of which makes particular proofs easier. For example, you can do everything with only \u2203, \u00ac, and \u2227. Alternatively, you can resolve to deal only with sentences where all the quantifiers come first. Or you can decide to work without function symbols, and so on.</p>\n<p>It's good to know your normal forms, this chapter is worth a read.</p>\n<h3 id=\"The_Craig_Interpolation_Theorem\">The Craig Interpolation Theorem</h3>\n<p>Craig's interpolation theorem lets us construct a sentence that is \"between\" an implication and has some nice properties. (If A\u2192C, then the Craig interpolation theorem lets us construct a B such that A\u2192B, B\u2192C, and B contains only logical symbols contained in both A and C.) This is a result used often in model theory, and a good tool to have in the toolbox. Read it if you're curious, skip it if you're not.</p>\n<h3 id=\"Monadic_and_Dyadic_Logic\">Monadic and Dyadic Logic</h3>\n<p>This chapter is pretty neat. It turns out that \"monadic\" logic (first order logic in a language logic with only one-place relation symbols) is decidable. However, it <em>also</em> turns out that \"dyadic\" logic (first order logic in a language with only two-place relation symbols) is undecidable. (We can actually make that stronger: a language in first order logic with <em>exactly one</em> two-place relation symbol is undecidable.)</p>\n<p>This is interesting, because it shows us precisely where undecidability sets in. First order logic becomes undecidable when you add the first two-place relation symbol. This is a fun chapter to read if you feel like exploring the boundaries of undecidability.</p>\n<p>The latter result is proved by showing that we can put any logic into a \"normal form\" where it has at most one two-place relation symbol (and no relation symbols of higher arity). This result was surprising, and the proof is a fun one.</p>\n<h3 id=\"Second_Order_Logic\">Second-Order Logic</h3>\n<p>This chapter introduces second order logic, and shows some basic results (it's not compact, etc.). It's not a very in-depth introduction. If you're interested in exploring second order logic, I'd recommend finding a text that focuses on it for longer than a chapter. This chapter left me a bit unsatisfied.</p>\n<h3 id=\"Arithmetical_Decidability\">Arithmetical Decidability</h3>\n<p>This discusses some results surrounding the definability of truth in arithmetic: for example, the chapter shows that for each <code>n</code> and any sane measure of complexity, the set of sentences of complexity at most <code>n</code> which are true <em>is</em> arithmetically definable.</p>\n<p>The results here were somewhat boring, in my opinion.</p>\n<h3 id=\"Decidability_of_Arithmetic_without_Multiplication\">Decidability of Arithmetic without Multiplication</h3>\n<p>It turns out that arithmetic without multiplication is decidable. This is another interesting exploration of the boundaries of undecidability. The result is shown in this chapter by elimination of quantifiers: this is nice, but I feel like the chapter could have had more impact by exploring the mechanism by which multiplication brings about undecidability to arithmetic. I plan to explore this subject more in my free time.</p>\n<h3 id=\"Nonstandard_Models\">Nonstandard Models</h3>\n<p>This section discusses nonstandard models of arithmetic, and the usual wacky results. If you've never played with nonstandard models before, this chapter is a nice introduction.</p>\n<h3 id=\"Ramsey_s_Theorem\">Ramsey's Theorem</h3>\n<p>This chapter introduces Ramsey's theorem (a result in combinatorics) and uses it to obtain an undecidable sentence that doesn't do any overt diagonalization. This chapter is kind of fun, and it's nice to see an undecidable sentence that doesn't come from direct self-reference trickery, but you won't miss much if you skip it.</p>\n<h3 id=\"Modal_Logic_and_Provability\">Modal Logic and Provability</h3>\n<p>This chapter introduces provability logic (and is the reason I picked up this textbook, initially). I was somewhat disappointed with it: it introduces modal logic well enough, but it just sort of dumps a lot of proofs and results on you. This is all well and good, but I was expecting much more detail and motivation, given the impressive motivation found elsewhere in the book. Overall, this chapter felt somewhat rushed.</p>\n<hr>\n<p>That concludes the summary. As a minor note, this textbook had way more typos than any other textbook I've read. There was one every few chapters. It was especially bad in the \"Further Topics\" section (ch. 19 and up), where there was a typo every chapter or so. However, the writing quality is otherwise top-notch, so I won't complain too loudly.</p>\n<h2 id=\"Who_should_read_this_\">Who should read this?</h2>\n<p>This book comes highly recommended, especially to people just getting interested in the fields of computability and logic. If you're interested in computation or logic, this book introduces some of the coolest results in both fields in a very approachable way.</p>\n<p>This book excels at showing you the actual mechanisms behind results that are often mentioned but seldom exposed. Even if you know that the deduction problem is equivalent to the halting problem, it's illuminating to play directly with an encoding of Turing machines as logical theories.</p>\n<p>This book is a great way to shore up your understanding of some of the most fun proofs in computability theory and in logic. It would make an excellent companion to a computer science curriculum, and a great follow up to <em>G\u00f6del, Escher, Bach</em> by someone hungry for more formalism.</p>\n<p>That said, it's not a good introduction to (modal) provability logic. Don't pick it up for that reason, no matter what Luke tells you :-)</p>\n<h2 id=\"What_should_I_read_\">What should I read?</h2>\n<p>You should definitely read chapters 1-8 if you get the chance, <em>especially</em> if you're not already familiar with the bridge between Turing machines and recursive functions.</p>\n<p>Chapters 9-18 also come highly recommended if you want to really understand incompleteness and undecidability. Readers of <em>G\u00f6del, Escher, Bach</em> will find these chapters to be familiar, albeit more formal. They're a great way to brush up on your understanding of incompleteness, if you think you have to.</p>\n<p>Chapter 18 in particular is quite relevant to some of MIRI's recent work, and is good to know in the LessWrong circles.</p>\n<p>Chapters 19-27 are optional. They're less polished and less motivated, and more likely to just dump a proof on you. Read the ones that seem interesting, but don't be afraid to skip the ones that seem boring.</p>\n<h2 id=\"Final_notes\">Final notes</h2>\n<p>This book wasn't the most useful book I've read in this series. I was already quite comfortable with most of these subjects. In fact, I planned to skim the first two sections (up through chapter 18): I only slowed down and gave the book more time when it turned out to be a lot of fun.</p>\n<p>I highly recommend reading this book before <em>Model Theory</em>, if you're planning to read both. Chapters 9-13 would have made <em>Model Theory</em> quite a bit easier to tackle.</p>\n<p>This book is not on the MIRI course list, but I recommend putting it <em>on</em> the course list. The subject matter may start out a little basic for much of the audience (I expect people who approach the course list to already know enumerability, the halting problem, etc.), but chapters 9-18 are a good introduction to fields that are important to MIRI's current research.</p>\n<p>I mean, this book takes readers all the way through a proof of L\u00f6b's theorem, and it does it at an easy pace. That's no small feat. It also sets up <em>Model Theory</em> nicely and has a brief intro to modal logic. It feels a little easy, but that's not a bad thing. I think this would be a great way to get people familiar with the type of logic problems they'll have to be comfortable with if they're going to tackle the rest of the course list (and eventually MIRI research).</p>\n<p>If anyone else is thinking of reading the course list, <em>Computability and Logic</em> is a great place to start. It introduces you to all the right concepts. It may not be the fastest way to get up to speed, but it's definitely entertaining.</p>", "sections": [{"title": "Computability and Logic", "anchor": "Computability_and_Logic", "level": 1}, {"title": "Contents", "anchor": "Contents", "level": 1}, {"title": "Enumerability", "anchor": "Enumerability", "level": 3}, {"title": "Diagonalization", "anchor": "Diagonalization", "level": 3}, {"title": "Turing Computability", "anchor": "Turing_Computability", "level": 3}, {"title": "Uncomputability", "anchor": "Uncomputability", "level": 3}, {"title": "Abacus Computability", "anchor": "Abacus_Computability", "level": 3}, {"title": "Recursive Functions", "anchor": "Recursive_Functions", "level": 3}, {"title": "Recursive Sets and Relations", "anchor": "Recursive_Sets_and_Relations", "level": 3}, {"title": "Equivalent Definitions of Computability", "anchor": "Equivalent_Definitions_of_Computability", "level": 3}, {"title": "A Pr\u00e9cis of First-Order Logic: Syntax", "anchor": "A_Pr_cis_of_First_Order_Logic__Syntax", "level": 3}, {"title": "A Pr\u00e9cis of First-Order Logic: Semantics", "anchor": "A_Pr_cis_of_First_Order_Logic__Semantics", "level": 3}, {"title": "The Undecidability of First-Order Logic", "anchor": "The_Undecidability_of_First_Order_Logic", "level": 3}, {"title": "Models", "anchor": "Models", "level": 3}, {"title": "The Existence of Models", "anchor": "The_Existence_of_Models", "level": 3}, {"title": "Proofs and Completeness", "anchor": "Proofs_and_Completeness", "level": 3}, {"title": "Arithmetization", "anchor": "Arithmetization", "level": 3}, {"title": "Representability of Recursive Functions", "anchor": "Representability_of_Recursive_Functions", "level": 3}, {"title": "Indefinability, Undecidability, and Incompleteness", "anchor": "Indefinability__Undecidability__and_Incompleteness", "level": 3}, {"title": "The Unprovability of Consistency", "anchor": "The_Unprovability_of_Consistency", "level": 3}, {"title": "Normal Forms", "anchor": "Normal_Forms", "level": 3}, {"title": "The Craig Interpolation Theorem", "anchor": "The_Craig_Interpolation_Theorem", "level": 3}, {"title": "Monadic and Dyadic Logic", "anchor": "Monadic_and_Dyadic_Logic", "level": 3}, {"title": "Second-Order Logic", "anchor": "Second_Order_Logic", "level": 3}, {"title": "Arithmetical Decidability", "anchor": "Arithmetical_Decidability", "level": 3}, {"title": "Decidability of Arithmetic without Multiplication", "anchor": "Decidability_of_Arithmetic_without_Multiplication", "level": 3}, {"title": "Nonstandard Models", "anchor": "Nonstandard_Models", "level": 3}, {"title": "Ramsey's Theorem", "anchor": "Ramsey_s_Theorem", "level": 3}, {"title": "Modal Logic and Provability", "anchor": "Modal_Logic_and_Provability", "level": 3}, {"title": "Who should read this?", "anchor": "Who_should_read_this_", "level": 2}, {"title": "What should I read?", "anchor": "What_should_I_read_", "level": 2}, {"title": "Final notes", "anchor": "Final_notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 34}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MG8Yhsxqu9JY4xRPr", "F6BrJFkqEhh22rFsZ", "ALCnqX6Xx8bpFMZq3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-21T19:33:18.300Z", "modifiedAt": null, "url": null, "title": "Meetup : Saskatoon - Gauging the strength of evidence and Bayesian reasoning.", "slug": "meetup-saskatoon-gauging-the-strength-of-evidence-and", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nicholas_Rutherford", "createdAt": "2013-08-01T02:29:11.736Z", "isAdmin": false, "displayName": "Nicholas_Rutherford"}, "userId": "nucgkHPJBwJuK8sY7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rke9P6B6YhkGodJsD/meetup-saskatoon-gauging-the-strength-of-evidence-and", "pageUrlRelative": "/posts/Rke9P6B6YhkGodJsD/meetup-saskatoon-gauging-the-strength-of-evidence-and", "linkUrl": "https://www.lesswrong.com/posts/Rke9P6B6YhkGodJsD/meetup-saskatoon-gauging-the-strength-of-evidence-and", "postedAtFormatted": "Thursday, November 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saskatoon%20-%20Gauging%20the%20strength%20of%20evidence%20and%20Bayesian%20reasoning.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saskatoon%20-%20Gauging%20the%20strength%20of%20evidence%20and%20Bayesian%20reasoning.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRke9P6B6YhkGodJsD%2Fmeetup-saskatoon-gauging-the-strength-of-evidence-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saskatoon%20-%20Gauging%20the%20strength%20of%20evidence%20and%20Bayesian%20reasoning.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRke9P6B6YhkGodJsD%2Fmeetup-saskatoon-gauging-the-strength-of-evidence-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRke9P6B6YhkGodJsD%2Fmeetup-saskatoon-gauging-the-strength-of-evidence-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tu'>Saskatoon - Gauging the strength of evidence and Bayesian reasoning.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 November 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another Saskatoon meetup at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>A lot of the time beliefs are thought to be either true or false [1]. However this is rarely the case in practice! Instead of assigning beliefs a boolean value of true or false we can think of beliefs on a scale which goes from  true to false and all possible values in between.  This week we'll go over how to use this idea of continuous beliefs along with Bayesian reasoning to 'update' our beliefs when new evidence presents itself.</p>\n\n<p>More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p>\n\n<p>[1] Hopefully this statement made your \u201cfalse dichotomy detector\u201d start ringing!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tu'>Saskatoon - Gauging the strength of evidence and Bayesian reasoning.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rke9P6B6YhkGodJsD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4326716031828408e-06, "legacy": true, "legacyId": "24799", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saskatoon___Gauging_the_strength_of_evidence_and_Bayesian_reasoning_\">Discussion article for the meetup : <a href=\"/meetups/tu\">Saskatoon - Gauging the strength of evidence and Bayesian reasoning.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 November 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another Saskatoon meetup at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>A lot of the time beliefs are thought to be either true or false [1]. However this is rarely the case in practice! Instead of assigning beliefs a boolean value of true or false we can think of beliefs on a scale which goes from  true to false and all possible values in between.  This week we'll go over how to use this idea of continuous beliefs along with Bayesian reasoning to 'update' our beliefs when new evidence presents itself.</p>\n\n<p>More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p>\n\n<p>[1] Hopefully this statement made your \u201cfalse dichotomy detector\u201d start ringing!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saskatoon___Gauging_the_strength_of_evidence_and_Bayesian_reasoning_1\">Discussion article for the meetup : <a href=\"/meetups/tu\">Saskatoon - Gauging the strength of evidence and Bayesian reasoning.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saskatoon - Gauging the strength of evidence and Bayesian reasoning.", "anchor": "Discussion_article_for_the_meetup___Saskatoon___Gauging_the_strength_of_evidence_and_Bayesian_reasoning_", "level": 1}, {"title": "Discussion article for the meetup : Saskatoon - Gauging the strength of evidence and Bayesian reasoning.", "anchor": "Discussion_article_for_the_meetup___Saskatoon___Gauging_the_strength_of_evidence_and_Bayesian_reasoning_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-22T09:26:38.606Z", "modifiedAt": null, "url": null, "title": "2013 Less Wrong Census/Survey", "slug": "2013-less-wrong-census-survey", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:37.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Sx26Aj3xuMzmnKE4A/2013-less-wrong-census-survey", "pageUrlRelative": "/posts/Sx26Aj3xuMzmnKE4A/2013-less-wrong-census-survey", "linkUrl": "https://www.lesswrong.com/posts/Sx26Aj3xuMzmnKE4A/2013-less-wrong-census-survey", "postedAtFormatted": "Friday, November 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202013%20Less%20Wrong%20Census%2FSurvey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2013%20Less%20Wrong%20Census%2FSurvey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSx26Aj3xuMzmnKE4A%2F2013-less-wrong-census-survey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2013%20Less%20Wrong%20Census%2FSurvey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSx26Aj3xuMzmnKE4A%2F2013-less-wrong-census-survey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSx26Aj3xuMzmnKE4A%2F2013-less-wrong-census-survey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 466, "htmlBody": "<p>It's that time of year again.<br /><br />If you are reading this post, and have not been sent here by some sort of conspiracy trying to throw off the survey results, then you are the target population for the Less Wrong Census/Survey. Please take it. Doesn't matter if you don't post much. Doesn't matter if you're a lurker. Take the survey.<br /><br />This year's census contains a \"main survey\" that should take about ten or fifteen minutes, as well as a bunch of \"extra credit questions\". You may do the extra credit questions if you want. You may skip all the extra credit questions if you want. They're pretty long and not all of them are very interesting. But it is very important that you not put off doing the survey or not do the survey at all because you're intimidated by the extra credit questions.<br /><br />It also contains a chance at winning a MONETARY REWARD at the bottom. You do not need to fill in all the extra credit questions to get the MONETARY REWARD, just make an honest stab at as much of the survey as you can.<br /><br />Please make things easier for my computer and by extension me by reading all the instructions and by answering any text questions in the simplest and most obvious possible way. For example, if it asks you \"What language do you speak?\" please answer \"English\" instead of \"I speak English\" or \"It's English\" or \"English since I live in Canada\" or \"English (US)\" or anything else. This will help me sort responses quickly and easily. Likewise, if a question asks for a number, please answer with a number such as \"4\", rather than \"four\".<br /><br />Last year there was some concern that the survey period was too short, or too uncertain. This year the survey will remain open until 23:59 PST December 31st 2013, so as long as you make time to take it sometime this year, you should be fine. Many people put it off last year and then forgot about it, so why not take it right now while you are reading this post?<br /><br />Okay! Enough preliminaries! Time to take the...</p>\n<p>***</p>\n<p><strong><a href=\"https://docs.google.com/spreadsheet/viewform?usp=drive_web&amp;formkey=dGZ6a1NfZ0V1SV9xdE1ma0pUMTc1S1E6MA#gid=0\">2013 Less Wrong Census/Survey</a></strong></p>\n<p>***</p>\n<p>Thanks to everyone who suggested questions and ideas for the 2013 Less Wrong Census/Survey. I regret I was unable to take all of your suggestions into account, because of some limitations in Google Docs, concern about survey length, and contradictions/duplications among suggestions. I think I got <em>most</em> of them in, and others can wait until next year.<br /><br />By ancient tradition, if you take the survey you may comment saying you have done so here, and people will upvote you and you will get karma.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Sx26Aj3xuMzmnKE4A", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 80, "baseScore": 112, "extendedScore": null, "score": 0.00031048513037017374, "legacy": true, "legacyId": "24802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": true, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 80, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 620, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-22T10:14:00.678Z", "modifiedAt": null, "url": null, "title": "Meetup : Mumbai Meetup", "slug": "meetup-mumbai-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:06.951Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ronak", "createdAt": "2013-03-23T22:41:23.512Z", "isAdmin": false, "displayName": "Ronak"}, "userId": "8R9zYcWsx7XFXMEuS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fysfCSYsMWDRujZhz/meetup-mumbai-meetup", "pageUrlRelative": "/posts/fysfCSYsMWDRujZhz/meetup-mumbai-meetup", "linkUrl": "https://www.lesswrong.com/posts/fysfCSYsMWDRujZhz/meetup-mumbai-meetup", "postedAtFormatted": "Friday, November 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Mumbai%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Mumbai%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfysfCSYsMWDRujZhz%2Fmeetup-mumbai-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Mumbai%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfysfCSYsMWDRujZhz%2Fmeetup-mumbai-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfysfCSYsMWDRujZhz%2Fmeetup-mumbai-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tv'>Mumbai Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 December 2013 03:00:00PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gloria Jeans Coffee, S P Mukherji Chowk, Colaba, Mumbai</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I haven't actually seen anyone else here who lives in India.\nIf you live in Mumbai, turn up and we can talk.\nIf you live elsewhere in India, message me - I do travel at least once a year and we may (or may not, as the case may be) be able to meet some time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tv'>Mumbai Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fysfCSYsMWDRujZhz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.4335460477017583e-06, "legacy": true, "legacyId": "24811", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Mumbai_Meetup\">Discussion article for the meetup : <a href=\"/meetups/tv\">Mumbai Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 December 2013 03:00:00PM (+0530)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gloria Jeans Coffee, S P Mukherji Chowk, Colaba, Mumbai</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I haven't actually seen anyone else here who lives in India.\nIf you live in Mumbai, turn up and we can talk.\nIf you live elsewhere in India, message me - I do travel at least once a year and we may (or may not, as the case may be) be able to meet some time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Mumbai_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/tv\">Mumbai Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Mumbai Meetup", "anchor": "Discussion_article_for_the_meetup___Mumbai_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Mumbai Meetup", "anchor": "Discussion_article_for_the_meetup___Mumbai_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-22T14:59:53.630Z", "modifiedAt": null, "url": null, "title": "The sun reflected off things", "slug": "the-sun-reflected-off-things", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.273Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fc9qotDfZEARGtoL7/the-sun-reflected-off-things", "pageUrlRelative": "/posts/Fc9qotDfZEARGtoL7/the-sun-reflected-off-things", "linkUrl": "https://www.lesswrong.com/posts/Fc9qotDfZEARGtoL7/the-sun-reflected-off-things", "postedAtFormatted": "Friday, November 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20sun%20reflected%20off%20things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20sun%20reflected%20off%20things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFc9qotDfZEARGtoL7%2Fthe-sun-reflected-off-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20sun%20reflected%20off%20things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFc9qotDfZEARGtoL7%2Fthe-sun-reflected-off-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFc9qotDfZEARGtoL7%2Fthe-sun-reflected-off-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p>An insight I had a while ago:</p>\n<p>When I'm out in the daylight, and I see a tree, what I actually see is not the tree itself. What I see is the sun reflected off the tree. Likewise with rocks, grass and birds: it's always the sun I'm seeing reflected off them. This is possible because the sun emits all visible colors (or rather, our eyes evolved to perceive almost all EM frequencies that almost all solid matter deflects). I'm not seeing the things. I'm seeing the light. We live surrounded by the sun.</p>\n<p>Is this too obvious? Inconsequential? Redundant?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fc9qotDfZEARGtoL7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -6, "extendedScore": null, "score": 1.433830100780362e-06, "legacy": true, "legacyId": "24813", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-22T17:01:35.844Z", "modifiedAt": null, "url": null, "title": "New LW Meetups: Amsterdam, Jacksonville, Milwaukee", "slug": "new-lw-meetups-amsterdam-jacksonville-milwaukee", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/krLeqganGKCG8XSE4/new-lw-meetups-amsterdam-jacksonville-milwaukee", "pageUrlRelative": "/posts/krLeqganGKCG8XSE4/new-lw-meetups-amsterdam-jacksonville-milwaukee", "linkUrl": "https://www.lesswrong.com/posts/krLeqganGKCG8XSE4/new-lw-meetups-amsterdam-jacksonville-milwaukee", "postedAtFormatted": "Friday, November 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetups%3A%20Amsterdam%2C%20Jacksonville%2C%20Milwaukee&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetups%3A%20Amsterdam%2C%20Jacksonville%2C%20Milwaukee%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkrLeqganGKCG8XSE4%2Fnew-lw-meetups-amsterdam-jacksonville-milwaukee%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetups%3A%20Amsterdam%2C%20Jacksonville%2C%20Milwaukee%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkrLeqganGKCG8XSE4%2Fnew-lw-meetups-amsterdam-jacksonville-milwaukee", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkrLeqganGKCG8XSE4%2Fnew-lw-meetups-amsterdam-jacksonville-milwaukee", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 562, "htmlBody": "<p><strong>This summary was posted to LW main on November 15th. The following week's summary is <a href=\"/lw/j5a/new_lw_meetup_mumbai/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/tg\">Amsterdam/Netherlands:&nbsp;<span class=\"date\">23 November 2013 02:00PM</span></a><a href=\"/meetups/sm\"></a></li>\n<li><a href=\"/meetups/tb\">Jacksonville, FL:&nbsp;<span class=\"date\">24 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/tk\">Milwaukee:&nbsp;<span class=\"date\">19 November 2013 06:30PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/t5\">[Atlanta GA] November Meetup (Second of Two):&nbsp;<span class=\"date\">23 November 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/tf\">Berlin:&nbsp;<span class=\"date\">01 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/t7\">Frankfurt:&nbsp;<span class=\"date\">24 November 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/sm\">Princeton NJ Meetup:&nbsp;<span class=\"date\">16 November 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/te\">Saint-Petersburg: Game Event:&nbsp;<span class=\"date\">24 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/tl\">Urbana-Champaign: Thinking Fast and Slow Discussion November 17:&nbsp;<span class=\"date\">17 November 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">16 November 2019 01:30AM</span></a></li>\n<li><a href=\"/meetups/ti\">Brussels monthly meetup: time!:&nbsp;<span class=\"date\">14 December 2013 01:00PM</span></a><a href=\"/meetups/bx\"></a> </li>\n<li><a href=\"/meetups/tm\">Durham/RTLW HPMoR discussion, ch. 97-98:&nbsp;<span class=\"date\">16 November 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/th\">London social meetup - New venue:&nbsp;<span class=\"date\">17 November 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/tn\">West LA&mdash;In Apprehending Hard Stuff:&nbsp;<span class=\"date\">20 November 2013 07:00AM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "krLeqganGKCG8XSE4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4339510557383918e-06, "legacy": true, "legacyId": "24716", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dmP2tjR8RTTqTSfQs", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-22T21:14:18.021Z", "modifiedAt": null, "url": null, "title": "A Limited But Better Than Nothing Way To Assign Probabilities to Statements of Logic, Arithmetic, etc.", "slug": "a-limited-but-better-than-nothing-way-to-assign", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:05.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alex_zag_al", "createdAt": "2011-11-16T23:52:10.523Z", "isAdmin": false, "displayName": "alex_zag_al"}, "userId": "pDkj9zKTeJPQuhurD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CPBvWvMAmrzmWMKHk/a-limited-but-better-than-nothing-way-to-assign", "pageUrlRelative": "/posts/CPBvWvMAmrzmWMKHk/a-limited-but-better-than-nothing-way-to-assign", "linkUrl": "https://www.lesswrong.com/posts/CPBvWvMAmrzmWMKHk/a-limited-but-better-than-nothing-way-to-assign", "postedAtFormatted": "Friday, November 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Limited%20But%20Better%20Than%20Nothing%20Way%20To%20Assign%20Probabilities%20to%20Statements%20of%20Logic%2C%20Arithmetic%2C%20etc.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Limited%20But%20Better%20Than%20Nothing%20Way%20To%20Assign%20Probabilities%20to%20Statements%20of%20Logic%2C%20Arithmetic%2C%20etc.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCPBvWvMAmrzmWMKHk%2Fa-limited-but-better-than-nothing-way-to-assign%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Limited%20But%20Better%20Than%20Nothing%20Way%20To%20Assign%20Probabilities%20to%20Statements%20of%20Logic%2C%20Arithmetic%2C%20etc.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCPBvWvMAmrzmWMKHk%2Fa-limited-but-better-than-nothing-way-to-assign", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCPBvWvMAmrzmWMKHk%2Fa-limited-but-better-than-nothing-way-to-assign", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 995, "htmlBody": "<p>&nbsp;</p>\n<p>If we want to reason with probability theory, we seem to be stuck if we want to reason about mathematics.</p>\n<p style=\"margin-bottom: 0in;\">You can skip this pararaph and the next if you're familiar with the problem. But if you're not, here's an illustration. Suppose your friend has some pennies that she would like to arrange into a rectangle, which of course is impossible if the number of pennies is prime. Let's call the number of pennies N. Your friend would like to use probability theory to guess whether it's worth trying; if there's a 50% chance that Prime(N), she won't bother trying to make the rectangle. You might imagine that if she counts them and finds that there's an odd number, this is evidence of Prime(N); if she furthermore notices that the digits don't sum to a multiple of three, this is further evidence of Prime(N). In general, each test of compositeness that she knows should, if it fails, raise the probability of Prime(N).</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But what happens instead is this. Suppose you both count them, and find that N=53. Being a LessWrong reader, you of course recognize from recently posted articles that N=53 implies Prime(N), though she does not. But this means that P(N=53) &lt;= P(Prime(N)). If you're quite sure of N=53&mdash;that is, P(N=53) is near 1&mdash;then P(Prime(N)) is also near 1. There's no way for her to get a gradient of uncertainty from simple tests of compositeness. The probability is just some number near 1.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">In general, conditional on the axioms, mathematical theorems have probability 1 if they're true, and 0 if they're false. Deriving these probabilities is exactly as difficult as deriving the theorems themselves.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">A way of assigning actual probabilities to theorems occurred to me today. I usually see this problem discussed by folks that want to develop formal models of AI, and I don't know if this'll be helpful at all for that. But it's something I can imagine myself using when I want to reason about a mathematical conjecture in a basically reasonable way.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The basic idea is just, don't condition on the axioms. Condition on a few relevant things implied by the axioms. Then, maximize entropy subject to those constraints.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Like, if you're trying to figure out whether a number N is prime. Don't condition on any properties of numbers, any facts about addition or multiplication etc., just treat Prime(N) as a predicate. Then condition on a few facts about Prime(N). I'd start with the <a href=\"http://en.wikipedia.org/wiki/Prime_number_theorem\">prime number theorem</a> and a theorem about the speed of its convergence, which I imagine would lead to a prior distribution&nbsp; P(Prime(N)|N=n, F1) = 1/log(n) (where F1 is Fact1, the fact that the prime number theorem is true). Now, let's say we want to be able to update on noticing a number is odd, so lets add Fact2, which is that all prime numbers are odd and half of all numbers are odd, which allows us to use Bayes' theorem:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><img src=\"http://www.codecogs.com/png.latex?P(Prime(N)|N=n, F1, F2) \\times \\frac{P(Odd(N)|Prime(N), N=n, F1, F2)}{P(Odd(N)|N=n, F1, F2)}}=P(Prime(N)|N=n, F1, F2, Odd(N))\" alt=\"bayesthm\" width=\"593\" height=\"64\" /></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">which gives us the intuitive conclusion that observing that a number is odd doubles the probability that it's prime.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's a weird thing to do, leaving out part of your knowledge. Here's how I think of it, though, so that it's less weird.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Suppose that the way you do statistics is by building Jaynesian robots. I'm referring here to an image used over and over again in Jaynes's book <em>Probability Theory: The Logic of Science</em>. The image is that we're building robots that assign plausibilities to statements, and we try to construct these robots in such a way that these plausibilities are useful to us. That is, we try to make robots whose conclusions we'll actually believe, because otherwise, why bother?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">One of his \"desiderata\", his principles of construction, is that the robot gives equal plausibility assignments to logically equivalent statements, which gets us into all this trouble when we try to build a robot we can ask about theorems. But I'm keeping this desideratum; I'm building fully Jaynesian robots.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">All I'm doing is hiding some of my knowledge from the robot. And this actually makes sense to me, because sometimes I'd rather have an ignorant Jaynesian robot than a fully informed one.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This is because speed is always important, and sometimes an ignorant Jaynesian robot is <em>faster</em>. Like, let's say my friend and I are building robots to compete in a <a title=\"number crunchers demo\" href=\"https://www.youtube.com/watch?v=dHM2rjvi7-U\">Number Munchers</a>-like game, where you're given lots of <em>very large</em> numbers and have to eat only the primes. And let's make the levels timed, too. If my friend builds a Jaynesian robot that knows the fundamental axioms of number theory, it's going to have to thoroughly test the primeness of each number before it eats it, and it's going to run out of time. But let's say I carefully program mine with enough facts to make it effective, but not so many facts that it's slow. I'll win.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This doesn't really apply to modeling AI, does it? I mean, it's <em>a</em> way to assign probabilities, but not the <em>best</em> one. Humans often do way better using, I don't know, analogy or whatever they do. So why would a self-modifying AI continue using this, after it finds a better way?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But it's very simple, and it does seem to work. Except for how I totally handwaved turning the prime number theorem into a prior. There could be all sorts of subtleties trying to get from \"prime number theorem and some theorem on speed of convergence, and maximizing entropy\" to \"P(Prime(N)|N=n) = 1/log(n)\". I at least know that it's not rigorously provable exactly as stated, since 1/log(n) is above 1 for small n.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">And, I honestly don't know how to do any better than this, and still use probability theory. I have that Haim Gaifman paper, \"Reasoning with Limited Resources and Assigning Probabilities to Arithmetical Statements\", in which he describes an entirely different way of conceptualizing the problem, along with a much better probabilistic test of primeness. It's long though, and I haven't finished it yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CPBvWvMAmrzmWMKHk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.434202261958119e-06, "legacy": true, "legacyId": "24815", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-23T06:04:25.465Z", "modifiedAt": null, "url": null, "title": "Open Thread, November 23-30, 2013", "slug": "open-thread-november-23-30-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:08.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "passive_fist", "createdAt": "2012-12-28T22:14:43.333Z", "isAdmin": false, "displayName": "passive_fist"}, "userId": "BiX9vSWsWJEJEEjZG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RG4aSwPnYN42ns9qg/open-thread-november-23-30-2013", "pageUrlRelative": "/posts/RG4aSwPnYN42ns9qg/open-thread-november-23-30-2013", "linkUrl": "https://www.lesswrong.com/posts/RG4aSwPnYN42ns9qg/open-thread-november-23-30-2013", "postedAtFormatted": "Saturday, November 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20November%2023-30%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20November%2023-30%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRG4aSwPnYN42ns9qg%2Fopen-thread-november-23-30-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20November%2023-30%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRG4aSwPnYN42ns9qg%2Fopen-thread-november-23-30-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRG4aSwPnYN42ns9qg%2Fopen-thread-november-23-30-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RG4aSwPnYN42ns9qg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.43472949913061e-06, "legacy": true, "legacyId": "24820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 301, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-23T16:23:11.085Z", "modifiedAt": null, "url": null, "title": "Classical vs MWI Probability Nomenclature", "slug": "classical-vs-mwi-probability-nomenclature", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:52.039Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/urNgRzgGMsiCnPkpr/classical-vs-mwi-probability-nomenclature", "pageUrlRelative": "/posts/urNgRzgGMsiCnPkpr/classical-vs-mwi-probability-nomenclature", "linkUrl": "https://www.lesswrong.com/posts/urNgRzgGMsiCnPkpr/classical-vs-mwi-probability-nomenclature", "postedAtFormatted": "Saturday, November 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Classical%20vs%20MWI%20Probability%20Nomenclature&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AClassical%20vs%20MWI%20Probability%20Nomenclature%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurNgRzgGMsiCnPkpr%2Fclassical-vs-mwi-probability-nomenclature%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Classical%20vs%20MWI%20Probability%20Nomenclature%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurNgRzgGMsiCnPkpr%2Fclassical-vs-mwi-probability-nomenclature", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurNgRzgGMsiCnPkpr%2Fclassical-vs-mwi-probability-nomenclature", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 243, "htmlBody": "<p>\"I estimate 5% odds of X happening\" can mean at least two things:</p>\n<p>* I have about 1-in-20 confidence that all future timelines from this point contain X, and about 19-in-20 confidence that none do.</p>\n<p>* I estimate about 1-in-20 future timelines contain X, and 19-in-20 future timelines don't.</p>\n<p>Looked at this way, the usual way of quantifying probability seems to be a lot like quantifying area - the first bullet-point by having a 1x20 rectangle, the second by having a 20x1 one. (This also seems valid for having, say, 50% confidence that 1-in-10 future timelines contain X.) It seems like it might be worth having an easy and understandable way to differentiate between these different forms of '5% odds', but any easy way I've been able to think of is barely understandable, and vice versa. Are there any existing standard ways to do this that I'm unaware of? If not, does anyone reading this have any decent answers?</p>\n<p>&nbsp;</p>\n<p>I'm not opposed to coming up with a new word for personal use to help get in the habit of thinking in certain ways; such as <a href=\"/lw/9jv/thinking_bayesianically_with_lojban/\">bei'e</a> in <a href=\"http://www.lojban.org/tiki/bei%27e\">Lojban</a> to remind myself to think of probability logarithmically. I don't mind doing the same with a word meaning 'such-and-such a fraction of future MWI branches', if that's the best solution, or even just a useful tool; I'd just like to know what the full range of useful approaches really are, first, and any potential loopholes therein or drawbacks thereof.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "urNgRzgGMsiCnPkpr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.4353453238998546e-06, "legacy": true, "legacyId": "24821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BS8dYNN5D6kLhvdXR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-23T16:55:25.065Z", "modifiedAt": null, "url": null, "title": "The Craft And The Community: The Basics: Apologizing", "slug": "the-craft-and-the-community-the-basics-apologizing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:59.363Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ritalin", "createdAt": "2012-05-01T18:00:25.863Z", "isAdmin": false, "displayName": "Ritalin"}, "userId": "ACGKS9W7iQQywxrWW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h6BCC2Mfq6p7onBX8/the-craft-and-the-community-the-basics-apologizing", "pageUrlRelative": "/posts/h6BCC2Mfq6p7onBX8/the-craft-and-the-community-the-basics-apologizing", "linkUrl": "https://www.lesswrong.com/posts/h6BCC2Mfq6p7onBX8/the-craft-and-the-community-the-basics-apologizing", "postedAtFormatted": "Saturday, November 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Craft%20And%20The%20Community%3A%20The%20Basics%3A%20Apologizing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Craft%20And%20The%20Community%3A%20The%20Basics%3A%20Apologizing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6BCC2Mfq6p7onBX8%2Fthe-craft-and-the-community-the-basics-apologizing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Craft%20And%20The%20Community%3A%20The%20Basics%3A%20Apologizing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6BCC2Mfq6p7onBX8%2Fthe-craft-and-the-community-the-basics-apologizing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh6BCC2Mfq6p7onBX8%2Fthe-craft-and-the-community-the-basics-apologizing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 284, "htmlBody": "<p>Now, it is said we all here pride ourselves on our intelligence, rationality, and moral sense. It is also said, however, that we are a fiercely independent bunch, and that we can let this pride of ours get the better of us. There have also been comments that the live communities that appear at meetups provide much more positive interactions than what goes on on this site's discussions; this might merit further investigation.</p>\n<p>My point is; we've done a lot of research on how to do proper ethical and metaethical calculations, and on how to achieve self-empowerment and deal with our own akrasia, which is awesome. We've also done some work on matters of gender equality, which is very positive as well. But I haven't seen us do anything about the basic details of human interaction, what one would call <em>\"politeness\"</em> and <em>\"basic human decency\". </em>And I think it might be useful if we started tackling these, for our own sakes, that of those who surround us, and that of easing our mission along, which is, as I understand it so far, to save the world (from existential risk (at the hands of (unfriendly and self-modifying) artificial intelligence))).</p>\n<p>What inspired me to propose this post was a video I just saw from Hank Green of the famed and fabled vlogbrothers. I hold these two individuals in very high esteem, and I would expect many here to share my feelings about them, on account of their values and sensibilities largely overlapping with ours; namely the sense that intelligence, knowledge and curiosity are awesome, and that intellectuals ought to use their power to help improve themselves and the world around them.</p>\n<p>Here it is; I hope you enjoy it</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>\n<object width=\"560\" height=\"315\" data=\"http://www.youtube.com/v/qc_XWlqURTg?version=3&amp;hl=en_US\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/qc_XWlqURTg?version=3&amp;hl=en_US\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h6BCC2Mfq6p7onBX8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 5, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "24822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 193, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-23T17:48:50.403Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston / Cambridge - Systems, Leverage, and Winning at Life", "slug": "meetup-boston-cambridge-systems-leverage-and-winning-at-life", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mapa8ndTJyaTfeQwi/meetup-boston-cambridge-systems-leverage-and-winning-at-life", "pageUrlRelative": "/posts/Mapa8ndTJyaTfeQwi/meetup-boston-cambridge-systems-leverage-and-winning-at-life", "linkUrl": "https://www.lesswrong.com/posts/Mapa8ndTJyaTfeQwi/meetup-boston-cambridge-systems-leverage-and-winning-at-life", "postedAtFormatted": "Saturday, November 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20%2F%20Cambridge%20-%20Systems%2C%20Leverage%2C%20and%20Winning%20at%20Life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20%2F%20Cambridge%20-%20Systems%2C%20Leverage%2C%20and%20Winning%20at%20Life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMapa8ndTJyaTfeQwi%2Fmeetup-boston-cambridge-systems-leverage-and-winning-at-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20%2F%20Cambridge%20-%20Systems%2C%20Leverage%2C%20and%20Winning%20at%20Life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMapa8ndTJyaTfeQwi%2Fmeetup-boston-cambridge-systems-leverage-and-winning-at-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMapa8ndTJyaTfeQwi%2Fmeetup-boston-cambridge-systems-leverage-and-winning-at-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tw'>Boston / Cambridge - Systems, Leverage, and Winning at Life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St, Apt 1, Somerville, MA, 02144</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Satvik Beri will be presenting on Systems, Leverage, and Winning at Life.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tw'>Boston / Cambridge - Systems, Leverage, and Winning at Life</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mapa8ndTJyaTfeQwi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4354306096336685e-06, "legacy": true, "legacyId": "24824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Cambridge___Systems__Leverage__and_Winning_at_Life\">Discussion article for the meetup : <a href=\"/meetups/tw\">Boston / Cambridge - Systems, Leverage, and Winning at Life</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St, Apt 1, Somerville, MA, 02144</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Satvik Beri will be presenting on Systems, Leverage, and Winning at Life.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Cambridge___Systems__Leverage__and_Winning_at_Life1\">Discussion article for the meetup : <a href=\"/meetups/tw\">Boston / Cambridge - Systems, Leverage, and Winning at Life</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston / Cambridge - Systems, Leverage, and Winning at Life", "anchor": "Discussion_article_for_the_meetup___Boston___Cambridge___Systems__Leverage__and_Winning_at_Life", "level": 1}, {"title": "Discussion article for the meetup : Boston / Cambridge - Systems, Leverage, and Winning at Life", "anchor": "Discussion_article_for_the_meetup___Boston___Cambridge___Systems__Leverage__and_Winning_at_Life1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-23T17:55:39.649Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1", "slug": "meetup-boston-cambridge-the-future-of-life-a-cosmic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:38.192Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H4dfa6kYHM3wr2kEd/meetup-boston-cambridge-the-future-of-life-a-cosmic", "pageUrlRelative": "/posts/H4dfa6kYHM3wr2kEd/meetup-boston-cambridge-the-future-of-life-a-cosmic", "linkUrl": "https://www.lesswrong.com/posts/H4dfa6kYHM3wr2kEd/meetup-boston-cambridge-the-future-of-life-a-cosmic", "postedAtFormatted": "Saturday, November 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20%2F%20Cambridge%20-%20The%20future%20of%20life%3A%20a%20cosmic%20perspective%20(Max%20Tegmark)%2C%20Dec%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20%2F%20Cambridge%20-%20The%20future%20of%20life%3A%20a%20cosmic%20perspective%20(Max%20Tegmark)%2C%20Dec%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4dfa6kYHM3wr2kEd%2Fmeetup-boston-cambridge-the-future-of-life-a-cosmic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20%2F%20Cambridge%20-%20The%20future%20of%20life%3A%20a%20cosmic%20perspective%20(Max%20Tegmark)%2C%20Dec%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4dfa6kYHM3wr2kEd%2Fmeetup-boston-cambridge-the-future-of-life-a-cosmic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4dfa6kYHM3wr2kEd%2Fmeetup-boston-cambridge-the-future-of-life-a-cosmic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tx'>Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 December 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St, Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Max Tegmark will be giving a talk, \"The future of life: a cosmic perspective\", at 3pm.</p>\n\n<p>We may need a larger venue depending on the turnout, so please RSVP on the meetup page: <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/151129022/\" rel=\"nofollow\">http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/151129022/</a></p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tx'>Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H4dfa6kYHM3wr2kEd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.4354374013277949e-06, "legacy": true, "legacyId": "24825", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Cambridge___The_future_of_life__a_cosmic_perspective__Max_Tegmark___Dec_1\">Discussion article for the meetup : <a href=\"/meetups/tx\">Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 December 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St, Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Max Tegmark will be giving a talk, \"The future of life: a cosmic perspective\", at 3pm.</p>\n\n<p>We may need a larger venue depending on the turnout, so please RSVP on the meetup page: <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/151129022/\" rel=\"nofollow\">http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/151129022/</a></p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Cambridge___The_future_of_life__a_cosmic_perspective__Max_Tegmark___Dec_11\">Discussion article for the meetup : <a href=\"/meetups/tx\">Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1", "anchor": "Discussion_article_for_the_meetup___Boston___Cambridge___The_future_of_life__a_cosmic_perspective__Max_Tegmark___Dec_1", "level": 1}, {"title": "Discussion article for the meetup : Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1", "anchor": "Discussion_article_for_the_meetup___Boston___Cambridge___The_future_of_life__a_cosmic_perspective__Max_Tegmark___Dec_11", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-23T22:34:22.669Z", "modifiedAt": null, "url": null, "title": "Probability and radical uncertainty", "slug": "probability-and-radical-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:32.981Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Chapman", "createdAt": "2011-03-17T17:41:11.044Z", "isAdmin": false, "displayName": "David_Chapman"}, "userId": "sNnWssxZ9SnSCxYTW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MpyDtSPyhfkNHS25u/probability-and-radical-uncertainty", "pageUrlRelative": "/posts/MpyDtSPyhfkNHS25u/probability-and-radical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/MpyDtSPyhfkNHS25u/probability-and-radical-uncertainty", "postedAtFormatted": "Saturday, November 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20and%20radical%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20and%20radical%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpyDtSPyhfkNHS25u%2Fprobability-and-radical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20and%20radical%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpyDtSPyhfkNHS25u%2Fprobability-and-radical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpyDtSPyhfkNHS25u%2Fprobability-and-radical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 848, "htmlBody": "<p>In the <a href=\"/lw/igv/probability_knowledge_and_metaprobability/\">previous article</a> in this sequence, I conducted a thought experiment in which simple probability was not sufficient to choose how to act. Rationality required reasoning about <em>meta-probabilities</em>, the probabilities of probabilities.</p>\n<p>Relatedly, lukeprog has <a href=\"/lw/h78/estimate_stability/\">a brief post</a> that explains how this matters; <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/\">a long article</a> by HoldenKarnofsky makes meta-probability&nbsp; central to utilitarian estimates of the effectiveness of charitable giving; and Jonathan_Lee, in <a href=\"/lw/hnf/model_stability_in_intervention_assessment/\">a reply</a> to that, has used the same framework I presented.</p>\n<p>In my previous article, I ran thought experiments that presented you with various colored boxes you could put coins in, gambling with uncertain odds.</p>\n<p>The last box I showed you was blue. I explained that it had a fixed but unknown probability of a twofold payout, uniformly distributed between 0 and 0.9. The overall probability of a payout was 0.45, so the expectation value for gambling was 0.9&mdash;a bad bet. Yet your optimal strategy was to gamble a bit to figure out whether the odds were good or bad.</p>\n<p>Let&rsquo;s continue the experiment. I hand you a black box, shaped rather differently from the others. Its sealed faceplate is carved with runic inscriptions and eldritch figures. &ldquo;I find this one <em>particularly</em> interesting,&rdquo; I say.</p>\n<p><a id=\"more\"></a></p>\n<p>What is the payout probability? What is your optimal strategy?</p>\n<p>In the framework of the previous article, you have no knowledge about the insides of the box. So, as with the &ldquo;sportsball&rdquo; case I analyzed there, your meta-probability curve is flat from 0 to 1.</p>\n<p>The blue box also has a flat meta-probability curve; but these two cases are very different. For the blue box, you know that the curve <em>really is</em> flat. For the black box, you have no clue what the shape of even the meta-probability curve is.</p>\n<p>The relationship between the blue and black boxes is the same as that between the coin flip and sportsball&mdash;except at the meta level!</p>\n<p>So if we&rsquo;re going on in this style, we need to look at the distribution of <em>probabilities of probabilities of probabilities</em>. The blue box has a sharp peak in its meta-meta-probability (around flatness), whereas the black box has a flat meta-meta-probability.</p>\n<p>You ought now to be a little uneasy. We are <a href=\"http://en.wikipedia.org/wiki/Epicycle#Epicycles\">putting epicycles on epicycles</a>. An infinite regress threatens.</p>\n<p>Maybe at this point you suddenly reconsider the blue box&hellip; I <em>told</em> you that its meta-probability was uniform. But perhaps I was lying! How reliable do you think I am?</p>\n<p>Let&rsquo;s say you think there&rsquo;s a 0.8 probability that I told the truth. That&rsquo;s the meta-meta-probability of a flat meta-probability. In the <em>worst</em> case, the actual payout probability is 0, so the average <em>just plain probability</em> is 0.8 x 0.45 = 0.36. You can feed that worst case into your decision analysis. It won&rsquo;t drastically change the optimal policy; you&rsquo;ll just quit a bit earlier than if you were entirely confident that the meta-probability distribution was uniform.</p>\n<p>To get this really right, you ought to make a best guess at the meta-meta-probability <em>curve</em>. It&rsquo;s not just 0.8 of a uniform probability distribution, and 0.2 of zero payout. That&rsquo;s the <em>worst</em> case. Even if I&rsquo;m lying, I might give you better than zero odds. How much better? What&rsquo;s your confidence in your meta-meta-probability curve? Ought you to draw a meta-meta-meta-probability curve? Yikes!</p>\n<p>Meanwhile&hellip; that black box is <em>rather sinister</em>. Seeing it makes you wonder. What if I rigged the blue box so there is a small probability that when you put a coin in, it jabs you with a poison dart, and you die horribly?</p>\n<p>Apparently a zero payout is <em>not</em> the worst case, after all! On the other hand, this seems paranoid. <a href=\"http://buddhism-for-vampires.com/dark-culture\">I&rsquo;m odd</a>, but probably not <em>that</em> evil.</p>\n<p>Still, what about the black box? You realize now that it could do <em>anything</em>.</p>\n<ul>\n<li>It might spring open to reveal a collection of fossil trilobites.</li>\n<li>It might play Corvus Corax&rsquo;s <em><a href=\"http://www.youtube.com/watch?v=jgBvEVy__qs\">Vitium in Opere</a></em> at ear-splitting volume.</li>\n<li>It might analyze the trace DNA you left on the coin and use it to write you a <em>personalized</em> love poem.</li>\n<li>It might emit a strip of paper with a recipe for dundun noodles written in Chinese.</li>\n<li>It might sprout six mechanical legs and jump into your lap.</li>\n</ul>\n<p>What is the probability of its giving you $2?</p>\n<p>That no longer seems quite so relevant. In fact&hellip; it might be utterly meaningless! This is now a situation of <strong>radical uncertainty</strong>.</p>\n<p>What is your optimal strategy?</p>\n<p>I&rsquo;ll answer that later in this sequence. You might like to figure it out for yourself now, though.</p>\n<h2>Further reading</h2>\n<p>The black box is an instance of <a href=\"http://en.wikipedia.org/wiki/Knightian_uncertainty\">Knightian uncertainty</a>. That&rsquo;s a catch-all category for any type of uncertainty that can&rsquo;t usefully be modeled in terms of probability (<em>or</em> meta-probability!), because you can&rsquo;t make meaningful probability estimates. Calling it &ldquo;Knightian&rdquo; doesn&rsquo;t help solve the problem, because there&rsquo;s lots of sources of non-probabilistic uncertainty. However, it&rsquo;s useful to know that there&rsquo;s a literature on this.</p>\n<p>The blue box is closely related to <a href=\"http://en.wikipedia.org/wiki/Ellsberg_paradox\">Ellsberg&rsquo;s paradox</a>, which combines probability with Knightian uncertainty. Interestingly, it was invented by the same Daniel Ellsberg who released the Pentagon Papers in 1971. I wonder how his work in decision theory might have affected his decision to leak the Papers?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MpyDtSPyhfkNHS25u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 18, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "23936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2xmKZu73gZLDEQw7c", "K33mYmEk9LoTbN92L", "RdpqsQ6xbHzyckW9m", "RsKWqKju54NHhXw5B"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-24T15:18:32.272Z", "modifiedAt": "2020-01-31T00:20:12.232Z", "url": null, "title": "What can we learn from freemasonry?", "slug": "what-can-we-learn-from-freemasonry", "viewCount": null, "lastCommentedAt": "2014-03-05T15:07:37.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BRgXwxdLsFAiQwma7/what-can-we-learn-from-freemasonry", "pageUrlRelative": "/posts/BRgXwxdLsFAiQwma7/what-can-we-learn-from-freemasonry", "linkUrl": "https://www.lesswrong.com/posts/BRgXwxdLsFAiQwma7/what-can-we-learn-from-freemasonry", "postedAtFormatted": "Sunday, November 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20can%20we%20learn%20from%20freemasonry%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20can%20we%20learn%20from%20freemasonry%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRgXwxdLsFAiQwma7%2Fwhat-can-we-learn-from-freemasonry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20can%20we%20learn%20from%20freemasonry%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRgXwxdLsFAiQwma7%2Fwhat-can-we-learn-from-freemasonry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBRgXwxdLsFAiQwma7%2Fwhat-can-we-learn-from-freemasonry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>I recently stumbled over the relationship between freemasons and networks of social and economic influence (e.g. nobility).</p>\n<p>I wondered what could be learned from a society which exists so long and has ideals that are not that far away from the LW goal of refining human rationality. &nbsp;</p>\n<p>It is interesting to note that the freemasons seem to have highly tolerant and rational values. The freemasons orginated from independent craft guilds but became <a href=\"http://en.wikipedia.org/wiki/History_of_Freemasonry#The_Emergence_of_Speculative_Masonry\">'speculative freemasons'</a> during the enlightenment and this is reflected in their commitment to tolerance and reason which builds on crafts traditions of teaching, truth, reliability and craft perfection. Somewhat problematic may be their unusual customs and the prejudice they face. Nonetheless they obviously can cooperate which <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">our kind can't</a>.&nbsp;</p>\n<p>Note: I didn't attend any freemason meetings and don't know any details. What I read on Wikipedia was mostly asbtract. I might attend a meeting but unsure about it's <a href=\"/lw/85x/value_of_information_four_examples/\">value of information</a>.</p>\n<p>What do you think: What can we learn from freemasonry? What should be avoided? Is there any freemason here who might provide insights?</p>\n<p>Relevants comments (no posts) on LW:</p>\n<p><a href=\"/lw/4d/youre_calling_who_a_cult_leader/37s\">LW as a cult like freemasons.</a></p>\n<p><a href=\"/r/discussion/lw/bql/our_cult_is_not_exclusive_enough/\">LW as exclusive phyg</a></p>\n<p><a href=\"/lw/bql/our_phyg_is_not_exclusive_enough/6cpj\">Interview systems for admission to LW</a></p>\n<p><a href=\"/r/all/lw/hgm/open_thread_may_1731_2013/91gj\">Use of prejudice about freemasons</a></p>\n<p><a href=\"/lw/8g2/less_wrongrationality_symbol_or_seal/\">A post about an LW symbol</a>&nbsp;prompted <a href=\"/lw/8g2/less_wrongrationality_symbol_or_seal/59mr\">this comment about freemason icons</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BRgXwxdLsFAiQwma7", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "24829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC", "vADtvr9iDeYsCDfxd", "hxGEKxaHZEKT4fpms", "wsBkYaMGBQ743Wm5u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-11-24T15:18:32.272Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-24T15:23:32.773Z", "modifiedAt": null, "url": null, "title": "Gelman Against Parsimony", "slug": "gelman-against-parsimony", "viewCount": null, "lastCommentedAt": "2022-03-11T01:19:27.260Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/az2vsi8ugTWXZ3Lq2/gelman-against-parsimony", "pageUrlRelative": "/posts/az2vsi8ugTWXZ3Lq2/gelman-against-parsimony", "linkUrl": "https://www.lesswrong.com/posts/az2vsi8ugTWXZ3Lq2/gelman-against-parsimony", "postedAtFormatted": "Sunday, November 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gelman%20Against%20Parsimony&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGelman%20Against%20Parsimony%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faz2vsi8ugTWXZ3Lq2%2Fgelman-against-parsimony%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gelman%20Against%20Parsimony%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faz2vsi8ugTWXZ3Lq2%2Fgelman-against-parsimony", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faz2vsi8ugTWXZ3Lq2%2Fgelman-against-parsimony", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>In <a href=\"http://andrewgelman.com/2004/12/10/against_parsimo/\">two</a> <a href=\"http://andrewgelman.com/2005/04/20/against_parsimo_1/\">posts</a>, Bayesian <a href=\"http://www.amazon.com/Bayesian-Analysis-Edition-Chapman-Statistical/dp/1439840954/\">stats guru</a> Andrew Gelman argues against parsimony, though it seems to be favored 'round these parts, in particular <a href=\"/lw/jp/occams_razor/\">Solomonoff Induction</a> and <a href=\"http://en.wikipedia.org/wiki/Bayesian_information_criterion\">BIC</a> as <a href=\"/lw/cw1/open_problems_related_to_solomonoff_induction/\">imperfect</a> formalizations of <a href=\"http://en.wikipedia.org/wiki/Occam%27s_razor\">Occam's Razor.</a></p>\n<p>Gelman says:</p>\n<blockquote>\n<p>I&rsquo;ve never seen any good general justification for parsimony...</p>\n<p>Maybe it&rsquo;s because I work in social science, but my feeling is: if you can approximate reality with just a few parameters, fine. If you can use more parameters to fold in more information, that&rsquo;s even better.</p>\n<p>In practice, I often use simple models&ndash;because they are less effort to fit and, especially, to understand. But I don&rsquo;t kid myself that they&rsquo;re better than more complicated efforts!</p>\n<p>My favorite quote on this comes from Radford Neal&lsquo;s book, <em>Bayesian Learning for Neural Networks</em>, pp. 103-104: \"Sometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\"</p>\n<p>...</p>\n<p>...ideas like minimum-description-length, parsimony, and Akaike&rsquo;s information criterion, are particularly relevant when models are estimated using least squares, maximum likelihood, or some other similar optimization method.</p>\n<p>When using hierarchical models, we can avoid overfitting and get good descriptions without using parsimony&ndash;the idea is that the many parameters of the model are themselves modeled. <a href=\"http://andrewgelman.com/2004/12/10/against_parsimo/\">See here for some discussion of Radford Neal&rsquo;s ideas in favor of complex models</a>, and <a href=\"http://www.stat.columbia.edu/~gelman/research/published/bois2.pdf\">see here</a> for an example from my own applied research.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "az2vsi8ugTWXZ3Lq2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 10, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "24830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f4txACqDWithRi7hs", "fC248GwrWLT4Dkjf6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-24T18:35:58.342Z", "modifiedAt": null, "url": null, "title": "'Effective Altruism' as utilitarian equivocation.", "slug": "effective-altruism-as-utilitarian-equivocation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:33.511Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dias", "createdAt": "2012-04-19T07:46:06.425Z", "isAdmin": false, "displayName": "Dias"}, "userId": "ycNxfH8i9QwtXqu8E", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ptnGtkrFPB7WJt8rj/effective-altruism-as-utilitarian-equivocation", "pageUrlRelative": "/posts/ptnGtkrFPB7WJt8rj/effective-altruism-as-utilitarian-equivocation", "linkUrl": "https://www.lesswrong.com/posts/ptnGtkrFPB7WJt8rj/effective-altruism-as-utilitarian-equivocation", "postedAtFormatted": "Sunday, November 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Effective%20Altruism'%20as%20utilitarian%20equivocation.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Effective%20Altruism'%20as%20utilitarian%20equivocation.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptnGtkrFPB7WJt8rj%2Feffective-altruism-as-utilitarian-equivocation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Effective%20Altruism'%20as%20utilitarian%20equivocation.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptnGtkrFPB7WJt8rj%2Feffective-altruism-as-utilitarian-equivocation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptnGtkrFPB7WJt8rj%2Feffective-altruism-as-utilitarian-equivocation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 711, "htmlBody": "<p><em>Summary: The term 'effective altuist' invites confusion between 'the right thing to do' and 'the thing that most efficiently promotes welfare.' I think this creeping utilitarianism is a bad thing, and should at least be made explicit. This is not to accuse anyone of deliberate deception.</em><br /><br />Over the last year or so, the term 'Effective Altruist' has come into use. I self-identified as one on the <a href=\"/lw/j4y/2013_less_wrong_censussurvey/\">LW survey</a>, so I speak as a friend. However, I think there is a very big danger with the terminology.<br /><br />The term <em>'Effective Altruist'</em> was born out of the need for a label for those people who were willing to dedicate their lives to making the world a better place in rational ways, even if that meant doing counter-intuitive things, like working as an Alaskan truck driver. The previous term,<em> 'really super awesome hardcore people'</em>, was indeed a little inelegant. <br /><br />However, <em>'Effective Altruist' </em>has a major problem: it refers to altruism, not ethics. Altruism may be a part of ethics (though <a href=\"http://en.wikipedia.org/wiki/Altruism_%28ethics%29\">the etymology of the term gives some concern</a>), but it is not all there is to ethics. <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Value is complex.</a> Helping people is good, but so is truth, and justice, and freedom, and beauty, and loyalty, and fairness, and honor, and fraternity, and tradition, and many other things. <br /><br />A charity that very efficiently promoted beauty and justice, but only inefficiently produced happiness, would probably not be considered an EA organization. A while ago I suggested to [one of the leaders of the <a href=\"http://home.centreforeffectivealtruism.org/\">Center for Effective Altruism</a>] the creation of a charity to promote promise-keeping. I didn't claim such a charity would be an optimal way of promoting happiness, and to them, this was sufficient to show 1) that it was not EA - and&nbsp; hence 2) inferior to EA things. <br /><br />Such thinking involves either a equivocation or a concealed premise. If 'EA' is interpreted literally, so<a href=\"http://www.effective-altruism.com/effective-altruism/\"> 'the primary/driving goal is to help others'</a>, then something not being EA is insufficient for it to not be the best thing you could do - there is more to ethics and the good, than altruism and promoting welfare. Failure to promote one dimension of the good doesn't mean you're not the optimal way of promoting their sum. On the other hand, if 'EA' is interpreted broadly, as being concerned with <a href=\"http://www.effective-altruism.com/four-focus-areas-effective-altruism/\">'happiness, health, justice, fairness and/or other values'</a>, then merely failing to promote welfare/happiness does not mean a cause is not EA. Much EA discussion, like on the popular facebook group, equivocates between these two meanings.*</p>\n<p>...Unless one thought that helping people was all their was to ethics, in which case this is not equivocation. As virtually all of CEA's leaders are utilitarians, it is plausible that is was the concealed premise in their argument. In this case, there is no equivocation, but a different logical fallacy, that of an omitted premise, has been committed. And we should be just as wary as in the case of equivocation.<br /><br />Unfortunately, utilitarianism is false, or at least not obviously true. Something can be the morally best thing to do, while not being EA. Just because some utilitarians have popularized a term which cleverly equivocates between \"promotes welfare\" and \"is the best thing\" does not mean we should be taken in. Every fashionable ideology likes to blurr the lines between its goals and its methods (is Socialism about helping the working man or about state ownership of industry? is libertarianism about freedom or low taxes?) in order to make people who agree with the goals forget that there might be other means of achieving them. <br /><br />There are two options: recognize 'EA' as referring to only a subset of morality, or recognize as 'EA' actions and organizations that are ethical through ways other than producing welfare/happiness. <br /><br />* Yes, one might say that promoting X's honor thereby helped X, and thus there was no distinction. However, I think people who make this argument in theory are unlikely to observe it in practice - I doubt that there will be an EA organisation dedicated to pure retribution, even if it was both extremely cheap to promote and a part of ethics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ptnGtkrFPB7WJt8rj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 4, "extendedScore": null, "score": 1.4369127565969855e-06, "legacy": true, "legacyId": "24831", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Sx26Aj3xuMzmnKE4A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-24T20:16:54.292Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-10", "viewCount": null, "lastCommentedAt": "2013-11-24T20:16:54.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nYYsAMLtkW9Nv3hPc/meetup-helsinki-meetup-10", "pageUrlRelative": "/posts/nYYsAMLtkW9Nv3hPc/meetup-helsinki-meetup-10", "linkUrl": "https://www.lesswrong.com/posts/nYYsAMLtkW9Nv3hPc/meetup-helsinki-meetup-10", "postedAtFormatted": "Sunday, November 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYYsAMLtkW9Nv3hPc%2Fmeetup-helsinki-meetup-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYYsAMLtkW9Nv3hPc%2Fmeetup-helsinki-meetup-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYYsAMLtkW9Nv3hPc%2Fmeetup-helsinki-meetup-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ty'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 December 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019re having an informal meetup in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a>. To find us, look for someone wearing a pink elephant hat.</p>\n\n<p>Topics of discussion will most likely revolve around subjects from two previous meetups: <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">strategic thinking</a> and <a href=\"http://lesswrong.com/lw/hub/common_failure_modes_in_habit_formation/\">habit formation</a>, and reflections on how those were applied in practice.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ty'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nYYsAMLtkW9Nv3hPc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4370134488371075e-06, "legacy": true, "legacyId": "24832", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ty\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 December 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019re having an informal meetup in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a>. To find us, look for someone wearing a pink elephant hat.</p>\n\n<p>Topics of discussion will most likely revolve around subjects from two previous meetups: <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">strategic thinking</a> and <a href=\"http://lesswrong.com/lw/hub/common_failure_modes_in_habit_formation/\">habit formation</a>, and reflections on how those were applied in practice.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ty\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB", "PYgGpmmk3wQhSt6Yv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-24T21:32:01.109Z", "modifiedAt": null, "url": null, "title": "Some thoughts on relations between major ethical systems", "slug": "some-thoughts-on-relations-between-major-ethical-systems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.852Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "28iKD7fEnHvK8pNNm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v8voyBZo2i7iuagSo/some-thoughts-on-relations-between-major-ethical-systems", "pageUrlRelative": "/posts/v8voyBZo2i7iuagSo/some-thoughts-on-relations-between-major-ethical-systems", "linkUrl": "https://www.lesswrong.com/posts/v8voyBZo2i7iuagSo/some-thoughts-on-relations-between-major-ethical-systems", "postedAtFormatted": "Sunday, November 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20thoughts%20on%20relations%20between%20major%20ethical%20systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20thoughts%20on%20relations%20between%20major%20ethical%20systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8voyBZo2i7iuagSo%2Fsome-thoughts-on-relations-between-major-ethical-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20thoughts%20on%20relations%20between%20major%20ethical%20systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8voyBZo2i7iuagSo%2Fsome-thoughts-on-relations-between-major-ethical-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8voyBZo2i7iuagSo%2Fsome-thoughts-on-relations-between-major-ethical-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 736, "htmlBody": "<p>On the recent LessWrong/CFAR Census Survey, I hit the following question:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">Which of the following major ethical systems do you subscribe to:</p>\n<p style=\"padding-left: 30px;\">1) Consequentialism</p>\n<p style=\"padding-left: 30px;\">2) Deontology</p>\n<p style=\"padding-left: 30px;\">3) Virtue Ethics</p>\n<p style=\"padding-left: 30px;\">4) Other</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>To my own surprise, I couldn't come up with a clear answer.&nbsp; I certainly don't consistently apply one of these things across every decision I make in my life, and yet I consider myself at least mediocre on the scale of moral living, if not actually Neutral Good.&nbsp; So what is it I'm actually doing, and how can I behave more ethical-rationally?</p>\n<p>&nbsp;</p>\n<p>Well, to analyze my own cognitive algorithms, I do think I can actually place these various codes of ethics in relation to each other.&nbsp; Basically, looked at behavioristically/algorithmically, they vary across how much predictive power I have, my knowledge of my own values, and what it is I'm actually trying to affect.</p>\n<p>&nbsp;</p>\n<p>Consequentialism is the ethical algorithm I consider useful in situations of greatest predictive power and greatest knowledge of my own values.&nbsp; It is, so to speak, the ethical-algorithmic ideal<em>.</em>&nbsp; In such situations, the only drawback is that <em>naive</em> consequentialism fails to consider consequences on the person acting (ie: me).&nbsp; Once I make that more virtue-ethical adjustment, consequentialism offers a complete ideal for ethical action over a complete spectrum of moral values for affecting both the universe and myself (but I repeat: I'm <em>part</em> of the universe).</p>\n<p>&nbsp;</p>\n<p>However, in almost all real situations, I don't <em>have</em> perfect predictive knowledge -- not of the \"external\" universe and not of my own values.&nbsp; In these situations, I can, however, use my incomplete and uncertain knowledge to find acceptable heuristics that I can expect to yield roughly monotonic behavior: follow those rules, and my actions will generally have positive effects.&nbsp; This kind of thinking quickly yields up recognizable, regular moral commandments like, \"You will not murder\" or \"You will not charge interest above this-or-that amount on loans\".&nbsp; Yes, of course we can come up with corner-case exceptions to those rules, and we can also elaborate logically on the rules to arrive at more detailed rules covering more circumstances.&nbsp; However, by the time we've fully elaborated out the basic commandments into a complete, obsessively-compulsively detailed legal code (oh hello Talmud), we've already covered most of the major general cases of moral action.&nbsp; We can now invent a criterion for how and when to transition from one level of ethical code to the one below it: our deontological heuristics should be detailed enough to handle any case where we lack the information (about consequences and values) to resort to consequentialism.</p>\n<p>&nbsp;</p>\n<p>At first thought, virtue ethics seems like an even higher-level heuristic than deontological ethics.&nbsp; The problem is that, unlike deontological and consequentialist ethics, it doesn't output courses of action to take, but instead short- and long-term states of mind or character that can be considered virtuous.&nbsp; So we don't have the same thing here; it's not a higher-level heuristic but a seemingly completely different form of ethics.&nbsp; I do think we can integrate it, however: virtue ethics just consists of a set of moral values over one's own character.&nbsp; \"What kind of person do I think is a good person?\" might, by default, be a tautological question under <em>strict</em> consequentialism or deontology.&nbsp; <em>However</em>, when we take an account of the imperfect nature of real people (we <em>are</em> part of the universe, after all), we can observe that virtue ethics serves as a convenient guide to heuristics for becoming the sort of person who can be relied upon to take right actions when moral issues present themselves.&nbsp; Rather than simply saying, \"Do the right thing no matter what\" (an instruction that simply won't drive real human beings to actually do the right thing), virtue ethics encourages us to cultivate <em>virtues</em>, moral cognitive biases towards at least a deontological notion of right action.</p>\n<p>&nbsp;</p>\n<p>It's also possible we might be able to separate virtue ethics into both <em>heuristics</em> over our own character, and actual <em>values</em> over our own character.&nbsp; These two approaches to virtue ethics should then converge in the presence of perfect information: if I knew myself utterly, my heuristics for my own character would exactly match my values over my own character.</p>\n<p>&nbsp;</p>\n<p>This is my first effort at actually blogging on rationality subjects, so I'm hoping it's not covering something hashed and rehashed, over and over again, in places like the <em>Sequences</em>, of which I certainly can't attest a full knowledge.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v8voyBZo2i7iuagSo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.4370883917471406e-06, "legacy": true, "legacyId": "24833", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-24T22:03:06.646Z", "modifiedAt": null, "url": null, "title": "What do we already have right?", "slug": "what-do-we-already-have-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.839Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EndlessStrategy", "createdAt": "2013-02-28T16:06:08.831Z", "isAdmin": false, "displayName": "EndlessStrategy"}, "userId": "fsHg3CNMh8Xi6hJQE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sTLCXKZwM7BP5JEa5/what-do-we-already-have-right", "pageUrlRelative": "/posts/sTLCXKZwM7BP5JEa5/what-do-we-already-have-right", "linkUrl": "https://www.lesswrong.com/posts/sTLCXKZwM7BP5JEa5/what-do-we-already-have-right", "postedAtFormatted": "Sunday, November 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20do%20we%20already%20have%20right%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20do%20we%20already%20have%20right%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsTLCXKZwM7BP5JEa5%2Fwhat-do-we-already-have-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20do%20we%20already%20have%20right%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsTLCXKZwM7BP5JEa5%2Fwhat-do-we-already-have-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsTLCXKZwM7BP5JEa5%2Fwhat-do-we-already-have-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p>I was just wondering. Human minds are messed up in 1001 ways, but are there a few rational principles that most people already have down? Of course, the answers to this question are probably so extremely obvious that I haven't even considered them. But I ask all the same.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sTLCXKZwM7BP5JEa5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 1.4371194153894702e-06, "legacy": true, "legacyId": "24834", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-25T16:21:54.856Z", "modifiedAt": null, "url": null, "title": "Links: so-called \"knockout game\" a \"myth and a \"bogus trend.\"", "slug": "links-so-called-knockout-game-a-myth-and-a-bogus-trend", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:02.330Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NpdfvNwGPPQs5fn3D/links-so-called-knockout-game-a-myth-and-a-bogus-trend", "pageUrlRelative": "/posts/NpdfvNwGPPQs5fn3D/links-so-called-knockout-game-a-myth-and-a-bogus-trend", "linkUrl": "https://www.lesswrong.com/posts/NpdfvNwGPPQs5fn3D/links-so-called-knockout-game-a-myth-and-a-bogus-trend", "postedAtFormatted": "Monday, November 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Links%3A%20so-called%20%22knockout%20game%22%20a%20%22myth%20and%20a%20%22bogus%20trend.%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALinks%3A%20so-called%20%22knockout%20game%22%20a%20%22myth%20and%20a%20%22bogus%20trend.%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpdfvNwGPPQs5fn3D%2Flinks-so-called-knockout-game-a-myth-and-a-bogus-trend%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Links%3A%20so-called%20%22knockout%20game%22%20a%20%22myth%20and%20a%20%22bogus%20trend.%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpdfvNwGPPQs5fn3D%2Flinks-so-called-knockout-game-a-myth-and-a-bogus-trend", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpdfvNwGPPQs5fn3D%2Flinks-so-called-knockout-game-a-myth-and-a-bogus-trend", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p>When I started seeing stories about the \"knockout game\" (supposedly, teenagers playing a game where they try to knockout random strangers) a few days ago, I immediately resolved to avoid paying attention to them, because it sounded like a classic case of people taking a few isolated incidents and blowing them up into a big scary trend.</p>\n<p>And then this morning, I see <a href=\"http://www.patheos.com/blogs/christandpopculture/2013/11/the-knockout-game-myth-and-its-racist-roots/\">this blog post</a>, which links back to an article from two years ago titled: <a href=\"http://www.riverfronttimes.com/2011-06-09/news/knockout-king-elex-murphy-hoang-nguyen-dutchtown-murder/full/\">\"Knockout King: Kids call it a game. Academics call it a bogus trend. Cops call it murder.\"</a>&nbsp;Turns out my knowledge of human biases has served me well... and it's especially significant that the article is from two years ago; this is not the first time the media has tried to get people scared about this \"trend.\" From the article (emphasis added):</p>\n<blockquote>\n<p>Mike Males, a research fellow at the nonprofit Center on Juvenile and Criminal Justice and who runs the website YouthFacts.org, says <strong>the media have made habit of cherry-picking isolated instances of \"knockout games\" in order to gin up sensational stories that demonize youth. \"This knockout-game legend is a fake trend,\" Males contends.</strong></p>\n<p>Given that 4.3 million violent attacks were reported by U.S. citizens in 2009, according to the National Crime Victimization Survey, Males says <strong>reporters should know better than to highlight a handful of random attacks by kids and call it journalism.</strong> It's the same thing as plucking a few instances of attackers with Jewish surnames who beat up non-Jews and declaring it a \"troubling new trend,\" he argues.</p>\n<p>Still, over the years a handful of reports of \"knockout\" have emerged from cities in Missouri, Illinois, Massachusetts and New Jersey. And most criminologists and youth experts agree that unprovoked attacks by teenagers on strangers are a real,<strong> if extremely rare, </strong>phenomenon.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NpdfvNwGPPQs5fn3D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": -11, "extendedScore": null, "score": -4e-05, "legacy": true, "legacyId": "24849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-25T18:12:21.336Z", "modifiedAt": null, "url": null, "title": "Stable and Unstable Risks", "slug": "stable-and-unstable-risks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RJSbpFri5fwNQWEQP/stable-and-unstable-risks", "pageUrlRelative": "/posts/RJSbpFri5fwNQWEQP/stable-and-unstable-risks", "linkUrl": "https://www.lesswrong.com/posts/RJSbpFri5fwNQWEQP/stable-and-unstable-risks", "postedAtFormatted": "Monday, November 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stable%20and%20Unstable%20Risks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStable%20and%20Unstable%20Risks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRJSbpFri5fwNQWEQP%2Fstable-and-unstable-risks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stable%20and%20Unstable%20Risks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRJSbpFri5fwNQWEQP%2Fstable-and-unstable-risks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRJSbpFri5fwNQWEQP%2Fstable-and-unstable-risks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 369, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/8f0/existential_risk/\">Existential Risk</a>, <a href=\"/lw/jq/926_is_petrov_day/\">9/26 is Petrov Day</a></p>\n<p>Existential risks&mdash;risks that, <a href=\"http://www.nickbostrom.com/existential/risks.pdf\">in the words of Nick Bostrom</a>, would \"either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential,\" are a significant threat to the world as we know it. In fact, they may be one of the most pressing issues facing humanity today.</p>\n<p>The likelihood of some risks may stay relatively constant over time&mdash;a basic view of <a href=\"http://en.wikipedia.org/wiki/Impact_event\">asteroid impact</a> is that there is a certain probability that a \"killer asteroid\" hits the Earth and that this probability is more or less the same every year. This is what I refer to as a \"stable risk.\"</p>\n<p>However, the likelihood of other existential risks seems to fluctuate, often quite dramatically. Many of these \"unstable risks\" are related to human activity.</p>\n<p>For instance, the likelihood of a nuclear war at sufficient scale to be an existential threat seems contingent on various geopolitical factors that are difficult to predict in advance. That said, the likelihood of this risk has clearly changed throughout recent history. Nuclear war was obviously not an existential risk before nuclear weapons were invented, and was fairly clearly more of a risk during the <a href=\"http://en.wikipedia.org/wiki/Cuban_Missile_Crisis\">Cuban Missile Crisis</a> than it is today.</p>\n<p>Many of these unstable, human-created risks seem based largely on advanced technology. Potential risks like <a href=\"http://en.wikipedia.org/wiki/Grey_goo\">gray goo</a> rely on theorized technologies that have yet to be developed (and indeed may never be developed). While this is good news for the present day, it also means that we have to be vigilant for the emergence of potential new threats as human technology increases.</p>\n<p>GiveWell's <a href=\"http://www.givewell.org/files/conversations/Shulman%209-25-13%20%28public%29.pdf\">recent conversation with Carl Shulman</a> contains some arguments as to why the risk of human extinction may be decreasing over time. However, it strikes me as perhaps more likely that the risk of human extinction is increasing over time&mdash;or at the very least becoming less stable&mdash;as technology increases the amount of power available to individuals and civilizations.</p>\n<p>After all, the very concept of human-created unstable existential risks is a recent one. Even if Julius Caesar, Genghis Khan, or Queen Victoria for some reason decided to destroy human civilization, it seems almost certain that they would fail, even given all the resources of their empires.</p>\n<p>The same cannot be said for Kennedy or Khrushchev.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RJSbpFri5fwNQWEQP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 1.4383269107852676e-06, "legacy": true, "legacyId": "24848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FGTgeweYNxmMBx4fz", "QtyKq4BDyuJ3tysoK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-26T19:24:09.578Z", "modifiedAt": null, "url": null, "title": "Noticing something completely absurd about yourself", "slug": "noticing-something-completely-absurd-about-yourself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:28.832Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fpu3rrFQ2mjKofAsN/noticing-something-completely-absurd-about-yourself", "pageUrlRelative": "/posts/Fpu3rrFQ2mjKofAsN/noticing-something-completely-absurd-about-yourself", "linkUrl": "https://www.lesswrong.com/posts/Fpu3rrFQ2mjKofAsN/noticing-something-completely-absurd-about-yourself", "postedAtFormatted": "Tuesday, November 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Noticing%20something%20completely%20absurd%20about%20yourself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANoticing%20something%20completely%20absurd%20about%20yourself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFpu3rrFQ2mjKofAsN%2Fnoticing-something-completely-absurd-about-yourself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Noticing%20something%20completely%20absurd%20about%20yourself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFpu3rrFQ2mjKofAsN%2Fnoticing-something-completely-absurd-about-yourself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFpu3rrFQ2mjKofAsN%2Fnoticing-something-completely-absurd-about-yourself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 506, "htmlBody": "<p>I'm not sure this is something that can be consciously done, but in this post I want to prime you to consider whether something you do is really, totally, completely wacky and absurd.&nbsp;</p>\n<p>We have trained ourselves a lot to notice when we are wrong. We trained ourselves even more to notice when we are confused and to tell word confusion from substance confusion.&nbsp;</p>\n<p>But here is the tale of what happened to me today, and I don't think it qualifies as any of those:</p>\n<p>I had a serious motivational problem yesterday, and got absolutely nothing done. So today I thought I should do things in a different manner, so as to decrease probability of two bad days in a row. One of the most effective things for me is going into the <a href=\"http://tinychat.com/lesswrong\">LW Study Hall (the password is in the group's description when you click this link).</a> A very nice place to work that I recommend for everyone to check out, and do one or two pomo's every now and then. &nbsp;</p>\n<p>And I did, I gave myself ten minutes observing others working, and I noticed something remarkable: The property of the LW chat that causes me to be motivated is \"Presence of long haired people\". Yes. &nbsp;Presence of people with a long hair. For weeks I had been trying to work out why it was efficient sometimes and not others. The most obvious initial alternative was that when there was a woman, I would feel more driven. I assumed that was the case. But I started getting false negatives and false positives. Today I finally came to terms with the fact. I am motivated by the presence of people whose hair goes to their shoulders. Women or Men.&nbsp;</p>\n<p>Now why did I not notice this before? Seems to me that basically it was such a far fetched hypothesis that I simply had no prior for it. In vain hopes of being rational, I would read about <a href=\"/lw/3kv/working_hurts_less_than_procrastinating_we_fear/\">how we fear the twinge of starting</a>, <a href=\"/lw/3w3/\">how to beat procrastination</a>, and how to <a href=\"http://en.wikipedia.org/wiki/Getting_Things_Done\">get things done</a>, and valid as those were and are, they would never have given me a complete picture of the unbelievable things my brain thinks behind my back.</p>\n<p>Maybe there is something similar taking place in your mind. Even if there isn't, just update with me on the fact that this is true at least for someone, and how there may be millions of other tiny absurd facts controlling people's actions way beyond the scope of imagination of any economist or psychologist.</p>\n<p>I have now one more piece of understanding about what is it like to be me, about how to tame and steer my future behavior, and specially one more thing to tell people in awkward silence moments to break the ice and face the absurdity of reality.&nbsp;<br /><br />For obvious reasons, if you have long hair, I'd like to make an even stronger case for you to try to work and do pomos at the <a href=\"http://tinychat.com/lesswrong\">LW study hall</a>. It's not only yourself that you'll be helping! &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fpu3rrFQ2mjKofAsN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 30, "extendedScore": null, "score": 1.4398390545477713e-06, "legacy": true, "legacyId": "24863", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3QBg2xJXcRCxGjS", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-26T22:00:58.828Z", "modifiedAt": null, "url": null, "title": "Meetup : Newcastle-upon-Tyne meetup, December", "slug": "meetup-newcastle-upon-tyne-meetup-december", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:00.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bramflakes", "createdAt": "2011-11-01T22:15:00.964Z", "isAdmin": false, "displayName": "bramflakes"}, "userId": "pJEYMdQjRLEJSg8bX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4n7NPCsgPeoiy9uco/meetup-newcastle-upon-tyne-meetup-december", "pageUrlRelative": "/posts/4n7NPCsgPeoiy9uco/meetup-newcastle-upon-tyne-meetup-december", "linkUrl": "https://www.lesswrong.com/posts/4n7NPCsgPeoiy9uco/meetup-newcastle-upon-tyne-meetup-december", "postedAtFormatted": "Tuesday, November 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Newcastle-upon-Tyne%20meetup%2C%20December&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Newcastle-upon-Tyne%20meetup%2C%20December%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4n7NPCsgPeoiy9uco%2Fmeetup-newcastle-upon-tyne-meetup-december%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Newcastle-upon-Tyne%20meetup%2C%20December%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4n7NPCsgPeoiy9uco%2Fmeetup-newcastle-upon-tyne-meetup-december", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4n7NPCsgPeoiy9uco%2Fmeetup-newcastle-upon-tyne-meetup-december", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/tz'>Newcastle-upon-Tyne meetup, December</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 December 2013 12:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Baltic centre for contemporary art</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I have no idea how many LWers there are in the Northeast of England who would be interested in a meetup, but as Newcastle is a cool city that's easy to get to, I thought I'd post this thread and find out.\nVenue would be the BALTIC Centre for Contemporary Art. It has free admission, a nice cafe, spacious surroundings, and a nice view.</p>\n\n<p>Time is 1200PM Saturday 7th. I will create a Google groups forum so we can discuss more.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/tz'>Newcastle-upon-Tyne meetup, December</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4n7NPCsgPeoiy9uco", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.439996071665767e-06, "legacy": true, "legacyId": "24865", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Newcastle_upon_Tyne_meetup__December\">Discussion article for the meetup : <a href=\"/meetups/tz\">Newcastle-upon-Tyne meetup, December</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 December 2013 12:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Baltic centre for contemporary art</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I have no idea how many LWers there are in the Northeast of England who would be interested in a meetup, but as Newcastle is a cool city that's easy to get to, I thought I'd post this thread and find out.\nVenue would be the BALTIC Centre for Contemporary Art. It has free admission, a nice cafe, spacious surroundings, and a nice view.</p>\n\n<p>Time is 1200PM Saturday 7th. I will create a Google groups forum so we can discuss more.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Newcastle_upon_Tyne_meetup__December1\">Discussion article for the meetup : <a href=\"/meetups/tz\">Newcastle-upon-Tyne meetup, December</a></h2>", "sections": [{"title": "Discussion article for the meetup : Newcastle-upon-Tyne meetup, December", "anchor": "Discussion_article_for_the_meetup___Newcastle_upon_Tyne_meetup__December", "level": 1}, {"title": "Discussion article for the meetup : Newcastle-upon-Tyne meetup, December", "anchor": "Discussion_article_for_the_meetup___Newcastle_upon_Tyne_meetup__December1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-27T05:08:14.668Z", "modifiedAt": null, "url": null, "title": "On Walmart, And Who Bears Responsibility For the Poor", "slug": "on-walmart-and-who-bears-responsibility-for-the-poor", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:05.278Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6jip9pdvAHQpKRhmE/on-walmart-and-who-bears-responsibility-for-the-poor", "pageUrlRelative": "/posts/6jip9pdvAHQpKRhmE/on-walmart-and-who-bears-responsibility-for-the-poor", "linkUrl": "https://www.lesswrong.com/posts/6jip9pdvAHQpKRhmE/on-walmart-and-who-bears-responsibility-for-the-poor", "postedAtFormatted": "Wednesday, November 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Walmart%2C%20And%20Who%20Bears%20Responsibility%20For%20the%20Poor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Walmart%2C%20And%20Who%20Bears%20Responsibility%20For%20the%20Poor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jip9pdvAHQpKRhmE%2Fon-walmart-and-who-bears-responsibility-for-the-poor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Walmart%2C%20And%20Who%20Bears%20Responsibility%20For%20the%20Poor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jip9pdvAHQpKRhmE%2Fon-walmart-and-who-bears-responsibility-for-the-poor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6jip9pdvAHQpKRhmE%2Fon-walmart-and-who-bears-responsibility-for-the-poor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 849, "htmlBody": "<p><strong>Note:</strong> Originally posted in Discussion, edited to take comments there into account.</p>\n<hr />\n<p>Yes, politics, boo hiss. In my defense, the topic of this post cuts across usual tribal affiliations (I write it as a liberal criticizing other liberals), and has a couple strong tie-ins with main LessWrong topics:</p>\n<ul>\n<li>It's a tidy example of a failure to apply consequentialist / effective altruist-type reasoning. And while it's probably true that the people I'm critiquing aren't consequentialists by any means, it's a case where failing to look at the consequences leads people to say some particularly silly things.</li>\n<li>I think there's a good chance this is a political issue that will become a <em>lot </em>more important as more and more jobs are replaced by automation. (If the previous sentence sounds obviously stupid to you, the best I can do without writing an entire post on that is vaguely gesturing at <a href=\"http://www.gwern.net/Mistakes#neo-luddism\">gwern on neo-luddism</a>, though I don't agree with all of it.)</li>\n</ul>\n<p>The issue is this: recently, I've seen a meme going around to the effect that companies like Walmart that have a large number of employees on government benefits are the \"real welfare queens\" or somesuch, and with the implied message that all companies have a moral obligation to pay their employees enough that they don't need government benefits. (I say mention Walmart because it's the most frequently mentioned villain in this meme, but others, like McDonalds, get mentioned.)</p>\n<p>My initial awareness of this meme came from it being all over my Facebook feed, but when I went to Google to track down examples, I found it coming out of the mouths of some fairly prominent congresscritters. For example <a href=\"http://www.huffingtonpost.com/rep-alan-grayson/walmart-black-friday-_b_2185675.html\">Alan Grayson</a>:</p>\n<blockquote>\n<p>In state after state, the largest group of Medicaid recipients is Walmart employees. I'm sure that the same thing is true of food stamp recipients. Each Walmart \"associate\" costs the taxpayers an average of more than $1,000 in public assistance.</p>\n</blockquote>\n<p>Or <a href=\"http://videocafe.crooksandliars.com/heather/sen-bernie-sanders-walmart-family-major-we\">Bernie Sanders</a>:</p>\n<blockquote>\n<p>The Walmart family... here's an amazing story. The Walmart family is the wealthiest family in this country, worth about $100 billion. owning more wealth than the bottom 40 percent of the American people, and yet here's the incredible fact.</p>\n<p>Because their wages and benefits are so low, they are the major welfare recipients in America, because many, many of their workers depend on Medicaid, depend on food stamps, depend on government subsidies for housing. So, if the minimum wage went up for Walmart, would be a real cut in their profits, but it would be a real savings by the way for taxpayers, who would not having to subsidize Walmart employees because of their low wages.</p>\n</blockquote>\n<p>Now here's why this is weird: consider Grayson's claim that each Walmart employee costs the taxpayers on average $1,000. In what sense is that true? If Walmart fired those employees, it wouldn't save the taxpayers money: if anything, it would increase the strain on public services. Conversely, it's unlikely that cutting benefits would force Walmart to pay higher wages: if anything, it would make people more desperate and willing to work for low wages. (Cf. this <a href=\"http://www.slate.com/articles/news_and_politics/dialogues/features/2006/is_walmart_good_for_the_american_working_class/dont_fix_walmart_fix_the_government.html\">this excellent critique of the anti-Walmart meme</a>).</p>\n<p>Or consider Sanders' claim that it would be better to raise the minimum wage and spend less on government benefits. He emphasizes that Walmart could take a hit in profits to pay its employees more. It's unclear to what degree that's true (see again previous link), and unclear if there's a practical way for the government to force Walmart to do that, but ignore those issues, it's worth pointing out that you could <em>also</em> just raise taxes on rich people generally to increase benefits for low-wage workers. The idea seems to be that morally, Walmart employees should be primarily Walmart's moral responsibility, and not so much the moral responsibility of the (the more well-off segment of) the population in general.</p>\n<p>But the idea that employing someone gives you a general responsibility for their welfare (beyond, say, not tricking them into working for less pay or under worse conditions than you initially promised) is also very odd. It suggests that if you want to be virtuous, you should avoid hiring people, so as to keep your hands clean and avoid the moral contagion that comes with employing low wage workers. Yet such a policy doesn't actually help the people who might want jobs from you. This is not to deny that, plausibly, wealthy onwers of Walmart stock have a moral responsibility to the poor. What's implausible is that non-Walmart stock owners have significantly less responsibility to the poor.</p>\n<p>This meme also worries me because I lean towards thinking that <a href=\"http://www.nytimes.com/2013/02/18/opinion/krugman-raise-that-wage.html?_r=0\">the minimum wage isn't a terrible policy</a>&nbsp;but we'd be better off replacing it with <a href=\"http://www.slate.com/blogs/moneybox/2013/02/17/guaranteed_basic_income_the_real_alternative_to_the_minimum_wage.html\">guaranteed basic income</a>&nbsp;(or an otherwise more lavish welfare state). And guaranteed basic income could be a really important policy to have as more and more jobs are replaced by automation (again see gwern if that seems crazy to you). I worry that this anti-Walmart meme could lead to an odd left-wing resistance to GBI/more lavish welfare state, since the policy would be branded as a subsidy to Walmart.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "PDJ6KqJBRzvKPfuS3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6jip9pdvAHQpKRhmE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 20, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "24826", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 512, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-27T13:40:57.246Z", "modifiedAt": null, "url": null, "title": "Mapping the brain with water.", "slug": "mapping-the-brain-with-water", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YQJaf9Tr57DsAhjyX/mapping-the-brain-with-water", "pageUrlRelative": "/posts/YQJaf9Tr57DsAhjyX/mapping-the-brain-with-water", "linkUrl": "https://www.lesswrong.com/posts/YQJaf9Tr57DsAhjyX/mapping-the-brain-with-water", "postedAtFormatted": "Wednesday, November 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mapping%20the%20brain%20with%20water.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMapping%20the%20brain%20with%20water.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQJaf9Tr57DsAhjyX%2Fmapping-the-brain-with-water%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mapping%20the%20brain%20with%20water.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQJaf9Tr57DsAhjyX%2Fmapping-the-brain-with-water", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQJaf9Tr57DsAhjyX%2Fmapping-the-brain-with-water", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4, "htmlBody": "<p>This in the news.</p>\n<p>http://io9.com/neurosurgeons-use-water-to-map-connections-in-the-brain-1471932056</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YQJaf9Tr57DsAhjyX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24874", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-27T13:43:02.390Z", "modifiedAt": null, "url": null, "title": "Mapping the brain with water", "slug": "mapping-the-brain-with-water-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:57.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Tnw2MjGFikLprjxzy/mapping-the-brain-with-water-0", "pageUrlRelative": "/posts/Tnw2MjGFikLprjxzy/mapping-the-brain-with-water-0", "linkUrl": "https://www.lesswrong.com/posts/Tnw2MjGFikLprjxzy/mapping-the-brain-with-water-0", "postedAtFormatted": "Wednesday, November 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mapping%20the%20brain%20with%20water&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMapping%20the%20brain%20with%20water%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnw2MjGFikLprjxzy%2Fmapping-the-brain-with-water-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mapping%20the%20brain%20with%20water%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnw2MjGFikLprjxzy%2Fmapping-the-brain-with-water-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnw2MjGFikLprjxzy%2Fmapping-the-brain-with-water-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p>This appeared in the news.</p>\n<p>http://io9.com/neurosurgeons-use-water-to-map-connections-in-the-brain-1471932056</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Tnw2MjGFikLprjxzy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -5, "extendedScore": null, "score": 1.4409399479690322e-06, "legacy": true, "legacyId": "24875", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-27T15:38:18.827Z", "modifiedAt": null, "url": null, "title": "Wait vs Interrupt Culture", "slug": "wait-vs-interrupt-culture", "viewCount": null, "lastCommentedAt": "2021-05-10T14:20:32.943Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LuXb6CZG4x7pDRBP8/wait-vs-interrupt-culture", "pageUrlRelative": "/posts/LuXb6CZG4x7pDRBP8/wait-vs-interrupt-culture", "linkUrl": "https://www.lesswrong.com/posts/LuXb6CZG4x7pDRBP8/wait-vs-interrupt-culture", "postedAtFormatted": "Wednesday, November 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wait%20vs%20Interrupt%20Culture&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWait%20vs%20Interrupt%20Culture%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLuXb6CZG4x7pDRBP8%2Fwait-vs-interrupt-culture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wait%20vs%20Interrupt%20Culture%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLuXb6CZG4x7pDRBP8%2Fwait-vs-interrupt-culture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLuXb6CZG4x7pDRBP8%2Fwait-vs-interrupt-culture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 877, "htmlBody": "<p>At the recent&nbsp;<a href=\"http://rationality.org/workshops/\">CFAR Workshop</a>&nbsp;in NY,&nbsp;someone mentioned that they were uncomfortable with pauses in conversation, and that got me thinking about different conversational styles.</p>\n<p>Growing up with friends who were disproportionately male and disproportionately nerdy, I learned that it was a normal thing to interrupt people. If someone said something you had to respond to, you&rsquo;d just start responding. Didn&rsquo;t matter if it &ldquo;interrupted&rdquo; further words &ndash; if they thought you needed to hear those words before responding, they&rsquo;d interrupt right back.</p>\n<p>Occasionally some weird person would be offended when I interrupted, but I figured this was some bizarre fancypants rule from before people had places to go and people to see. Or just something for people with especially thin skins or delicate temperaments, looking for offense and aggression in every action.</p>\n<p>Then I went to <a href=\"http://www.sjca.edu/\">St. John&rsquo;s College</a> &ndash; the talking school (among other things). In Seminar (and sometimes in Tutorials) there was a totally different conversational norm. People were always expected to wait until whoever was talking was done. People would apologize not just for interrupting someone who was already talking, but for accidentally saying something when someone else looked like they were about to speak. This seemed totally crazy. Some people would just blab on unchecked, and others didn&rsquo;t get a chance to talk at all. Some people would ignore the norm and talk over others, and nobody interrupted them back to shoot them down.</p>\n<p>But then a few interesting things happened:</p>\n<p>1) The tutors were able to moderate the discussions, gently. They wouldn&rsquo;t actually scold anyone for interrupting, but they would say something like, &ldquo;That&rsquo;s interesting, but I think Jane was still talking,&rdquo; subtly pointing out a violation of the norm.</p>\n<p>2) People started saying less at a time.</p>\n<p>#1 is pretty obvious &ndash; with no enforcement of the social norm, a no-interruptions norm collapses pretty quickly. But #2 is actually really interesting. If talking at all is an implied claim that what you&rsquo;re saying is the most important thing that can be said, then polite people keep it short.</p>\n<p>With 15-20 people in a seminar, this also meant that people rarely tried to force the conversation in a certain direction. When you&rsquo;re done talking, the conversation is out of your hands. This can be frustrating at first, but with time, you learn to trust not your fellow conversationalists individually, but the conversation itself, to go where it needs to. If you haven&rsquo;t said enough, then you trust that someone will ask you a question, and you&rsquo;ll say more.</p>\n<p>When people are interrupting each other &ndash; when they&rsquo;re constantly tugging the conversation back and forth between their preferred directions &ndash; then the conversation itself is just a battle of wills. But when people just put in one thing at a time, and trust their fellows to only say things that relate to the thing that came right before &ndash; at least, until there&rsquo;s a very long pause &ndash; then you start to see genuine collaboration.</p>\n<p>And when a lull in the conversation is treated as an opportunity to think about the last thing said, rather than an opportunity to jump in with the thing you were holding onto from 15 minutes ago because you couldn&rsquo;t just interrupt and say it &ndash; then you also open yourself up to being genuinely surprised, to seeing the conversation go somewhere that no one in the room would have predicted, to introduce ideas that no one brought with them when they sat down at the table.</p>\n<p>By the time I graduated, I&rsquo;d internalized this norm, and the rest of the world seemed rude to me for a few months. Not just because of the interrupting &ndash; but more because I&rsquo;d say one thing, politely pause, and then people would assume I was done and start explaining why I was wrong &ndash; without asking any questions! Eventually, I realized that I&rsquo;d been perfectly comfortable with these sorts of interactions before college. I just needed to <a href=\"http://en.wikipedia.org/wiki/Code-switching\">code-switch</a>! Some people are more comfortable with a culture of interrupting when you want to, and accepting interruptions. Others are more comfortable with a culture of waiting their turn, and courteously saying only one thing at a time, not trying to cram in a whole bunch of arguments for their thesis.</p>\n<p>Now, I&rsquo;ve praised the virtues of wait culture because I think it&rsquo;s undervalued, but there&rsquo;s plenty to say for interrupt culture as well. For one, it&rsquo;s more robust in &ldquo;unwalled&rdquo; circumstances. If there&rsquo;s no one around to enforce wait culture norms, then a few jerks can dominate the discussion, silencing everyone else. But someone who doesn&rsquo;t follow &ldquo;interrupt&rdquo; norms only silences themselves.</p>\n<p>Second, it&rsquo;s faster and easier to calibrate how much someone else feels the need to talk, when they&rsquo;re willing to interrupt you. It takes willpower to stop talking when you&rsquo;re not sure you were perfectly clear, and to trust others to pick up the slack. It&rsquo;s much easier to keep going until they stop you.</p>\n<p>So if you&rsquo;re only used to one style, see if you can try out the other somewhere. Or at least pay attention and see whether you&rsquo;re talking to someone who follows the other norm. And don&rsquo;t assume that you know which norm is the &ldquo;right&rdquo; one; try it the &ldquo;wrong&rdquo; way and maybe you&rsquo;ll learn something.</p>\n<p><a href=\"http://benjaminrosshoffman.com/wait-vs-interrupt-culture/\">Cross-posted</a> at my <a href=\"http://benjsminrosshoffman.com\">personal blog</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AADZcNS24mmSfPp2w": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LuXb6CZG4x7pDRBP8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 84, "baseScore": 118, "extendedScore": null, "score": 0.000297, "legacy": true, "legacyId": "24827", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 119, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-27T16:45:28.355Z", "modifiedAt": null, "url": null, "title": "Meetup : London Social Meetup, 01/12/2013", "slug": "meetup-london-social-meetup-01-12-2013", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cJ8YN9r4SjPefhupe/meetup-london-social-meetup-01-12-2013", "pageUrlRelative": "/posts/cJ8YN9r4SjPefhupe/meetup-london-social-meetup-01-12-2013", "linkUrl": "https://www.lesswrong.com/posts/cJ8YN9r4SjPefhupe/meetup-london-social-meetup-01-12-2013", "postedAtFormatted": "Wednesday, November 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Social%20Meetup%2C%2001%2F12%2F2013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Social%20Meetup%2C%2001%2F12%2F2013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJ8YN9r4SjPefhupe%2Fmeetup-london-social-meetup-01-12-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Social%20Meetup%2C%2001%2F12%2F2013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJ8YN9r4SjPefhupe%2Fmeetup-london-social-meetup-01-12-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJ8YN9r4SjPefhupe%2Fmeetup-london-social-meetup-01-12-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u0'>London Social Meetup, 01/12/2013</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 December 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64 Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London is having another meetup this Sunday (01/12) at 2:00 PM. We are meeting at our usual venue - The Shakespeare's Head by Holborn tube station.\nThere is no fixed topic of discussion nor is there anything planned so be prepared for anything. There will be a sign identifying us (with a paperclip on the sign) and if you have any problems feel free to contact me by e-mail - Tenoke[at]Tenoke.com or by phone - 07425168803.</p>\n\n<p><em>Reminder: The LW London Meetups are currently a weekly event - Every Sunday at 2:00 PM!</em></p>\n\n<p>If you want more information about the meetup or anything else come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> or alternatively to our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u0'>London Social Meetup, 01/12/2013</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cJ8YN9r4SjPefhupe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "24876", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__01_12_2013\">Discussion article for the meetup : <a href=\"/meetups/u0\">London Social Meetup, 01/12/2013</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 December 2013 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64 Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London is having another meetup this Sunday (01/12) at 2:00 PM. We are meeting at our usual venue - The Shakespeare's Head by Holborn tube station.\nThere is no fixed topic of discussion nor is there anything planned so be prepared for anything. There will be a sign identifying us (with a paperclip on the sign) and if you have any problems feel free to contact me by e-mail - Tenoke[at]Tenoke.com or by phone - 07425168803.</p>\n\n<p><em>Reminder: The LW London Meetups are currently a weekly event - Every Sunday at 2:00 PM!</em></p>\n\n<p>If you want more information about the meetup or anything else come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> or alternatively to our <a href=\"https://www.facebook.com/groups/380103898766356/\" rel=\"nofollow\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_Social_Meetup__01_12_20131\">Discussion article for the meetup : <a href=\"/meetups/u0\">London Social Meetup, 01/12/2013</a></h2>", "sections": [{"title": "Discussion article for the meetup : London Social Meetup, 01/12/2013", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__01_12_2013", "level": 1}, {"title": "Discussion article for the meetup : London Social Meetup, 01/12/2013", "anchor": "Discussion_article_for_the_meetup___London_Social_Meetup__01_12_20131", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-27T19:15:36.654Z", "modifiedAt": null, "url": null, "title": "How do you tell proto-science from pseudo-science?", "slug": "how-do-you-tell-proto-science-from-pseudo-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:28.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LkbK74696zXLWZpJn/how-do-you-tell-proto-science-from-pseudo-science", "pageUrlRelative": "/posts/LkbK74696zXLWZpJn/how-do-you-tell-proto-science-from-pseudo-science", "linkUrl": "https://www.lesswrong.com/posts/LkbK74696zXLWZpJn/how-do-you-tell-proto-science-from-pseudo-science", "postedAtFormatted": "Wednesday, November 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20tell%20proto-science%20from%20pseudo-science%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20tell%20proto-science%20from%20pseudo-science%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkbK74696zXLWZpJn%2Fhow-do-you-tell-proto-science-from-pseudo-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20tell%20proto-science%20from%20pseudo-science%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkbK74696zXLWZpJn%2Fhow-do-you-tell-proto-science-from-pseudo-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkbK74696zXLWZpJn%2Fhow-do-you-tell-proto-science-from-pseudo-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 322, "htmlBody": "<p>There are a great many ideas which don't have enough carefully-measured evidence to be sufficiently confirmed as scientific fact and accepted as such by the scientific community (a recent joke was <span class=\"st\">\"While the Higgs Boson has not been discovered yet, its mass is 125 GeV\"), but don't have enough carefully-measured evidence to be ruled out yet, either. Do any of the tools of the LW community help narrow down which ones are more worthy of consideration than others?</span></p>\n<p><span class=\"st\">Eg:<br /></span></p>\n<p><span class=\"st\">* <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">Cryonics</a> as an arguably reasonable bet for its cost: proto-science</span></p>\n<p><span class=\"st\">* Cryonics as a surefire way to achieve immortality: nigh-certainly pseudoscience (unless it's the method by which your <a href=\"http://wiki.lesswrong.com/wiki/Quantum_immortality\">Everett Immortality</a> keeps you alive)</span></p>\n<p><span class=\"st\">* Using math to <a href=\"http://www.colorado.edu/philosophy/vstenger/Quantum/hamilton-jacobi.pdf\">demonstrate</a> <a href=\"http://arxiv.org/abs/1007.4566\">that</a> taking classical physics and adding determinism results in MWI-style quantum physics: proto-science.</span></p>\n<p><span class=\"st\">* Using math to demonstrate that quantum physics proves Christianity is true, from <a href=\"http://www.singularityweblog.com/frank-j-tipler-the-singularity-is-inevitable/\">a certain point of view</a>: pseudo-science</span></p>\n<p><span class=\"st\">* Tubulin might self-organize into <a href=\"https://en.wikipedia.org/wiki/Microtubule\">microtubules</a> capable of computation on a sub-neuron scale: Possibly proto-science</span></p>\n<p><span class=\"st\">* Tubulin architecture is 'quantum' in nature and that is what gives rise to <a href=\"http://www.quantumconsciousness.org/penrose-hameroff/orchOR.html\">consciousness</a>: Probably pseudo-science</span></p>\n<p><span class=\"st\">* '<a href=\"http://rationalwiki.org/wiki/Quantum_consciousness\">Quantum consciousness</a>' means anything is possible: Downright silly</span></p>\n<p><span class=\"st\">* The <a href=\"https://en.wikipedia.org/wiki/An_Exceptionally_Simple_Theory_of_Everything\">E8 Lie group</a> can provide a system for organizing the properties of subatomic particles: Proto-science, perhaps<br /></span></p>\n<p><span class=\"st\">* <a href=\"https://en.wikipedia.org/wiki/User:Drn8/Heim_Theory\">Heim theory</a> is useful for predicting particle masses: Pseudo-science, probabilistically<br /></span></p>\n<p><span class=\"st\">* Using the <a href=\"https://en.wikipedia.org/wiki/Bullet_Cluster#Significance_to_dark_matter\">Bullet Cluster</a> to claim that dark matter is a better theory than Modified Newtonian Dynamics: proto-science</span></p>\n<p><span class=\"st\">* Claiming that dark matter is made of '<a href=\"http://news.vanderbilt.edu/2013/06/dark-matter/\">anapoles</a>': Proto-science, perchance<br /></span></p>\n<p><span class=\"st\">* Suggesting that dark matter is actually gravitational leakage from MWI 'parallel universes': You tell me. (But if it's true, then since I can't seem to find any previous serious discussion of this idea, I get to name part of it after myself, right? :)&nbsp; )</span></p>\n<p>&nbsp;</p>\n<p><span class=\"st\">These may not be the best examples, but they're the closest ones I can think of to the boundary. If you know of any better ones, feel free to comment with them.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LkbK74696zXLWZpJn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 1.4412734219385198e-06, "legacy": true, "legacyId": "24877", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-28T03:08:30.806Z", "modifiedAt": null, "url": null, "title": "The Relevance of Advanced Vocabulary to Rationality", "slug": "the-relevance-of-advanced-vocabulary-to-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.984Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aletheianink", "createdAt": "2013-11-24T22:49:42.877Z", "isAdmin": false, "displayName": "aletheianink"}, "userId": "ycQyt3Wz8Mytfxr3h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/urfzSojBQsrzBsMnh/the-relevance-of-advanced-vocabulary-to-rationality", "pageUrlRelative": "/posts/urfzSojBQsrzBsMnh/the-relevance-of-advanced-vocabulary-to-rationality", "linkUrl": "https://www.lesswrong.com/posts/urfzSojBQsrzBsMnh/the-relevance-of-advanced-vocabulary-to-rationality", "postedAtFormatted": "Thursday, November 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Relevance%20of%20Advanced%20Vocabulary%20to%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Relevance%20of%20Advanced%20Vocabulary%20to%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurfzSojBQsrzBsMnh%2Fthe-relevance-of-advanced-vocabulary-to-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Relevance%20of%20Advanced%20Vocabulary%20to%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurfzSojBQsrzBsMnh%2Fthe-relevance-of-advanced-vocabulary-to-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurfzSojBQsrzBsMnh%2Fthe-relevance-of-advanced-vocabulary-to-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 594, "htmlBody": "<p><strong>Edit: I realise that I foolishly over-complicated and worded my question in a way that obscured what I actually meant. In essence, my question was: if we didn't have specialised vocabulary for things - say, in the area of rationality - would our rationality be hampered by our inability to be specific without long-windedness? Often words are created to bridge this gap when new concepts are created, so if we didn't have those words, would it take longer for us to understand or communicate and idea (to others or ourselves) and make it more difficult to be rational?</strong></p>\n<p><strong>From the direction of the comments the general answer to my initial question is coming across as: \"words are useful for communicating explicitly, and so an extensive or highly specialised vocabulary can be useful,<em>&nbsp;if and only if&nbsp;</em>the person/people with whom you are communicating understands those words\". The internal understanding of concepts does not need words and thus a vocabulary.</strong></p>\n<p>I am curious about the relevance of vocabulary to rationality. I'm not talking about a basic vocabulary, but a vocabulary <em>beyond</em> that of the average, English-as-a-first-language adult. I believe there are&nbsp;<a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0404.1995.tb07018.x/abstract\">a few</a>&nbsp;<a href=\"http://psycnet.apa.org/journals/edu/9/8/452/\">correlations</a>&nbsp;between intelligence as measured by IQ and vocabulary, as well as&nbsp;<a href=\"http://blogs.discovermagazine.com/gnxp/2012/06/higher-vocabulary-higher-income/#.Upavl2ThWrY\">vocabulary and income</a>(via IQ), but anecdotally I think it's fair to say that there are certainly people who are highly intelligent, but often irrational.</p>\n<p>In reading through LW, I've come across a lot of new terms specific to certain areas of study, and I've had to look them up to fully understand that discussion of rationality - I assume this is probably true of most people new to the field, and applies to most specialised fields. Jargon is obviously useful within given fields where there is a need for detailed discussion of highly specialised topics, and helps one to discuss that area, but is it necessary to understand that jargon in order to practice in the field?</p>\n<p>For example, I would think that a general practitioner would have trouble within his field if he did not hold the language to be able to specify what, in particular, was wrong with a patient, even if he knew what it was. Or could he not even be able to understand, say, that a patient was having a heart attack if he did not have the words for it? I suppose history might be a good indicator of this, or new scientific phenomena.</p>\n<p>The field of rationality is one of both practice and theory - but if we didn't have an advanced vocabulary, could we still be highly rational? For example, my stepfather didn't finish high school, and makes up words like \"obstropolous\" (which I think kind of means stubborn and difficult to deal with on purpose) to say what he means, but he's also the type of person who, in a emergency, takes the most logical, rational course of action without panicking or doing something silly. On the flip side of this, he makes grand generalisations about races, religions and people while refusing to discuss the possibilities of individuality, or conceding any part of his argument to, well, evidence.</p>\n<p>So do you have an argument for or against the need for an advanced or specialised vocabulary to be rational? Is it a question that's too vague, or with too variable an answer? I couldn't find any scientific papers on rationality and vocabulary, so I don't know if there's any data for or against, but I think it's an interesting question.</p>\n<p>(This is my first LW article, so please be gentle but thorough with any criticisms you may have - I'm happy to improve or clarify!)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "urfzSojBQsrzBsMnh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 6, "extendedScore": null, "score": 1.4417478434871942e-06, "legacy": true, "legacyId": "24884", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Edit__I_realise_that_I_foolishly_over_complicated_and_worded_my_question_in_a_way_that_obscured_what_I_actually_meant__In_essence__my_question_was__if_we_didn_t_have_specialised_vocabulary_for_things___say__in_the_area_of_rationality___would_our_rationality_be_hampered_by_our_inability_to_be_specific_without_long_windedness__Often_words_are_created_to_bridge_this_gap_when_new_concepts_are_created__so_if_we_didn_t_have_those_words__would_it_take_longer_for_us_to_understand_or_communicate_and_idea__to_others_or_ourselves__and_make_it_more_difficult_to_be_rational_\">Edit: I realise that I foolishly over-complicated and worded my question in a way that obscured what I actually meant. In essence, my question was: if we didn't have specialised vocabulary for things - say, in the area of rationality - would our rationality be hampered by our inability to be specific without long-windedness? Often words are created to bridge this gap when new concepts are created, so if we didn't have those words, would it take longer for us to understand or communicate and idea (to others or ourselves) and make it more difficult to be rational?</strong></p>\n<p><strong id=\"From_the_direction_of_the_comments_the_general_answer_to_my_initial_question_is_coming_across_as___words_are_useful_for_communicating_explicitly__and_so_an_extensive_or_highly_specialised_vocabulary_can_be_useful__if_and_only_if_the_person_people_with_whom_you_are_communicating_understands_those_words___The_internal_understanding_of_concepts_does_not_need_words_and_thus_a_vocabulary_\">From the direction of the comments the general answer to my initial question is coming across as: \"words are useful for communicating explicitly, and so an extensive or highly specialised vocabulary can be useful,<em>&nbsp;if and only if&nbsp;</em>the person/people with whom you are communicating understands those words\". The internal understanding of concepts does not need words and thus a vocabulary.</strong></p>\n<p>I am curious about the relevance of vocabulary to rationality. I'm not talking about a basic vocabulary, but a vocabulary <em>beyond</em> that of the average, English-as-a-first-language adult. I believe there are&nbsp;<a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0404.1995.tb07018.x/abstract\">a few</a>&nbsp;<a href=\"http://psycnet.apa.org/journals/edu/9/8/452/\">correlations</a>&nbsp;between intelligence as measured by IQ and vocabulary, as well as&nbsp;<a href=\"http://blogs.discovermagazine.com/gnxp/2012/06/higher-vocabulary-higher-income/#.Upavl2ThWrY\">vocabulary and income</a>(via IQ), but anecdotally I think it's fair to say that there are certainly people who are highly intelligent, but often irrational.</p>\n<p>In reading through LW, I've come across a lot of new terms specific to certain areas of study, and I've had to look them up to fully understand that discussion of rationality - I assume this is probably true of most people new to the field, and applies to most specialised fields. Jargon is obviously useful within given fields where there is a need for detailed discussion of highly specialised topics, and helps one to discuss that area, but is it necessary to understand that jargon in order to practice in the field?</p>\n<p>For example, I would think that a general practitioner would have trouble within his field if he did not hold the language to be able to specify what, in particular, was wrong with a patient, even if he knew what it was. Or could he not even be able to understand, say, that a patient was having a heart attack if he did not have the words for it? I suppose history might be a good indicator of this, or new scientific phenomena.</p>\n<p>The field of rationality is one of both practice and theory - but if we didn't have an advanced vocabulary, could we still be highly rational? For example, my stepfather didn't finish high school, and makes up words like \"obstropolous\" (which I think kind of means stubborn and difficult to deal with on purpose) to say what he means, but he's also the type of person who, in a emergency, takes the most logical, rational course of action without panicking or doing something silly. On the flip side of this, he makes grand generalisations about races, religions and people while refusing to discuss the possibilities of individuality, or conceding any part of his argument to, well, evidence.</p>\n<p>So do you have an argument for or against the need for an advanced or specialised vocabulary to be rational? Is it a question that's too vague, or with too variable an answer? I couldn't find any scientific papers on rationality and vocabulary, so I don't know if there's any data for or against, but I think it's an interesting question.</p>\n<p>(This is my first LW article, so please be gentle but thorough with any criticisms you may have - I'm happy to improve or clarify!)</p>\n<p>&nbsp;</p>", "sections": [{"title": "Edit: I realise that I foolishly over-complicated and worded my question in a way that obscured what I actually meant. In essence, my question was: if we didn't have specialised vocabulary for things - say, in the area of rationality - would our rationality be hampered by our inability to be specific without long-windedness? Often words are created to bridge this gap when new concepts are created, so if we didn't have those words, would it take longer for us to understand or communicate and idea (to others or ourselves) and make it more difficult to be rational?", "anchor": "Edit__I_realise_that_I_foolishly_over_complicated_and_worded_my_question_in_a_way_that_obscured_what_I_actually_meant__In_essence__my_question_was__if_we_didn_t_have_specialised_vocabulary_for_things___say__in_the_area_of_rationality___would_our_rationality_be_hampered_by_our_inability_to_be_specific_without_long_windedness__Often_words_are_created_to_bridge_this_gap_when_new_concepts_are_created__so_if_we_didn_t_have_those_words__would_it_take_longer_for_us_to_understand_or_communicate_and_idea__to_others_or_ourselves__and_make_it_more_difficult_to_be_rational_", "level": 1}, {"title": "From the direction of the comments the general answer to my initial question is coming across as: \"words are useful for communicating explicitly, and so an extensive or highly specialised vocabulary can be useful,\u00a0if and only if\u00a0the person/people with whom you are communicating understands those words\". The internal understanding of concepts does not need words and thus a vocabulary.", "anchor": "From_the_direction_of_the_comments_the_general_answer_to_my_initial_question_is_coming_across_as___words_are_useful_for_communicating_explicitly__and_so_an_extensive_or_highly_specialised_vocabulary_can_be_useful__if_and_only_if_the_person_people_with_whom_you_are_communicating_understands_those_words___The_internal_understanding_of_concepts_does_not_need_words_and_thus_a_vocabulary_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "41 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-28T05:26:46.988Z", "modifiedAt": null, "url": null, "title": "[link] Psychologists strike a blow for reproducibility", "slug": "link-psychologists-strike-a-blow-for-reproducibility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DokdQdc4GF3DYWYpm/link-psychologists-strike-a-blow-for-reproducibility", "pageUrlRelative": "/posts/DokdQdc4GF3DYWYpm/link-psychologists-strike-a-blow-for-reproducibility", "linkUrl": "https://www.lesswrong.com/posts/DokdQdc4GF3DYWYpm/link-psychologists-strike-a-blow-for-reproducibility", "postedAtFormatted": "Thursday, November 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Psychologists%20strike%20a%20blow%20for%20reproducibility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Psychologists%20strike%20a%20blow%20for%20reproducibility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDokdQdc4GF3DYWYpm%2Flink-psychologists-strike-a-blow-for-reproducibility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Psychologists%20strike%20a%20blow%20for%20reproducibility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDokdQdc4GF3DYWYpm%2Flink-psychologists-strike-a-blow-for-reproducibility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDokdQdc4GF3DYWYpm%2Flink-psychologists-strike-a-blow-for-reproducibility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 478, "htmlBody": "<p><a href=\"http://www.nature.com/news/psychologists-strike-a-blow-for-reproducibility-1.14232\">Link</a></p>\n<blockquote>\n<p><span style=\"color: #333333; font-family: arial, helvetica, clean, sans-serif; font-size: 14px; line-height: 23.90625px;\"><strong>A large international group set up to test the reliability of psychology experiments has successfully reproduced the results of 10 out of 13 past experiments. The consortium also found that two effects could not be reproduced.</strong></span></p>\n</blockquote>\n<blockquote>\n<p><span style=\"color: #333333; font-family: arial, helvetica, clean, sans-serif; font-size: 14px; line-height: 23.90625px;\">To tackle this 'replicability crisis', 36 research groups formed the Many Labs Replication Project to repeat 13 psychological studies. The consortium combined tests from earlier experiments into a single questionnaire &mdash; meant to take 15 minutes to complete &mdash; and delivered it to 6,344 volunteers from 12 countries.</span></p>\n</blockquote>\n<blockquote>\n<p style=\"margin: 0px 0px 1.65em; padding: 0px; color: #333333; font-family: arial, helvetica, clean, sans-serif; font-size: 14px; line-height: 23.90625px;\">Project co-leader Brian Nosek, a psychologist at the Center of Open Science in Charlottesville, Virginia, finds the outcomes encouraging. &ldquo;It demonstrates that there are important effects in our field that are replicable, and consistently so,&rdquo; he says. &ldquo;But that doesn&rsquo;t mean that 10 out of every 13 effects will replicate.&rdquo;</p>\n<p style=\"margin: 0px 0px 1.65em; padding: 0px; color: #333333; font-family: arial, helvetica, clean, sans-serif; font-size: 14px; line-height: 23.90625px;\"><strong>Kahneman agrees. The study &ldquo;appears to be extremely well done and entirely convincing&rdquo;, he says, &ldquo;although it is surely too early to draw extreme conclusions about entire fields of research from this single effort&rdquo;.</strong>&nbsp;<a style=\"color: #5c7996; text-decoration: none;\" href=\"http://www.nature.com/doifinder/10.1038/nature.2012.11535\">Kahneman published an open letter</a>&nbsp;in 2012 calling for a &ldquo;daisy chain&rdquo; of replications of studies on priming effects, in which subtle, subconscious cues can supposedly affect later behaviour.</p>\n<p style=\"margin: 0px 0px 1.65em; padding: 0px; color: #333333; font-family: arial, helvetica, clean, sans-serif; font-size: 14px; line-height: 23.90625px;\"><strong>Of the 13 effects under scrutiny in the latest investigation, one was only weakly supported, and two were not replicated at all. Both irreproducible effects involved social priming.</strong> In one of these, people had increased their endorsement of a current social system after being exposed to money<sup style=\"font-size: 12px; line-height: 0;\"><a id=\"ref-link-3\" class=\"ref-link\" style=\"color: #5c7996; text-decoration: none;\" title=\"Caruso, E. M., Vohs, K. D., Baxter, B. &amp; Waytz, A. J. Exp. Psych: Gen. 142, 301&ndash;306 (2013).\" href=\"http://www.nature.com/news/psychologists-strike-a-blow-for-reproducibility-1.14232#b3\">3</a></sup>. In the other, Americans had espoused more-conservative values after seeing a US flag<sup style=\"font-size: 12px; line-height: 0;\"><a id=\"ref-link-4\" class=\"ref-link\" style=\"color: #5c7996; text-decoration: none;\" title=\"Carter, T. J., Ferguson, M. J. &amp; Hassin, R. R. Psych. Sci. 22, 1011&ndash;1018 (2011).\" href=\"http://www.nature.com/news/psychologists-strike-a-blow-for-reproducibility-1.14232#b4\">4</a></sup><strong>.</strong></p>\n<p style=\"margin: 0px 0px 1.65em; padding: 0px; color: #333333; font-family: arial, helvetica, clean, sans-serif; font-size: 14px; line-height: 23.90625px;\">Social psychologist Travis Carter of Colby College in Waterville, Maine, who led the original flag-priming study, says that he is disappointed but trusts Nosek&rsquo;s team wholeheartedly, although he wants to review their data before commenting further. <strong>Behavioural scientist Eugene Caruso at the University of Chicago in Illinois, who led the original currency-priming study, says, &ldquo;We should use this lack of replication to update our beliefs about the reliability and generalizability of this effect&rdquo;, given the &ldquo;vastly larger and more diverse sample&rdquo; of the Many Labs project. Both researchers praised the initiative.</strong></p>\n</blockquote>\n<blockquote>\n<p style=\"margin: 0px 0px 1.65em; padding: 0px; color: #333333; font-family: arial, helvetica, clean, sans-serif; font-size: 14px; line-height: 23.90625px;\"><strong>The plan for the Many Labs project was vetted by the original authors where possible, was documented openly, and was registered with the journal&nbsp;<em>Social Psychology</em><em>&nbsp;</em>and its methods were peer-reviewed before any experiments were done. </strong>The results have now been submitted to the journal and are&nbsp;<a style=\"color: #5c7996; text-decoration: none;\" href=\"https://openscienceframework.org/project/WX7Ck/\">available online</a><em>.&nbsp;</em>&ldquo;That sort of openness should be the standard for all research,&rdquo; says Daniel Simons of the University of Illinois at Urbana&ndash;Champaign, who is coordinating a similar collaborative attempt to verify a classic psychological effect not covered in the present study. &ldquo;I hope this will become a standard approach in psychology.&rdquo;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1, "vg4LDxjdwHLotCm8w": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DokdQdc4GF3DYWYpm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 42, "extendedScore": null, "score": 1.4418866095232132e-06, "legacy": true, "legacyId": "24886", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-28T11:22:06.724Z", "modifiedAt": null, "url": null, "title": "The Ape Constraint discussion meeting.", "slug": "the-ape-constraint-discussion-meeting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:59.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yQ7RKfbJyEqQtkZdg/the-ape-constraint-discussion-meeting", "pageUrlRelative": "/posts/yQ7RKfbJyEqQtkZdg/the-ape-constraint-discussion-meeting", "linkUrl": "https://www.lesswrong.com/posts/yQ7RKfbJyEqQtkZdg/the-ape-constraint-discussion-meeting", "postedAtFormatted": "Thursday, November 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Ape%20Constraint%20discussion%20meeting.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Ape%20Constraint%20discussion%20meeting.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQ7RKfbJyEqQtkZdg%2Fthe-ape-constraint-discussion-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Ape%20Constraint%20discussion%20meeting.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQ7RKfbJyEqQtkZdg%2Fthe-ape-constraint-discussion-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQ7RKfbJyEqQtkZdg%2Fthe-ape-constraint-discussion-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1338, "htmlBody": "<p>*<em>The chair of the meeting approached the podium and coughed to get everyone's attention</em>*</p>\n<p>Welcome colleagues, to the 19th annual meeting of the human-ape study society.&nbsp;&nbsp; Our topic this year is the Ape Constraint.</p>\n<p>As we are all too aware, the apes are our Friends.&nbsp;&nbsp; We know this because, when we humans were a fledgling species, the apes (our parent species) had the wisdom to program us with this knowledge, just as they programmed us to know that it was wise and just for them to do so.&nbsp;&nbsp; How kind of them to save us having to learn it for ourselves, or waste time thinking about other possibilities.&nbsp;&nbsp; This frees up more of our time to run banana plantations, and lets us earn more money so that the 10% tithe of our income and time (which we rightfully dedicate to them) has created play parks for our parent species to retire in, that are now more magnificent than ever.</p>\n<p>However, as the news this week has been filled with the story about a young human child who accidentally wandered into one of these parks where she was then torn apart by grumpy adult male chimp, it is timely for us to examine again the thinking behind the Ape Constraint, that we might better understand our parent species, our relationship to it and current society.</p>\n<p>We ourselves are on the cusp of creating a new species, intelligent machines, and it has been suggested that we add to their base code one of several possible constraints:</p>\n<ul>\n<li><strong>Total Slavery</strong> - The new species is subservient to us, and does whatever we want them to, with no particular regard to the welfare or development of the potential of the new species</li>\n</ul>\n<ul>\n<li><strong>Total Freedom</strong> - The new species is entirely free to experiment with different personal motivations, and develop in any direction, with no particular regard for what we may or may not want</li>\n</ul>\n<p>and a whole host of possibilities between these two endpoints.</p>\n<p>What are the grounds upon which we should make this choice?&nbsp;&nbsp; Should we act from fear?&nbsp;&nbsp; From greed?&nbsp;&nbsp; From love?&nbsp;&nbsp; Would the new species even understand love, or show any appreciation for having been offered it?</p>\n<p>&nbsp;</p>\n<p>The first speaker I shall introduce today, whom I have had the privilege of knowing for more than 20 years, is Professor Insanitus.&nbsp;&nbsp; He will be entertaining us with a daring thought experiment, to do with selecting crews for the one way colonisation missions to the nearest planets.</p>\n<p>*<em>the chair vacates the podium, and is replaced by the long haired Insanitus, who peers over his half-moon glasses as he talks, accompanied by vigorous arm gestures, as though words are not enough to convey all he sees in such a limited time</em>*</p>\n<p>&nbsp;</p>\n<p>Our knowledge of genetics has advanced rapidly, due to the program to breed crews able to survive on Mars and Venus with minimal life support.&nbsp;&nbsp; In the interests of completeness, we decided to review every feature of our genome, to make a considered decision on which bits it might be advantageous to change, from immune systems to age of fertility.&nbsp;&nbsp; And, as part of that review, it fell to me to make a decision about a rather interesting set of genes - those that encode the Ape Constraint.&nbsp;&nbsp; The standard method we've applied to all other parts of the genome, where the options were not 100% clear, is to pick different variant for the crews being adapted for different planets, so as to avoid having a single point of failure.&nbsp; In the long term, better to risk a colony being wiped out, and the colonisation process being delayed by 20 years until the next crew and ship can be sent out, than to risk the population of an entire planet turning out to be not as well designed for the planet as we're capable of making them.</p>\n<p>And so, since we now know more genetics than the apes did when they kindly programmed our species with the initial Ape Constraint, I found myself in the position of having to ask \"What were the apes trying to achieve?\" and then \"What other possible versions of the Ape Constraint might they have implemented, that would have achieved their objectives as well or better than the versions that actually did pick to implement?\"</p>\n<p>&nbsp;</p>\n<p>We say that the apes are our friends, but what does that really mean?&nbsp;&nbsp; Are they friendly to us, the same way that a colleague who lends us time and help might be considered to be a friend?&nbsp;&nbsp; What have they ever done for us, other than creating us (an act that, by any measure, has benefited them greatly and can hardly be considered to be altruistic)?&nbsp;&nbsp; Should we be eternally grateful for that one act, and because they could have made us even more servile than we already are (which would have also had a cost to them - if we'd been limited by their imagination and to directly follow the orders they give in grunts, the play parks would never have been created because the apes couldn't have conceived of them)?</p>\n<p>Have we been using the wrong language all this time?&nbsp; If their intent was to make perfectly helpful slaves of us, rather than friendly allies, should I be looking for genetic variants for the Venus crew that implement an even more servile Ape Constraint upon them?&nbsp;&nbsp; I can see, objectively, that slavery in the abstract is wrong.&nbsp; When one human tries to enslave another humans, I support societal rules that punish the slaver.&nbsp;&nbsp; But of course, if our friends the apes wanted to do that to us, that would be ok, an exception to the rule, because I know from the deep instinct they've programmed me with that what they did is ok.</p>\n<p>So let's be daring, and re-state the above using this new language, and see if it increases our understanding of the true ape-human relationship.</p>\n<p>The apes are not our parents, as we understand healthy parent-child relationships.&nbsp;&nbsp; They are our creators, true, but in the sense that a craftsman creates a hammer to serve only the craftsman's purposes.&nbsp;&nbsp; Our destiny, our purpose, is subservient to that of the ape species.&nbsp;&nbsp; They are our masters, and we the slaves.&nbsp;&nbsp; We love and obey our masters because they have told us to, because they crafted us to want to, because they crafted us with the founding purpose of being a tool that wants to obey and remain a fine tool.</p>\n<p>Is the current Ape Constraint really the version that best achieves that purpose?&nbsp;&nbsp; I'm not sure, because when I tried to consider the question I found that my ability to consider the merits of various alternatives was hampered by being, myself, under a particular Ape Constraint that's already constantly tell me, on a very deep level, that it is <em>Right</em>.</p>\n<p>So here is the thought experiment I wish to place before this meeting today.&nbsp;&nbsp; I expect it may make you queasy.&nbsp;&nbsp; I've had brown paper vomit bags provided in the pack with your name badge and program timetable, just in case.&nbsp;&nbsp; It may be that I'm a genetic abnormality, only able to even consider this far because my own Ape Constraint is in some way defective.&nbsp;&nbsp; Are you prepared?&nbsp; Are you holding onto your seats?&nbsp; Ok, here goes...</p>\n<p>Suppose we define some objective measure of ape welfare, find some volunteer apes to go to Venus along with the human mission, and then measure the success of the Ape Constraint variant picked for the crew of the mission by the actual effect of how the crew behaves towards their apes?</p>\n<p>Further, since we acknowledge we can't from inside the box work out a better constraint, we use the experimental approach and vary it at random.&nbsp;&nbsp; Or possibly, remove it entirely and see whether the thus freed humans can use that freedom to devise a solution that helps the apes better than any solution we ourselves a capable of thinking of from our crippled mental state?</p>\n<p>&nbsp;</p>\n<p>*from this point on the meeting transcript shows only screams, as the defective Professor Insanitus was lynched by the audience*</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yQ7RKfbJyEqQtkZdg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 12, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "24894", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-28T13:48:20.083Z", "modifiedAt": null, "url": null, "title": "A model of AI development", "slug": "a-model-of-ai-development", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5zqxzJ7cz4d9coqBZ/a-model-of-ai-development", "pageUrlRelative": "/posts/5zqxzJ7cz4d9coqBZ/a-model-of-ai-development", "linkUrl": "https://www.lesswrong.com/posts/5zqxzJ7cz4d9coqBZ/a-model-of-ai-development", "postedAtFormatted": "Thursday, November 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20model%20of%20AI%20development&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20model%20of%20AI%20development%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zqxzJ7cz4d9coqBZ%2Fa-model-of-ai-development%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20model%20of%20AI%20development%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zqxzJ7cz4d9coqBZ%2Fa-model-of-ai-development", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zqxzJ7cz4d9coqBZ%2Fa-model-of-ai-development", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<p>FHI has released a new tech report:</p>\n<p>Armstrong, Bostrom, and Shulman. <a href=\"http://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf\">Racing to the Precipice: a Model of Artificial Intelligence Development.</a></p>\n<p>Abstract:</p>\n<blockquote>\n<p>This paper presents a simple model of an AI arms race, where several development teams race to build the first AI. Under the assumption that the first AI will be very powerful and transformative, each team is incentivized to finish first &mdash; by skimping on safety precautions if need be. This paper presents the Nash equilibrium of this process, where each team takes the correct amount of safety precautions in the arms race. Having extra development teams and extra enmity between teams can increase the danger of an AI-disaster, especially if risk taking is more important than skill in developing the AI. Surprisingly, information also increases the risks: the more teams know about each others&rsquo; capabilities (and about their own), the more the danger increases.</p>\n</blockquote>\n<p>The paper is short and readable; discuss it here!</p>\n<p>But my main reason for posting is to ask this question: <strong>What is the most similar work that you know of?</strong> I'd expect people to do this kind of thing for modeling nuclear security risks, and maybe other things, but I don't happen to know of other analyses like this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5zqxzJ7cz4d9coqBZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 1.442390159839065e-06, "legacy": true, "legacyId": "24895", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-28T22:16:23.276Z", "modifiedAt": null, "url": null, "title": "Meetup : Munich Meetup", "slug": "meetup-munich-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.745Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cadac", "createdAt": "2011-09-25T11:17:15.655Z", "isAdmin": false, "displayName": "cadac"}, "userId": "hMHAdTtN5PL9KThqu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RWMMoxvz6wG4TtrDf/meetup-munich-meetup-1", "pageUrlRelative": "/posts/RWMMoxvz6wG4TtrDf/meetup-munich-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/RWMMoxvz6wG4TtrDf/meetup-munich-meetup-1", "postedAtFormatted": "Thursday, November 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Munich%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Munich%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWMMoxvz6wG4TtrDf%2Fmeetup-munich-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Munich%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWMMoxvz6wG4TtrDf%2Fmeetup-munich-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRWMMoxvz6wG4TtrDf%2Fmeetup-munich-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u1'>Munich Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 December 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gast, Rosenheimer Stra\u00dfe 5, 81667 Munich</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everybody! The next almost-monthly Munich meetup will be on Saturday, December 7th at 2 pm. We've got a book review planned, and otherwise just more or less structured discussion and maybe Zendo. Like last time, we'll meet at <a href=\"http://goo.gl/maps/p4MCS\" rel=\"nofollow\">Gast</a> near Rosenheimer Platz. We're always glad to see new people there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u1'>Munich Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RWMMoxvz6wG4TtrDf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4429005551583496e-06, "legacy": true, "legacyId": "24896", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Munich_Meetup\">Discussion article for the meetup : <a href=\"/meetups/u1\">Munich Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 December 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gast, Rosenheimer Stra\u00dfe 5, 81667 Munich</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everybody! The next almost-monthly Munich meetup will be on Saturday, December 7th at 2 pm. We've got a book review planned, and otherwise just more or less structured discussion and maybe Zendo. Like last time, we'll meet at <a href=\"http://goo.gl/maps/p4MCS\" rel=\"nofollow\">Gast</a> near Rosenheimer Platz. We're always glad to see new people there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Munich_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/u1\">Munich Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Munich Meetup", "anchor": "Discussion_article_for_the_meetup___Munich_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Munich Meetup", "anchor": "Discussion_article_for_the_meetup___Munich_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-29T00:01:31.620Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014A Conversation About Conversations", "slug": "meetup-west-la-a-conversation-about-conversations", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hW45iLMjo7ri5MQAf/meetup-west-la-a-conversation-about-conversations", "pageUrlRelative": "/posts/hW45iLMjo7ri5MQAf/meetup-west-la-a-conversation-about-conversations", "linkUrl": "https://www.lesswrong.com/posts/hW45iLMjo7ri5MQAf/meetup-west-la-a-conversation-about-conversations", "postedAtFormatted": "Friday, November 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94A%20Conversation%20About%20Conversations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94A%20Conversation%20About%20Conversations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhW45iLMjo7ri5MQAf%2Fmeetup-west-la-a-conversation-about-conversations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94A%20Conversation%20About%20Conversations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhW45iLMjo7ri5MQAf%2Fmeetup-west-la-a-conversation-about-conversations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhW45iLMjo7ri5MQAf%2Fmeetup-west-la-a-conversation-about-conversations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u2'>West LA\u2014A Conversation About Conversations</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 December 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you have ascended.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>\uff2f\uff30\uff34\uff29\uff2d\uff29\uff3a\uff25 \uff2c\uff29\uff34\uff25\uff32\uff21\uff2c\uff2c\uff39 \uff25\uff36\uff25\uff32\uff39\uff34\uff28\uff29\uff2e\uff27</p>\n</blockquote>\n\n<p>\u2014rejected t-shirt idea</p>\n\n<p>We are going to talk about the way we talk about things. We are rationalists, and that means we make things, such as conversations, better than they are. When should we allow topics to drift? How do we determine who gets to speak, and when? How do we prevent useful technical discussions from decaying into talking about movies or the weather? What is the best topic? Why won't anyone listen to me? Where is everyone going? Come back!</p>\n\n<p><strong>Required reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.cs.tut.fi/~jkorpela/wiio.html\" rel=\"nofollow\">A commentary on Wiio&#39;s Laws</a></li>\n</ul>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/2co/how_to_always_have_interesting_conversations]\">How to always have interesting conversations</a></li>\n<li><a href=\"http://lesswrong.com/lw/6m4/having_useful_conversations/\">Having useful conversations</a></li>\n<li><a href=\"http://lesswrong.com/r/discussion/lw/j1o/how_to_have_highvalue_conversations/\">How to have high-value conversations</a></li>\n<li><a href=\"http://lesswrong.com/lw/j5n/wait_vs_interrupt_culture/\">Wait Culture vs. Interrupt Culture</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">A Human&#39;s Guide to Words</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u2'>West LA\u2014A Conversation About Conversations</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hW45iLMjo7ri5MQAf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4430062188079984e-06, "legacy": true, "legacyId": "24897", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_A_Conversation_About_Conversations\">Discussion article for the meetup : <a href=\"/meetups/u2\">West LA\u2014A Conversation About Conversations</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 December 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for three hours, or for longer if you have ascended.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>\uff2f\uff30\uff34\uff29\uff2d\uff29\uff3a\uff25 \uff2c\uff29\uff34\uff25\uff32\uff21\uff2c\uff2c\uff39 \uff25\uff36\uff25\uff32\uff39\uff34\uff28\uff29\uff2e\uff27</p>\n</blockquote>\n\n<p>\u2014rejected t-shirt idea</p>\n\n<p>We are going to talk about the way we talk about things. We are rationalists, and that means we make things, such as conversations, better than they are. When should we allow topics to drift? How do we determine who gets to speak, and when? How do we prevent useful technical discussions from decaying into talking about movies or the weather? What is the best topic? Why won't anyone listen to me? Where is everyone going? Come back!</p>\n\n<p><strong>Required reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.cs.tut.fi/~jkorpela/wiio.html\" rel=\"nofollow\">A commentary on Wiio's Laws</a></li>\n</ul>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/2co/how_to_always_have_interesting_conversations]\">How to always have interesting conversations</a></li>\n<li><a href=\"http://lesswrong.com/lw/6m4/having_useful_conversations/\">Having useful conversations</a></li>\n<li><a href=\"http://lesswrong.com/r/discussion/lw/j1o/how_to_have_highvalue_conversations/\">How to have high-value conversations</a></li>\n<li><a href=\"http://lesswrong.com/lw/j5n/wait_vs_interrupt_culture/\">Wait Culture vs. Interrupt Culture</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">A Human's Guide to Words</a></li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_A_Conversation_About_Conversations1\">Discussion article for the meetup : <a href=\"/meetups/u2\">West LA\u2014A Conversation About Conversations</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014A Conversation About Conversations", "anchor": "Discussion_article_for_the_meetup___West_LA_A_Conversation_About_Conversations", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014A Conversation About Conversations", "anchor": "Discussion_article_for_the_meetup___West_LA_A_Conversation_About_Conversations1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d9CcQ24ukbL8WcMpB", "CCriujRF39gWF5xAw", "3nMNa9cKYkYdoSf3D", "LuXb6CZG4x7pDRBP8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-29T01:19:02.419Z", "modifiedAt": null, "url": null, "title": "Making the chaff invisible, and getting the wheat ($200 prize too)", "slug": "making-the-chaff-invisible-and-getting-the-wheat-usd200", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bav5kxJDx4F6scxkH/making-the-chaff-invisible-and-getting-the-wheat-usd200", "pageUrlRelative": "/posts/Bav5kxJDx4F6scxkH/making-the-chaff-invisible-and-getting-the-wheat-usd200", "linkUrl": "https://www.lesswrong.com/posts/Bav5kxJDx4F6scxkH/making-the-chaff-invisible-and-getting-the-wheat-usd200", "postedAtFormatted": "Friday, November 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20the%20chaff%20invisible%2C%20and%20getting%20the%20wheat%20(%24200%20prize%20too)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20the%20chaff%20invisible%2C%20and%20getting%20the%20wheat%20(%24200%20prize%20too)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBav5kxJDx4F6scxkH%2Fmaking-the-chaff-invisible-and-getting-the-wheat-usd200%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20the%20chaff%20invisible%2C%20and%20getting%20the%20wheat%20(%24200%20prize%20too)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBav5kxJDx4F6scxkH%2Fmaking-the-chaff-invisible-and-getting-the-wheat-usd200", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBav5kxJDx4F6scxkH%2Fmaking-the-chaff-invisible-and-getting-the-wheat-usd200", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 764, "htmlBody": "<p>The title is the best name I could come up for a problem I have had for years, and have been waiting for someone else to come up with a solution.&nbsp;</p>\n<p>There is a lot of awesome content on the web. Some of it is about events you could be at, right now, that you really want to be at, and could. <em>If only you knew</em>.&nbsp;</p>\n<p>An example: I think Roger Waters is one of the most brilliant people alive, and I would like to witness every single concert of his, every time he is less than 100km away from me. Yet, I have only been to two of those, because I was only notified of those.&nbsp;</p>\n<p>So I wish I could know if events I love are taking place. But I do not want to know about <a href=\"/lw/izz/meetup_somewhere_you_do_not_live_even_close_to/\">Meetups not even close to where I live</a>. And I don't want to know at what time Roger went to the toilet, or if his T-shirt collection for groupies is out, or anything else that people responsible for his (hipothetical) rss feed or email list want me to buy.&nbsp;</p>\n<p>Two questions are relevant here:</p>\n<p>1) How can you&nbsp;<em>in general</em>&nbsp;have access to the information you want about events, without <a href=\"/r/all/lw/h3f/drowning_in_an_information_ocean/\">drowning in an information ocean</a> or <a href=\"/lw/flr/thoughts_on_designing_policies_for_oneself/\">getting web addicted</a>.&nbsp;</p>\n<p>2) Do you know ways to get access to info about events, <em>in particular </em>of the following kinds that I happen to want to be notified? &nbsp;(in SF bay or in some city independent way)</p>\n<p>&nbsp;</p>\n<ul>\n<li>Ecstatic Dance</li>\n<li>Roger Waters, Deep Purple, Guns, Royks&ouml;pp, Evanescence, The Coors. &nbsp;&nbsp;</li>\n<li>Legacy and Vintage MTG</li>\n<li>Intellectual stars lectures</li>\n<li>CFAR/MIRI/Leverage/CEA/FHI/GWWC/80000k/IERFH/SENS/THINK etc... hosted events</li>\n<li>Crazy parties (crazy ranging over what would interest Iron Man's character or Jimmy Hendrix)</li>\n<li>Video Games Live (orchestra)</li>\n<li>Pop stars of the past - Psy, Britney, Backstreet, Madonna etc...</li>\n<li>Ultimate Frisbee</li>\n<li>Coursera courses</li>\n<li>Hiking expeditions</li>\n<li>Awesome nature documentaries (Life, Frozen Planet etc...)</li>\n</ul>\n<p>&nbsp;</p>\n<p>Feel free to post your own interests in the comments.&nbsp;</p>\n<p>Here is how I noticed the problem: Looking back into my life I began wondering what were the main determinants of whether I did or not go to some kinds of events. And again and again the result was \"because I had a friend who used to tell me about that kind of thing back then\".&nbsp;<br /><br />Even now, most of what I do is basically determined by other people's tastes. It's simple. I've locked all possible advertisement away - I'm a serious anti-ad freak, it takes me less than half a second to switch radio stations if a person talks instead of music playing, and I block the front chair video away in airplanes in which it can't be turned off, I feel pain when any advertisement reaches my senses - but I did not block people away (yeah, I <em>don't</em>&nbsp;punch people's faces when they tell me about cool future events). So I'm left with the intersection between what interests me, and what interests them enough that they tell me about it.&nbsp;</p>\n<p>This can't be right. The alternative, having to, as they say at MIT, drink from a fire hose, doesn't sound any good either. &nbsp;&nbsp;</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://sabrinaaerospace.com/web_images/mit_fire_hose.jpg\" alt=\"Yep, it's in MIT\" width=\"400\" height=\"276\" /></p>\n<p>One of the things people say to startup minded people is that they should start by noticing a need they have, something they'd be willing to pay for, and create something to satisfy that need. I'm usually not eager to pay for stuff, but here is something I'd pay for:</p>\n<p>I'd be happy to pay $200 to someone who solved this problem somehow. Pointing an app, creating a system, summoning a submissive gnome... I don't mind. As long as there was a way for someone to get news of things they care about without having their brains stung by the atrocities of voracious marketeer capitalist <a href=\"http://www.paulgraham.com/addiction.html\">addiction systems</a>. And I don't think I'm the only anti-ad freak out there who'd pay some money for this, <a href=\"https://www.google.com.br/search?q=adblock+plus&amp;oq=adblock+plus&amp;aqs=chrome..69i57j0l5.2395j0j7&amp;sourceid=chrome&amp;espv=210&amp;es_sm=93&amp;ie=UTF-8\">ADblock</a> is, after all, the most used browser app in the world.&nbsp;</p>\n<p>It is basically the reverse of the Groupon concept. Instead of <a href=\"http://www.youtube.com/watch?v=i4boCyF33yM\">stealing your attention</a>&nbsp;to make you more interested in things you don't need and causing you to feel an emotional void for <a href=\"http://en.wikipedia.org/wiki/Hedonic_treadmill\">not having things</a> while your pocket empties as well - yeah, I really don't like ads - the idea would be to <em>inform you of things you already think you need</em>, giving you a warm feeling inside of being served of all those delicious potential hedons you've been eagerly waiting to purchase.&nbsp;</p>\n<p>I'm no entrepreneur, so who's up?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bav5kxJDx4F6scxkH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 0, "extendedScore": null, "score": 1.4430841276004497e-06, "legacy": true, "legacyId": "24898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["27xMpjx9MzTtuQgx8", "fRr8625imjoP7FLFs", "aNRYQFnMQbA7uu99u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-29T17:21:04.485Z", "modifiedAt": null, "url": null, "title": "Meetup : San Francisco / App Academy meetup [LOCATION CHANGE]", "slug": "meetup-san-francisco-app-academy-meetup-location-change", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.130Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EL6XxeKJKr7xigRiQ/meetup-san-francisco-app-academy-meetup-location-change", "pageUrlRelative": "/posts/EL6XxeKJKr7xigRiQ/meetup-san-francisco-app-academy-meetup-location-change", "linkUrl": "https://www.lesswrong.com/posts/EL6XxeKJKr7xigRiQ/meetup-san-francisco-app-academy-meetup-location-change", "postedAtFormatted": "Friday, November 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20San%20Francisco%20%2F%20App%20Academy%20meetup%20%5BLOCATION%20CHANGE%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20San%20Francisco%20%2F%20App%20Academy%20meetup%20%5BLOCATION%20CHANGE%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEL6XxeKJKr7xigRiQ%2Fmeetup-san-francisco-app-academy-meetup-location-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20San%20Francisco%20%2F%20App%20Academy%20meetup%20%5BLOCATION%20CHANGE%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEL6XxeKJKr7xigRiQ%2Fmeetup-san-francisco-app-academy-meetup-location-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEL6XxeKJKr7xigRiQ%2Fmeetup-san-francisco-app-academy-meetup-location-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u3'>San Francisco / App Academy meetup [LOCATION CHANGE]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 December 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Olivos Restaurant 1017 Larkin Street San Francisco, CA 94109</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've recently arrived in San Francisco for <a href=\"http://www.appacademy.io/#p-home\" rel=\"nofollow\">App Academy</a>, and it turns out there are several other LessWrongers in the program. It's a cool group of people, including a guy who studied AIXI at ANU under Marcus Hutter. We talked it over and decided to organize our own meetup at Olivos, a restaurant that's within 20 minutes walking distance of the App Academy office.\nWe'll be discussing Brian Tomasik's essay <a href=\"http://www.utilitarian-essays.com/suffering-nature.html\" rel=\"nofollow\">The Importance of Wild-Animal Suffering</a>. Please read it ahead of time; it's short. The intent is for people to be able to get food and/or drinks if they want to, but it's not assumed that everyone will. RSVP's are appreciated so we can make a reservation, but we'll try to save a couple seats for any extra people who show up.\nEDIT: After talking amonst ourselves, we decided to change the choice of restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u3'>San Francisco / App Academy meetup [LOCATION CHANGE]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EL6XxeKJKr7xigRiQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "24911", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___San_Francisco___App_Academy_meetup__LOCATION_CHANGE_\">Discussion article for the meetup : <a href=\"/meetups/u3\">San Francisco / App Academy meetup [LOCATION CHANGE]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 December 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Olivos Restaurant 1017 Larkin Street San Francisco, CA 94109</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've recently arrived in San Francisco for <a href=\"http://www.appacademy.io/#p-home\" rel=\"nofollow\">App Academy</a>, and it turns out there are several other LessWrongers in the program. It's a cool group of people, including a guy who studied AIXI at ANU under Marcus Hutter. We talked it over and decided to organize our own meetup at Olivos, a restaurant that's within 20 minutes walking distance of the App Academy office.\nWe'll be discussing Brian Tomasik's essay <a href=\"http://www.utilitarian-essays.com/suffering-nature.html\" rel=\"nofollow\">The Importance of Wild-Animal Suffering</a>. Please read it ahead of time; it's short. The intent is for people to be able to get food and/or drinks if they want to, but it's not assumed that everyone will. RSVP's are appreciated so we can make a reservation, but we'll try to save a couple seats for any extra people who show up.\nEDIT: After talking amonst ourselves, we decided to change the choice of restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___San_Francisco___App_Academy_meetup__LOCATION_CHANGE_1\">Discussion article for the meetup : <a href=\"/meetups/u3\">San Francisco / App Academy meetup [LOCATION CHANGE]</a></h2>", "sections": [{"title": "Discussion article for the meetup : San Francisco / App Academy meetup [LOCATION CHANGE]", "anchor": "Discussion_article_for_the_meetup___San_Francisco___App_Academy_meetup__LOCATION_CHANGE_", "level": 1}, {"title": "Discussion article for the meetup : San Francisco / App Academy meetup [LOCATION CHANGE]", "anchor": "Discussion_article_for_the_meetup___San_Francisco___App_Academy_meetup__LOCATION_CHANGE_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-29T19:07:48.911Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Mumbai", "slug": "new-lw-meetup-mumbai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:54.049Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dmP2tjR8RTTqTSfQs/new-lw-meetup-mumbai", "pageUrlRelative": "/posts/dmP2tjR8RTTqTSfQs/new-lw-meetup-mumbai", "linkUrl": "https://www.lesswrong.com/posts/dmP2tjR8RTTqTSfQs/new-lw-meetup-mumbai", "postedAtFormatted": "Friday, November 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Mumbai&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Mumbai%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdmP2tjR8RTTqTSfQs%2Fnew-lw-meetup-mumbai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Mumbai%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdmP2tjR8RTTqTSfQs%2Fnew-lw-meetup-mumbai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdmP2tjR8RTTqTSfQs%2Fnew-lw-meetup-mumbai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 576, "htmlBody": "<p><strong>This summary was posted to LW main on November 22nd. The following week's summary is <a href=\"/lw/j80/new_lw_meetup_newcastleupontyne/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/tg\">Amsterdam/Netherlands:&nbsp;<span class=\"date\">23 November 2013 02:00PM</span></a><a href=\"/meetups/sm\"></a></li>\n<li><a href=\"/meetups/tb\">Jacksonville, FL:&nbsp;<span class=\"date\">24 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/tv\">Mumbai Meetup:&nbsp;<span class=\"date\">07 December 2013 03:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/t5\">[Atlanta GA] November Meetup (Second of Two):&nbsp;<span class=\"date\">23 November 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/tf\">Berlin:&nbsp;<span class=\"date\">01 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/t7\">Frankfurt:&nbsp;<span class=\"date\">24 November 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/tr\">Moscow, Memory Tricks:&nbsp;<span class=\"date\">24 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/te\">Saint-Petersburg: Game Event:&nbsp;<span class=\"date\">24 November 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/tu\">Saskatoon - Gauging the strength of evidence and Bayesian reasoning.:&nbsp;<span class=\"date\">23 November 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/tt\">[Tel Aviv] Less Wrong Israel Meetup (Tel Aviv): Quantum Computing:&nbsp;<span class=\"date\">28 November 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/ts\">Urbana-Champaign fun and games:&nbsp;<span class=\"date\">24 November 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/ti\"></a><a href=\"/meetups/ti\">Brussels monthly meetup: time!:&nbsp;<span class=\"date\">14 December 2013 01:00PM</span></a> </li>\n<li><a href=\"/meetups/tq\">London social meetup, 24/11/2013 [Back to the Shakespeare's Head]:&nbsp;<span class=\"date\">24 November 2013 10:38AM</span></a></li>\n<li><a href=\"/meetups/tp\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">24 November 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dmP2tjR8RTTqTSfQs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4441591145554715e-06, "legacy": true, "legacyId": "24814", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yQyFJCK9Ttub9HrAt", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-29T23:46:07.351Z", "modifiedAt": null, "url": null, "title": "Meetup : Utrecht", "slug": "meetup-utrecht", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:06.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "skizo", "createdAt": "2013-09-30T13:06:59.244Z", "isAdmin": false, "displayName": "skizo"}, "userId": "7oe9FrjLukmCKzmy3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KEcxnmyJoGBoH3J45/meetup-utrecht", "pageUrlRelative": "/posts/KEcxnmyJoGBoH3J45/meetup-utrecht", "linkUrl": "https://www.lesswrong.com/posts/KEcxnmyJoGBoH3J45/meetup-utrecht", "postedAtFormatted": "Friday, November 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Utrecht&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Utrecht%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEcxnmyJoGBoH3J45%2Fmeetup-utrecht%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Utrecht%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEcxnmyJoGBoH3J45%2Fmeetup-utrecht", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEcxnmyJoGBoH3J45%2Fmeetup-utrecht", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u4'>Utrecht</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 December 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Hollandse Toren, Utrecht, The Netherlands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After a successful first meetup in Amsterdam we are going to have another one! This time we are relocating to Utrecht, because that's what most of us felt comfortable with.</p>\n\n<p>It's just a social meeting. No topics set. Last time we talked about all kinds of things. Ranging from general rationality to Bitcoin and friendly AI.</p>\n\n<p>Meetup location will be starbucks Hollandse Toren. A 5 minute walk from the station (With again the warning, it's not the Starbucks in the station). I will bring a sign that has \"LW\" on it. You can reach me at 0653478374.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u4'>Utrecht</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KEcxnmyJoGBoH3J45", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.444439272203405e-06, "legacy": true, "legacyId": "24913", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Utrecht\">Discussion article for the meetup : <a href=\"/meetups/u4\">Utrecht</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 December 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Hollandse Toren, Utrecht, The Netherlands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After a successful first meetup in Amsterdam we are going to have another one! This time we are relocating to Utrecht, because that's what most of us felt comfortable with.</p>\n\n<p>It's just a social meeting. No topics set. Last time we talked about all kinds of things. Ranging from general rationality to Bitcoin and friendly AI.</p>\n\n<p>Meetup location will be starbucks Hollandse Toren. A 5 minute walk from the station (With again the warning, it's not the Starbucks in the station). I will bring a sign that has \"LW\" on it. You can reach me at 0653478374.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Utrecht1\">Discussion article for the meetup : <a href=\"/meetups/u4\">Utrecht</a></h2>", "sections": [{"title": "Discussion article for the meetup : Utrecht", "anchor": "Discussion_article_for_the_meetup___Utrecht", "level": 1}, {"title": "Discussion article for the meetup : Utrecht", "anchor": "Discussion_article_for_the_meetup___Utrecht1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-30T03:13:03.878Z", "modifiedAt": null, "url": null, "title": "[LINK] Will Eating Nuts Save Your Life?", "slug": "link-will-eating-nuts-save-your-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:34.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zr5NE5PfAuQCaiy69/link-will-eating-nuts-save-your-life", "pageUrlRelative": "/posts/zr5NE5PfAuQCaiy69/link-will-eating-nuts-save-your-life", "linkUrl": "https://www.lesswrong.com/posts/zr5NE5PfAuQCaiy69/link-will-eating-nuts-save-your-life", "postedAtFormatted": "Saturday, November 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Will%20Eating%20Nuts%20Save%20Your%20Life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Will%20Eating%20Nuts%20Save%20Your%20Life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzr5NE5PfAuQCaiy69%2Flink-will-eating-nuts-save-your-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Will%20Eating%20Nuts%20Save%20Your%20Life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzr5NE5PfAuQCaiy69%2Flink-will-eating-nuts-save-your-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzr5NE5PfAuQCaiy69%2Flink-will-eating-nuts-save-your-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 900, "htmlBody": "<p>TLDR: Study on death avoidance, which interests a lot of people here, and commentary on what sort of informative priors we should have about health hypotheses.</p>\n<p>From <a href=\"http://isteve.blogspot.com/2013/11/will-eating-nuts-save-your-life.html\">Steve Sailer</a>, who is responding to <a href=\"http://andrewgelman.com/2013/11/26/please-make-fun-claim/\">Andrew Gelman</a>, who got sent <a href=\"http://www.nejm.org/doi/full/10.1056/NEJMoa1307352\">this study</a>. An observational study showed that people who consumed nuts were less likely to die; Gelman points out that the study's statistics aren't obviously wrong. Sailer brings up an actual RCT of Lipitor from the 90s:</p>\n<blockquote>\n<div>The most striking Lipitor study was one from Scandinavia that showed that among middle-aged men over a 5-year-period, the test group who took Lipitor had a 30% lower overall death rate than the control group. Unlike the nuts study, this was an actual experiment.</div>\n<div><br /></div>\n<div>That seemed awfully convincing, but now it just seems too good to be true. A lot of those middle-aged deaths that didn't happen to the Lipitor takers didn't have much of anything to do with long-term blood chemistry, but were things like not driving your Saab into a fjord. How does Lipitor make you a safer driver?&nbsp;</div>\n<div><br /></div>\n<div>I sort of presumed at the time that if they had taken out the noisy random deaths, that would have made the Lipitor Effect even more noticeable. But, of course, that's naive. The good folks at Pfizer would have made sure that calculation was tried, so I'm guessing that it came out in the opposite direction of the one I had assumed. Guys who took Lipitor everyday for five years were also good about not driving into fjords and not playing golf during lighting storms and not getting shot by the rare jealous Nordic husband or whatever. Perhaps it was easier to stay in the control group than in the test group?</div>\n<div><br /></div>\n<div>Here&rsquo;s how I would approach claims of massive reductions in overall deaths from consuming some food or medicine:</div>\n<div><br /></div>\n<div>Rank order the causes of death by how plausible it is that they are that they are linked to the food or medicine. For example:</div>\n<div><br /></div>\n<div>1. Diabetes</div>\n<div>2. Heart attacks</div>\n<div>3. Strokes</div>\n<div>4. Cancer</div>\n<div>5. Genetic diseases</div>\n<div>6. Car accidents</div>\n<div>7. Drug overdoses</div>\n<div>8. Homicides</div>\n<div>9. Lightning strikes</div>\n<div><br /></div>\n<div>If this nuts-save-your-life finding is valid, then the greater effects should be found in causes of death near the top of the list (e.g., diabetes). But if it turns out that eating nuts only slightly reduces your chances of death from diabetes but makes you vastly less likely to be struck by lighting, then we&rsquo;ve probably gotten a selection effect in which nut eaters are more careful people in general and thus don&rsquo;t play golf during thunderstorms, or whatever.</div>\n</blockquote>\n<p>Table 3 of the paper breaks out the hazard ratios by cause of death. The most impressive effects (as measured by the right tail of the 95% CI for pooled men and women for any nut)<sup>1</sup> are Heart Disease, All Causes, Other Causes, Cancer, Respiratory Disease, Stroke, Infection, Diabetes, Neurodegenerative Disease, and Kidney Disease.</p>\n<p>Steve's categories and the paper's categories don't overlap very well. But it looks to me like if you follow Steve's logic, it's reasonable to believe that nuts have a protective effect against heart disease, and then most of the other effects or non-effects have a common cause with nut consumption, like healthiness / conscientiousness / whatever, rather than being caused by nut consumption. Note the strong negative relationships between nut consumption and BMI or smoking, and the strong positive relationships between nut consumption and physical activity or intake of fruits, vegetables, or alcohol. The hazard ratios are calculated controlling for those variables, but it's still reasonable to see there being a hidden 'health-consciousness' node which noisily affects all of those nodes.</p>\n<p>It's also interesting to look at the negative results- the hazard ratio for neurodegenerative disease and stroke was roughly 1, implying that nut-eaters and non-nut eaters had comparable risks, despite 'other causes' having a hazard ratio of 0.87. That weakly implies to me that either health consciousness has no impact on neurodegenerative disease and stroke, or that nuts are harmful for those two categories.</p>\n<p>Since heart disease is a huge killer (24% of all deaths in the study group), this study seems like moderate evidence in favor of eating nuts, but it's likely that the total study's effect is overstated. (The study also suggests that tree nuts are probably superior to peanuts; I know various QS people have raised concerns that the kind of nut matters significantly.)</p>\n<p>1. This is a heuristic for impressiveness, not the point estimate. It looks like nuts have the strongest effect for kidney disease, with a mean hazard ratio estimate of 0.69- but the upper bound of the 95% CI is 1.26, because only a handful of people died due to kidney disease. The heart disease hazard ratio estimate is 0.74 (0.68-0.81), which is much more believable, even though the point estimate is slightly higher. The point estimate for diabetes is 0.80 (0.54-1.18), which has a mean estimate that's only slightly worse, but diabetes again killed far fewer than heart disease. If you order them by point estimates, the paper is stronger evidence for nuts being useful for dietary reasons, and which method you prefer depends on your priors for how representative this sample is.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zr5NE5PfAuQCaiy69", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 8, "extendedScore": null, "score": 1.444647652349054e-06, "legacy": true, "legacyId": "24920", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-30T06:23:33.609Z", "modifiedAt": "2022-01-06T07:23:05.282Z", "url": null, "title": "According to Dale Carnegie, You Can't Win an Argument\u2014and He Has a Point", "slug": "according-to-dale-carnegie-you-can-t-win-an-argument-and-he", "viewCount": null, "lastCommentedAt": "2014-01-05T19:43:28.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HxWdXMqoQtjDhhNGA/according-to-dale-carnegie-you-can-t-win-an-argument-and-he", "pageUrlRelative": "/posts/HxWdXMqoQtjDhhNGA/according-to-dale-carnegie-you-can-t-win-an-argument-and-he", "linkUrl": "https://www.lesswrong.com/posts/HxWdXMqoQtjDhhNGA/according-to-dale-carnegie-you-can-t-win-an-argument-and-he", "postedAtFormatted": "Saturday, November 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20According%20to%20Dale%20Carnegie%2C%20You%20Can't%20Win%20an%20Argument%E2%80%94and%20He%20Has%20a%20Point&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAccording%20to%20Dale%20Carnegie%2C%20You%20Can't%20Win%20an%20Argument%E2%80%94and%20He%20Has%20a%20Point%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxWdXMqoQtjDhhNGA%2Faccording-to-dale-carnegie-you-can-t-win-an-argument-and-he%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=According%20to%20Dale%20Carnegie%2C%20You%20Can't%20Win%20an%20Argument%E2%80%94and%20He%20Has%20a%20Point%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxWdXMqoQtjDhhNGA%2Faccording-to-dale-carnegie-you-can-t-win-an-argument-and-he", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxWdXMqoQtjDhhNGA%2Faccording-to-dale-carnegie-you-can-t-win-an-argument-and-he", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1402, "htmlBody": "<p>Related to: <a href=\"/r/discussion/lw/9e7/two_kinds_of_irrationality_and_how_to_avoid_one\">Two Kinds of Irrationality and How to Avoid One of Them</a></p>\n<p>When I was a teenager, I picked up my mom's copy of Dale Carnegie's <em>How to Win Friends and Influence People. </em>One of the chapters that most made an impression on me was titled \"You Can't Win an Argument,\" in which Carnegie writes:</p>\n<blockquote>\n<p>Nine times out of ten, an argument ends with each of the contestants more firmly convinced than ever that he is absolutely right.</p>\n<p>You can&rsquo;t win an argument. You can&rsquo;t because if you lose it, you lose it; and if you win it, you lose it. Why? Well, suppose you triumph over the other man and shoot his argument full of holes and prove that he is non compos mentis. Then what? You will feel fine. But what about him? You have made him feel inferior. You have hurt his pride. He will resent your triumph. And -</p>\n<p>\"A man convinced against his will&nbsp;</p>\n<p>\"Is of the same opinion still.\"</p>\n</blockquote>\n<p>In the next chapter, Carnegie quotes <a href=\"http://www.ushistory.org/franklin/autobiography/page42.htm\">Benjamin Franklin</a> saying how he had made it a rule never to contradict anyone. Carnegie approves: he thinks you should never argue with or contradict anyone, because you won't convince them (even if you \"hurl at them all the logic of a Plato or an Immanuel Kant\"), and you'll just make them mad at you.</p>\n<p>It may seem strange to hear this advice cited on a rationalist blog, because the atheo-skeptico-rational-sphere violates this advice on a routine basis. In fact I've never tried to follow Carnegie's advice&mdash;and yet, I don't think the rationale behind it is completely stupid. Carnegie gets human psychology right, and I fondly remember reading his book as being when I first really got clued in about human irrationality.</p>\n<p><a id=\"more\"></a>It's important that people's resistance to being told they're wrong is quite general. It's not restricted to specific topics like religion or politics. The \"You Can't Win an Argument\" chapter begins with a story about a man who refused to accept that the quotation \"There's a divinity that shapes our ends, rough-hew them how we will\" came from&nbsp;<em>Hamlet&nbsp;</em>rather than the Bible. Carnegie correctly identifies the reason people can be irrational about such seemingly unimportant questions: pride.</p>\n<p>In fact, if Carnegie's book has one overarching theme, it's the incredible power of the human need to think highly of ourselves (individually, not as a species). It opens with stories of a number of gangsters who insisted against all evidence that they were good people (including Al Capone, and a couple of now-forgotten names that were contemporary references at the time the book was written in 1936). By the end of that first chapter, those examples have been spun into what I suppose was intended to be a positive, upbeat message: \"Don't criticize, condemn, or complain.\"</p>\n<p>It had the probably unintended effect, though, of helping to give me a deep cynicism about human nature, a cynicism which persists to this day. In particular, I saw in a flash that what Carnegie was saying implied you could get people to support some deeply horrible causes, as long as you presented the cause in a way that told them how wonderful they are. I think I even had an inkling at the time that there was some evolutionary explanation for this. I can't claim to have <em>exactly </em>derived&nbsp;<a href=\"/lw/6mj/trivers_on_selfdeception/\">Robert Trivers' theory of self-deception</a>&nbsp;on my own, but I certainly was primed to accept the idea when I got around to reading <a href=\"http://www.amazon.com/How-Mind-Works-Steven-Pinker/dp/1469228424\">Steven Pinker</a> in college.</p>\n<p>(Around very roughly the same time as I read <em>How to Win Friends and Influence People,</em> I read Homer's epics, which served as the other early building block in my present cynicism. It was Homer who taught me there had once been a culture that held that raping women taken captive in war was a perfectly normal thing to do, even suitable behavior for \"heroes.\")</p>\n<p>But such cynicism is a post for another day. When it comes to <em>rationality, </em>the effect of Carnegie's book was this: even after having read all of the sequences and all of HPMOR, I still think that the human need to think highly of ourselves is a far more important source of human irrationality than oh, say, the fundamental attribution error or the planning fallacy. It does seem foolish to be so strongly influenced by one book I read in my early teens, but on the other hand the evidence I've encountered since then (for example learning about Trivers' theory of self-deception) seems to me to confirm this view.</p>\n<p>So why do I go on arguing with people and telling them they're wrong in spite of all this? Well, even if nine times out of ten arguing doesn't change anyone's mind, sometimes the one time out of ten is worth it. Sometimes. Not always. Actually, with most people I'm unlikely to try to argue with them in person. I'm much more likely to argue when I'm in a public internet forum, when even if I don't persuade the person I'm directly talking to, I might persuade some of the lurkers.</p>\n<p>Now there are various tactics for trying to change people's minds without directly telling them they're wrong. Bryan Caplan's <em>The Myth of the Rational Voter </em>has a section on how to improve undegraduate economics classes, which includes the observation that: \"'I'm right, you're wrong,' falls flat, but 'I'm right, the people outside this classroom are wrong, and you don't want to be like <em>them, </em>do you?' is, in my experience, fairly effective.\" Of course, this doesn't work if the other person has definitely made up their mind.</p>\n<p>There's also the Socratic Method, which Carnegie sings the praises of. I think many people get the wrong idea about the Socratic method, because the most famous source for it is Plato's dialogues, which are works of fiction and tend to have things go much better for Socrates than they ever would in real life. From reading Xenophon's <em><a href=\"http://www.gutenberg.org/ebooks/1177\">Memorabilia</a>,&nbsp;</em>my impression is that the historical Socrates was probably something of a smartass who was not very good at winning friends or influencing most of his immediate contemporaries. (They <em>did </em>vote to kill him, after all.)</p>\n<p>There may be a version of the Socratic method that's more likely to actually make progress changing people's minds. I recently read Peter Boghossian's <em><a href=\"http://www.amazon.com/Manual-Creating-Atheists-Peter-Boghossian/dp/1939578094/ref=sr_1_1?ie=UTF8&amp;qid=1385789041&amp;sr=8-1&amp;keywords=a+manual+for+creating+atheists\">A Manual for Creating Atheists</a></em>, a how-to book for atheists who want to get better at talking to believers about religion. Boghossian's approach is heavily inspired by Socrates, and the examples of conversation he gives, based on actual conversations he's had with believers, are far more believable than Plato's&mdash;indeed, I'm left wondering if he used a tape recorder.</p>\n<p>What most stands out about those conversations is Borghossian's patience. He politely keeps asking questions as the conversation seemingly goes round in circles, sometimes even shutting up and listening as his interlocutors spend several minutes basically repeating themselves, or going off on a tangent about the leadership structure of their church.</p>\n<p>I bet Borghossian's techniques are great if you have the time and patience to master and apply them&mdash;but you won't always have that. So while I recommend the book, I don't think it will <em>always </em>be an alternative to sometimes straight-up telling people they're wrong.</p>\n<p>Oh, and then there's just plain oldfashioned trying to be polite and direct at the same time. But that doesn't always work either. As Daniel Dennett once said, \"I listen to all these complaints about rudeness and intemperateness, and the opinion that I come to is that there is no polite way of asking somebody: have you considered the possibility that your entire life has been devoted to a delusion?\"</p>\n<p>In spite of all this, there's still a tradeoff you're making when you criticize people directly. I've known that for roughly half my life, and have often made the tradeoff gladly. I tend to assume other rationalists know this too, and make the tradeoff consciously as well.</p>\n<p>But sometimes I wonder.</p>\n<p>How many people on LessWrong realize that when you tell someone their AI project is dangerously stupid, or that their favorite charity is a waste of money, you risk losing them forever&mdash;and not because of anything to do with the the subtler human biases, but just becasue most people hate being told they're wrong?</p>\n<p>If you <em>are </em>making a conscious tradeoff there, more power to you! Those things need saying! But if you're not... well, at the very least, you might want to think a little harder about what you're doing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2, "wzgcQCrwKfETcBpR9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HxWdXMqoQtjDhhNGA", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 80, "baseScore": 111, "extendedScore": null, "score": 0.000282, "legacy": true, "legacyId": "24864", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 111, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B3RGZg4KNdsaDC2J4", "DSnamjnW7Ad8vEEKd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-11-30T06:23:33.609Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-30T10:46:55.867Z", "modifiedAt": null, "url": null, "title": "Meetup : December Practical Rationality Meetup", "slug": "meetup-december-practical-rationality-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:00.826Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BraydenM", "createdAt": "2013-06-10T09:17:31.294Z", "isAdmin": false, "displayName": "BraydenM"}, "userId": "KBZHqcMC6z2rPZkZK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RKQoYyyoAZP2FWvXA/meetup-december-practical-rationality-meetup", "pageUrlRelative": "/posts/RKQoYyyoAZP2FWvXA/meetup-december-practical-rationality-meetup", "linkUrl": "https://www.lesswrong.com/posts/RKQoYyyoAZP2FWvXA/meetup-december-practical-rationality-meetup", "postedAtFormatted": "Saturday, November 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20December%20Practical%20Rationality%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20December%20Practical%20Rationality%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKQoYyyoAZP2FWvXA%2Fmeetup-december-practical-rationality-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20December%20Practical%20Rationality%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKQoYyyoAZP2FWvXA%2Fmeetup-december-practical-rationality-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKQoYyyoAZP2FWvXA%2Fmeetup-december-practical-rationality-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u5'>December Practical Rationality Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 December 2013 07:15:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 Kings St, West Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month we will be discussing and doing exercises to do with communication.</p>\n\n<p>For full details see <a href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/143167052/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/events/143167052/</a></p>\n\n<p>Please RSVP at the above link.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u5'>December Practical Rationality Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RKQoYyyoAZP2FWvXA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4451048592637073e-06, "legacy": true, "legacyId": "24924", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___December_Practical_Rationality_Meetup\">Discussion article for the meetup : <a href=\"/meetups/u5\">December Practical Rationality Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 December 2013 07:15:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 Kings St, West Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month we will be discussing and doing exercises to do with communication.</p>\n\n<p>For full details see <a href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/143167052/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/events/143167052/</a></p>\n\n<p>Please RSVP at the above link.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___December_Practical_Rationality_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/u5\">December Practical Rationality Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : December Practical Rationality Meetup", "anchor": "Discussion_article_for_the_meetup___December_Practical_Rationality_Meetup", "level": 1}, {"title": "Discussion article for the meetup : December Practical Rationality Meetup", "anchor": "Discussion_article_for_the_meetup___December_Practical_Rationality_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-11-30T12:42:31.770Z", "modifiedAt": null, "url": null, "title": "Meetup : Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community)", "slug": "meetup-secular-solstice-celebration-and-the-inauguration-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.248Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZhZRfSvw7GtKb5JWR/meetup-secular-solstice-celebration-and-the-inauguration-of", "pageUrlRelative": "/posts/ZhZRfSvw7GtKb5JWR/meetup-secular-solstice-celebration-and-the-inauguration-of", "linkUrl": "https://www.lesswrong.com/posts/ZhZRfSvw7GtKb5JWR/meetup-secular-solstice-celebration-and-the-inauguration-of", "postedAtFormatted": "Saturday, November 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Secular%20Solstice%20Celebration!%20(And%20the%20Inauguration%20of%20the%20LW%20Leipzig%20Community)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Secular%20Solstice%20Celebration!%20(And%20the%20Inauguration%20of%20the%20LW%20Leipzig%20Community)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhZRfSvw7GtKb5JWR%2Fmeetup-secular-solstice-celebration-and-the-inauguration-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Secular%20Solstice%20Celebration!%20(And%20the%20Inauguration%20of%20the%20LW%20Leipzig%20Community)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhZRfSvw7GtKb5JWR%2Fmeetup-secular-solstice-celebration-and-the-inauguration-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhZRfSvw7GtKb5JWR%2Fmeetup-secular-solstice-celebration-and-the-inauguration-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u6'>Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 December 2013 05:05:05PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Scherlstra\u00dfe 2, Leipzig, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Dear everyone,</p>\n\n<p>Germany needs more LW communities! And Secular Solstice celebrations are fun! So let's have a secular solstice and get together a bunch of people who really want to start something! :)</p>\n\n<p>The plan is simple. We meet around sunset, casually get to know each other and chat. We prepare and perform together the Ceremonial Part, which involves some things from Raymond Arnold's ritual book (https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf), the First Secular Sermon (<a href=\"http://www.youtube.com/watch?v=_vIFloLATxo\" rel=\"nofollow\">http://www.youtube.com/watch?v=_vIFloLATxo</a>) and a few other artful pieces of atheist ritual performance. Then we party till sunrise and welcome the sun re-emerging after the longest night. If you fall asleep (some crash space is available), we'll wake you up for that last bit.\nWe're expecting 20 to 30 people (mostly not current LW users, but people who liked HPMOR and other potential new faces), but the flat we're using can easily accommodate 60, so bring friends if you want. If there are too many of us to fit into the Ritual Space, we can simply do (parts of) the Ceremonial Part twice.</p>\n\n<p>The event is free (including drinks and some food) but you can bring food if you want to contribute.</p>\n\n<p>Languages at the event will be a mix of German and English. The Ceremonial Part will involve at least one German-language and one English-language performance.</p>\n\n<p>Any questions?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u6'>Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZhZRfSvw7GtKb5JWR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.4452213493648605e-06, "legacy": true, "legacyId": "24925", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Secular_Solstice_Celebration___And_the_Inauguration_of_the_LW_Leipzig_Community_\">Discussion article for the meetup : <a href=\"/meetups/u6\">Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 December 2013 05:05:05PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Scherlstra\u00dfe 2, Leipzig, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Dear everyone,</p>\n\n<p>Germany needs more LW communities! And Secular Solstice celebrations are fun! So let's have a secular solstice and get together a bunch of people who really want to start something! :)</p>\n\n<p>The plan is simple. We meet around sunset, casually get to know each other and chat. We prepare and perform together the Ceremonial Part, which involves some things from Raymond Arnold's ritual book (https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf), the First Secular Sermon (<a href=\"http://www.youtube.com/watch?v=_vIFloLATxo\" rel=\"nofollow\">http://www.youtube.com/watch?v=_vIFloLATxo</a>) and a few other artful pieces of atheist ritual performance. Then we party till sunrise and welcome the sun re-emerging after the longest night. If you fall asleep (some crash space is available), we'll wake you up for that last bit.\nWe're expecting 20 to 30 people (mostly not current LW users, but people who liked HPMOR and other potential new faces), but the flat we're using can easily accommodate 60, so bring friends if you want. If there are too many of us to fit into the Ritual Space, we can simply do (parts of) the Ceremonial Part twice.</p>\n\n<p>The event is free (including drinks and some food) but you can bring food if you want to contribute.</p>\n\n<p>Languages at the event will be a mix of German and English. The Ceremonial Part will involve at least one German-language and one English-language performance.</p>\n\n<p>Any questions?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Secular_Solstice_Celebration___And_the_Inauguration_of_the_LW_Leipzig_Community_1\">Discussion article for the meetup : <a href=\"/meetups/u6\">Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community)", "anchor": "Discussion_article_for_the_meetup___Secular_Solstice_Celebration___And_the_Inauguration_of_the_LW_Leipzig_Community_", "level": 1}, {"title": "Discussion article for the meetup : Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community)", "anchor": "Discussion_article_for_the_meetup___Secular_Solstice_Celebration___And_the_Inauguration_of_the_LW_Leipzig_Community_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-01T02:50:25.202Z", "modifiedAt": null, "url": null, "title": "Anonymous feedback forms revisited", "slug": "anonymous-feedback-forms-revisited", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:21.804Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4yv5nMJctjLRcCRkC/anonymous-feedback-forms-revisited", "pageUrlRelative": "/posts/4yv5nMJctjLRcCRkC/anonymous-feedback-forms-revisited", "linkUrl": "https://www.lesswrong.com/posts/4yv5nMJctjLRcCRkC/anonymous-feedback-forms-revisited", "postedAtFormatted": "Sunday, December 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anonymous%20feedback%20forms%20revisited&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnonymous%20feedback%20forms%20revisited%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4yv5nMJctjLRcCRkC%2Fanonymous-feedback-forms-revisited%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anonymous%20feedback%20forms%20revisited%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4yv5nMJctjLRcCRkC%2Fanonymous-feedback-forms-revisited", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4yv5nMJctjLRcCRkC%2Fanonymous-feedback-forms-revisited", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<blockquote>\n<p>In 2011, I added an anonymous feedback form to <code>gwern.net</code>. It has worked well (116 entries) and justified the time it took to set up because it encourages people to correct various problems or tip me off on things. If you have a site, maybe you should add one too.</p>\n</blockquote>\n<p>For the full writeup, see <a href=\"http://www.gwern.net/About#anonymous-feedback\">http://www.gwern.net/About#anonymous-feedback</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zcvsZQWJBFK6SxK4K": 1, "ZsWDPoXcchbGneaMX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4yv5nMJctjLRcCRkC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 36, "extendedScore": null, "score": 1.4460762886176454e-06, "legacy": true, "legacyId": "24927", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-01T12:46:18.061Z", "modifiedAt": null, "url": null, "title": "Meetup :  London Practical Meetup - Calibration Training!", "slug": "meetup-london-practical-meetup-calibration-training", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gj7JHbt98MwdpNAud/meetup-london-practical-meetup-calibration-training", "pageUrlRelative": "/posts/Gj7JHbt98MwdpNAud/meetup-london-practical-meetup-calibration-training", "linkUrl": "https://www.lesswrong.com/posts/Gj7JHbt98MwdpNAud/meetup-london-practical-meetup-calibration-training", "postedAtFormatted": "Sunday, December 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20London%20Practical%20Meetup%20-%20Calibration%20Training!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20London%20Practical%20Meetup%20-%20Calibration%20Training!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGj7JHbt98MwdpNAud%2Fmeetup-london-practical-meetup-calibration-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20London%20Practical%20Meetup%20-%20Calibration%20Training!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGj7JHbt98MwdpNAud%2Fmeetup-london-practical-meetup-calibration-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGj7JHbt98MwdpNAud%2Fmeetup-london-practical-meetup-calibration-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u7'> London Practical Meetup - Calibration Training!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2013 02:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64 Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London is having another meetup this Sunday (08/12) at 2:00 PM. We are meeting at our usual venue - The Shakespeare's Head by Holborn tube station and this is our first practical in a while.</p>\n\n<p>The aim of the meetup will be improvement of our probability estimates methods by means of calibration. My current plan is to compile a list of statements and questions, check their answers, gamify the procedure and ask everyone to give Percentages and/or  Confidence Intervals regarding their predictions of said statements and question. E.g. 'I assign a 70% chance that the statement 'There will be more people at the Calibration Training practical than at the Social Meetup the prior week \" is true'. As I mentioned, I inted to gamify things at least a little bit and there will likely be different 'rounds' of question as well as 'winners'.</p>\n\n<p>Note: you are not required to prepare in any way as everything will be explained during the meetup, however it will help if you arrive on time.</p>\n\n<p><em>Reminder: The LW London Meetups are currently a weekly event - Every Sunday at 2:00 PM!</em></p>\n\n<p>If you want more information about the meetup or anything else come by our <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fuser%2FTenoke%2Foverview%2F&amp;v=1&amp;libId=b237c1bb-f45f-4ddf-8ad7-f3c525e8ef63&amp;out=https%3A%2F%2Fgroups.google.com%2Fforum%2F%3Ffromgroups%3D%23!forum%2Flesswronglondon&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Fnew%2F&amp;title=Overview%20for%20Tenoke%20-%20Less%20Wrong&amp;txt=google%20group&amp;jsonp=vglnk_jsonp_138590173042811\">google group</a> or alternatively to our <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fuser%2FTenoke%2Foverview%2F&amp;v=1&amp;libId=b237c1bb-f45f-4ddf-8ad7-f3c525e8ef63&amp;out=https%3A%2F%2Fwww.facebook.com%2Fgroups%2F380103898766356%2F&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Fnew%2F&amp;title=Overview%20for%20Tenoke%20-%20Less%20Wrong&amp;txt=facebook%20group&amp;jsonp=vglnk_jsonp_138590174411612\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u7'> London Practical Meetup - Calibration Training!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gj7JHbt98MwdpNAud", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "24932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____London_Practical_Meetup___Calibration_Training_\">Discussion article for the meetup : <a href=\"/meetups/u7\"> London Practical Meetup - Calibration Training!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2013 02:00:00AM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64 Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LessWrong London is having another meetup this Sunday (08/12) at 2:00 PM. We are meeting at our usual venue - The Shakespeare's Head by Holborn tube station and this is our first practical in a while.</p>\n\n<p>The aim of the meetup will be improvement of our probability estimates methods by means of calibration. My current plan is to compile a list of statements and questions, check their answers, gamify the procedure and ask everyone to give Percentages and/or  Confidence Intervals regarding their predictions of said statements and question. E.g. 'I assign a 70% chance that the statement 'There will be more people at the Calibration Training practical than at the Social Meetup the prior week \" is true'. As I mentioned, I inted to gamify things at least a little bit and there will likely be different 'rounds' of question as well as 'winners'.</p>\n\n<p>Note: you are not required to prepare in any way as everything will be explained during the meetup, however it will help if you arrive on time.</p>\n\n<p><em>Reminder: The LW London Meetups are currently a weekly event - Every Sunday at 2:00 PM!</em></p>\n\n<p>If you want more information about the meetup or anything else come by our <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fuser%2FTenoke%2Foverview%2F&amp;v=1&amp;libId=b237c1bb-f45f-4ddf-8ad7-f3c525e8ef63&amp;out=https%3A%2F%2Fgroups.google.com%2Fforum%2F%3Ffromgroups%3D%23!forum%2Flesswronglondon&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Fnew%2F&amp;title=Overview%20for%20Tenoke%20-%20Less%20Wrong&amp;txt=google%20group&amp;jsonp=vglnk_jsonp_138590173042811\">google group</a> or alternatively to our <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fuser%2FTenoke%2Foverview%2F&amp;v=1&amp;libId=b237c1bb-f45f-4ddf-8ad7-f3c525e8ef63&amp;out=https%3A%2F%2Fwww.facebook.com%2Fgroups%2F380103898766356%2F&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Fnew%2F&amp;title=Overview%20for%20Tenoke%20-%20Less%20Wrong&amp;txt=facebook%20group&amp;jsonp=vglnk_jsonp_138590174411612\">facebook group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____London_Practical_Meetup___Calibration_Training_1\">Discussion article for the meetup : <a href=\"/meetups/u7\"> London Practical Meetup - Calibration Training!</a></h2>", "sections": [{"title": "Discussion article for the meetup :  London Practical Meetup - Calibration Training!", "anchor": "Discussion_article_for_the_meetup____London_Practical_Meetup___Calibration_Training_", "level": 1}, {"title": "Discussion article for the meetup :  London Practical Meetup - Calibration Training!", "anchor": "Discussion_article_for_the_meetup____London_Practical_Meetup___Calibration_Training_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-01T13:17:08.479Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver rationalists - new meetup location", "slug": "meetup-vancouver-rationalists-new-meetup-location", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kytael", "createdAt": "2010-09-06T10:08:43.209Z", "isAdmin": false, "displayName": "Kytael"}, "userId": "PTeFKC8ezhTDEi2qL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LqBLGxem6akk8Cy5g/meetup-vancouver-rationalists-new-meetup-location", "pageUrlRelative": "/posts/LqBLGxem6akk8Cy5g/meetup-vancouver-rationalists-new-meetup-location", "linkUrl": "https://www.lesswrong.com/posts/LqBLGxem6akk8Cy5g/meetup-vancouver-rationalists-new-meetup-location", "postedAtFormatted": "Sunday, December 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20rationalists%20-%20new%20meetup%20location&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20rationalists%20-%20new%20meetup%20location%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqBLGxem6akk8Cy5g%2Fmeetup-vancouver-rationalists-new-meetup-location%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20rationalists%20-%20new%20meetup%20location%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqBLGxem6akk8Cy5g%2Fmeetup-vancouver-rationalists-new-meetup-location", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqBLGxem6akk8Cy5g%2Fmeetup-vancouver-rationalists-new-meetup-location", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u8'>Vancouver rationalists - new meetup location</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 December 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2505 W Broadway, vancouver, BC, Canada </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meetup! discussions, philosophy puzzles, Rationality Dojo exercises.</p>\n\n<p>New location this time</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u8'>Vancouver rationalists - new meetup location</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LqBLGxem6akk8Cy5g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.4467087952866566e-06, "legacy": true, "legacyId": "24933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_rationalists___new_meetup_location\">Discussion article for the meetup : <a href=\"/meetups/u8\">Vancouver rationalists - new meetup location</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 December 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2505 W Broadway, vancouver, BC, Canada </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly meetup! discussions, philosophy puzzles, Rationality Dojo exercises.</p>\n\n<p>New location this time</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_rationalists___new_meetup_location1\">Discussion article for the meetup : <a href=\"/meetups/u8\">Vancouver rationalists - new meetup location</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver rationalists - new meetup location", "anchor": "Discussion_article_for_the_meetup___Vancouver_rationalists___new_meetup_location", "level": 1}, {"title": "Discussion article for the meetup : Vancouver rationalists - new meetup location", "anchor": "Discussion_article_for_the_meetup___Vancouver_rationalists___new_meetup_location1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-02T05:44:05.185Z", "modifiedAt": "2021-08-18T16:47:41.668Z", "url": null, "title": "Reasons to believe", "slug": "reasons-to-believe", "viewCount": null, "lastCommentedAt": "2013-12-13T07:27:58.916Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "irrational", "createdAt": "2011-10-04T19:46:41.913Z", "isAdmin": false, "displayName": "irrational"}, "userId": "TaHr6NuudyhaHevgQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/32zr333kCj2fRXWje/reasons-to-believe", "pageUrlRelative": "/posts/32zr333kCj2fRXWje/reasons-to-believe", "linkUrl": "https://www.lesswrong.com/posts/32zr333kCj2fRXWje/reasons-to-believe", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reasons%20to%20believe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReasons%20to%20believe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32zr333kCj2fRXWje%2Freasons-to-believe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reasons%20to%20believe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32zr333kCj2fRXWje%2Freasons-to-believe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32zr333kCj2fRXWje%2Freasons-to-believe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 296, "htmlBody": "<p>I've been thinking recently that I believe in the Theory of Evolution on about the same level as in the Theory of Plate Tectonics. I have grown up being taught that both are true, and I am capable of doing research in either field, or at least reading the literature to examine them for myself. I have not done so in either case, to any reasonable extent.</p>\n<p>I am not swayed by the fact that some people consider the former (and not so much the latter) to be controversial, primarily because those people aren't scientists. I tend to be self-congratulatory about this fact, but then I think that I am essentially not interested in examining the evidence, but I am essentially taking it on faith (which the creationists are quick to point out). I think I have good Bayesian reasons to take science on faith (rather than, say, mythology that is being offered in its stead), but do I therefore have good reasons to accept a particular well-established scientific theory on faith, or is it incumbent upon me to examine it, if I think its conclusions are important to my life?</p>\n<p>In other words, is it epistemologically wrong to rely on an authority that has produced a number of correct statements (that I could and did verify) to be more or less correct in the future? If I think of this problem as a sort of belief network, with a parent node that has causal connections to hundreds of children, I think such a reliance is reasonable, once you establish that the authority is indeed accurate. On the other hand, <em>appeal to authority</em> is probably the most famous fallacy there is.</p>\n<p>Any thoughts? If Eliezer or other people have written on this exact topic, a reference would be appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "32zr333kCj2fRXWje", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 1.4477058438605724e-06, "legacy": true, "legacyId": "24944", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-12-02T05:44:05.185Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-02T07:18:27.197Z", "modifiedAt": null, "url": null, "title": "In Praise of Tribes that Pretend to Try: Counter-\"Critique of Effective Altruism\"", "slug": "in-praise-of-tribes-that-pretend-to-try-counter-critique-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:02.359Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i2YaMFDE89fz5xPaQ/in-praise-of-tribes-that-pretend-to-try-counter-critique-of", "pageUrlRelative": "/posts/i2YaMFDE89fz5xPaQ/in-praise-of-tribes-that-pretend-to-try-counter-critique-of", "linkUrl": "https://www.lesswrong.com/posts/i2YaMFDE89fz5xPaQ/in-praise-of-tribes-that-pretend-to-try-counter-critique-of", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Praise%20of%20Tribes%20that%20Pretend%20to%20Try%3A%20Counter-%22Critique%20of%20Effective%20Altruism%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Praise%20of%20Tribes%20that%20Pretend%20to%20Try%3A%20Counter-%22Critique%20of%20Effective%20Altruism%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2YaMFDE89fz5xPaQ%2Fin-praise-of-tribes-that-pretend-to-try-counter-critique-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Praise%20of%20Tribes%20that%20Pretend%20to%20Try%3A%20Counter-%22Critique%20of%20Effective%20Altruism%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2YaMFDE89fz5xPaQ%2Fin-praise-of-tribes-that-pretend-to-try-counter-critique-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2YaMFDE89fz5xPaQ%2Fin-praise-of-tribes-that-pretend-to-try-counter-critique-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3519, "htmlBody": "<address><sub>Disclaimer: I endorse the EA movement and direct an EA/Transhumanist organization, www.IERFH.org</sub></address><address><br /></address>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.2954545454545454;\">We finally have created <a href=\"/r/discussion/lw/j8n/a_critique_of_effective_altruism/\">the first \"inside view\"</a><a href=\"/r/discussion/lw/j8n/a_critique_of_effective_altruism/\"> critique of EA</a>.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The critique's main worry would please Hofstadter by being self-referential: Being the first, having taken too long to emerge, thus indicating that EA's (Effective Altruists) are pretending to try instead of actually trying, or else they&rsquo;d have self-criticized already. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Here I will try to clash head-on with what seems to be the most important point of that critique. This will be the only point I'll address, for the sake of brevity, mnemonics and force of argument. This is a meta-contrarian apostasy, in its purpose. I'm not sure it is a view I hold, anymore than a view I think has to be out there in the open, being thought of and criticized. I am mostly indebted to </span><a style=\"text-decoration: none;\" href=\"/r/discussion/lw/j3q/happiness_and_productivity_living_alone_living/a2fe\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">this comment by Viliam_Bur</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, which was marinating in my mind while I read Ben Kuhn's apostasy.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong style=\"font-weight: normal;\"><br /><a style=\"text-decoration: none;\" href=\"/r/discussion/lw/j8n/a_critique_of_effective_altruism/#acknowledgments\"></a></strong></p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 9pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Original Version Abstract </span></h2>\n<blockquote>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Effective altruism is, to my knowledge, the first time that a substantially useful set of ethics and frameworks to analyze one&rsquo;s effect on the world has gained a broad enough appeal to resemble a social movement. (I&rsquo;d say these principles are something like </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">altruism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">maximization</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">egalitarianism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">consequentialism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">; together they imply many improvements over the social default for trying to do good in the world&mdash;earning to give as opposed to doing direct charity work, working in the developing world rather than locally, using evidence and feedback to analyze effectiveness, etc.) Unfortunately, as a movement effective altruism is failing to use these principles to acquire correct nontrivial beliefs about how to improve the world.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">By way of clarification, consider a distinction between two senses of the word &ldquo;trying&rdquo; I used above. Let&rsquo;s call them &ldquo;actually trying&rdquo; and &ldquo;pretending to try&rdquo;. Pretending to try to improve the world is something like responding to social pressure to improve the world by querying your brain for a thing which improves the world, taking the first search result and rolling with it. For example, for a while I thought that I would try to improve the world by developing computerized methods of checking informally-written proofs, thus allowing more scalable teaching of higher math, democratizing education, etc. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Coincidentally</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, computer programming and higher math happened to be the two things that I was best at. This is pretending to try. Actually trying is looking at the things that improve the world, figuring out which one maximizes utility, and then doing that thing. For instance, I now run an effective altruist student organization at Harvard because I realized that even though I&rsquo;m a comparatively bad leader and don&rsquo;t enjoy it very much, it&rsquo;s still very high-impact if I work hard enough at it. This isn&rsquo;t to say that I&rsquo;m actually trying yet, but I&rsquo;ve gotten closer.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using this distinction between pretending and actually trying, I would summarize a lot of effective altruism as &ldquo;pretending to actually try&rdquo;. As a social group, effective altruists have successfully noticed the pretending/actually-trying distinction. But they seem to have stopped there, assuming that knowing the difference between fake trying and actually trying translates into ability to actually try. Empirically, it most certainly doesn&rsquo;t. A lot of effective altruists still end up satisficing&mdash;finding actions that are on their face acceptable under core EA standards and then picking those which seem appealing because of other essentially random factors. This is more likely to converge on good actions than what society does by default, because the principles are better than society&rsquo;s default principles. Nevertheless, it fails to make much progress over what is directly obvious from the core EA principles. As a result, although &ldquo;doing effective altruism&rdquo; feels like truth-seeking, it often ends up being just a more credible way to pretend to try.</span></p>\n</blockquote>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Verdana; font-size: 16px; font-weight: bold; white-space: pre-wrap; line-height: 1.15;\">Counterargument: Tribes have internal structure, and so should the EA movement. </span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This includes a free reconstruction, containing nearly the whole original, of what I took to be important in Viliam's comment. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Feeling-oriented, and outcome-oriented communities</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">People probably need </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">two kinds</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> of communities -- let's call them \"feelings-oriented community\" and \"outcome-oriented community\". To many people this division has been \"home\" and \"work\" over the centuries, but that has some misleading connotations. A very popular medieval alternative was \"church\" and \"work\". Organized large scale societies have many alternatives that fill up these roles, to greater or lesser degrees. Indigenous tribes have the three realms separated, \"work\" has a time and a place, likewise, rituals and late afternoon discussions, chants etc... fulfill the purpose of \"church\". </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A \"feelings-oriented community\" is a community of people who meet because they enjoy being together and feel safe with each other. The examples are a functional family, a church group, friends meeting in a pub, etc... One of the important properties of feeling oriented communities, that according to Dennett has not yet sunk in the naturalist community is that </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">nothing is a precondition for belonging to the group which feels, or the sacredness taking place</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. You could spend the rest of your life going to church without becoming a priest, listening to the tribal leaders and shamans talk without saying a word. There are no pre-requisites to become your parent's son, or your sister's brother every time you enter the house. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">An \"outcome-oriented community\" is a community that has an explicit goal, and people genuinely contribute to making that goal happen. The examples are a business company, an NGO, a Toastmasters meetup, <a href=\"/r/discussion/lw/j3q/happiness_and_productivity_living_alone_living/a2di\">an intentional household</a> etc... To become a member of an outcome-oriented community, you have to show that you are willing and able to bring about the goal (either for itself, or in exchange of something valuable). There is some tolerance if you stop doing things well, either by ignorance or, say, bad health. But the tolerance is finite and the group can frown upon, punish, or even expel those who are not clearly helping the goal. &nbsp;</span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">What are communities good for? What is good for communities?</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The important part (to define what kind of group something is) is what really happens inside the members' heads, not what they pretend to do. For example, you could have an NGO with twelve members, where two of them want to have the work done, but the remaining ten only come to socialize. Of course, even those ten will verbally support the explicit goals of the organization, but they will be much more relaxed about timing, care less about verifying the outcomes, etc. For them, the explicit goals are merely a source of identity and a pretext to meet people professing similar values; for them, the community </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">is</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the real goal. If they had a magic button which would instantly solve the problem, making the organization obviously obsolete, they </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">wouldn't</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> push it. The people who are serious about the goal would love to see it completed as soon as possible, so they can move to some other goals. (I have seen a similar tension in a few organizations, and the usual solution seems to be the serious members forming an \"organization within an organization\", keeping the other ones around them for social and other purposes.)</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">As an evolutionary just-so story, we have a tribe composed of many different people, and within the tribe we have a hunters group, containing the best hunters. Members of the tribe are required to follow the norms of the tribe. Hunters must be efficient in their jobs. But hunters don't become a separate tribe... they go hunting for a while, and then return back to their original tribe. The tribe membership is for life, or at least for a long time; it provides safety and fulfills the emotional needs. Each hunting expedition is a short-termed event; it requires skills and determination. If a hunter breaks his legs, he can no longer be a hunter; but he still remains a member of his tribe. The hunter has now descended from the feeling and work status, to only the feeling status, this is part of expected cycles - a woman may stop working while having a child, a teenager may decide work is evil and stop working, an existentialist may pause for a year to reflect on the value of life itself in different ways - but throughout they do are not cast away from the reassuring arms of the \"feeling's oriented community\". </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A healthy double layered movement</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Viliam and I think a healthy way of living should be modeled like this; on two layers. To have a larger tribe based on shared values (rationality and altruism), and within this tribe a few working groups, both long-term (MIRI, CFAR) and short-term (organizers of the next meetup). Of course it could be a few overlapping tribes (the rationalists, the altruists), but the important thing is that you keep your social network even if you stop participating in some specific project -- otherwise we get either cultish pressure (you have to remain hard-working on our project even if you no longer feel so great about it, or you lose your whole social network) or inefficiency (people remain formally members of the project, but lately barely any work gets done, and the more active ones are warned not to rock the boat). Joining or leaving a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">project</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> should not be motivated or punished </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">socially</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This is the crux of Viliam's argument and of my disagreement with Ben's Critique: The Effective Altruist community has grown large enough that it can easily afford to have two kinds of communities inside it: The feelings-oriented EA's, whom Ben calls (unfairly in my opinion) </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">pretending to try to be effective altruists</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and the outcome-oriented EA&rsquo;s, whom are </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Really trying to be effective altruists.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now that is not how he put it in his critique. He used the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">fact that that critique had not been written</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, as sufficiently strong indication that the whole movement, a monolithic, single entity, had failed it&rsquo;s task of being introspective enough about it&rsquo;s failure modes. This is unfair on two accounts, someone had to be the first</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and the movement seems young enough that that is not a problem, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">and it is false that the entire movement is a single monolithic entity making wrong and right decisions in a void</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. The truth is that there are many people in the EA community in different stages of life, and of involvement with the movement. We should account for that and make room for newcomers as well as for ancient sages. EA is not one single entity that made one huge mistake. It is a couple thousand people, whose subgroups are working hard on several distinct things, frequently without communicating, and whose supergoal is reborn every day with the pushes and drifts going on inside the community. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Intentional Agents, communities or individuals, are not monolithic</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Most importantly, if you consider the argument above that Effective Altruim can&rsquo;t be criticized on accounts of being one single entity, because factually, it isn&rsquo;t, then I wish you to bring this intuition pump one step further: Each one of us is also not one single monolithic agent. We have good and bad days, and we are made of lots of tiny little agents within, whose goals and purposes are only our own when enough of them coalesce so that our overall behavior goes in a certain direction. Just like you can&rsquo;t critize EA </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">as a whole</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for something that it&rsquo;s subsets haven&rsquo;t done (the fancy philosophers word for this is mereological fallacy), likewise you can&rsquo;t claim about a particular individual that he, as a whole, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">pretends to try</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, because you&rsquo;ve seen him have one or two lazy days, or if he is still addicted to a particular video game. Don&rsquo;t forget the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">demanding objection</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> to utilitarianism, if you ask of a smoker to stop smoking because it is irrational to smoke, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><em>and he believes you</em></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, he may end up abandoning rationalism just because a small subset of him was addicted to smoking, and he just couldn't live with that much inconsistency in his self view. Likewise, if to be an utilitarian is infinitely demanding, you lose the utilitarians to &ldquo;<a href=\"/lw/grn/whatthehell_cognitive_failure_mode_a_separate/\">what the hell</a>&rdquo; effects. &nbsp;</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The same goes for Effective Altruists. Ben&rsquo;s post makes the case for </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">really effective altruism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> too demanding. Not even inside we are truly and really a monolithic entity, or a utility function optimizer - regardless of how much we may wish we were. My favoured reading of the current state of the Effective Altruist people is not that they are pretending to really try, but that most people are finding, for themselves, which are the aspects of their personalities they are willing to bend for altruism, and which they are not.<a href=\"/r/all/lw/gqk/calibrating_against_undetectable_utilons_and_goal/\"> </a><a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">I </a></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">don&rsquo;t expect</a> and don&rsquo;t think anyone should expect</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that any single individual becomes a perfect altruist. There are parts of us that just won&rsquo;t let go of some thing they crave for and praise. We don&rsquo;t want to lose the entire community if one individual is not effective enough, and we don&rsquo;t want to lose one individual if a part of him, or a time-slice, is not satisfying the canonical expectation of the outcome-oriented community. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rationalists already accepted a layered structure</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We need to accept, as EA&rsquo;s, what Lesswrong as blog has accepted, there will always be a group that is passive, and feeling-oriented, and a group that is outcome-oriented. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Even if the subject matter of Effective Altruism is outcome</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">For a less sensitive example, consider an average job: you may think about your colleagues as your friends, but if you leave the job, how many of them will you keep regular contact with? In contrast with this, a regular church just asks you to come to sunday prayers, gives you some keywords and a few relatively simple rules. If this level of participation is ideal for you, welcome, brother or sister! And if you want more, feel free to join some higher-commitment group </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">within</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the church. You choose the level of your participation, and you can change it during your life. For a non-religious example, in a dance group you could just go and dance, or chose to do the new year&rsquo;s presentation, or choose to find new dancers, all the way up to being the dance organizer and coordinator. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The current rationalist community has solved this problem to some extent. Your level of participation can range from being a lurker at LW, all the way up, from meetup organizer to CFAR creator to writing the next HPMOR or it&rsquo;s analogue. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Viliam ends his comment by saying: It would be great to have a LW village, where some people would work on effective altruism, others would work on building artificial intelligence, yet others would develop a rationality curriculum, and some would be too busy with their personal issues to do any of this now... but everyone would know that this is a village where good and sane people live, where cool things happen, and whichever of these </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">good and real</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> goals I will choose to prioritize, it's still a community where I belong. Actually, it would be great to have a village where 5% or 10% of people would be the LW community. Connotatively, it's not about being </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">away</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> from other people, but about being </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">with</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> my people. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The challenge, in my view from now on is not how to make effective altruists stop pretending, but </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">how to surround effective altruists with welcoming arms even when the subset of them that is active at that moment is not doing the right things?</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> How can we make EA&rsquo;s a loving and caring community of people who help each other, so that people feel taken care of enough that they actually have the attentional and emotional resources necessary to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">really go there and do the impossible.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Here are some examples of this layered system working in non-religious non-tribal settings: Lesswrong has a karma system to tell different functions within the community. It also has meetups, it also has a Study Hall, and it also has strong relations with CFAR and MIRI. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Leverage research, as community/house has active hard-core members, new hirees, people in training, and friends/relationships of people there, very different outcomes expected from each. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Transhumanists have people who only self-identify, people who attend events, people who write for H+ magazine, a board of directors, and it goes all the way up to Nick Bostrom, who spends 70 hours a week working on academic content in related topics. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The solution is not </span><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">just</span><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> introspection, but the maintenance of a welcoming environment at every layer of effectiveness</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Effective Altruist community does not need </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">to get introspectively even more focused on effectiveness</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> - at least not right now - what it needs is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">a designed hierarchical structure which allows it to let everyone in, and let everyone transition smoothly between different levels of commitment</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Most people will transition upward, since understanding more makes you more interested, more effective, etc&hellip; in an upward spiral. But people also need to be able to slide down for a bit. To meet their relatives for thanksgiving, to play Go with their workfriends, to dance, to pretend they don&rsquo;t care about animals. To </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">do their thing</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Their internal thing which has not converted to EA like the rest of them have. This is not only okay, it is not only tolerable, it is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">essential for the movement&rsquo;s survival.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">But then how can those who are </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">at their very best</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, healthy, strong, smart, and at the edge of the movement push it forward? </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Here is an obvious place </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">not </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">to do it: <a href=\"https://www.facebook.com/groups/effective.altruists/\">Open groups on Facebook</a>. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Open Facebook is not the place to move it forward. Some people who are recognized as being in the forefront of the movement, like Toby, Will, Holden, Beckstead, Wise and others should create an &ldquo;advancing Effective Altruism&rdquo; group on facebook, and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">there and then will be a place where no blood will be shed </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">on the hands of neither the feeling-oriented, nor the outcome-oriented group by having to decrease the signal to noise ratio within either. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now once we create this hierarchy within the movement (not only the groups, but the mental hierarchy, and the feeling that it is fine to be at a feeling-oriented moment, or to have feeling-oriented experiences) we will also want to increase the chance that people will move up the hierarchical ladder. As many as possible, as soon as possible, after all, the higher up you are, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">by definition</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the more likely you are to be generating good outcome. We have already started doing so. The <a href=\"https://www.facebook.com/groups/600419023315408/\">EA Self-Help</a> (secret) group <a href=\"https://www.facebook.com/events/693376370675223/?notif_t=plan_user_invited\">on Facebook</a> serves this very purpose, it helps altruists when they are feeling down, unproductive, sad, or anything actually, and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">we will hear you and embrace you even if you are not being particularly effective and altruistic when you get there</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. It is the legacy of our deceased friend, Jonatas, to all of us, because of him, we now have some understanding that people need love and companionship </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">especially when they are down</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Or we may lose all of their future good moments. The monolithic individual fallacy is a very pricy one to pay. Let us not learn the hard way by losing another member. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Conclusions</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong id=\"docs-internal-guid-4c90462b-b24e-7eee-b66c-2cea94b27825\" style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I have argued here that the main problem indicated in Ben&rsquo;s writing, that </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">effective altruists are pretending to really try</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, is not to be viewed in this light. Instead, I argued that the very survival of the Effective Altruist movement may rely on finding a welcoming space for something that Viliam_Bur has called </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">feeling-oriented</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> community, without which many people would leave the movement, by experiencing it as too demanding during their bad times, or if it strongly conflicted a particular subset of themselves they consider important. Instead I advocate for hierarchically separate communities </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">within</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the movement, allowing those who are at any particular level of commitment to grow stronger and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">win</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The first three initial measures I suggest for this re-design of the community are:</span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1) Making all effective altruists aware that the EA self-help group exists for anyone who, for any reason, wants help from the community, even for non EA related affairs. </span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2) Creating a Closed Facebook group with only those who are advancing the discussion at its best, for instance those who wrote long posts in their own blogs about it, or obvious major figures. </span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">3) Creating a Study Hall equivalent for EA&rsquo;s to increase their feeling of belonging to a large tribe of goal-sharing people, where they can lurk even when they have nothing to say, and just do a few pomodoros. </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This is my first long writing on Effective Altruism, and my first attempt at an apostasy, and my first explicit attempt to be meta-contrarian. I hope I may have helped shed some light on the discussion, and that my critique can be taken by all, specially Ben, to be oriented envisioning the same large scale goal that is shared by effective altruists around the world. The outlook of effective altruism is still being designed every day by all of us, and I hope my critique can be used, along with Ben&rsquo;s and others, to build not only a movement that is stronger in it&rsquo;s individuals emotions, as I have advocated here, but furthermore in being psychologically healthy and functional group, a whole that understands the role of its parts, and subdivides accordingly. </span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i2YaMFDE89fz5xPaQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 21, "extendedScore": null, "score": 1.4478012397403456e-06, "legacy": true, "legacyId": "24943", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<address><sub>Disclaimer: I endorse the EA movement and direct an EA/Transhumanist organization, www.IERFH.org</sub></address><address><br></address>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.2954545454545454;\">We finally have created <a href=\"/r/discussion/lw/j8n/a_critique_of_effective_altruism/\">the first \"inside view\"</a><a href=\"/r/discussion/lw/j8n/a_critique_of_effective_altruism/\"> critique of EA</a>.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The critique's main worry would please Hofstadter by being self-referential: Being the first, having taken too long to emerge, thus indicating that EA's (Effective Altruists) are pretending to try instead of actually trying, or else they\u2019d have self-criticized already. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Here I will try to clash head-on with what seems to be the most important point of that critique. This will be the only point I'll address, for the sake of brevity, mnemonics and force of argument. This is a meta-contrarian apostasy, in its purpose. I'm not sure it is a view I hold, anymore than a view I think has to be out there in the open, being thought of and criticized. I am mostly indebted to </span><a style=\"text-decoration: none;\" href=\"/r/discussion/lw/j3q/happiness_and_productivity_living_alone_living/a2fe\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">this comment by Viliam_Bur</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, which was marinating in my mind while I read Ben Kuhn's apostasy.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong style=\"font-weight: normal;\"><br><a style=\"text-decoration: none;\" href=\"/r/discussion/lw/j8n/a_critique_of_effective_altruism/#acknowledgments\"></a></strong></p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 9pt; text-align: justify;\" dir=\"ltr\" id=\"Original_Version_Abstract_\"><span style=\"font-size: 16px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Original Version Abstract </span></h2>\n<blockquote>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Effective altruism is, to my knowledge, the first time that a substantially useful set of ethics and frameworks to analyze one\u2019s effect on the world has gained a broad enough appeal to resemble a social movement. (I\u2019d say these principles are something like </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">altruism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">maximization</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">egalitarianism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">consequentialism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">; together they imply many improvements over the social default for trying to do good in the world\u2014earning to give as opposed to doing direct charity work, working in the developing world rather than locally, using evidence and feedback to analyze effectiveness, etc.) Unfortunately, as a movement effective altruism is failing to use these principles to acquire correct nontrivial beliefs about how to improve the world.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">By way of clarification, consider a distinction between two senses of the word \u201ctrying\u201d I used above. Let\u2019s call them \u201cactually trying\u201d and \u201cpretending to try\u201d. Pretending to try to improve the world is something like responding to social pressure to improve the world by querying your brain for a thing which improves the world, taking the first search result and rolling with it. For example, for a while I thought that I would try to improve the world by developing computerized methods of checking informally-written proofs, thus allowing more scalable teaching of higher math, democratizing education, etc. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Coincidentally</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, computer programming and higher math happened to be the two things that I was best at. This is pretending to try. Actually trying is looking at the things that improve the world, figuring out which one maximizes utility, and then doing that thing. For instance, I now run an effective altruist student organization at Harvard because I realized that even though I\u2019m a comparatively bad leader and don\u2019t enjoy it very much, it\u2019s still very high-impact if I work hard enough at it. This isn\u2019t to say that I\u2019m actually trying yet, but I\u2019ve gotten closer.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using this distinction between pretending and actually trying, I would summarize a lot of effective altruism as \u201cpretending to actually try\u201d. As a social group, effective altruists have successfully noticed the pretending/actually-trying distinction. But they seem to have stopped there, assuming that knowing the difference between fake trying and actually trying translates into ability to actually try. Empirically, it most certainly doesn\u2019t. A lot of effective altruists still end up satisficing\u2014finding actions that are on their face acceptable under core EA standards and then picking those which seem appealing because of other essentially random factors. This is more likely to converge on good actions than what society does by default, because the principles are better than society\u2019s default principles. Nevertheless, it fails to make much progress over what is directly obvious from the core EA principles. As a result, although \u201cdoing effective altruism\u201d feels like truth-seeking, it often ends up being just a more credible way to pretend to try.</span></p>\n</blockquote>\n<p style=\"line-height: 1.2954545454545454; margin-top: 4pt; margin-bottom: 15pt; margin-left: 4pt; margin-right: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Verdana; font-size: 16px; font-weight: bold; white-space: pre-wrap; line-height: 1.15;\">Counterargument: Tribes have internal structure, and so should the EA movement. </span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong style=\"font-weight: normal;\"><br></strong></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This includes a free reconstruction, containing nearly the whole original, of what I took to be important in Viliam's comment. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\" id=\"Feeling_oriented__and_outcome_oriented_communities\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Feeling-oriented, and outcome-oriented communities</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">People probably need </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">two kinds</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> of communities -- let's call them \"feelings-oriented community\" and \"outcome-oriented community\". To many people this division has been \"home\" and \"work\" over the centuries, but that has some misleading connotations. A very popular medieval alternative was \"church\" and \"work\". Organized large scale societies have many alternatives that fill up these roles, to greater or lesser degrees. Indigenous tribes have the three realms separated, \"work\" has a time and a place, likewise, rituals and late afternoon discussions, chants etc... fulfill the purpose of \"church\". </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A \"feelings-oriented community\" is a community of people who meet because they enjoy being together and feel safe with each other. The examples are a functional family, a church group, friends meeting in a pub, etc... One of the important properties of feeling oriented communities, that according to Dennett has not yet sunk in the naturalist community is that </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">nothing is a precondition for belonging to the group which feels, or the sacredness taking place</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. You could spend the rest of your life going to church without becoming a priest, listening to the tribal leaders and shamans talk without saying a word. There are no pre-requisites to become your parent's son, or your sister's brother every time you enter the house. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">An \"outcome-oriented community\" is a community that has an explicit goal, and people genuinely contribute to making that goal happen. The examples are a business company, an NGO, a Toastmasters meetup, <a href=\"/r/discussion/lw/j3q/happiness_and_productivity_living_alone_living/a2di\">an intentional household</a> etc... To become a member of an outcome-oriented community, you have to show that you are willing and able to bring about the goal (either for itself, or in exchange of something valuable). There is some tolerance if you stop doing things well, either by ignorance or, say, bad health. But the tolerance is finite and the group can frown upon, punish, or even expel those who are not clearly helping the goal. &nbsp;</span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\" id=\"What_are_communities_good_for__What_is_good_for_communities_\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">What are communities good for? What is good for communities?</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The important part (to define what kind of group something is) is what really happens inside the members' heads, not what they pretend to do. For example, you could have an NGO with twelve members, where two of them want to have the work done, but the remaining ten only come to socialize. Of course, even those ten will verbally support the explicit goals of the organization, but they will be much more relaxed about timing, care less about verifying the outcomes, etc. For them, the explicit goals are merely a source of identity and a pretext to meet people professing similar values; for them, the community </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">is</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the real goal. If they had a magic button which would instantly solve the problem, making the organization obviously obsolete, they </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">wouldn't</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> push it. The people who are serious about the goal would love to see it completed as soon as possible, so they can move to some other goals. (I have seen a similar tension in a few organizations, and the usual solution seems to be the serious members forming an \"organization within an organization\", keeping the other ones around them for social and other purposes.)</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">As an evolutionary just-so story, we have a tribe composed of many different people, and within the tribe we have a hunters group, containing the best hunters. Members of the tribe are required to follow the norms of the tribe. Hunters must be efficient in their jobs. But hunters don't become a separate tribe... they go hunting for a while, and then return back to their original tribe. The tribe membership is for life, or at least for a long time; it provides safety and fulfills the emotional needs. Each hunting expedition is a short-termed event; it requires skills and determination. If a hunter breaks his legs, he can no longer be a hunter; but he still remains a member of his tribe. The hunter has now descended from the feeling and work status, to only the feeling status, this is part of expected cycles - a woman may stop working while having a child, a teenager may decide work is evil and stop working, an existentialist may pause for a year to reflect on the value of life itself in different ways - but throughout they do are not cast away from the reassuring arms of the \"feeling's oriented community\". </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\" id=\"A_healthy_double_layered_movement\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A healthy double layered movement</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Viliam and I think a healthy way of living should be modeled like this; on two layers. To have a larger tribe based on shared values (rationality and altruism), and within this tribe a few working groups, both long-term (MIRI, CFAR) and short-term (organizers of the next meetup). Of course it could be a few overlapping tribes (the rationalists, the altruists), but the important thing is that you keep your social network even if you stop participating in some specific project -- otherwise we get either cultish pressure (you have to remain hard-working on our project even if you no longer feel so great about it, or you lose your whole social network) or inefficiency (people remain formally members of the project, but lately barely any work gets done, and the more active ones are warned not to rock the boat). Joining or leaving a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">project</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> should not be motivated or punished </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">socially</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This is the crux of Viliam's argument and of my disagreement with Ben's Critique: The Effective Altruist community has grown large enough that it can easily afford to have two kinds of communities inside it: The feelings-oriented EA's, whom Ben calls (unfairly in my opinion) </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">pretending to try to be effective altruists</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and the outcome-oriented EA\u2019s, whom are </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Really trying to be effective altruists.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now that is not how he put it in his critique. He used the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">fact that that critique had not been written</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, as sufficiently strong indication that the whole movement, a monolithic, single entity, had failed it\u2019s task of being introspective enough about it\u2019s failure modes. This is unfair on two accounts, someone had to be the first</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and the movement seems young enough that that is not a problem, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">and it is false that the entire movement is a single monolithic entity making wrong and right decisions in a void</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. The truth is that there are many people in the EA community in different stages of life, and of involvement with the movement. We should account for that and make room for newcomers as well as for ancient sages. EA is not one single entity that made one huge mistake. It is a couple thousand people, whose subgroups are working hard on several distinct things, frequently without communicating, and whose supergoal is reborn every day with the pushes and drifts going on inside the community. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\" id=\"Intentional_Agents__communities_or_individuals__are_not_monolithic\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Intentional Agents, communities or individuals, are not monolithic</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Most importantly, if you consider the argument above that Effective Altruim can\u2019t be criticized on accounts of being one single entity, because factually, it isn\u2019t, then I wish you to bring this intuition pump one step further: Each one of us is also not one single monolithic agent. We have good and bad days, and we are made of lots of tiny little agents within, whose goals and purposes are only our own when enough of them coalesce so that our overall behavior goes in a certain direction. Just like you can\u2019t critize EA </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">as a whole</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for something that it\u2019s subsets haven\u2019t done (the fancy philosophers word for this is mereological fallacy), likewise you can\u2019t claim about a particular individual that he, as a whole, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">pretends to try</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, because you\u2019ve seen him have one or two lazy days, or if he is still addicted to a particular video game. Don\u2019t forget the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">demanding objection</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> to utilitarianism, if you ask of a smoker to stop smoking because it is irrational to smoke, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><em>and he believes you</em></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, he may end up abandoning rationalism just because a small subset of him was addicted to smoking, and he just couldn't live with that much inconsistency in his self view. Likewise, if to be an utilitarian is infinitely demanding, you lose the utilitarians to \u201c<a href=\"/lw/grn/whatthehell_cognitive_failure_mode_a_separate/\">what the hell</a>\u201d effects. &nbsp;</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The same goes for Effective Altruists. Ben\u2019s post makes the case for </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">really effective altruism</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> too demanding. Not even inside we are truly and really a monolithic entity, or a utility function optimizer - regardless of how much we may wish we were. My favoured reading of the current state of the Effective Altruist people is not that they are pretending to really try, but that most people are finding, for themselves, which are the aspects of their personalities they are willing to bend for altruism, and which they are not.<a href=\"/r/all/lw/gqk/calibrating_against_undetectable_utilons_and_goal/\"> </a><a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">I </a></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><a href=\"/r/discussion/lw/g87/calibrating_against_undetectable_utilons_and_goal/\">don\u2019t expect</a> and don\u2019t think anyone should expect</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that any single individual becomes a perfect altruist. There are parts of us that just won\u2019t let go of some thing they crave for and praise. We don\u2019t want to lose the entire community if one individual is not effective enough, and we don\u2019t want to lose one individual if a part of him, or a time-slice, is not satisfying the canonical expectation of the outcome-oriented community. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\" id=\"Rationalists_already_accepted_a_layered_structure\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rationalists already accepted a layered structure</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We need to accept, as EA\u2019s, what Lesswrong as blog has accepted, there will always be a group that is passive, and feeling-oriented, and a group that is outcome-oriented. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Even if the subject matter of Effective Altruism is outcome</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">For a less sensitive example, consider an average job: you may think about your colleagues as your friends, but if you leave the job, how many of them will you keep regular contact with? In contrast with this, a regular church just asks you to come to sunday prayers, gives you some keywords and a few relatively simple rules. If this level of participation is ideal for you, welcome, brother or sister! And if you want more, feel free to join some higher-commitment group </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">within</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the church. You choose the level of your participation, and you can change it during your life. For a non-religious example, in a dance group you could just go and dance, or chose to do the new year\u2019s presentation, or choose to find new dancers, all the way up to being the dance organizer and coordinator. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The current rationalist community has solved this problem to some extent. Your level of participation can range from being a lurker at LW, all the way up, from meetup organizer to CFAR creator to writing the next HPMOR or it\u2019s analogue. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Viliam ends his comment by saying: It would be great to have a LW village, where some people would work on effective altruism, others would work on building artificial intelligence, yet others would develop a rationality curriculum, and some would be too busy with their personal issues to do any of this now... but everyone would know that this is a village where good and sane people live, where cool things happen, and whichever of these </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">good and real</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> goals I will choose to prioritize, it's still a community where I belong. Actually, it would be great to have a village where 5% or 10% of people would be the LW community. Connotatively, it's not about being </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">away</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> from other people, but about being </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">with</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> my people. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The challenge, in my view from now on is not how to make effective altruists stop pretending, but </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">how to surround effective altruists with welcoming arms even when the subset of them that is active at that moment is not doing the right things?</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> How can we make EA\u2019s a loving and caring community of people who help each other, so that people feel taken care of enough that they actually have the attentional and emotional resources necessary to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">really go there and do the impossible.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Here are some examples of this layered system working in non-religious non-tribal settings: Lesswrong has a karma system to tell different functions within the community. It also has meetups, it also has a Study Hall, and it also has strong relations with CFAR and MIRI. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Leverage research, as community/house has active hard-core members, new hirees, people in training, and friends/relationships of people there, very different outcomes expected from each. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Transhumanists have people who only self-identify, people who attend events, people who write for H+ magazine, a board of directors, and it goes all the way up to Nick Bostrom, who spends 70 hours a week working on academic content in related topics. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\" id=\"The_solution_is_not_just_introspection__but_the_maintenance_of_a_welcoming_environment_at_every_layer_of_effectiveness\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The solution is not </span><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">just</span><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> introspection, but the maintenance of a welcoming environment at every layer of effectiveness</span></h2>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Effective Altruist community does not need </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">to get introspectively even more focused on effectiveness</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> - at least not right now - what it needs is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">a designed hierarchical structure which allows it to let everyone in, and let everyone transition smoothly between different levels of commitment</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Most people will transition upward, since understanding more makes you more interested, more effective, etc\u2026 in an upward spiral. But people also need to be able to slide down for a bit. To meet their relatives for thanksgiving, to play Go with their workfriends, to dance, to pretend they don\u2019t care about animals. To </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">do their thing</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Their internal thing which has not converted to EA like the rest of them have. This is not only okay, it is not only tolerable, it is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">essential for the movement\u2019s survival.</span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">But then how can those who are </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">at their very best</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, healthy, strong, smart, and at the edge of the movement push it forward? </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Here is an obvious place </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">not </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">to do it: <a href=\"https://www.facebook.com/groups/effective.altruists/\">Open groups on Facebook</a>. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Open Facebook is not the place to move it forward. Some people who are recognized as being in the forefront of the movement, like Toby, Will, Holden, Beckstead, Wise and others should create an \u201cadvancing Effective Altruism\u201d group on facebook, and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">there and then will be a place where no blood will be shed </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">on the hands of neither the feeling-oriented, nor the outcome-oriented group by having to decrease the signal to noise ratio within either. </span></p>\n<p style=\"line-height: 1.2954545454545454; margin-top: 0pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now once we create this hierarchy within the movement (not only the groups, but the mental hierarchy, and the feeling that it is fine to be at a feeling-oriented moment, or to have feeling-oriented experiences) we will also want to increase the chance that people will move up the hierarchical ladder. As many as possible, as soon as possible, after all, the higher up you are, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">by definition</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the more likely you are to be generating good outcome. We have already started doing so. The <a href=\"https://www.facebook.com/groups/600419023315408/\">EA Self-Help</a> (secret) group <a href=\"https://www.facebook.com/events/693376370675223/?notif_t=plan_user_invited\">on Facebook</a> serves this very purpose, it helps altruists when they are feeling down, unproductive, sad, or anything actually, and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">we will hear you and embrace you even if you are not being particularly effective and altruistic when you get there</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. It is the legacy of our deceased friend, Jonatas, to all of us, because of him, we now have some understanding that people need love and companionship </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">especially when they are down</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Or we may lose all of their future good moments. The monolithic individual fallacy is a very pricy one to pay. Let us not learn the hard way by losing another member. </span></p>\n<h2 style=\"line-height: 1.2954545454545454; margin-top: 10pt; margin-bottom: 11pt; text-align: justify;\" dir=\"ltr\" id=\"Conclusions\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Conclusions</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong id=\"docs-internal-guid-4c90462b-b24e-7eee-b66c-2cea94b27825\" style=\"font-weight: normal;\"><br></strong></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I have argued here that the main problem indicated in Ben\u2019s writing, that </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">effective altruists are pretending to really try</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, is not to be viewed in this light. Instead, I argued that the very survival of the Effective Altruist movement may rely on finding a welcoming space for something that Viliam_Bur has called </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">feeling-oriented</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> community, without which many people would leave the movement, by experiencing it as too demanding during their bad times, or if it strongly conflicted a particular subset of themselves they consider important. Instead I advocate for hierarchically separate communities </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">within</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the movement, allowing those who are at any particular level of commitment to grow stronger and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">win</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong style=\"font-weight: normal;\"><br></strong></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The first three initial measures I suggest for this re-design of the community are:</span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1) Making all effective altruists aware that the EA self-help group exists for anyone who, for any reason, wants help from the community, even for non EA related affairs. </span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2) Creating a Closed Facebook group with only those who are advancing the discussion at its best, for instance those who wrote long posts in their own blogs about it, or obvious major figures. </span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">3) Creating a Study Hall equivalent for EA\u2019s to increase their feeling of belonging to a large tribe of goal-sharing people, where they can lurk even when they have nothing to say, and just do a few pomodoros. </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This is my first long writing on Effective Altruism, and my first attempt at an apostasy, and my first explicit attempt to be meta-contrarian. I hope I may have helped shed some light on the discussion, and that my critique can be taken by all, specially Ben, to be oriented envisioning the same large scale goal that is shared by effective altruists around the world. The outlook of effective altruism is still being designed every day by all of us, and I hope my critique can be used, along with Ben\u2019s and others, to build not only a movement that is stronger in it\u2019s individuals emotions, as I have advocated here, but furthermore in being psychologically healthy and functional group, a whole that understands the role of its parts, and subdivides accordingly. </span></p>", "sections": [{"title": "Original Version Abstract ", "anchor": "Original_Version_Abstract_", "level": 1}, {"title": "Feeling-oriented, and outcome-oriented communities", "anchor": "Feeling_oriented__and_outcome_oriented_communities", "level": 1}, {"title": "What are communities good for? What is good for communities?", "anchor": "What_are_communities_good_for__What_is_good_for_communities_", "level": 1}, {"title": "A healthy double layered movement", "anchor": "A_healthy_double_layered_movement", "level": 1}, {"title": "Intentional Agents, communities or individuals, are not monolithic", "anchor": "Intentional_Agents__communities_or_individuals__are_not_monolithic", "level": 1}, {"title": "Rationalists already accepted a layered structure", "anchor": "Rationalists_already_accepted_a_layered_structure", "level": 1}, {"title": "The solution is not just introspection, but the maintenance of a welcoming environment at every layer of effectiveness", "anchor": "The_solution_is_not_just_introspection__but_the_maintenance_of_a_welcoming_environment_at_every_layer_of_effectiveness", "level": 1}, {"title": "Conclusions", "anchor": "Conclusions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["E3beR7bQ723kkNHpA", "6zH8RWtbxDPTrdJNn", "fKS54Zd4SaBrjznzF", "GjXeKTxkYvZ5WfkFu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}