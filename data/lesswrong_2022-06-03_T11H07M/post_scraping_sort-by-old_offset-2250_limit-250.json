{"results": [{"createdAt": null, "postedAt": "2010-11-22T11:11:53.201Z", "modifiedAt": null, "url": null, "title": "Does cognitive therapy encourage bias?", "slug": "does-cognitive-therapy-encourage-bias-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qCYqcrNKzKFeskt2q/does-cognitive-therapy-encourage-bias-1", "pageUrlRelative": "/posts/qCYqcrNKzKFeskt2q/does-cognitive-therapy-encourage-bias-1", "linkUrl": "https://www.lesswrong.com/posts/qCYqcrNKzKFeskt2q/does-cognitive-therapy-encourage-bias-1", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20cognitive%20therapy%20encourage%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20cognitive%20therapy%20encourage%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqCYqcrNKzKFeskt2q%2Fdoes-cognitive-therapy-encourage-bias-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20cognitive%20therapy%20encourage%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqCYqcrNKzKFeskt2q%2Fdoes-cognitive-therapy-encourage-bias-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqCYqcrNKzKFeskt2q%2Fdoes-cognitive-therapy-encourage-bias-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 634, "htmlBody": "<div>\r\n<p>&nbsp;</p>\r\n<p>(This post might suffer from formatting problems. I'm pretty dumb with computers, so it's not a surprise to me, but if anyone out there knows how to fix it, I'd be grateful for the help.)</p>\r\n<address>Summary: Cognitive therapy may encourage [motivated cognition](<a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\"><span style=\"color: #8a8a8b;\">http://lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/</span></a>). My main source for this post is Judith Beck's [Cognitive Therapy: Basics and Beyond](<a href=\"http://www.amazon.com/Cognitive-Therapy-Judith-Beck-Phd/dp/0898628474/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418167&amp;sr=1-1\"><span style=\"color: #6a8a6b;\">http://www.amazon.com/Cognitive-Therapy-Judith-Beck-Phd/dp/0898628474/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418167&amp;sr=1-1</span></a>)</address>\r\n<p>\"[Cognitive behavioral therapy](<a href=\"http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy\"><span style=\"color: #8a8a8b;\">http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy</span></a>)\" (CBT) is a catch-all term for a variety of therapeutic practices and theories. Among other things, it aims to teach patients to modify their own beliefs. The rationale seems to be this:</p>\r\n<p>(1) Affect, behavior, and cognition are interrelated such that changes in one of the three will lead to changes in the other two.&nbsp;</p>\r\n<p>(2) Affective problems, such as depression, can thus be addressed in a roundabout fashion: modifying the beliefs from which the undesired feelings stem.</p>\r\n<p>So far, so good.&nbsp;And how does one modify destructive beliefs? CBT offers many techniques.</p>\r\n<p>Alas, included among them seems to be motivated skepticism. For example, consider a depressed college student. She and her therapist decide that one of her bad beliefs is \"I'm inadequate.\" They want to replace that bad one with a more positive one, namely, \"I'm adequate in most ways (but I'm only human, too).\" Their method is to do a worksheet comparing evidence for and against the old, negative belief. Listen to their dialog:</p>\r\n<p style=\"padding-left: 30px;\">[Therapist]: What evidence do you have that you're inadequate?</p>\r\n<p style=\"padding-left: 30px;\">[Patient]: Well, I didn't understand a concept my economics professor presented in class today.</p>\r\n<p style=\"padding-left: 30px;\">T: Okay, write that down on the right side, then put a big \"BUT\" next to it...Now, let's see if there could be another explanation for why you might not have understood the concept <em>other</em> than that you're inadequate.</p>\r\n<p style=\"padding-left: 30px;\">P: Well, it was the first time she talked about it. And it wasn't in the readings.</p>\r\n<p>Thus the bad belief is treated with suspicion.&nbsp;What's wrong with that? Well, see what they do about evidence <em>against</em> her inadequacy:</p>\r\n<p style=\"padding-left: 30px;\">&nbsp;T: Okay, let's try the left side now. What evidence do you have from <em>today</em> that you <em>are</em> adequate at many things? I'll warn you, this can be hard if your screen is operating.</p>\r\n<p style=\"padding-left: 30px;\">P: Well, I worked on my literature paper.</p>\r\n<p style=\"padding-left: 30px;\">T: Good. Write that down. What else?</p>\r\n<p style=\"padding-left: 30px;\">(pp. 179-180; ellipsis and emphasis both in the original)</p>\r\n<p>When they encounter evidence for the patient's bad belief, they investigate further, looking for ways to avoid inferring that she is inadequate. However, when they find evidence against the bad belief, they just chalk it up.</p>\r\n<p>This is not how one should approach evidence...assuming one wants correct beliefs.</p>\r\n<p>So why does Beck advocate this approach? Here are some possible reasons.</p>\r\n<p>A. If beliefs are keeping you depressed, maybe you should fight them even at the cost of a little correctness (and of the increased habituation to motivated cognition).</p>\r\n<p>B. Depressed patients are already predisposed to find the downside of any given event. They don't need help doubting themselves. Therefore, therapists' encouraging them to seek alternative explanations for negative events doesn't skew their beliefs. On the contrary, it helps to bring the depressed patients' beliefs back into correspondence with reality.</p>\r\n<p>C. Strictly speaking, this motivated cognition does not lead to false beliefs because beliefs of the form \"I'm inadequate,\" along with its more helpful replacement, are not truth-apt. They can't be true or false. After all, what experiences do they induce believers to [anticipate] (<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\"><span style=\"color: #8a8a8b;\">http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/</span></a>)? (If this were the rationale, then what&nbsp;would the sense of the term \"evidence\" be in this context?)</p>\r\n<p>What do you guys think? Is this common to other CBT authors as well?&nbsp;I've only read two other books in this vein (Albert Ellis and Robert A. Harper's [A Guide to Rational Living](<a href=\"http://www.amazon.com/Guide-Rational-Living-Albert-Ellis/dp/0879800429/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418180&amp;sr=1-1\"><span style=\"color: #6a8a6b;\">http://www.amazon.com/Guide-Rational-Living-Albert-Ellis/dp/0879800429/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418180&amp;sr=1-1</span></a>) and Jacqueline Persons' [Cognitive Therapy in Practice: A Case Formulation Approach](<a href=\"http://www.amazon.com/Cognitive-Therapy-Practice-Formulation-Approach/dp/0393700771/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1290420954&amp;sr=1-2\"><span style=\"color: #8a8a8b;\">http://www.amazon.com/Cognitive-Therapy-Practice-Formulation-Approach/dp/0393700771/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1290420954&amp;sr=1-2</span></a>)) and I can't recall either one explicitly doing this, but I may have missed it. I do remember that Ellis and Harper seemed to conflate instrumental and epistemic rationality.</p>\r\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qCYqcrNKzKFeskt2q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L32LHWzy9FzSDazEg", "a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T11:31:53.303Z", "modifiedAt": null, "url": null, "title": "Does cognitive therapy encourage bias?", "slug": "does-cognitive-therapy-encourage-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:38.267Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FJkfboEQjN3ritXk9/does-cognitive-therapy-encourage-bias", "pageUrlRelative": "/posts/FJkfboEQjN3ritXk9/does-cognitive-therapy-encourage-bias", "linkUrl": "https://www.lesswrong.com/posts/FJkfboEQjN3ritXk9/does-cognitive-therapy-encourage-bias", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20cognitive%20therapy%20encourage%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20cognitive%20therapy%20encourage%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJkfboEQjN3ritXk9%2Fdoes-cognitive-therapy-encourage-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20cognitive%20therapy%20encourage%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJkfboEQjN3ritXk9%2Fdoes-cognitive-therapy-encourage-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFJkfboEQjN3ritXk9%2Fdoes-cognitive-therapy-encourage-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 610, "htmlBody": "<div><address>Summary: Cognitive therapy may encourage <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">motivated cognition</a>. My main source for this post is Judith Beck's <a href=\"http://www.amazon.com/Cognitive-Therapy-Judith-Beck-Phd/dp/0898628474/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418167&amp;sr=1-1\">Cognitive Therapy: Basics and Beyond</a>.&nbsp;</address>\r\n<p>\"<a href=\"http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy\">Cognitive behavioral therapy</a>\" (CBT) is a catch-all term for a variety of therapeutic practices and theories. Among other things, it aims to teach patients to modify their own beliefs. The rationale seems to be this:</p>\r\n<p>(1) Affect, behavior, and cognition are interrelated such that changes in one of the three will lead to changes in the other two.&nbsp;</p>\r\n<p>(2) Affective problems, such as depression, can thus be addressed in a roundabout fashion: modifying the beliefs from which the undesired feelings stem.</p>\r\n<p>So far, so good.&nbsp;And how does one modify destructive beliefs? CBT offers many techniques.</p>\r\n<p>Alas, included among them seems to be motivated skepticism. For example, consider a depressed college student. She and her therapist decide that one of her bad beliefs is \"I'm inadequate.\" They want to replace that bad one with a more positive one, namely, \"I'm adequate in most ways (but I'm only human, too).\" Their method is to do a worksheet comparing evidence for and against the old, negative belief. Listen to their dialog:</p>\r\n<p style=\"padding-left: 30px;\">[Therapist]: What evidence do you have that you're inadequate?</p>\r\n<p style=\"padding-left: 30px;\">[Patient]: Well, I didn't understand a concept my economics professor presented in class today.</p>\r\n<p style=\"padding-left: 30px;\">T: Okay, write that down on the right side, then put a big \"BUT\" next to it...Now, let's see if there could be another explanation for why you might not have understood the concept <em>other</em> than that you're inadequate.</p>\r\n<p style=\"padding-left: 30px;\">P: Well, it was the first time she talked about it. And it wasn't in the readings.</p>\r\n<p>Thus the bad belief is treated with suspicion.&nbsp;What's wrong with that? Well, see what they do about evidence <em>against</em> her inadequacy:</p>\r\n<p style=\"padding-left: 30px;\">&nbsp;T: Okay, let's try the left side now. What evidence do you have from <em>today</em> that you <em>are</em> adequate at many things? I'll warn you, this can be hard if your screen is operating.</p>\r\n<p style=\"padding-left: 30px;\">P: Well, I worked on my literature paper.</p>\r\n<p style=\"padding-left: 30px;\">T: Good. Write that down. What else?</p>\r\n<p style=\"padding-left: 30px;\">(pp. 179-180; ellipsis and emphasis both in the original)</p>\r\n<p>When they encounter evidence for the patient's bad belief, they investigate further, looking for ways to avoid inferring that she is inadequate. However, when they find evidence against the bad belief, they just chalk it up.</p>\r\n<p>This is not how one should approach evidence...assuming one wants correct beliefs.</p>\r\n<p>So why does Beck advocate this approach? Here are some possible reasons.</p>\r\n<p>A. If beliefs are keeping you depressed, maybe you should fight them even at the cost of a little correctness (and of the increased habituation to motivated cognition).</p>\r\n<p>B. Depressed patients are already predisposed to find the downside of any given event. They don't need help doubting themselves. Therefore, therapists' encouraging them to seek alternative explanations for negative events doesn't skew their beliefs. On the contrary, it helps to bring the depressed patients' beliefs back into correspondence with reality.</p>\r\n<p>C. Strictly speaking, this motivated cognition does not lead to false beliefs because beliefs of the form \"I'm inadequate,\" along with its more helpful replacement, are not truth-apt. They can't be true or false. After all, what experiences do they induce believers to <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">anticipate</a>? (If this were the rationale, then what&nbsp;would the sense of the term \"evidence\" be in this context?)</p>\r\n<p>What do you guys think? Is this common to other CBT authors as well?&nbsp;I've only read two other books in this vein (Albert Ellis and Robert A. Harper's <a href=\"http://www.amazon.com/Guide-Rational-Living-Albert-Ellis/dp/0879800429/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418180&amp;sr=1-1\">A Guide to Rational Living</a> and Jacqueline Persons' <a href=\"http://www.amazon.com/Cognitive-Therapy-Practice-Formulation-Approach/dp/0393700771/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1290420954&amp;sr=1-2\">Cognitive Therapy in Practice: A Case Formulation Approach</a>) and I can't recall either one explicitly doing this, but I may have missed it. I do remember that Ellis and Harper seemed to conflate instrumental and epistemic rationality.</p>\r\n<p>Edit: Thanks a lot to Vaniver for the help on link formatting.</p>\r\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6v2FHy8dtyCYg9Kz4": 3, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FJkfboEQjN3ritXk9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 6.482773306114807e-07, "legacy": true, "legacyId": "4066", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L32LHWzy9FzSDazEg", "a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T14:00:31.150Z", "modifiedAt": null, "url": null, "title": "Psychopathy and the Wason Selection Task", "slug": "psychopathy-and-the-wason-selection-task", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:39.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uh7gTTZqmtbku6z6g/psychopathy-and-the-wason-selection-task", "pageUrlRelative": "/posts/uh7gTTZqmtbku6z6g/psychopathy-and-the-wason-selection-task", "linkUrl": "https://www.lesswrong.com/posts/uh7gTTZqmtbku6z6g/psychopathy-and-the-wason-selection-task", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Psychopathy%20and%20the%20Wason%20Selection%20Task&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APsychopathy%20and%20the%20Wason%20Selection%20Task%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuh7gTTZqmtbku6z6g%2Fpsychopathy-and-the-wason-selection-task%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Psychopathy%20and%20the%20Wason%20Selection%20Task%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuh7gTTZqmtbku6z6g%2Fpsychopathy-and-the-wason-selection-task", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuh7gTTZqmtbku6z6g%2Fpsychopathy-and-the-wason-selection-task", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 257, "htmlBody": "<p>The Wason Selection Task is the somewhat famous experimental problem that requires attempting to falsify a hypothesis in order to get the correct answer. From the <a href=\"http://en.wikipedia.org/wiki/Wason_selection_task\">wikipedia article</a>:</p>\n<blockquote>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\"><em>You are shown a set of four cards placed on a table, each of which has a number on one side and a colored patch on the other side. The visible faces of the cards show 3, 8, red and brown. Which card(s) should you turn over in order to test the truth of the proposition that if a card shows an even number on one face, then its opposite face is red?</em></span></p>\n</blockquote>\n<p>Aside from an illustration of the rampancy of confirmation bias (only 10-20% of people get it right), the task is interesting for another reason: when framed in terms of social interactions, people's performance dramatically improves:</p>\n<blockquote>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\"><em>For example, if the rule used is \"If you are drinking alcohol then you must be over 18\", and the cards have an age on one side and beverage on the other, e.g., \"17\", \"beer\", \"22\", \"coke\", most people have no difficulty in selecting the correct cards (\"17\" and \"beer\").</em></span></p>\n</blockquote>\n<p>However, apparently psychopaths perform nearly as badly on the \"social contract\" versions of this experiment as they do on the normal one. From <a href=\"http://www.economist.com/node/17460702\">the Economist</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Verdana, Arial, sans-serif; font-size: 13px; color: #333333; line-height: 20px;\">For problems cast as social contracts or as questions of risk avoidance, by contrast, non-psychopaths got it right about 70% of the time. Psychopaths scored much less&mdash;around 40%&mdash;and those in the middle of the psychopathy scale scored midway between the two.</span></p>\n</blockquote>\n<p>The original (gated) research appears to be <a href=\"http://pss.sagepub.com/content/early/2010/09/07/0956797610384148.abstract\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"v4pviL33XGMuTpSNs": 1, "dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uh7gTTZqmtbku6z6g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 6.483135965670263e-07, "legacy": true, "legacyId": "4067", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T19:26:30.131Z", "modifiedAt": null, "url": null, "title": "Volitive Rationality", "slug": "volitive-rationality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NihilCredo", "createdAt": "2009-04-22T23:40:56.227Z", "isAdmin": false, "displayName": "NihilCredo"}, "userId": "W6f2cwKiKSroig5kb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YcPRdQwgoSFeev6Df/volitive-rationality", "pageUrlRelative": "/posts/YcPRdQwgoSFeev6Df/volitive-rationality", "linkUrl": "https://www.lesswrong.com/posts/YcPRdQwgoSFeev6Df/volitive-rationality", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Volitive%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVolitive%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcPRdQwgoSFeev6Df%2Fvolitive-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Volitive%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcPRdQwgoSFeev6Df%2Fvolitive-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcPRdQwgoSFeev6Df%2Fvolitive-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": null, "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YcPRdQwgoSFeev6Df", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4068", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": null, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T21:37:47.173Z", "modifiedAt": null, "url": null, "title": "Morality in Fantasy Worlds", "slug": "morality-in-fantasy-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lucidfox", "createdAt": "2010-11-22T06:58:06.993Z", "isAdmin": false, "displayName": "lucidfox"}, "userId": "hNnKSqvajCMRj9eKK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3E5HAQ45etewgmhn9/morality-in-fantasy-worlds", "pageUrlRelative": "/posts/3E5HAQ45etewgmhn9/morality-in-fantasy-worlds", "linkUrl": "https://www.lesswrong.com/posts/3E5HAQ45etewgmhn9/morality-in-fantasy-worlds", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%20in%20Fantasy%20Worlds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%20in%20Fantasy%20Worlds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3E5HAQ45etewgmhn9%2Fmorality-in-fantasy-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%20in%20Fantasy%20Worlds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3E5HAQ45etewgmhn9%2Fmorality-in-fantasy-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3E5HAQ45etewgmhn9%2Fmorality-in-fantasy-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p>This comment by Carinthium in <a href=\"/lw/34v/the_selfreinforcing_binary/\">my earlier post</a> has got me to think, and my apologies if this subject was raised before - couldn't find it.</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">1- In a hypothetical world where God actually existed, it would be an OBJECTIVE belief and thus it would be worth saving people. (Just as with people killing themselves with harmful drugs under almost all real circumstances)</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">\n<div>I admit I haven't really thought much about such a topic, so perhaps it deserves a separate discussion.</div>\n<div>Imagine you live in a fantasy world (like one of the Dungeons &amp; Dragons universes) where the god you worship places you into better afterlife for following virtues that it considers moral. Suppose that we have evidence for the existence of the good and bad afterlives - testimony of resurrected people, etc. - so we know it's not just a groundless threat. Some people would, naturally, not care and engage in behavior that would land them in the bad afterlife. Would it be moral or rational to try and save them from their own folly, or to write it up to personal choice instead?</div>\n<div>Things get even more complicated when there are multiple gods, each with its own concept of morality, and you have a choice between them. But that's a whole separate can of worms...</div>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3E5HAQ45etewgmhn9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 6.484251923922895e-07, "legacy": true, "legacyId": "4069", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ihzDAw6shkSKqM4fj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T23:34:01.635Z", "modifiedAt": null, "url": null, "title": "Advanced neurotypicallity", "slug": "advanced-neurotypicallity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sCfnBJBjLrKigN98z/advanced-neurotypicallity", "pageUrlRelative": "/posts/sCfnBJBjLrKigN98z/advanced-neurotypicallity", "linkUrl": "https://www.lesswrong.com/posts/sCfnBJBjLrKigN98z/advanced-neurotypicallity", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advanced%20neurotypicallity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvanced%20neurotypicallity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCfnBJBjLrKigN98z%2Fadvanced-neurotypicallity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advanced%20neurotypicallity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCfnBJBjLrKigN98z%2Fadvanced-neurotypicallity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCfnBJBjLrKigN98z%2Fadvanced-neurotypicallity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<p><a id=\"__mce\" href=\"http://rm.livejournal.com/1967682.html?nc=4\">A description of auditioning</a></p>\n<p>--</p>\n<blockquote>As an auditioner, this means I try to respect the people behind the table, be genuine and keep them from being bored. I want them to know that I appreciate their efforts, know that their side of the table is awkward too, and thank them for seeing me. And a lot of this, I have to show, don't tell. It's hard. Especially when you've also got to show up with the skills (also, seriously, it's weird do be affable and connected and then be Lady Anne, because she's a lot of things, but affable not so much).  If you're auditioning for something, and especially if you're new to auditioning, often, if you're like me, you'll consider your odds of getting cast, and your computations will be quite grim. Well let me tell you something, stop that right now.  Because if you can come into the room, say hello to me, make chit-chat for 30 seconds and do your monologue actually facing the table -- you are so ahead of the game. If you haven't sat behind the table, you think I'm joking, but I'm not. I've had people do monologues with their back to me because, they explained, they were nervous. I've had people build a jury box out of chairs (while my mouth hung open) and then proceed to do a spot-on imitation of Jack Nicholson in A Few Good Men. There was the guy with the gun. The people who brought their boyfriends (fine as a safety precaution if I'm auditioning you in a non-standard space; a complete distraction if I'm auditioning you at a rehearsal studio and they want in the room with you).</blockquote>\n<p>I found that interesting because I'm apt to space out around people, and the passage is by someone who's \"on\" a lot more of the time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sCfnBJBjLrKigN98z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 6.484535663820151e-07, "legacy": true, "legacyId": "4070", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-23T14:08:33.563Z", "modifiedAt": null, "url": null, "title": "Rationality is Not an Attractive Tribe", "slug": "rationality-is-not-an-attractive-tribe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:07.140Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v76DQ9hWkT9WEyRgs/rationality-is-not-an-attractive-tribe", "pageUrlRelative": "/posts/v76DQ9hWkT9WEyRgs/rationality-is-not-an-attractive-tribe", "linkUrl": "https://www.lesswrong.com/posts/v76DQ9hWkT9WEyRgs/rationality-is-not-an-attractive-tribe", "postedAtFormatted": "Tuesday, November 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20is%20Not%20an%20Attractive%20Tribe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20is%20Not%20an%20Attractive%20Tribe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv76DQ9hWkT9WEyRgs%2Frationality-is-not-an-attractive-tribe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20is%20Not%20an%20Attractive%20Tribe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv76DQ9hWkT9WEyRgs%2Frationality-is-not-an-attractive-tribe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv76DQ9hWkT9WEyRgs%2Frationality-is-not-an-attractive-tribe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1268, "htmlBody": "<p><em>Summary: I wonder how attractive rationality as a tribe and worldview is to the average person, when the competition is not constrained by verifiability or consistency and is therefore able optimize around offering imaginary status superstimuli to its adherents.</em></p>\n<p><em><br /></em></p>\n<blockquote>\n<p>Anti-intellectualism has been a constant thread winding its way through our political and cultural life, nurtured by the false notion that democracy means that 'my ignorance is just as good as your knowledge.'</p>\n<p>&mdash; Isaac Asimov</p>\n</blockquote>\n<p>I've long been puzzled by the capability of people to reject obvious conclusions and opt for convoluted arguments that boil down to logical fallacies when it comes to defending a belief they have a stake in. When someone resists doing the math, despite an obvious capability to do so in other similar cases, we are right to suspect external factors at play. A framework that seems congruent with the evolutionary history of our species is that of beliefs as signals of loyalty to a tribe. Such a framework would explain the rejection of evolution and other scientific theories by large swathes of the world's population, especially religious population, despite access to a flood of evidence in support.&nbsp;</p>\n<p>I will leave support of the tribal signalling framework to others, and examine the consequences for popular support of rationality and science if indeed such a framework&nbsp;successfully&nbsp;approximates reality.&nbsp;The best way I can do that is by examining one&nbsp;popular&nbsp;alternative: The&nbsp;Christian&nbsp;religion which I am most familiar with, in particular its evangelical protestant branch. I am fairly confident that this narrative can be ported to other branches of&nbsp;Christianity&nbsp;and abrahamic&nbsp;faiths fairly easily and the equivalents for other large religions can be constructed with some extra effort.</p>\n<blockquote>\n<p>\"Blessed are the meek, for they will inherit the earth\"</p>\n<p>&mdash;&nbsp;The Bible (New International Version), Matthew 5:5</p>\n</blockquote>\n<p>What is the narrative that an evangelical&nbsp;Christian&nbsp;buys into regarding their own status? They belong to the 'chosen people', worshipping a god that loves them, personally, created them with special care, and has a plan for their individual lives. They are taking part in a battle with absolute evil, that represents everything disgusting and despicable, which is manifested in the various&nbsp;difficulties&nbsp;they face in their lives. The end-game however is known. The believers, once their faith is tested in this world, are destined for an eternity of bliss with their brethren in the presence of their god, while the enemies will be thrown in the eternal fire for eternal torment. In this narrative, the disadvantaged in this life are very important. There exist propositions which can be held with absolute certainty. This presents a black-white divide in which moral judgements are easy, as long as corner cases can be swept under the rug. Each and every person, regardless of their social standing or capability, can be of utmost importance. Everyone can potentially save a soul for all eternity! In fact,&nbsp;the gospels place emphasis on the humble and the meek:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px; color: #001320; line-height: 21px;\">So those who are last now will be first then, and those who are first will be last.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px; color: #001320; line-height: 21px;\"><span style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small;\">&mdash;&nbsp;The Bible (New International Version), Matthew 20:16</span></span></p>\n</blockquote>\n<p>What is the rational alternative to this battle-hardened, well-optimized worldview? That there is no grand narrative. If such a narrative exists, (pursuit of truth,&nbsp;combating&nbsp;existential risk, &lt;insert yours here&gt;), the stars of this narrative are those blessed with intelligence and education such that they can digest the background material and make these pursuits on the cutting edge. It turns out, your enemies are not innately evil, either. You may have just misunderstood each other. You have to constantly struggle to fight your own biases, to no certain outcome. In fact, we are to hold no proposition with 100% certainty. On the plus side, science and rationality offers, or at least aspires to offer, a consistent worldview free from cognitive dissonance for those that can detect the alternative's contradictions. On the status side, for those of high intelligence, it puts them at the top of the hierarchy, being in the line of the great heroes of thought that have gone before, uncovering all the knowledge we have so far. But this is not hard to perceive as elitism, especially since the barriers to entry are difficult, if not impossible, to overcome for the vast majority of humans. Rationality may have an edge if it can be shown to improve an individual's life prospects. I am not aware of such research, especially one that untangles rationality from intelligence. Perhaps the most successful example, Pick-up artists, are out of limits for this community because their terminals are deemed offensive. While we define rationality as the way to win, the win that we focus on in this community is a collective one, therefore unlikely to confer an individual with high status in the meantime if this individual does not belong to the intellectually gifted few.</p>\n<p>So what does rationality have to offer to the common man to gain their support? The role of hard-working donor, whose contribution is in a replaceable commodity, e.g. money? The role of passive&nbsp;consumer&nbsp;of scientific products and documentaries? It seems to me that in the marketplace of worldview-tribes, rationality and science do not present themselves an attractive option for large swathes of the earth's population, and why would they? They were never developed as such. To make things worse, the alternatives have&nbsp;millennia&nbsp;of cultural evolution to better fit their targets, unconstrained by mundane burdens such as verifiability and consistency. I can perfectly see the attraction of the 'rational irrationality' point of view where someone would compartmentalises&nbsp;rationality&nbsp;into result-sensitive 'get things done' areas, while choosing to affirm unverifiable and/or incoherent propositions that nevertheless superstimulate one's feel-good status receptors.</p>\n<p>I see two routes here: The one is that we decide that popular support is not necessary. We focus our recruiting efforts on the upper strata of intelligence and influence. If it's a narrative that they need, we can't help them. We're in the business of providing raw truth. Humans are barely on the brink of general intelligence,&nbsp;anyway. A recent post claimed that an IQ of 130+ was practically a prerequisite for appreciating and comprehending the sequences. The truths are hard to grasp and inconvenient, but ultimately it doesn't matter if a narrative can be developed for the common person. They can keep believing in creationism, and we'll save humanity for them anyway.</p>\n<p>On the other hand, just because the scientific/rational&nbsp;worldview has not been fitted to the common man, it doesn't mean it can't be done. (But there is no guarantee that it can.) The alternative is to explore the open avenues that may lead to a more palatable narrative, including popularising many of the rationality themes that are articulated in this community. People show interest when I speak to them about cognitive biases but I have no accessible resources to give them that would start from there as a beachhead and progress into other more esoteric topics. And I don't find it incredible that rationality could provably aid in better individual outcomes, we just need solid research around the proposition. (The effects of various valleys of bad rationality or shifts in terminals due to rationality exposure may complicate that).</p>\n<p>I am not taking a position on which course of action is superior, or that these are the only alternatives. But it does seem to me that, if my reasoning and assumptions are correct, we have to make up our mind on what exactly it is we want to do as the Less Wrong community.</p>\n<p><strong>Edit/Note: I initially planned for this to be posted as a normal article, but seeing how the voting is... equal in both directions, but that there is a lively discussion developing, I think this article is just fine in the discussion section.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v76DQ9hWkT9WEyRgs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 16, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "4076", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-23T15:49:07.296Z", "modifiedAt": null, "url": null, "title": "Fiction: Tasmanian Devil ch. 1", "slug": "fiction-tasmanian-devil-ch-1", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.645Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fictiona", "createdAt": "2010-11-23T14:55:27.642Z", "isAdmin": false, "displayName": "fictiona"}, "userId": "pXq2T4wi7cEW67oSd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GDG4FdagtkHwg3jjq/fiction-tasmanian-devil-ch-1", "pageUrlRelative": "/posts/GDG4FdagtkHwg3jjq/fiction-tasmanian-devil-ch-1", "linkUrl": "https://www.lesswrong.com/posts/GDG4FdagtkHwg3jjq/fiction-tasmanian-devil-ch-1", "postedAtFormatted": "Tuesday, November 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fiction%3A%20Tasmanian%20Devil%20ch.%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFiction%3A%20Tasmanian%20Devil%20ch.%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDG4FdagtkHwg3jjq%2Ffiction-tasmanian-devil-ch-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fiction%3A%20Tasmanian%20Devil%20ch.%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDG4FdagtkHwg3jjq%2Ffiction-tasmanian-devil-ch-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDG4FdagtkHwg3jjq%2Ffiction-tasmanian-devil-ch-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>Tasmanian Devil is a story of college love, mystery, magic, logic and ethics. It parodises the human nature to the core. It can get very dark and is based loosely on rationality.</p>\n<p><a href=\"http://alpha.fictiona.com/novels/tasmanian-devil/1\">Read Chapter 1 here</a>. (PG-13; warnings: very dark themes, a few vulgar words)&nbsp;</p>\n<p>Teaser:</p>\n<blockquote>\n<p><span style=\"font-family: verdana; font-size: 13px; line-height: 20px; \">Baby tooth was a market in which there were several sellers but only one buyer. Economics textbooks called this type of market&nbsp;<em>monopsony</em>. Theory predicted that, due to her bargaining power, the Tooth Fairy could be bitchy all she wanted, buying teeth at a very low price if she wanted to. However, according to Penny&rsquo;s research, the Tooth Fairy was generous enough to pay up to $6 per tooth! Why?</span></p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GDG4FdagtkHwg3jjq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "4077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-23T15:52:36.992Z", "modifiedAt": null, "url": null, "title": "New comments on the recent psi study", "slug": "new-comments-on-the-recent-psi-study", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.152Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "miS4XR3NQGgxkzKu9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CCr3QJxNo3cQokReW/new-comments-on-the-recent-psi-study", "pageUrlRelative": "/posts/CCr3QJxNo3cQokReW/new-comments-on-the-recent-psi-study", "linkUrl": "https://www.lesswrong.com/posts/CCr3QJxNo3cQokReW/new-comments-on-the-recent-psi-study", "postedAtFormatted": "Tuesday, November 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20comments%20on%20the%20recent%20psi%20study&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20comments%20on%20the%20recent%20psi%20study%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCCr3QJxNo3cQokReW%2Fnew-comments-on-the-recent-psi-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20comments%20on%20the%20recent%20psi%20study%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCCr3QJxNo3cQokReW%2Fnew-comments-on-the-recent-psi-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCCr3QJxNo3cQokReW%2Fnew-comments-on-the-recent-psi-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 14, "htmlBody": "<p>HT reddit/r/science: http://www.ruudwetzels.com//articles/Wagenmakersetal_subm.pdf</p>\n<p>Probably nobody is surprised here, but I thought one might be interested.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CCr3QJxNo3cQokReW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 20, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "4078", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-23T17:58:04.506Z", "modifiedAt": null, "url": null, "title": "Compatibilism in action", "slug": "compatibilism-in-action", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:48.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rwallace", "createdAt": "2009-03-01T16:13:25.493Z", "isAdmin": false, "displayName": "rwallace"}, "userId": "cPhXNeZvnK7LgPMnv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gzbKnX5foFPNb47ef/compatibilism-in-action", "pageUrlRelative": "/posts/gzbKnX5foFPNb47ef/compatibilism-in-action", "linkUrl": "https://www.lesswrong.com/posts/gzbKnX5foFPNb47ef/compatibilism-in-action", "postedAtFormatted": "Tuesday, November 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Compatibilism%20in%20action&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACompatibilism%20in%20action%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgzbKnX5foFPNb47ef%2Fcompatibilism-in-action%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Compatibilism%20in%20action%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgzbKnX5foFPNb47ef%2Fcompatibilism-in-action", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgzbKnX5foFPNb47ef%2Fcompatibilism-in-action", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 455, "htmlBody": "<p>A practical albeit fictional application of the philosophical conclusion that free will is compatible with determinism came up today in a discussion about a setting element from the role-playing game <a href=\"http://en.wikipedia.org/wiki/Exalted\">Exalted</a></p>\n<p>(5:31:44 PM) Nekira Sudacne: So during the pirmodial war, one Yozi got his fetch killed and he reincarnated as Sachervell, He Who Knows The Shape of Things To Come. And he reincarnated asleep. and he has remained asleep. And the other primordials do all in their power to keep him asleep. and he wants to be asleep.</p>\n<p>For you see, for as long as he sleeps, he dreams only of the present. should he awaken, he will see the totaltiy of exsistance, all things past and future exsactly as they will happen. quantumly speaking he will lock the universe into a single shape. All things that happen will happen as he sees them happen and there will be no chance for anyone to change it. effectivly nullifying chance for change. Even he cannot alter his vision for his vision takes into account all attempts to alter it.</p>\n<p>And there's a big debate over rather or not this is a game ending thing. Essentially, does predestination negate freewill or not</p>\n<p>(5:32:17 PM) Nekira Sudacne: and this is important, because one of the requirements for Exaltation to function is freewill. if Sachervell is able to negate freewill, then Exaltations will cease to function</p>\n<p>(5:32:44 PM) Nekira Sudacne: and maddenly enough the game authors are also on the thread arguing because THEY don't agree where to go with it either :)&nbsp;</p>\n<p>(5:38:02 PM) rw271828: ah, well I happen to know the answer :-)</p>\n<p>(5:39:23 PM) rw271828: one of the most important discoveries of 20th-century mathematics is that in general the behavior of a complex system cannot be predicted -- or rather, there is no easier way to predict it than to run it and see what happens. Note in particular:</p>\n<p>(5:39:41 PM) rw271828: 1. This is a mathematical fact, so it applies in all possible universes, including Exalted</p>\n<p>(5:40:01 PM) rw271828: 2. Humans and other sentient lifeforms are complex systems in the relevant sense</p>\n<p>(5:41:33 PM) rw271828: so if you postulate an entity that can actually see the future (as opposed to just extrapolate what is likely to happen unless something intervenes), the only way to do that is for that entity to run a perfect simulation, a complete copy of the universe&nbsp;</p>\n<p>(5:42:50 PM) rw271828: &nbsp;if you're willing to postulate that, well fine, continue the game, and just note that you are running it in the copy the entity is using to make the prediction - the people in the setting still have free will, it is their actions that determine the future, and thus the result of the prediction ^.^</p>\n<p>(5:43:04 PM) Nekira Sudacne: Hah. nice one</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gzbKnX5foFPNb47ef", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -5, "extendedScore": null, "score": 6.487231697829474e-07, "legacy": true, "legacyId": "4079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T03:40:17.056Z", "modifiedAt": null, "url": null, "title": "Inherited Improbabilities: Transferring the Burden of Proof", "slug": "inherited-improbabilities-transferring-the-burden-of-proof", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.414Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ATgFZpZCh4rS8fCGD/inherited-improbabilities-transferring-the-burden-of-proof", "pageUrlRelative": "/posts/ATgFZpZCh4rS8fCGD/inherited-improbabilities-transferring-the-burden-of-proof", "linkUrl": "https://www.lesswrong.com/posts/ATgFZpZCh4rS8fCGD/inherited-improbabilities-transferring-the-burden-of-proof", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inherited%20Improbabilities%3A%20Transferring%20the%20Burden%20of%20Proof&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInherited%20Improbabilities%3A%20Transferring%20the%20Burden%20of%20Proof%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATgFZpZCh4rS8fCGD%2Finherited-improbabilities-transferring-the-burden-of-proof%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inherited%20Improbabilities%3A%20Transferring%20the%20Burden%20of%20Proof%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATgFZpZCh4rS8fCGD%2Finherited-improbabilities-transferring-the-burden-of-proof", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATgFZpZCh4rS8fCGD%2Finherited-improbabilities-transferring-the-burden-of-proof", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2383, "htmlBody": "<blockquote>\r\n<p>One person's <a href=\"http://en.wikipedia.org/wiki/Modus_ponens\">modus ponens</a> is another's <a href=\"http://en.wikipedia.org/wiki/Modus_tollens\">modus tollens</a>.</p>\r\n</blockquote>\r\n<p>- Common saying among philosophers and other people who know what these terms mean.</p>\r\n<blockquote>\r\n<p>If you believe A =&gt; B, then you have to ask yourself: which do I believe more? A, or not B?</p>\r\n</blockquote>\r\n<p>- <a href=\"http://nlpers.blogspot.com/2010/04/b-and-not-b-not.html\">Hal Daume III</a>, quoted by&nbsp;<a href=\"/lw/32p/study_shows_existence_of_psychic_powers/2y2p?c=1&amp;context=1#2y2p\">Vladimir Nesov</a>.</p>\r\n<p><strong>Summary:</strong>&nbsp;<em>Rules of logic have counterparts in probability theory. This post discusses the probabilistic analogue of </em>modus tollens<em>&nbsp;(the rule that if A=&gt;B is true and B is false, then A is false), which is the inequality </em>P(A)&nbsp;&le; P(B)/P(B|A). <em>What this says, in ordinary language, is that if A strongly implies B, then proving A is approximately as difficult as proving B.&nbsp;</em></p>\r\n<p>The appeal trial for <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">Amanda Knox and Raffaele Sollecito</a>&nbsp;starts today, and so to mark the occasion I thought I'd present an observation about probabilities that&nbsp;occurred to me while studying the <a href=\"http://www.seattlepi.com/dayart/20100318/knox_opinion.pdf\">\"motivation document\"</a><sup>(1)</sup>,&nbsp;or judges' report, from the first-level trial.</p>\r\n<p>One of the \"pillars\" of the case against Knox and Sollecito is the idea that the apparent burglary in the house where the murder was committed -- a house shared by&nbsp;four people, namely Meredith Kercher (the victim), Amanda Knox, and two Italian women -- was staged. That is, the signs of a burglary were supposedly faked by Knox and&nbsp;Sollecito in order to deflect suspicion from themselves. (Unsuccessfully, of course...)</p>\r\n<p>As the authors of the report, presiding judge Giancarlo Massei and his assistant Beatrice Cristiani, put it (p.44):</p>\r\n<blockquote>\r\n<p>What has been explained up to this point leads one to conclude that the situation of disorder in Romanelli's room and the breaking of the window constitute an&nbsp;artificially created production, with the purpose of directing investigators toward someone without a key to the entrance, who would have had to enter the house via&nbsp;the window whose glass had been broken and who would then have perpetrated the violence against Meredith that caused her death.</p>\r\n</blockquote>\r\n<p><a id=\"more\"></a></p>\r\n<p>Now, even before examining \"what has been explained up to this point\", i.e. the reasons that Massei and Cristiani (and the police before them) were led to this&nbsp;conclusion, we can pretty easily agree that <em>if</em>&nbsp;it is correct -- that is, if Knox and Sollecito did in fact stage the burglary in Filomena Romanelli's room -- then it&nbsp;is extremely likely that they are guilty of participation in Kercher's murder. After all, what are the chances that they <em>just happened</em>&nbsp;to engage in the bizarre&nbsp;offense of making it look like there was a burglary in the house, <em>on the very same night</em>&nbsp;as a murder occurred, in that very house? Now, one could still&nbsp;hypothetically argue about what their precise role was (e.g. whether they actually physically caused Kercher's death, or merely participated in some sort of conspiracy&nbsp;to make the latter happen via the actions of known burglar and undisputed culprit Rudy Guede), and thus possibly about how severely they should be treated by the&nbsp;justice system; but in any case I think I'm on quite solid ground in asserting that a faked burglary by Knox and Sollecito would <em>very strongly</em>&nbsp;imply that Knox and&nbsp;Sollecito are criminally culpable in the death of Meredith Kercher.</p>\r\n<p>...which is in fact quite a problem for Massei and Cristiani, as I'll now explain.</p>\r\n<p>Probability theory can and should be thought of as a quantitative version -- indeed, a generalization -- of the \"rules of logic\" that underpin &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Traditional_rationality\">Traditional Rationality</a>.&nbsp;(Agreement with the previous sentence is essentially what it means to be a <a href=\"http://wiki.lesswrong.com/wiki/Bayesian\">Bayesian</a>.) One of these rules is this:</p>\r\n<p>(1) If A implies B, then not-B implies not-A.</p>\r\n<p>For example, all squares are in fact rectangles; which means that if something isn't a rectangle, it can't possibly be a square. Likewise, if \"it's raining\" implies&nbsp;\"the sidewalk is wet\", and you know the sidewalk isn't wet, then you know it's not raining.</p>\r\n<p>The rule that gets you from \"A implies B\" and \"A\" to \"B\" is called <em>modus ponens</em>, which is Latin for \"method that puts\". The rule that gets you from \"A implies B\" and&nbsp;\"not-B\" to \"not-A\" is called <em>modus tollens</em>, which is Latin for \"method that takes away\". As the saying goes, and as we have just seen, they are really one and the&nbsp;same.&nbsp;</p>\r\n<p>If, for a moment, we were to think about the Meredith Kercher case as a matter of pure logic -- that is, where inferences were always absolutely certain, with zero&nbsp;uncertainty -- then we could say that if we know that \"burglary is fake\" implies \"Knox and Sollecito are guilty\", and we also know that the burglary was in fact fake,&nbsp;then we know that Knox and Sollecito are guilty.</p>\r\n<p>But, of course, there's another way to say the same thing: if we know that \"burglary is fake\" implies \"Knox and Sollecito are guilty\", and we also know that Knox and&nbsp;Sollecito are <em>innocent</em>, then we know that the burglary wasn't fake. (And that to the extent Massei and Cristiani say it was, they must be mistaken.)</p>\r\n<p>In other words, so long as one accepts the implication \"burglary fake =&gt; Knox and Sollecito guilty\", one can't consistently hold that the burglary was fake and that&nbsp;Knox and Sollecito are innocent, but one <em>can</em>&nbsp;consistently hold either that the burglary was fake and Knox and Sollecito are guilty, or that Knox and Sollecito are&nbsp;innocent and the burglary was <em>not&nbsp;</em>fake.</p>\r\n<p>The question of <em>which</em>&nbsp;of these two alternatives to believe thus reduces to the question of whether, given the evidence in the case, it's more believable that Knox&nbsp;and Sollecito are guilty, or that the burglary was \"authentic\". Massei and Cristiani, of course, aim to convince us that the latter is the more improbable.</p>\r\n<p>But notice what this means! This means that <em>the proposition that the burglary was fake assumes, or inherits, the same high burden of proof as the proposition that&nbsp;Knox and Sollecito committed murder</em>! Unfortunately for Massei and Cristiani, there's no way to \"bootstrap up\" from the mundane sort of evidence that seemingly&nbsp;suffices to show that a couple of youngsters engaged in some deception, to the much stronger sort of evidence required to prove that two honor students<sup>(2)</sup> with gentle personalities suddenly decided, on an unexpectedly free&nbsp;evening, to force a friend into a deadly sex game with a local drifter they barely knew, for the sake of a bit of thrill-seeking<sup>(3)</sup>.</p>\r\n<div>\r\n<div>You may have noticed that, two paragraphs ago, I left the logical regime of implication, consistency, and absolute certainty, and entered the probability-theoretic&nbsp;realm of belief, uncertainty, and burdens of proof. So to make the point rigorous, we'll have to switch from pure logic to its quantitative generalization, the&nbsp;mathematics of probability theory. &nbsp;</div>\r\n<div>\r\n<div>When logical statements are translated into their probabilistic analogues, a statement like \"A is true\" is converted to something like \"P(A) is high\"; \"A implies B\"&nbsp;becomes \"A is (strong) &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Evidence\">evidence</a>&nbsp;&nbsp;of B\"; and rules such as (1) above turn into &nbsp;<em>bounds</em>&nbsp;on the probabilities of some hypotheses in terms of others.</div>\r\n<div>\r\n<div>Specifically, the translation of (1) into probabilistic language would be something like:</div>\r\n<div>(2) If A is (sufficiently) strong evidence of B, and B is unlikely, then A is unlikely.</div>\r\n<div>or</div>\r\n<div>(2') If A is (sufficiently) strong evidence of B, then the prior probability of A can't be much higher than the prior probability of B.</div>\r\n</div>\r\n</div>\r\n</div>\r\n<p>Let's prove this:</p>\r\n<p>Suppose that A is strong evidence of B -- that is, that P(B|A) is close to 1. We'll represent this as P(B|A) &ge;&nbsp;1-&epsilon;, where &epsilon;&nbsp;is a small number. Then,&nbsp;via &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Bayes'_theorem\">Bayes' theorem</a>, this tells us that</p>\r\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=P(B)\\frac{P(A|B)}{P(A)} \\geq 1-\\epsilon\" target=\"_blank\"><img title=\"P(B)\\frac{P(A|B)}{P(A)} \\geq 1-\\epsilon\" src=\"http://latex.codecogs.com/gif.latex?P(B)\\frac{P(A|B)}{P(A)} \\geq 1-\\epsilon\" alt=\"\" /></a></p>\r\n<p>or</p>\r\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\frac{P(A|B)}{P(A)} \\geq \\frac{1-\\epsilon}{P(B)}\" target=\"_blank\"><img title=\"\\frac{P(A|B)}{P(A)} \\geq \\frac{1-\\epsilon}{P(B)}\" src=\"http://latex.codecogs.com/gif.latex?\\frac{P(A|B)}{P(A)} \\geq \\frac{1-\\epsilon}{P(B)}\" alt=\"\" /></a></p>\r\n<p>so that</p>\r\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=\\frac{P(A)}{P(A|B)} \\leq \\frac{P(B)}{1-\\epsilon}\" target=\"_blank\"><img title=\"\\frac{P(A)}{P(A|B)} \\leq \\frac{P(B)}{1-\\epsilon}\" src=\"http://latex.codecogs.com/gif.latex?\\frac{P(A)}{P(A|B)} \\leq \\frac{P(B)}{1-\\epsilon}\" alt=\"\" /></a></p>\r\n<p>and thus</p>\r\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=P(A) \\leq \\frac{P(A|B)P(B)}{1-\\epsilon} \\leq \\frac{P(B)}{1-\\epsilon}\" target=\"_blank\"><img title=\"P(A) \\leq \\frac{P(A|B)P(B)}{1-\\epsilon} \\leq \\frac{P(B)}{1-\\epsilon}\" src=\"http://latex.codecogs.com/gif.latex?P(A) \\leq \\frac{P(A|B)P(B)}{1-\\epsilon} \\leq \\frac{P(B)}{1-\\epsilon}\" alt=\"\" /></a></p>\r\n<p>&nbsp;</p>\r\n<p>since P(A|B) &le;&nbsp;1. Hence we get an upper bound of P(B)/(1-&epsilon;) on P(A). For instance, if P(B) is 0.001, and P(B|A) is at least 0.95, then P(A) can't be any&nbsp;larger than 0.001/0.95 = 0.001052...</p>\r\n<p>Actually, there's a simpler proof, direct from the definition of P(B|A), which goes like this: P(B|A) = P(A&amp;B)/P(A), whence P(A) = P(A&amp;B)/P(B|A) &le;&nbsp;P(B)/P(B|A). (Note the use of the conjunction rule: P(A&amp;B)&nbsp;&le; P(B).)</p>\r\n<p>The statement</p>\r\n<p>(3)</p>\r\n<p>&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=P(A) \\leq \\frac{P(B)}{P(B|A)}\" target=\"_blank\"><img style=\"border: 0px initial initial;\" title=\"P(A) \\leq \\frac{P(B)}{P(B|A)}\" src=\"http://latex.codecogs.com/gif.latex?P(A) \\leq \\frac{P(B)}{P(B|A)}\" alt=\"\" /></a>&nbsp;&nbsp;</p>\r\n<p>is a quantitative version of &nbsp;<em>modus tollens</em>, just as the equivalent statement&nbsp;</p>\r\n<p>(4)</p>\r\n<p><a href=\"http://www.codecogs.com/eqnedit.php?latex=P(B) \\geq P(A)P(B|A)\" target=\"_blank\"><img title=\"P(B) \\geq P(A)P(B|A)\" src=\"http://latex.codecogs.com/gif.latex?P(B) \\geq P(A)P(B|A)\" alt=\"\" /></a></p>\r\n<p>is a quantitative version of &nbsp;<em>modus ponens</em>.&nbsp;Assuming P(B|A) is high, what (4) says is that if P(A) is high, so is P(B); what (3) says is that if P(B) is low, so is&nbsp;P(A).</p>\r\n<p>Or, in other words, that the improbability &nbsp;-- burden of proof -- of B is &nbsp;<em>transferred to</em>, or &nbsp;<em>inherited by</em>, A.</p>\r\n<p>...which means you cannot simultaneously believe that (1) Knox and Sollecito's staging of the burglary would be strong evidence of their guilt; (2) proving their guilt&nbsp;is hard; and (3) proving they staged the burglary is easy. Something has to give; hard work must be done somewhere.</p>\r\n<p>Of their 427-page report, Massei and Cristiani devote approximately 20 pages (mainly pp. 27-49) to their argument that the burglary was staged by Knox and Sollecito&nbsp;rather than being the work of known burglar Rudy Guede (including a strange section devoted to the refuting the hypothesis that the burglary was &nbsp;<em>staged by Guede</em>).&nbsp;But think about it: if they were &nbsp;<em>really&nbsp;</em>&nbsp;able to demonstrate this, they would scarcely have needed to bother writing the remaining 400-odd pages of the report! For,&nbsp;if it is granted that Knox and Sollecito staged the burglary, then, in the absence of any other explanation for the staging (like November 1 being Annual Stage-Burglary Day for some group to which Knox or Sollecito belonged) it easily follows with conviction-level confidence that they were involved in a conspiracy that&nbsp;resulted in the death of Meredith Kercher. You would hardly need to bother with DNA, luminol, or the cell phone traffic of the various \"protagonists\".&nbsp;</p>\r\n<p>Yet it doesn't appear that Massei and Cristiani have much conception of the burden they face in trying to prove something that would so strongly imply their hugely a-priori-improbable ultimate thesis. Their arguments purporting to show that Knox and Sollecito faked the burglary are quite weak -- and, indeed, are reminiscent of those used time and again by their lower-status counterparts, conspiracy theorists of all&nbsp;types, from 9/11 \"truthers\" to the-Moon-landing-was-faked-ists. Here's a sample, from p.39:</p>\r\n<blockquote>\r\n<p>Additionally, the fragments of broken glass were scattered in a homogeneous manner on the internal and external windowsill, without any noticeable displacement and&nbsp;without any piece of glass being found on the surface below the window. This circumstance...rules out the possibility that the stone was thrown from outside the house&nbsp;to allow access inside via the window after the glass was broken. The climber, in leaning his hands and then his feet or knees on the windowsill, would have caused&nbsp;some of the glass to fall, or at least would have had to move some of the pieces lest they form a trap and cause injury. However, no piece of glass was found under the&nbsp;window and no sign of injury was discovered on the glass found in Romanelli's room.</p>\r\n</blockquote>\r\n<div>\r\n<div>(The question to ask, when confronted with an argument like this, is: <em>\"rules out\" with what confidence</em>? If Massei and Cristiani think this is strong evidence&nbsp;against the hypothesis that the stone was thrown from outside the house, then that means they have a model that makes &nbsp;<em>highly specific predictions</em>&nbsp;about the behavior&nbsp;of glass fragments when a stone is thrown from inside, versus when it is thrown from outside. Predictions which can be tested<sup>(4)</sup>. This is&nbsp;one reason why &nbsp;<a href=\"/lw/2sl/the_irrationality_game/2qdu?c=1\">I advocate using numbers</a>&nbsp;&nbsp;in arguments; if Massei and Cristiani had been required to think carefully enough to give a number, that would have forced them&nbsp;to examine their assumptions more critically, rather than &nbsp;<a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">stopping</a>&nbsp;&nbsp;on plausible-sounding arguments consistent with their already-arrived-at &nbsp;<a href=\"/lw/js/the_bottom_line/\">bottom line</a>.)</div>\r\n<div>\r\n<div>The impression one gets is that Massei and Cristiani thought, on some level, that all they needed to do was make the fake-burglary hypothesis &nbsp;<em>sound coherent </em>&nbsp;-- and&nbsp;that if they did so, that would count as a few points against Knox and Sollecito. They could then do the same thing with regard to the other pieces of evidence in the&nbsp;case, each time coming up with an explanation of the facts in terms of an assumption that Knox and Sollecito are guilty, and each time thereby scoring a few more&nbsp;points against them -- points which would presumably add up to a substantial number by the end of the report.</div>\r\n</div>\r\n<div>\r\n<div>But, of course, the mathematics of probability theory don't work that way. It's not enough for a hypothesis, such as that the apparent burglary in Filomena Romanelli's room was staged, &nbsp;to merely be able to explain the data; it must do so &nbsp;<em>better than its negation</em>. And, &nbsp;<em>in the absence of the assumption that Knox and Sollecito are&nbsp;guilty</em>&nbsp;-- if we're presuming them to be innocent, as the law requires, or assigning a tiny prior probability to their guilt, as &nbsp;<a href=\"/lw/31/what_do_we_mean_by_rationality/\">epistemic rationality</a>&nbsp; requires --&nbsp;<em>this contest is rigged</em>. The standards for \"explaining well\" that the fake-burglary hypothesis has to meet in order to be taken seriously are &nbsp;<em>much higher &nbsp;</em>than&nbsp;those that its negation has to meet, because of the dependence relation that exists between the fake-burglary question and the murder question. Any hypothesis that&nbsp;requires the assumption that Knox and Sollecito are guilty of murder inherits the full \"explanatory inefficiency penalty\" (i.e. prior improbability) of the latter&nbsp;proposition.</div>\r\n</div>\r\n<div>If A implies B, then not-B implies not-A. It goes both ways.</div>\r\n</div>\r\n<p>&nbsp;</p>\r\n<hr />\r\n<p><span style=\"font-size: small;\"><span style=\"font-size: 11px;\"><br /></span></span></p>\r\n<p>Notes</p>\r\n<p><span style=\"font-size: small;\"><span style=\"font-size: 11px; \"><sup>(1)</sup></span>&nbsp;Some &nbsp;<a href=\"http://truejustice.org\">pro-guilt advocates</a>&nbsp;&nbsp;have apparently produced a translation, but I haven't looked at it and can't vouch for it. Translations of passages appearing in this post are my own.</span></p>\r\n<p><span style=\"font-size: 11px; \"><sup>(2)</sup></span>&nbsp;One of whom, incidentally, is &nbsp;<a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer#v=t&amp;n=ee95701e-065e-43b1-ad6e-2c3f2d5226d7&amp;b=0\">known</a>&nbsp;&nbsp;to be enjoying &nbsp;<em><a href=\"http://www.fanfiction.net/s/5782108/1/Harry_Potter_and_the_Methods_of_Rationality\">Harry Potter and the Methods of Rationalit</a></em><a href=\"http://www.fanfiction.net/s/5782108/1/Harry_Potter_and_the_Methods_of_Rationality\">y</a>&nbsp; -- so take &nbsp;<em>that &nbsp;</em>for whatever it's worth<span style=\"font-size: small;\"><span style=\"font-size: 11px;\">.&nbsp;</span></span></p>\r\n<p><span style=\"font-size: small;\"><span style=\"font-size: 11px;\"><span style=\"font-size: small;\"><span style=\"font-size: 11px; \"><sup>(3)</sup></span>&nbsp;From p. 422 of the report:<span style=\"font-size: small;\"><span style=\"font-size: 11px;\">&nbsp;</span></span></span></span></span></p>\r\n<blockquote>\r\n<p><span style=\"font-size: small;\"><span style=\"font-size: 11px;\">The criminal acts turned out to be the result of purely accidental circumstances which came together to create a situation which, in the combination of the various factors, made the crimes against Meredith possible: Amanda and Raffaele, who happened to find themselves without any commitments, randomly met up with Rudy Guede (there is no trace of any planned appointment), and found themselves together in the house on Via Della Pergola where, that very evening, Meredith was alone. A crime which came into being, therefore, without any premeditation, without any animosity or rancorous feeling toward the victim...</span></span></p>\r\n</blockquote>\r\n<p><span style=\"font-size: 11px; \"><sup>(4)</sup></span>&nbsp;And sure enough, during the trial, the defense hired a ballistics expert who &nbsp;<em>conducted experiments&nbsp;</em>&nbsp;showing that a rock thrown from the outside would produce patterns of glass, etc. similar to what was found at the scene -- results which forced the prosecutors to admit that the rock was probably thrown from the outside, but which were simply &nbsp;<em>ignored&nbsp;</em>&nbsp;by Massei and Cristiani! (See p. 229 of &nbsp;<a href=\"http://www.injusticeinperugia.org/Raffaele_Sollecito_Appeal.pdf\">Sollecito's appeal document</a>, if you can read Italian.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ATgFZpZCh4rS8fCGD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 45, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "4081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G9dptrW9CJi7wNg3b", "L32LHWzy9FzSDazEg", "34XxbRFe54FycoCDw", "RcZCwxFiZzE6X7nsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T07:28:36.950Z", "modifiedAt": null, "url": null, "title": "By using Cost Basis Working Capital as the number", "slug": "by-using-cost-basis-working-capital-as-the-number", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.045Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "meidang", "createdAt": "2010-11-24T07:24:48.672Z", "isAdmin": false, "displayName": "meidang"}, "userId": "XFpNWRSoshMfN5fSc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xLFhZtfMHjRZHuXEP/by-using-cost-basis-working-capital-as-the-number", "pageUrlRelative": "/posts/xLFhZtfMHjRZHuXEP/by-using-cost-basis-working-capital-as-the-number", "linkUrl": "https://www.lesswrong.com/posts/xLFhZtfMHjRZHuXEP/by-using-cost-basis-working-capital-as-the-number", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20By%20using%20Cost%20Basis%20Working%20Capital%20as%20the%20number&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABy%20using%20Cost%20Basis%20Working%20Capital%20as%20the%20number%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLFhZtfMHjRZHuXEP%2Fby-using-cost-basis-working-capital-as-the-number%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=By%20using%20Cost%20Basis%20Working%20Capital%20as%20the%20number%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLFhZtfMHjRZHuXEP%2Fby-using-cost-basis-working-capital-as-the-number", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLFhZtfMHjRZHuXEP%2Fby-using-cost-basis-working-capital-as-the-number", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<p>Investing is a long-term, <a href=\"http://www.pandora-bracelets-australia.com/pandora-beads/\"><strong>pandora  beads</strong></a> personal, goal orientated, non- competitive, hands on,  decision-making process that does not require advanced degrees or a rocket  scientist IQ. In fact, being too smart can be a problem if you have a tendency  to over analyze things. It is helpful to establish guidelines for selecting  securities, <a href=\"http://www.pandora-bracelets-australia.com/pandora-charms/\"><strong>pandora  charms</strong></a> and for disposing of them. For example, limit Equity  involvement to Investment Grade, NYSE, dividend paying, profitable, and widely  held companies.</p>\n<p>So is there really such a thing as an Income Portfolio that needs to be  managed? .For Fixed Income, <a href=\"http://www.pandora-bracelets-australia.com/pandora-beads/\"><strong></strong>cheap  pandora beads</a> focus on Investment Grade securities, with above average but  not &ldquo;highest in class&rdquo; yields. With Variable Income securities, avoid purchase  near 52-week highs, and keep individual holdings well below 5%. <a href=\"http://www.pandora-bracelets-australia.com/\"><strong>cheap pandora  bracelets</strong></a> Keep individual Preferred Stocks and Bonds well below 5%  as well. Closed End Fund positions may be slightly higher than 5%, depending on  type.</p>\n<p>Take a reasonable profit more than one years&rsquo; income for starters as soon as  possible. With a 60% Equity Allocation, 60% of profits and interest would be  allocated to stocks.Monitoring Investment Performance the Wall Street way is  inappropriate and problematic for goal-orientated investors. <a href=\"http://www.pandora-bracelets-australia.com/pandora-charms/\"><strong>cheap  pandora charms</strong></a> Or are we really just dealing with an investment  portfolio that needs its Asset Allocation tweaked occasionally as we approach  the time in life when it has to provide the yacht&hellip; and the gas money to run it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xLFhZtfMHjRZHuXEP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4083", "legacySpam": true, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T08:03:48.409Z", "modifiedAt": null, "url": null, "title": "Depression and Rationality", "slug": "depression-and-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:39.476Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9aDYihurHjcqpjhoa/depression-and-rationality", "pageUrlRelative": "/posts/9aDYihurHjcqpjhoa/depression-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/9aDYihurHjcqpjhoa/depression-and-rationality", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Depression%20and%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADepression%20and%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aDYihurHjcqpjhoa%2Fdepression-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Depression%20and%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aDYihurHjcqpjhoa%2Fdepression-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aDYihurHjcqpjhoa%2Fdepression-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 653, "htmlBody": "<div class=\"usertext-body\">\n<div class=\"md\">\n<p>Okay, this post might be more personal than most on LessWrong. But I think it might serve a more general function for LessWrong readers. It's commonly known that depression is often triggered (and maintained) by fundamentally irrational thoughts. Many thoughts associated with depression are simultaneously thoughts that are fundamentally irrational. Feelings of guilt, feelings of hopelessness, and feelings of worthlessness - many of those beliefs assume conceptions that do not correspond with the real world. And so a *rational* person may ostensibly be less prone to depression. But depression involves other things that can still strike a fully rational person. Loss of capacity to enjoy things. And loss of energy, focus, and ability to concentrate.</p>\n<p>So here is my story:</p>\n<p>I've finally corrected all my thinking and reconciled myself with the notion that there's a very high chance that my ex (of 7 months) will probably never talk to me again and that i must prepare to live a life without her. She has already refused contact with me for 6 months, and I've pretty much been emotionally broken for that same period of time. There has been one phase of improvement (mostly since I got more information and accordingly corrected my thought patterns), but the improving has finally stagnated.</p>\n<p>My problem is, that I think I've lost my capacity to enjoy things. I simply don't enjoy anything anymore. Occasionally, I can try to find things to laugh at, but those things are usually only temporary sources of laughter (the more \"severe\" the norm-violation, the funnier - if we go by Robin Hanson's definition of \"humor\"). They're not even sustainable sources of laughter (or enjoyment), since almost all of them involve trolling to one extent or another. I have some problems with energy/concentration, but my Adderall for ADD helps with them. But it's still difficult for me to maintain the attention span to do most other things.</p>\n<p>Yes, I do have friends, and yes, I do talk to people. The problem is that talking to people doesn't make me feel any less lonely anymore. Sometimes it makes me temporarily feel better. I know that the world is interesting, that I have friends to talk to, that there are so many things I can do. And my past 12-year old self would be SO happy if he could exchange spots with me. But in the end, I just get bored with everything so quickly. Sometimes I can briefly find things to laugh at. But those are only funny for a short time.</p>\n<p>So that's what I'm trying to find a solution for. Maybe it sounds like depression - I don't know. It's not full head-on clinical depression, but since I'm frequently sad despite correcting all my destructive thought patterns, I don't know what it is anymore.</p>\n<p>So maybe I'm trying to find the best meds for that. But I'm very skeptical of SSRIs because they're no better than placebo for mild-moderate depression. I have no sources for weed or other psychedelics. therapy might not even work because i've corrected my thinking (and even convinced myself that i probably will eventually land someone, simply because i'm super-exceptional at advertising myself online and that the chances will improve as I get older). but it's not helping me feel any better.</p>\n<p>===</p>\n<p>The problem is, I probably don't even qualify for a diagnosis of depression. Here are the symptoms of dysthymia:</p>\n<p>\"\"To be diagnosed, an adult must experience 2 or more of the following symptoms for at least two years:[5]</p>\n<pre><code>Feelings of hopelessness<br />Insomnia or hypersomnia<br />Poor concentration or difficulty making decisions<br />Poor appetite or overeating<br />Low energy or fatigue<br />Low self-esteem<br />Low sex drive<br />Irritability [1]\"\"<br /></code></pre>\n<p>But I have none of those other than irritability. My main problem is simply that I'm still lonely and sad. And it's persisted, even though I've changed my thinking. I know that I have things better off than most people, but it's not going to help if I've lost my capacity for enjoying things.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9aDYihurHjcqpjhoa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 6.489295842968854e-07, "legacy": true, "legacyId": "4084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T10:08:26.453Z", "modifiedAt": null, "url": null, "title": "Why abortion looks more okay to us than killing babies", "slug": "why-abortion-looks-more-okay-to-us-than-killing-babies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:07.879Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BzddatoNo3XPyHtpk/why-abortion-looks-more-okay-to-us-than-killing-babies", "pageUrlRelative": "/posts/BzddatoNo3XPyHtpk/why-abortion-looks-more-okay-to-us-than-killing-babies", "linkUrl": "https://www.lesswrong.com/posts/BzddatoNo3XPyHtpk/why-abortion-looks-more-okay-to-us-than-killing-babies", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20abortion%20looks%20more%20okay%20to%20us%20than%20killing%20babies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20abortion%20looks%20more%20okay%20to%20us%20than%20killing%20babies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzddatoNo3XPyHtpk%2Fwhy-abortion-looks-more-okay-to-us-than-killing-babies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20abortion%20looks%20more%20okay%20to%20us%20than%20killing%20babies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzddatoNo3XPyHtpk%2Fwhy-abortion-looks-more-okay-to-us-than-killing-babies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBzddatoNo3XPyHtpk%2Fwhy-abortion-looks-more-okay-to-us-than-killing-babies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 378, "htmlBody": "<p>Some thoughts that I don't remember anyone expressing on LW.</p>\n<p>First let's get this out of the way: life does <em>not</em> \"begin at birth\". As far as life can be said to \"begin\" anywhen, it begins at conception. Moreover, the child's intellectual abilities, self-awareness or similar qualities don't undergo any abrupt change at birth. It's just an arbitrary moment in the child's development. So it would seem that allowing killing kids <em>only before they're born</em> is illogical. What are the odds that your threshold for \"personhood\" coincides so well with the moment of birth? Could it be okay to kill kids up to 2 years old, say? CronoDAS voices this opinion <a href=\"/lw/2l/closet_survey_1/1ou?c=1\">here</a>.</p>\n<p>But there's another argument in favor of considering the moment of birth \"special\".&nbsp;Eliezer&nbsp;<a href=\"/lw/yj/an_especially_elegant_evpsych_experiment/\">linked</a>&nbsp;to a study showing that the degree of parental grief over a child's death, when plotted against the child's age, follows the same curve as the child's reproductive potential plotted against age. Now, the reproductive potential of an unborn kid depends on its <em>chance of survival,</em>&nbsp;and the moment of birth is special in this respect. In the ancestral environment many kids used to die at birth. And mothers died often too, which made their kids less likely to survive. An unborn kid is a creature that hasn't yet passed this big and sharply defined hurdle, so we instinctively discount our sympathy for its reproductive potential by a large factor without knowing why.</p>\n<p>How much this should influence our modern attitudes toward abortion, if at all, is another question entirely. As medicine becomes better, kids and mothers become more likely to survive. So if our attitudes were allowed to drift toward a new evolutionary equilibrium which took account of technology, we'd come to hate abortions <span style=\"text-decoration: line-through;\">again</span>&nbsp;(thx Morendil). But then again, the new evolutionary equilibrium is probably a very nasty system of values that no one in their right mind would embrace now (won't spell it out, use your imagination).</p>\n<p>Ultimately your morality is up to you and the little voices in your head. You think womens' rights trump kids' rights or the other way round, okay. But if you use factual arguments, try to make sure they are correct.</p>\n<p><strong>ETA:</strong> see DanArmak's and Sniffnoy's comments for simpler explanations. Taken together, they sound more convincing to me than my own idea.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BzddatoNo3XPyHtpk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 25, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "4085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["J4vdsSKB7LzAvaAMB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T13:19:12.265Z", "modifiedAt": null, "url": null, "title": "Do IQ tests measure g?", "slug": "do-iq-tests-measure-g", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:38.139Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QhrbBzFAaTQbee4zH/do-iq-tests-measure-g", "pageUrlRelative": "/posts/QhrbBzFAaTQbee4zH/do-iq-tests-measure-g", "linkUrl": "https://www.lesswrong.com/posts/QhrbBzFAaTQbee4zH/do-iq-tests-measure-g", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20IQ%20tests%20measure%20g%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20IQ%20tests%20measure%20g%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhrbBzFAaTQbee4zH%2Fdo-iq-tests-measure-g%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20IQ%20tests%20measure%20g%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhrbBzFAaTQbee4zH%2Fdo-iq-tests-measure-g", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhrbBzFAaTQbee4zH%2Fdo-iq-tests-measure-g", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>I'm indulging in the simple pleasure of drawing large conclusions from a single study....  <a href=\"http://blog.webnographer.com/2010/11/why-exams-mean-nothing-out-of-context/\">Why exams are nothing out of context:</a></p>\n<blockquote>the story about the maths ability of Brazilian street kids living in the in the favelas of Recife. This story helped both of us realise the importance of carrying out usability tests in context.  Three researchers (see: Carraher, Carraher, and Schliemann 1985) carried out research with children aged 9 to 15.  These kids had dropped out of school, and were selling sun screen, and chewing gum on the streets. The researchers worked out that they could set the kids questions by purchasing goods off them. For example, 1,000 minus 300  is the same as giving the kid a 1,000 Cruzeiros note for a product that costs 300 Cruzeiros. Multiplication can be done by asking the kids how much 3 of a product would cost.  In these tests the Brazilian street kids scored 98%. But when they were put into a formalised test setting, and asked instead of how much would 3 apples cost or what 3&times;9 is, the kids performance dropped to just 37%.</blockquote>\n<blockquote>What is scary is that the researchers later tested middle class children in a private school. These kids did very well in the formal exam. But when they had to do transactions with real money in the street, using the same maths, they failed in being able to do the transactions.</blockquote>\n<p>Is it possible that the correlation between g and success isn't about raw intelligence, it's about being able to access one's intelligence in situations (like classrooms) which involve thresholds for easily improving one's status?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4cKQgA4S7xfNeeWXg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QhrbBzFAaTQbee4zH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 28, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "4086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T15:24:16.699Z", "modifiedAt": null, "url": null, "title": "The Cult Of Reason", "slug": "the-cult-of-reason", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:37.631Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WZudcjaMbSjfm3E6y/the-cult-of-reason", "pageUrlRelative": "/posts/WZudcjaMbSjfm3E6y/the-cult-of-reason", "linkUrl": "https://www.lesswrong.com/posts/WZudcjaMbSjfm3E6y/the-cult-of-reason", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Cult%20Of%20Reason&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Cult%20Of%20Reason%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWZudcjaMbSjfm3E6y%2Fthe-cult-of-reason%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Cult%20Of%20Reason%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWZudcjaMbSjfm3E6y%2Fthe-cult-of-reason", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWZudcjaMbSjfm3E6y%2Fthe-cult-of-reason", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p>So... while investigating Wikipedia I found out about an actual Cult. Of Reason. Revolutionary France. From the description, it sounds pretty awesome.<a href=\"/&lt;a href=&quot;url goes here&quot;&gt;Text goes here&lt;/a&gt;\"> </a><a href=\"http://en.wikipedia.org/wiki/Cult_of_Reason\">Here's he link</a>. Is this denomination usable? Is it useful? Can it be resurrected? Should it be? Is it compatible with what we stand for? Discuss. Also, note that in French \"Culte\" does not mean \"Sect\", it means \"the act of worshipping\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WZudcjaMbSjfm3E6y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "4088", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T17:02:11.537Z", "modifiedAt": null, "url": null, "title": "Probability and Politics", "slug": "probability-and-politics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:37.737Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YafmHeLuxfRNRkgN2/probability-and-politics", "pageUrlRelative": "/posts/YafmHeLuxfRNRkgN2/probability-and-politics", "linkUrl": "https://www.lesswrong.com/posts/YafmHeLuxfRNRkgN2/probability-and-politics", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20and%20Politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20and%20Politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYafmHeLuxfRNRkgN2%2Fprobability-and-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20and%20Politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYafmHeLuxfRNRkgN2%2Fprobability-and-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYafmHeLuxfRNRkgN2%2Fprobability-and-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1600, "htmlBody": "<p><strong style=\"font-weight: bold; \">Follow-up to</strong>:&nbsp;<a href=\"/lw/2qq/politics_as_charity/\">Politics as Charity</a></p>\n<p>Can we think well about courses of action with low probabilities of high payoffs? &nbsp;</p>\n<p><a href=\"http://www.givingwhatwecan.org/\">Giving What We Can (GWWC)</a>, whose members pledge to donate a portion of their income to <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">most efficiently</a> help the global poor, says that evaluating spending on&nbsp;<a href=\"http://www.givingwhatwecan.org/resources/political-change.php\">political advocacy</a>&nbsp;is very hard:</p>\n<blockquote style=\"border-left-width: 2px; border-left-style: solid; border-left-color: #336699; padding-left: 4px; margin-top: 5px; margin-bottom: 5px; margin-left: 5px; margin-right: 15px; \">\n<p>Such changes could have enormous effects, but the cost-effectiveness of supporting them is very difficult to quantify as one needs to determine both the value of the effects and the degree to which your donation increases the probability of the change occurring. Each of these is very difficult to estimate and since the first is potentially very large and the second very small [1], it is very challenging to work out which scale will dominate.</p>\n</blockquote>\n<p>This sequence attempts to actually work out a first approximation of an answer to this question, piece by piece.&nbsp;Last time, I <a href=\"/lw/2qq/politics_as_charity/\">discussed</a> the evidence, especially from randomized experiments, that money spent on campaigning can elicit marginal votes quite cheaply. Today, I'll present the state-of-the-art in estimating the chance that those votes will directly swing an election outcome.</p>\n<p><strong>Disclaimer</strong></p>\n<p><strong></strong><a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is a mind-killer</a>:&nbsp;tribal feelings readily degrade the analytical skill and impartiality of otherwise very sophisticated thinkers, and so discussion of politics (even in a descriptive empirical way, or in meta-level fashion) signals an increased probability of poor analysis. I am not a political partisan and am raising the subject primarily for its illustrative value in thinking about small probabilities of large payoffs.</p>\n<p><a id=\"more\"></a></p>\n<div><strong style=\"font-weight: bold; \"><br />Two routes from vote to policy: electing and affecting</strong></div>\n<div>\n<p>In thinking about the effects of an additional vote on policy, we can distinguish between&nbsp;<a href=\"http://www.princeton.edu/~davidlee/wp/voterspolicies.pdf\">two ways</a>&nbsp;to affect public policy:&nbsp;<em style=\"font-style: italic; \">electing</em>&nbsp;politicians disposed to implement certain policies, or&nbsp;<em style=\"font-style: italic; \">affecting&nbsp;</em><span style=\"font-style: italic; \">[2]</span>&nbsp;the policies of existing and future officeholders who base their decisions on electoral statistics (including that marginal vote and its effects). Models of the probability of a marginal vote swaying an election are most obviously relevant to the electing approach, but the affecting route will also depend on such models, as they are used by politicians.&nbsp;</p>\n<p><strong>The surprising virtues of naive Fermi calculation</strong></p>\n</div>\n<div>\n<div>In my previous post I linked to Eric Schwitzgebel's &nbsp;<a href=\"http://experimentalphilosophy.typepad.com/experimental_philosophy/2009/07/professors-on-the-morality-of-voting.html\">discussion</a>&nbsp;&nbsp;of politics as charity, in which he guesstimated that the probability of a U.S. Presidential election being tied was 1/<em>n</em>&nbsp;where <em>n</em>&nbsp;is the number of voters. So with an estimate of 100 million U.S. voters in presidential elections he gave a 1/100,000,000 probability of a marginal vote swaying the election. This is a suspiciously&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">available</a>&nbsp;number.&nbsp;It seems to be derived from a simple model in which we imagine drawing randomly from all the possible divisions of the electorate between two candidates, when only one division would make the marginal vote decisive. But of course we know that voting won't involve a uniform distribution.</div>\n<div><br />One objection comes from modeling each vote as a flip of a biased coin. If the coin is exactly fair, then the chance of a tie goes with 1/(sqrt(<em>n</em>)). But if the coin is even slightly removed from exact fairness, then the chance of a tie rapidly falls to neglible levels. This was actually one of the first models in the &nbsp;<a href=\"http://www.springerlink.com/content/v7740xw683l67462/\">literature</a>, and &nbsp;<a href=\"/lw/2qq/politics_as_charity/2ovp?c=1\">recapitulated</a>&nbsp;by LessWrongers in comments last time.</div>\n<div><br />However, if we instead think of the bias of the coin itself as sampled from a uniform distribution, then we get the same &nbsp;<a href=\"http://www.springerlink.com/content/g5l731k042958483/\">result</a>&nbsp;as Schwitzgebel. In the electoral context, we can think of the coin's bias as reflecting factors with correlated effects on many voters, e.g. the state of the economy, with good economic results favoring incumbents and their parties.</div>\n<div><br /></div>\n<div>Of course, it's clear that electoral outcomes are not uniformly sampled: we see few 90%-10% outcomes in national American elections. Electoral competition and &nbsp;<a href=\"http://en.wikipedia.org/wiki/Median_voter_theorem\">Median Voter Theorem</a>&nbsp; effects, along with the stability of partisan identifications, will tend to keep candidates roughly balanced and limit the quantity of true swing voters. Within that range, unpredictable large \"wild card\" influences like the economy will shift the result from year to year, forcing us to spread our probability mass fairly evenly over a large region. Depending on our estimates of that range, we would need to multiply Schwitzgebel's estimate by a fudge factor <em>c</em> to get a probability of a tie of &nbsp;<em>c/n &nbsp;</em>for a random election, with 1&lt;<em>c&lt;</em>100 if we bound from above based on the idea that elections are very unlikely fought in a band of 1% of the electorate.</div>\n<div><br /></div>\n<p><strong>Fermi, meet data<br /></strong></p>\n<p>How well does this hold up against empirical data? In two papers from&nbsp;<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1083825\">1998</a>&nbsp; and &nbsp;<a href=\"http://www.nber.org/papers/w15220.pdf\">2009</a>, Andrew Gelman and coauthors attempt to estimate the probability a voter going into past U.S. Presidential elections should have assigned to casting a decisive vote. They use standard models that take inputs like party self-identification, economic growth, and incumbent approval ratings to predict electoral outcomes. These models have proven quite reliable in predicting candidate vote share and no more accurate methods are known. So we can take their output as a first approximation of the individual voter's rational estimates [3].</p>\n<div><br /></div>\n<div>Their first paper considers:</div>\n<blockquote>\n<div>... the 1952-1988 elections. For six of the elections, the probability is fairly independent of state size (slightly higher for the smallest states) and is near 1 in 10 million. For the other three elections (1964, 1972, and 1984, corresponding to the landslide victories of Johnson, Nixon, and Reagan [incumbents with favorable economic conditions]), the probability is much smaller, on the order of 1 in hundreds of millions for all of the states.<br /></div>\n</blockquote>\n<div>The result for 1992 was near 1 in 10 million. In 2008, which had economic and other conditions strongly favoring Obama, they found the following:<br /><br /></div>\n<blockquote>\n<div>probabilities a week before the 2008 presidential election, using state-by-state election forecasts based on the latest polls. The states where a single vote was most likely to matter are New Mexico, Virginia, New Hampshire, and Colorado, where your vote had an approximate 1 in 10 million chance of determining the national election outcome. On average, a[n actual] voter in America had a 1 in 60 million chance of being decisive in the presidential election.</div>\n</blockquote>\n<div>All told, these place the average value of &nbsp;<em>c</em>&nbsp;a little under the middle of the range given by the Fermi calculation above, and are very far from &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Pascal's_mugging\">Pascal's Mugging</a>&nbsp; territory.<br /><br /></div>\n<div><strong>Voting vs campaign contributions</strong></div>\n<div><br /></div>\n<div>What are the implications for a causal decision theorist who wants to dedicate a modest effort to efficient do-gooding? The exact value of voting depends on many other factors, e.g. the value of policies, but we can at least compare ways to deliver votes.<br /><br /></div>\n<div>Which has more bang per buck: voting in your jurisdiction or taking the hour or so to earn money and make campaign contributions? Last time I &nbsp;<a href=\"/lw/2qq/politics_as_charity/\">estimated</a>&nbsp; a cost of $50 to $500 per vote from contributions, more in more competitive races (diminishing returns). So unless you have a high opportunity cost, you'd do better to vote yourself than contribute to a campaign in your own jurisdiction. The standard heuristic that everyone should vote seems to have been defended.<br /><br /></div>\n<div>But let's avoid&nbsp;<a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">motivated stopping</a>.&nbsp;The above data indicate frequent differences of 1-2 orders of magnitude across jurisdictions. So someone in an uncompetitive New York district would often do better to donate less than $50 (to a competitive race) than to vote. (On the other hand, if you live in a competitive district [4], replacing your vote with donations might cost a sizable portion of your charitable budget.)<br /><br /></div>\n<div>When we take into differences between election cycles, usually another 1-2 orders of magnitude, the value of voting in a \"safe\" jurisdiction in an election which is not close winds up negligible (if your reaction to this fact is not independent of others'). For those spending on political advocacy, this provides a route for increased cost-effectiveness: by switching from an even distribution of spending to focus on the (forecast) closest third of elections, you can nearly double your expected effectiveness. Even more extreme \"wait-in-reserve\" strategies could pay off, but are limited by the imperfection of forecasting methods.<br /><br /></div>\n<div><strong>Ties, recounts, and lawyers&nbsp;<br /><br /></strong></div>\n</div>\n<div>Does the possibility of &nbsp;<a href=\"/lw/2qq/politics_as_charity/2ofh?c=1\">recounts</a>&nbsp;disrupt the above analysis?</div>\n<div><br /></div>\n<div>It turns out that it doesn't.&nbsp;In countries with reasonably clean elections, a candidate with a large enough margin of victory is almost certain to be declared the winner. Say that a \"large enough\" margin is 5,000 votes, and that a candidate is 99% likely to be declared the winner given that margin. Then Candace the Candidate must go from a 1% probability of victory to a 99% probability of victory as we consider vote totals from a 5,000 vote shortfall to a 5,000 vote lead. So, on average within that range, each marginal vote must increase her probability of victory by 0.0098%. Since there are 10,000 possibilities to hit within the range, so long as they have roughly similar prospective probabilities the expected value of the marginal vote will be almost the same as the single \"deciding vote\" model.<br /><br /></div>\n<div>\n<div><strong>Summary</strong></div>\n</div>\n<p>It is possible to make sensible estimates of the probability of at least some events that have never happened before, like tied presidential elections, and use them in attempting efficient philanthropy.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>[1] At least for&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">two-boxers</a>. More on one-boxing decision theorists at a later date.</p>\n<p>[2]&nbsp;There are a number of arguments that voters' role in affecting policies is more important, e.g. in this Less Wrong&nbsp;<a href=\"/lw/mh/the_american_system_and_misleading_labels/\">post</a>&nbsp;by Eliezer. More on this later.</p>\n<p>[3] Although for very low values, the possibility that our models are fundamentally mistaken looms progressively larger. See &nbsp;<a href=\"http://arxiv.org/abs/0810.5515\">Ord et al</a>.</p>\n<p>[4] Including other relevant sorts of competitiveness, e.g. California is typically a safe state in Presidential elections, but there are usually competitive ballot initiatives.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "EeSkeTcT4wtW2fWsL": 1, "AeqCtS3BaY3cwzKAs": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YafmHeLuxfRNRkgN2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 28, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "3699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong style=\"font-weight: bold; \">Follow-up to</strong>:&nbsp;<a href=\"/lw/2qq/politics_as_charity/\">Politics as Charity</a></p>\n<p>Can we think well about courses of action with low probabilities of high payoffs? &nbsp;</p>\n<p><a href=\"http://www.givingwhatwecan.org/\">Giving What We Can (GWWC)</a>, whose members pledge to donate a portion of their income to <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">most efficiently</a> help the global poor, says that evaluating spending on&nbsp;<a href=\"http://www.givingwhatwecan.org/resources/political-change.php\">political advocacy</a>&nbsp;is very hard:</p>\n<blockquote style=\"border-left-width: 2px; border-left-style: solid; border-left-color: #336699; padding-left: 4px; margin-top: 5px; margin-bottom: 5px; margin-left: 5px; margin-right: 15px; \">\n<p>Such changes could have enormous effects, but the cost-effectiveness of supporting them is very difficult to quantify as one needs to determine both the value of the effects and the degree to which your donation increases the probability of the change occurring. Each of these is very difficult to estimate and since the first is potentially very large and the second very small [1], it is very challenging to work out which scale will dominate.</p>\n</blockquote>\n<p>This sequence attempts to actually work out a first approximation of an answer to this question, piece by piece.&nbsp;Last time, I <a href=\"/lw/2qq/politics_as_charity/\">discussed</a> the evidence, especially from randomized experiments, that money spent on campaigning can elicit marginal votes quite cheaply. Today, I'll present the state-of-the-art in estimating the chance that those votes will directly swing an election outcome.</p>\n<p><strong id=\"Disclaimer\">Disclaimer</strong></p>\n<p><strong></strong><a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is a mind-killer</a>:&nbsp;tribal feelings readily degrade the analytical skill and impartiality of otherwise very sophisticated thinkers, and so discussion of politics (even in a descriptive empirical way, or in meta-level fashion) signals an increased probability of poor analysis. I am not a political partisan and am raising the subject primarily for its illustrative value in thinking about small probabilities of large payoffs.</p>\n<p><a id=\"more\"></a></p>\n<div><strong style=\"font-weight: bold; \"><br>Two routes from vote to policy: electing and affecting</strong></div>\n<div>\n<p>In thinking about the effects of an additional vote on policy, we can distinguish between&nbsp;<a href=\"http://www.princeton.edu/~davidlee/wp/voterspolicies.pdf\">two ways</a>&nbsp;to affect public policy:&nbsp;<em style=\"font-style: italic; \">electing</em>&nbsp;politicians disposed to implement certain policies, or&nbsp;<em style=\"font-style: italic; \">affecting&nbsp;</em><span style=\"font-style: italic; \">[2]</span>&nbsp;the policies of existing and future officeholders who base their decisions on electoral statistics (including that marginal vote and its effects). Models of the probability of a marginal vote swaying an election are most obviously relevant to the electing approach, but the affecting route will also depend on such models, as they are used by politicians.&nbsp;</p>\n<p><strong id=\"The_surprising_virtues_of_naive_Fermi_calculation\">The surprising virtues of naive Fermi calculation</strong></p>\n</div>\n<div>\n<div>In my previous post I linked to Eric Schwitzgebel's &nbsp;<a href=\"http://experimentalphilosophy.typepad.com/experimental_philosophy/2009/07/professors-on-the-morality-of-voting.html\">discussion</a>&nbsp;&nbsp;of politics as charity, in which he guesstimated that the probability of a U.S. Presidential election being tied was 1/<em>n</em>&nbsp;where <em>n</em>&nbsp;is the number of voters. So with an estimate of 100 million U.S. voters in presidential elections he gave a 1/100,000,000 probability of a marginal vote swaying the election. This is a suspiciously&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Availability_heuristic\">available</a>&nbsp;number.&nbsp;It seems to be derived from a simple model in which we imagine drawing randomly from all the possible divisions of the electorate between two candidates, when only one division would make the marginal vote decisive. But of course we know that voting won't involve a uniform distribution.</div>\n<div><br>One objection comes from modeling each vote as a flip of a biased coin. If the coin is exactly fair, then the chance of a tie goes with 1/(sqrt(<em>n</em>)). But if the coin is even slightly removed from exact fairness, then the chance of a tie rapidly falls to neglible levels. This was actually one of the first models in the &nbsp;<a href=\"http://www.springerlink.com/content/v7740xw683l67462/\">literature</a>, and &nbsp;<a href=\"/lw/2qq/politics_as_charity/2ovp?c=1\">recapitulated</a>&nbsp;by LessWrongers in comments last time.</div>\n<div><br>However, if we instead think of the bias of the coin itself as sampled from a uniform distribution, then we get the same &nbsp;<a href=\"http://www.springerlink.com/content/g5l731k042958483/\">result</a>&nbsp;as Schwitzgebel. In the electoral context, we can think of the coin's bias as reflecting factors with correlated effects on many voters, e.g. the state of the economy, with good economic results favoring incumbents and their parties.</div>\n<div><br></div>\n<div>Of course, it's clear that electoral outcomes are not uniformly sampled: we see few 90%-10% outcomes in national American elections. Electoral competition and &nbsp;<a href=\"http://en.wikipedia.org/wiki/Median_voter_theorem\">Median Voter Theorem</a>&nbsp; effects, along with the stability of partisan identifications, will tend to keep candidates roughly balanced and limit the quantity of true swing voters. Within that range, unpredictable large \"wild card\" influences like the economy will shift the result from year to year, forcing us to spread our probability mass fairly evenly over a large region. Depending on our estimates of that range, we would need to multiply Schwitzgebel's estimate by a fudge factor <em>c</em> to get a probability of a tie of &nbsp;<em>c/n &nbsp;</em>for a random election, with 1&lt;<em>c&lt;</em>100 if we bound from above based on the idea that elections are very unlikely fought in a band of 1% of the electorate.</div>\n<div><br></div>\n<p><strong id=\"Fermi__meet_data\">Fermi, meet data<br></strong></p>\n<p>How well does this hold up against empirical data? In two papers from&nbsp;<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1083825\">1998</a>&nbsp; and &nbsp;<a href=\"http://www.nber.org/papers/w15220.pdf\">2009</a>, Andrew Gelman and coauthors attempt to estimate the probability a voter going into past U.S. Presidential elections should have assigned to casting a decisive vote. They use standard models that take inputs like party self-identification, economic growth, and incumbent approval ratings to predict electoral outcomes. These models have proven quite reliable in predicting candidate vote share and no more accurate methods are known. So we can take their output as a first approximation of the individual voter's rational estimates [3].</p>\n<div><br></div>\n<div>Their first paper considers:</div>\n<blockquote>\n<div>... the 1952-1988 elections. For six of the elections, the probability is fairly independent of state size (slightly higher for the smallest states) and is near 1 in 10 million. For the other three elections (1964, 1972, and 1984, corresponding to the landslide victories of Johnson, Nixon, and Reagan [incumbents with favorable economic conditions]), the probability is much smaller, on the order of 1 in hundreds of millions for all of the states.<br></div>\n</blockquote>\n<div>The result for 1992 was near 1 in 10 million. In 2008, which had economic and other conditions strongly favoring Obama, they found the following:<br><br></div>\n<blockquote>\n<div>probabilities a week before the 2008 presidential election, using state-by-state election forecasts based on the latest polls. The states where a single vote was most likely to matter are New Mexico, Virginia, New Hampshire, and Colorado, where your vote had an approximate 1 in 10 million chance of determining the national election outcome. On average, a[n actual] voter in America had a 1 in 60 million chance of being decisive in the presidential election.</div>\n</blockquote>\n<div>All told, these place the average value of &nbsp;<em>c</em>&nbsp;a little under the middle of the range given by the Fermi calculation above, and are very far from &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Pascal's_mugging\">Pascal's Mugging</a>&nbsp; territory.<br><br></div>\n<div><strong>Voting vs campaign contributions</strong></div>\n<div><br></div>\n<div>What are the implications for a causal decision theorist who wants to dedicate a modest effort to efficient do-gooding? The exact value of voting depends on many other factors, e.g. the value of policies, but we can at least compare ways to deliver votes.<br><br></div>\n<div>Which has more bang per buck: voting in your jurisdiction or taking the hour or so to earn money and make campaign contributions? Last time I &nbsp;<a href=\"/lw/2qq/politics_as_charity/\">estimated</a>&nbsp; a cost of $50 to $500 per vote from contributions, more in more competitive races (diminishing returns). So unless you have a high opportunity cost, you'd do better to vote yourself than contribute to a campaign in your own jurisdiction. The standard heuristic that everyone should vote seems to have been defended.<br><br></div>\n<div>But let's avoid&nbsp;<a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">motivated stopping</a>.&nbsp;The above data indicate frequent differences of 1-2 orders of magnitude across jurisdictions. So someone in an uncompetitive New York district would often do better to donate less than $50 (to a competitive race) than to vote. (On the other hand, if you live in a competitive district [4], replacing your vote with donations might cost a sizable portion of your charitable budget.)<br><br></div>\n<div>When we take into differences between election cycles, usually another 1-2 orders of magnitude, the value of voting in a \"safe\" jurisdiction in an election which is not close winds up negligible (if your reaction to this fact is not independent of others'). For those spending on political advocacy, this provides a route for increased cost-effectiveness: by switching from an even distribution of spending to focus on the (forecast) closest third of elections, you can nearly double your expected effectiveness. Even more extreme \"wait-in-reserve\" strategies could pay off, but are limited by the imperfection of forecasting methods.<br><br></div>\n<div><strong>Ties, recounts, and lawyers&nbsp;<br><br></strong></div>\n</div>\n<div>Does the possibility of &nbsp;<a href=\"/lw/2qq/politics_as_charity/2ofh?c=1\">recounts</a>&nbsp;disrupt the above analysis?</div>\n<div><br></div>\n<div>It turns out that it doesn't.&nbsp;In countries with reasonably clean elections, a candidate with a large enough margin of victory is almost certain to be declared the winner. Say that a \"large enough\" margin is 5,000 votes, and that a candidate is 99% likely to be declared the winner given that margin. Then Candace the Candidate must go from a 1% probability of victory to a 99% probability of victory as we consider vote totals from a 5,000 vote shortfall to a 5,000 vote lead. So, on average within that range, each marginal vote must increase her probability of victory by 0.0098%. Since there are 10,000 possibilities to hit within the range, so long as they have roughly similar prospective probabilities the expected value of the marginal vote will be almost the same as the single \"deciding vote\" model.<br><br></div>\n<div>\n<div><strong>Summary</strong></div>\n</div>\n<p>It is possible to make sensible estimates of the probability of at least some events that have never happened before, like tied presidential elections, and use them in attempting efficient philanthropy.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>[1] At least for&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">two-boxers</a>. More on one-boxing decision theorists at a later date.</p>\n<p>[2]&nbsp;There are a number of arguments that voters' role in affecting policies is more important, e.g. in this Less Wrong&nbsp;<a href=\"/lw/mh/the_american_system_and_misleading_labels/\">post</a>&nbsp;by Eliezer. More on this later.</p>\n<p>[3] Although for very low values, the possibility that our models are fundamentally mistaken looms progressively larger. See &nbsp;<a href=\"http://arxiv.org/abs/0810.5515\">Ord et al</a>.</p>\n<p>[4] Including other relevant sorts of competitiveness, e.g. California is typically a safe state in Presidential elections, but there are usually competitive ballot initiatives.</p>", "sections": [{"title": "Disclaimer", "anchor": "Disclaimer", "level": 1}, {"title": "The surprising virtues of naive Fermi calculation", "anchor": "The_surprising_virtues_of_naive_Fermi_calculation", "level": 1}, {"title": "Fermi, meet data", "anchor": "Fermi__meet_data", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "31 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SgZ2mhvDbneBusFEB", "3p3CYauiX8oLjmwRF", "L32LHWzy9FzSDazEg", "ZXuqNhMDcs6mYtb6i"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T18:12:15.828Z", "modifiedAt": null, "url": null, "title": "2005 short story loosely about P-zombies", "slug": "2005-short-story-loosely-about-p-zombies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.961Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Airedale", "createdAt": "2010-03-14T19:20:44.438Z", "isAdmin": false, "displayName": "Airedale"}, "userId": "iQo3csv2cgdsjfLnY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mo4FaABprMtnWpPow/2005-short-story-loosely-about-p-zombies", "pageUrlRelative": "/posts/Mo4FaABprMtnWpPow/2005-short-story-loosely-about-p-zombies", "linkUrl": "https://www.lesswrong.com/posts/Mo4FaABprMtnWpPow/2005-short-story-loosely-about-p-zombies", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202005%20short%20story%20loosely%20about%20P-zombies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2005%20short%20story%20loosely%20about%20P-zombies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMo4FaABprMtnWpPow%2F2005-short-story-loosely-about-p-zombies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2005%20short%20story%20loosely%20about%20P-zombies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMo4FaABprMtnWpPow%2F2005-short-story-loosely-about-p-zombies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMo4FaABprMtnWpPow%2F2005-short-story-loosely-about-p-zombies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>I thought some of you might enjoy this 2005 story published in Asimov's:</p>\n<p><a href=\"http://www.asimovs.com/_issue_0702/Secondperson.shtml\">Second Person, Present Tense</a> by Daryl Gregory (plus some of the <a href=\"http://www.darylgregory.com/stories/SecondPersonPresentTense.aspx\">author's notes</a> on the story)</p>\n<p>The story involves a a drug that (for a period of time) turns someone into a P-zombie.</p>\n<p>It's currently available for free on the Asimov's page, but it may only be there temporarily.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mo4FaABprMtnWpPow", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 6.490785787245333e-07, "legacy": true, "legacyId": "4089", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T21:13:45.409Z", "modifiedAt": null, "url": null, "title": "Startups", "slug": "startups", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:38.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wE3nf7NRCwj9yJd3K/startups", "pageUrlRelative": "/posts/wE3nf7NRCwj9yJd3K/startups", "linkUrl": "https://www.lesswrong.com/posts/wE3nf7NRCwj9yJd3K/startups", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Startups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStartups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE3nf7NRCwj9yJd3K%2Fstartups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Startups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE3nf7NRCwj9yJd3K%2Fstartups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE3nf7NRCwj9yJd3K%2Fstartups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>There seems to be a non-negligible deal of overlap between this community and <a href=\"http://news.ycombinator.com\">Hacker News</a>, both in terms of material and members. For those not aware of HN, it's a news aggregator for people interested in startups, technology, and other intellectually interesting topics, with a reputation for high-quality material and discourse.</p>\n<p>While rationality and LessWrong gets its fair share of attention over at HN, I haven't heard of much discussion about startups over here. Off-line, I've heard a claim that in terms of contribution to existential risk prevention charities, startups are suboptimal when compared to jobs in finance, but not much else other than that. I find this odd, as many of the contributors in this site seem to be prime founder material, and rationality should really be of use when working in a high-stakes ever-changing environment.</p>\n<p>My intention with this post is simply to kickstart a discussion around startups and gauge the attitudes of fellow LessWrongers. Does anyone (else) aspire to becoming a startup founder in the next few years? Do you believe startup founding to be a viable means of contributing to groups existential risk prevention?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wE3nf7NRCwj9yJd3K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 6.491229604477327e-07, "legacy": true, "legacyId": "4090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T21:40:00.597Z", "modifiedAt": null, "url": null, "title": "Future of Humanity Institute at Oxford hiring postdocs", "slug": "future-of-humanity-institute-at-oxford-hiring-postdocs", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i5bez6nwcGHCF6i7s/future-of-humanity-institute-at-oxford-hiring-postdocs", "pageUrlRelative": "/posts/i5bez6nwcGHCF6i7s/future-of-humanity-institute-at-oxford-hiring-postdocs", "linkUrl": "https://www.lesswrong.com/posts/i5bez6nwcGHCF6i7s/future-of-humanity-institute-at-oxford-hiring-postdocs", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Future%20of%20Humanity%20Institute%20at%20Oxford%20hiring%20postdocs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFuture%20of%20Humanity%20Institute%20at%20Oxford%20hiring%20postdocs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi5bez6nwcGHCF6i7s%2Ffuture-of-humanity-institute-at-oxford-hiring-postdocs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Future%20of%20Humanity%20Institute%20at%20Oxford%20hiring%20postdocs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi5bez6nwcGHCF6i7s%2Ffuture-of-humanity-institute-at-oxford-hiring-postdocs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi5bez6nwcGHCF6i7s%2Ffuture-of-humanity-institute-at-oxford-hiring-postdocs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.fhi.ox.ac.uk/news/2010/vacancies\">http://www.fhi.ox.ac.uk/news/2010/vacancies</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i5bez6nwcGHCF6i7s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.491293807386318e-07, "legacy": true, "legacyId": "4091", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-24T22:31:57.215Z", "modifiedAt": null, "url": null, "title": "$100 for the best article on efficient charity -- deadline Wednesday 1st December", "slug": "usd100-for-the-best-article-on-efficient-charity-deadline", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:45.849Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4amcyxad5bnBR9Afm/usd100-for-the-best-article-on-efficient-charity-deadline", "pageUrlRelative": "/posts/4amcyxad5bnBR9Afm/usd100-for-the-best-article-on-efficient-charity-deadline", "linkUrl": "https://www.lesswrong.com/posts/4amcyxad5bnBR9Afm/usd100-for-the-best-article-on-efficient-charity-deadline", "postedAtFormatted": "Wednesday, November 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%24100%20for%20the%20best%20article%20on%20efficient%20charity%20--%20deadline%20Wednesday%201st%20December&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%24100%20for%20the%20best%20article%20on%20efficient%20charity%20--%20deadline%20Wednesday%201st%20December%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4amcyxad5bnBR9Afm%2Fusd100-for-the-best-article-on-efficient-charity-deadline%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%24100%20for%20the%20best%20article%20on%20efficient%20charity%20--%20deadline%20Wednesday%201st%20December%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4amcyxad5bnBR9Afm%2Fusd100-for-the-best-article-on-efficient-charity-deadline", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4amcyxad5bnBR9Afm%2Fusd100-for-the-best-article-on-efficient-charity-deadline", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 471, "htmlBody": "<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; margin-top: 8px; margin-right: 8px; margin-bottom: 8px; margin-left: 8px; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; padding-top: 0.5em; padding-right: 0.5em; padding-bottom: 0.5em; padding-left: 0.5em; \">\n<p style=\"font-family: Verdana, Arial, Helvetica, sans-serif; \">Reposted from a few days ago, noting that&nbsp;<a style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\" href=\"/user/jsalvatier/\">jsalvatier</a>&nbsp;(kudos to him for putting up the prize money, <em style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">very </em>community spirited) &nbsp; has promised <strong style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">$100 to the winner</strong>, and I have decided to set a deadline of&nbsp;<strong style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">Wednesday 1st December</strong> for submissions, as my friend has called me and asked me where the article I promised him is. This guy wants his&nbsp;god-damn&nbsp;rationality already, people!&nbsp;</p>\n<p style=\"font-family: Verdana, Arial, Helvetica, sans-serif; \">My friend is currently in a potentially lucrative management consultancy career, but is considering getting a job in eco-tourism because he \"wants to&nbsp;make the world a better place\" and we got into a debate about Efficient Charity, Roles&nbsp;vs. Goals, and&nbsp;<a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Optimizing versus Acquiring Warm Fuzzies</a>.&nbsp;</p>\n<p style=\"font-family: Verdana, Arial, Helvetica, sans-serif; \">I thought that there would be a good article here that I could send him to, but there&nbsp;isn't. So I've decided to ask people to write such an article.&nbsp;What I am looking for is an article that is <strong>less than 1800 words long</strong>, and explains the&nbsp;following ideas:&nbsp;</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: decimal; list-style-position: outside; list-style-image: initial; \">\n<li>Charity should be about actually trying to do as much expected good as possible for a&nbsp;given amount of resource (time, $), in a quantified sense. I.e. \"5000 lives saved in&nbsp;expectation\", not \"we made a big difference\".&nbsp;</li>\n<li>The norms and framing of our society regarding charity currently get it wrong, i.e.&nbsp;people send lots of $ to charities that do a lot less good than other charities. The&nbsp;\"inefficiency\" here is very large, i.e. Givewell estimates by a factor of <a href=\"http://blog.givewell.org/2010/01/28/can-choosing-the-right-charity-double-your-impact/\"><strong>1000</strong> at least</a>. &nbsp;Our norm of ranking charities by % spent on overheads is very very silly.&nbsp;</li>\n<li>It is usually <a href=\"http://www.utilitarian-essays.com/make-money.html\">better to work a highly-paid job</a> and donate because if you work for a&nbsp;charity you replace the person who would have been hired had you not applied</li>\n<li>Our instincts will tend to tempt us to optimize for signalling, this is to be&nbsp;resisted unless (or to the extent that)&nbsp;it is what you actually want to do.&nbsp;Our instincts will also tend to want to optimize for \"<a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Warm Fuzzies</a>\". These&nbsp;should be purchased separately from <em>actual good outcomes</em>.&nbsp;</li>\n<li>Our human intuition about how to allocate resources is extremely bad. Moreover, since charity is typically for the <a href=\"http://www.amazon.co.uk/Dead-Aid-working-another-Africa/dp/1846140064\">so-called benefit</a> of someone else, you, the donor, usually don't get to see the result. Lacking this feedback from experience, one tends to make all kinds of gigantic mistakes.&nbsp;</li>\n</ol>\n<p style=\"color: #000000; font-size: small;\">but without using any unexplained LW Jargon. (Utilons, Warm Fuzzies, optimizing).&nbsp;Linking to posts explaining jargon is&nbsp;<strong style=\"font-weight: bold; \"><span style=\"font-family: mceinline;\">NOT</span></strong><strong><span style=\"font-family: mceinline;\">&nbsp;OK</span></strong>. Just don't use any LW Jargon at all. I will judge the winner based upon these criteria and the score that the article gets&nbsp;on LW. Maybe the winning article will not rigidly meet all criteria: there is some flexibility. The point of the article is to persuade people who are, at least to some extent charitable and who are smart (university educated at a top university or equivalent) to seriously consider investing more time in rationality when they want to do charitable things.&nbsp;</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4amcyxad5bnBR9Afm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "4092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T02:12:59.245Z", "modifiedAt": null, "url": null, "title": "What Science got Wrong and Why ", "slug": "what-science-got-wrong-and-why", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:39.242Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vj3ChjWrchLAZwn38/what-science-got-wrong-and-why", "pageUrlRelative": "/posts/vj3ChjWrchLAZwn38/what-science-got-wrong-and-why", "linkUrl": "https://www.lesswrong.com/posts/vj3ChjWrchLAZwn38/what-science-got-wrong-and-why", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Science%20got%20Wrong%20and%20Why%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Science%20got%20Wrong%20and%20Why%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvj3ChjWrchLAZwn38%2Fwhat-science-got-wrong-and-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Science%20got%20Wrong%20and%20Why%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvj3ChjWrchLAZwn38%2Fwhat-science-got-wrong-and-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvj3ChjWrchLAZwn38%2Fwhat-science-got-wrong-and-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p><a href=\"http://www.edge.org/3rd_culture/thaler10/thaler10_index.html\">An article at The Edge has scientific experts in various fields give their favorite examples of theories that were wrong in their fields.</a> Most relevantly to Less Wrong, many of those scientists discuss what their disciplines did that was wrong which resulted in the misconceptions. For example, Irene Pepperberg not surprisingly discusses the failure for scientists to appreciate avian intelligence. She emphasizes that this failure resulted from a combination of different factors, including the lack of appreciation that high level cognition could occur without the mammalian cortex, and that many early studies used pigeons which just aren't that bright.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vj3ChjWrchLAZwn38", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 6.491961450571569e-07, "legacy": true, "legacyId": "4093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T06:57:23.015Z", "modifiedAt": null, "url": null, "title": "Science and rationalism - a brief epistemological exploration", "slug": "science-and-rationalism-a-brief-epistemological-exploration", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.930Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fictiona", "createdAt": "2010-11-23T14:55:27.642Z", "isAdmin": false, "displayName": "fictiona"}, "userId": "pXq2T4wi7cEW67oSd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a3sMDCTtwTyWgobpc/science-and-rationalism-a-brief-epistemological-exploration", "pageUrlRelative": "/posts/a3sMDCTtwTyWgobpc/science-and-rationalism-a-brief-epistemological-exploration", "linkUrl": "https://www.lesswrong.com/posts/a3sMDCTtwTyWgobpc/science-and-rationalism-a-brief-epistemological-exploration", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%20and%20rationalism%20-%20a%20brief%20epistemological%20exploration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%20and%20rationalism%20-%20a%20brief%20epistemological%20exploration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3sMDCTtwTyWgobpc%2Fscience-and-rationalism-a-brief-epistemological-exploration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%20and%20rationalism%20-%20a%20brief%20epistemological%20exploration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3sMDCTtwTyWgobpc%2Fscience-and-rationalism-a-brief-epistemological-exploration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3sMDCTtwTyWgobpc%2Fscience-and-rationalism-a-brief-epistemological-exploration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 514, "htmlBody": "<p>Is rationalism scientific? Yes. Is science rationalistic? Depends.</p>\n<p>Within scientific disciplines, I believe that computer science is more rationalistic than others due to its deductibility (can be proved mathematically).</p>\n<p>How about other fields?</p>\n<p>Physics? Chemistry? Biology? They rely mostly on empirical results to support their arguments and theories. They can observe. They can experiment. Some of them claim they can prove... but to what extent can they be so confident? Can we really bridge empiricism to rationalism?&nbsp;</p>\n<p><strong>Verificationism.&nbsp;</strong>Sure, scientific theory should be able to be supported by empirical evidence. But lack of contradicting evidence doesn't necessarily mean that the theory is true. It just means that the theory <em>isn't yet made false</em>, even if&nbsp;a hypothesis can be empirically tested and the study has been replicated again and again. <em>Falsifiability&nbsp;</em>is like a time bomb. You don't know the conditions in which a said theory doesn't apply. There may be unknown unknowns, like&nbsp;Newton didn't know his theory didn't apply in the outer space.</p>\n<p>Moreover, some fields can not be experimented, and in some cases, observed. Examples: astronomy, natural history. This is more of a speculation - yet does not receive as much scepticism as social science. Big Bang theory, how dinosaurs were extincted, etc.. &nbsp;cannot be replicated or confirmed given current technology. Scientists in those areas are playing on \"what ifs\"... trying to explain possible causes without really knowing how cause-effect relationships may have been different in prehistorical times. I don't find them very different from, say, political analysts trying to explain why Kennedy was assassinated.</p>\n<p>I myself am a <em>fallibist</em>. But I won't go as far as supporting the&nbsp;<em>M&uuml;nchhausen Trilemma</em>.&nbsp;To science: I have reasonable doubts, but I believe that reasonable (blind) faith is necessary for practicality/pragmatism.</p>\n<p><strong>Relativism.</strong>&nbsp;A proposition is only true relative to a particular perspective. Like the story of <a href=\"http://en.wikipedia.org/wiki/Blind_men_and_an_elephant\">blind men and the elephant</a>. How can scientists be sure that they see the whole 'truth', if 'truth' is definable at all? &nbsp;Maybe the elephant is too large? Maybe blind men cook the results in order to get their opinions published? Maybe blind men lack a good common measurement (e.g. eyes of the same quality) to give the elephant a 'fair' assessment?</p>\n<p>Maybe it's not blind men and the elephant - it's Plato's Allegory of the Cave (in modern days, it's called <em>Matrix the movie</em>)?</p>\n<p>The point here is that to scientists need to use judgment in measuring and interpreting the results, and this process relies on the limits and sharpness of their senses, intellect, measurement equipment as well as their experience. Why is light year used to measure distances in space? Why is IQ used to measure intelligence? There are limitations from both cognitive and methodological points of view.</p>\n<p><strong>Subjectivism.</strong>&nbsp;Whatever methods scientists use to gain confidence in theory from empirical evidence, they \"participate\" in measuring the results, rather than \"observing them objectively\". When you use a ruler to measure the length of something, are you sure you have good eyes? Your visual ability remains constant the whole time? The ruler doesn't contract or expand while measuring the object? This is particularly present in measuring behaviour of waves and particles at atomic and subatomic scales.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a3sMDCTtwTyWgobpc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -6, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "4095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T07:33:38.809Z", "modifiedAt": null, "url": null, "title": "localroger: Replotting The Transmigration of Prime Intellect", "slug": "localroger-replotting-the-transmigration-of-prime-intellect", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:37.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZAFXt94q8SDwBeSDe/localroger-replotting-the-transmigration-of-prime-intellect", "pageUrlRelative": "/posts/ZAFXt94q8SDwBeSDe/localroger-replotting-the-transmigration-of-prime-intellect", "linkUrl": "https://www.lesswrong.com/posts/ZAFXt94q8SDwBeSDe/localroger-replotting-the-transmigration-of-prime-intellect", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20localroger%3A%20Replotting%20The%20Transmigration%20of%20Prime%20Intellect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Alocalroger%3A%20Replotting%20The%20Transmigration%20of%20Prime%20Intellect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAFXt94q8SDwBeSDe%2Flocalroger-replotting-the-transmigration-of-prime-intellect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=localroger%3A%20Replotting%20The%20Transmigration%20of%20Prime%20Intellect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAFXt94q8SDwBeSDe%2Flocalroger-replotting-the-transmigration-of-prime-intellect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAFXt94q8SDwBeSDe%2Flocalroger-replotting-the-transmigration-of-prime-intellect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.kuro5hin.org/story/2010/9/3/20910/05774\">http://www.kuro5hin.org/story/2010/9/3/20910/05774</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZAFXt94q8SDwBeSDe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 4, "extendedScore": null, "score": 6.492744644389158e-07, "legacy": true, "legacyId": "4098", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T09:32:04.283Z", "modifiedAt": null, "url": null, "title": "Rational Project Management", "slug": "rational-project-management", "viewCount": null, "lastCommentedAt": "2021-10-02T10:23:53.940Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uBZwWKYsmuy5tWewf/rational-project-management", "pageUrlRelative": "/posts/uBZwWKYsmuy5tWewf/rational-project-management", "linkUrl": "https://www.lesswrong.com/posts/uBZwWKYsmuy5tWewf/rational-project-management", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Project%20Management&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Project%20Management%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBZwWKYsmuy5tWewf%2Frational-project-management%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Project%20Management%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBZwWKYsmuy5tWewf%2Frational-project-management", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBZwWKYsmuy5tWewf%2Frational-project-management", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 345, "htmlBody": "<p>There's a lot of discussion on the site about <a title=\"akrasia\" href=\"/lw/1sm/akrasia_tactics_review/\">akrasia</a> (failure of willpower), but most of it focuses on the individual. Obviously, though, organizations also can be said to suffer from akrasia. Even when there is clear agreement among the leaders or members of a business, charity, political party, etc. about what the organization should try to accomplish, it's often the case that the daily tasks performed by the people who work there bear only the loosest of resemblance to the tasks that would be picked by an ideal decision-maker.</p>\n<p>There are lots of different issues here -- what should be done, when it should be done, who should do it, how much of a budget in money, office space, website space, etc. a project should receive, when and how to evaluate the success of a project...</p>\n<p>I've come across several good books on how to cope with office politics, how to build a financially successful company, and how to motivate people to perform at a high level, but none on how to manage an organization so that it can fulfill its mission most efficiently.</p>\n<p>Does anyone know of a standard (or cutting-edge) text (or program!) in this area? Books, articles, videos, and other media are all welcome, even if they're behind a paywall. Sometime in the next 20 years, I hope to revolutionize the efficiency of America's legal system by developing project management software, automatic litigation tools, machine-readable law libraries, consulting, and/or teaching. Basically I think there is no good reason why the current system, which involves otherwise smart people solving the same problems over and over again for different clients for decades on end, should not give way to an entrepreneurial system where problems get solved once or twice and then the solutions get propagated across the society. Since this is an <a title=\"impossible\" href=\"/lw/qs/einsteins_superpowers/\">impossible</a> <a href=\"/lw/un/on_doing_the_impossible/\">problem</a>, I will have plenty of wheels to invent, and see no need to reinvent any wheels that already exist. So...know any wheels? Even if it seems obvious to you, I might have missed it.</p>\n<p>Thank you oodles &amp; kaboodles in advance,</p>\n<p>Mass_Driver</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uBZwWKYsmuy5tWewf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.49303560850265e-07, "legacy": true, "legacyId": "4099", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rRmisKb45dN7DK4BW", "5o4EZJyqmHY4XgRCY", "fpecAJLG9czABgCe9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T12:44:45.663Z", "modifiedAt": null, "url": null, "title": "Good news about the Big Bang", "slug": "good-news-about-the-big-bang", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SbkXTZBivh9v976H6/good-news-about-the-big-bang", "pageUrlRelative": "/posts/SbkXTZBivh9v976H6/good-news-about-the-big-bang", "linkUrl": "https://www.lesswrong.com/posts/SbkXTZBivh9v976H6/good-news-about-the-big-bang", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20news%20about%20the%20Big%20Bang&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20news%20about%20the%20Big%20Bang%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSbkXTZBivh9v976H6%2Fgood-news-about-the-big-bang%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20news%20about%20the%20Big%20Bang%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSbkXTZBivh9v976H6%2Fgood-news-about-the-big-bang", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSbkXTZBivh9v976H6%2Fgood-news-about-the-big-bang", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>(Disclaimer: very poor knowledge of physics here, just interpreting the article)</p>\n<p><a href=\"http://www.physorg.com/print209708826.html\">http://www.physorg.com/print209708826.html</a></p>\n<p>- looks like there are many of them, as non-creationists would expect</p>\n<p>The really good news is</p>\n<p>&gt; In the past, Penrose has investigated cyclic cosmology models because he  has noticed another shortcoming of the much more widely accepted  inflationary theory: it cannot explain why there was such low entropy at  the beginning of the universe. The low entropy state (or high degree of  order) was essential for making complex matter possible.</p>\n<p>Which I interpret to mean information passes through the Big Crunch/Big Bang cycle. No heat death, information passes through - good news for transhumanists?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SbkXTZBivh9v976H6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 1, "extendedScore": null, "score": 6.493507096354836e-07, "legacy": true, "legacyId": "4100", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T13:02:13.989Z", "modifiedAt": null, "url": null, "title": "Ideological bullies?", "slug": "ideological-bullies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:51.915Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fictiona", "createdAt": "2010-11-23T14:55:27.642Z", "isAdmin": false, "displayName": "fictiona"}, "userId": "pXq2T4wi7cEW67oSd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RMAK6eHCyn9cjjsjj/ideological-bullies", "pageUrlRelative": "/posts/RMAK6eHCyn9cjjsjj/ideological-bullies", "linkUrl": "https://www.lesswrong.com/posts/RMAK6eHCyn9cjjsjj/ideological-bullies", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ideological%20bullies%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdeological%20bullies%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMAK6eHCyn9cjjsjj%2Fideological-bullies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ideological%20bullies%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMAK6eHCyn9cjjsjj%2Fideological-bullies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMAK6eHCyn9cjjsjj%2Fideological-bullies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 343, "htmlBody": "<p>Science has long questioned theism by examining design flaws in the nature. However, it seems to me that some science followers don't like being questioned about the scientific methodology. Just like theists don't like being questioned about existence of God.</p>\n<p>Of course, science lovers pride themselves on scientific enquiries and methods. But does such pride translate to ego and honour - something that should not be challenged?&nbsp;</p>\n<p>I have met several scientists and science Ph.D. students. Some - not all -&nbsp;&nbsp;of them&nbsp;seem to treat <em>fallibists </em>as some kind of nonsense and <em>anti-positivists</em> as a science equivalent of Satan. Although they argue politely, this saddens me. They are not as open-minded as philosophers I know. Philosophers tend to be happy when receiving challenges about their fundamental beliefs. Theists and scientists seem to be otherwise. This is just from my experience though. It is not meant to be generalisable.</p>\n<p>Science is not rationalism. It is an attempt to translate empirical results into knowledge with logic and statistics. So, science stands on two foundations - empiricism and rationalism. If one believes in science, they are likely to believe in both&nbsp;empiricism and rationalism. But because empiricism cannot be conclusive, neither can science. Why do scientific 'facts' are sometimes spoken as if they were certain? Why do some scientists fail to notice that those 'facts' are subject to <em>falsification </em>when&nbsp;new evidence is introduced?</p>\n<p>Right, I may have done some silly stuffs like <a href=\"http://en.wikipedia.org/wiki/Grounded_theory\">grounded theory</a>, which isn't compatible with the scientific methods. But I acknowledge that I can be flawed, and my methods are <em>certainly </em>flawed. Some - not all - natural scientists, on the other hand, seem to have so much faith in scientific methods, which I believe is somewhat flawed as well.&nbsp;As a social scientist, I am sometimes intellectually group-raped by scientists as though I offend them by questioning the scientific methods. * sighs *</p>\n<p>If you are a scientist or a science follower, may I ask, what do you think about social scientists? Sometimes I feel like scientists look down on social scientists, and I don't feel comfortable working with them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RMAK6eHCyn9cjjsjj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -10, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "4101", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T16:42:12.751Z", "modifiedAt": null, "url": null, "title": "Eating et al.: Study on High/Low Protein | Glycemic-Index", "slug": "eating-et-al-study-on-high-low-protein-or-glycemic-index", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:37.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "miS4XR3NQGgxkzKu9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/in45jgNyRgtQNCDmQ/eating-et-al-study-on-high-low-protein-or-glycemic-index", "pageUrlRelative": "/posts/in45jgNyRgtQNCDmQ/eating-et-al-study-on-high-low-protein-or-glycemic-index", "linkUrl": "https://www.lesswrong.com/posts/in45jgNyRgtQNCDmQ/eating-et-al-study-on-high-low-protein-or-glycemic-index", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eating%20et%20al.%3A%20Study%20on%20High%2FLow%20Protein%20%7C%20Glycemic-Index&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEating%20et%20al.%3A%20Study%20on%20High%2FLow%20Protein%20%7C%20Glycemic-Index%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fin45jgNyRgtQNCDmQ%2Feating-et-al-study-on-high-low-protein-or-glycemic-index%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eating%20et%20al.%3A%20Study%20on%20High%2FLow%20Protein%20%7C%20Glycemic-Index%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fin45jgNyRgtQNCDmQ%2Feating-et-al-study-on-high-low-protein-or-glycemic-index", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fin45jgNyRgtQNCDmQ%2Feating-et-al-study-on-high-low-protein-or-glycemic-index", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<p>Nutrition and related topics have been a topic a few times here and on related blogs, starting back with <a href=\"/lw/a6/the_unfinished_mystery_of_the_shangrila_diet/\">Shangri-La</a>, <a href=\"/lw/af/missed_distinctions/\">hypoglycemia</a>, and <a href=\"/lw/2bd/what_should_i_eat_a_case_study_in_rational/\">what</a> <a href=\"/lw/2qa/rational_health_optimization/\">we</a> <a href=\"/lw/31p/twinkie_diet_helps_nutrition_professor_lose_27/\">should</a> <a href=\"http://www.sentientdevelopments.com/2010/08/optimize-your-health-with-zone-and.html\">eat</a>.</p>\n<p>Now, HT reddit, there <a href=\"http://www.physorg.com/news/2010-11-danish-obesity-riddle.html\">seems to be some (new) evidence</a> pro the position I think that quite some people here have: high-protein, low-glycemic-index. So, some people here can be a little bit more sure that they made the right bet earlier on -- but how have you actually arrived at those conclusions earlier? I see the evolutionary argument, but by itself alone, it is not that convincing. There must have been data, ...</p>\n<p>So, any recommendations on further sources/high-quality collections?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "in45jgNyRgtQNCDmQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "4102", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BD4oExxQguTgpESdm", "Afvk6GGfoo8mea5cb", "DxHmpiGS5quixEyjz", "BTXdajWzoN2YRbgjG", "Ys8oHYgdGmma5KuJT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-25T20:46:56.517Z", "modifiedAt": null, "url": null, "title": "Transhumanism thread in progress at Reddit", "slug": "transhumanism-thread-in-progress-at-reddit", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:39.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Document", "createdAt": "2010-02-08T04:14:47.949Z", "isAdmin": false, "displayName": "Document"}, "userId": "vaMNHjzaCGqF8yTMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wifMjDqjsNpGSWFZK/transhumanism-thread-in-progress-at-reddit", "pageUrlRelative": "/posts/wifMjDqjsNpGSWFZK/transhumanism-thread-in-progress-at-reddit", "linkUrl": "https://www.lesswrong.com/posts/wifMjDqjsNpGSWFZK/transhumanism-thread-in-progress-at-reddit", "postedAtFormatted": "Thursday, November 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transhumanism%20thread%20in%20progress%20at%20Reddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATranshumanism%20thread%20in%20progress%20at%20Reddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwifMjDqjsNpGSWFZK%2Ftranshumanism-thread-in-progress-at-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transhumanism%20thread%20in%20progress%20at%20Reddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwifMjDqjsNpGSWFZK%2Ftranshumanism-thread-in-progress-at-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwifMjDqjsNpGSWFZK%2Ftranshumanism-thread-in-progress-at-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>Starting with <a href=\"http://www.reddit.com/r/pics/comments/ebgqx/you_were_born_too_soon/c16u58p\">this reply</a>&nbsp;to \"You were born too soon\":</p>\n<p>&gt; depending on when exactly we achieve this, this could be the best time to be born ever, because it will be the absolute earliest anybody will have achieved immortality. Someone born within 20 years of this moment could one day be the oldest human, sentient, or even living being in the Universe.</p>\n<p>The comments are currently split between arguing and agreeing with this. So far, no mention of cryonics. One post presents a possibly interesting technical argument that <a href=\"http://www.reddit.com/r/pics/comments/ebgqx/you_were_born_too_soon/c16uzcf?context=3\">our current knowledge/technology is centuries away</a>&nbsp;from mind uploading/whole-brain emulation.</p>\n<p>(Also posted to <a href=\"/lw/2s9/the_singularity_in_the_zeitgeist\">The Singularity in the Zeitgeist</a>, but that thread seems to have been mostly forgotten.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wifMjDqjsNpGSWFZK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 6.494687199317689e-07, "legacy": true, "legacyId": "4103", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5tBNPapegcMnXWiA4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-26T04:48:46.744Z", "modifiedAt": null, "url": null, "title": "Buy Insurance -- Bet Against Yourself", "slug": "buy-insurance-bet-against-yourself", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:09.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JDKfPsHvBwgq4Knn9/buy-insurance-bet-against-yourself", "pageUrlRelative": "/posts/JDKfPsHvBwgq4Knn9/buy-insurance-bet-against-yourself", "linkUrl": "https://www.lesswrong.com/posts/JDKfPsHvBwgq4Knn9/buy-insurance-bet-against-yourself", "postedAtFormatted": "Friday, November 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Buy%20Insurance%20--%20Bet%20Against%20Yourself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuy%20Insurance%20--%20Bet%20Against%20Yourself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJDKfPsHvBwgq4Knn9%2Fbuy-insurance-bet-against-yourself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Buy%20Insurance%20--%20Bet%20Against%20Yourself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJDKfPsHvBwgq4Knn9%2Fbuy-insurance-bet-against-yourself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJDKfPsHvBwgq4Knn9%2Fbuy-insurance-bet-against-yourself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 601, "htmlBody": "<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">My friend and housemate, User:Kevin, makes a very pleasant living selling opioids on the&nbsp;internet, a living he expects to continue for some time, unless something awful&nbsp;happens like Obama losing the next election. The Intrade contract for Obama's&nbsp;loss is currently trading at 42% -- what can User:Kevin do about this?</div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">My suggestion was that he bet heavily on Obama's loss. Say he spends $4200 buying not-Obama futures. If Obama wins, that money becomes worthless, but he gets four years selling kratom regulation-free. On the other hand, say Palin&nbsp;takes a surprise victory and institutes draconian regulation on various&nbsp;substances -- User:Kevin's $4,200 has just become $10,000, leaving a $5800&nbsp;windfall to help him while he finds his next muse.</div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">This is nothing more than what we normally call buying insurance, just extended&nbsp;to whatever outcomes you may want to insure against. Let's talk about some of the&nbsp;effects of this action.<a id=\"more\"></a></div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\"><strong>Leaving a Line of Retreat</strong></div>\n<div><span style=\"font-family: arial, sans-serif;\"><span style=\"border-collapse: collapse;\">Let's say that User:Kevin now finds Obama's loss more or less unthinkable (I&nbsp;don't mean to impugn his rationality -- the article's more or less fictional&nbsp;from here on). Well, that's not really very good -- he&nbsp;<em> needs &nbsp;</em>to think about&nbsp;it. I suppose the proper answer is something like \"<a title=\"The Meditation on Curiosity\" href=\"/lw/jz/the_meditation_on_curiosity/\">If Obama will win the election, I want to believe..</a>.\"&nbsp;-- but it may&nbsp;<em> also &nbsp;</em>help to think that, yes, a Republican win would suck, but it would also come with&nbsp;this huge financial windfall up front. That is, by flattening your outcome curve&nbsp;a bit, you can &nbsp;<a title=\"Leave a Line of Retreat\" href=\"/lw/o4/leave_a_line_of_retreat/\">reduce your attachment to individual outcomes</a>, and help yourself&nbsp;to make more rational&nbsp;judgments.</span></span></div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">Of course, if the market gives Obama 60%, User:Kevin should probably just believe&nbsp;that. That's what markets are for. Which brings us to:</div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\"><strong>Helping the Market</strong></div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">You might think that User:Kevin's (hypothetical) actions are antisocial. That by&nbsp;buying not-Obama futures, he's sending a false signal to the market decreasing&nbsp;Obama's chances. If so, &nbsp;<a title=\"A Manipulator Can Aid Prediction Market Accuracy\" href=\"http://hanson.gmu.edu/biashelp.pdf\">Robin Hanson might like to have a word with you</a>. Here's why:</div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">Let's say that Clippy is a rational speculator in this market. He doesn't care&nbsp;whether Obama wins or not, he just wants to maximize his monetary outcome so&nbsp;that he can purchase materials with which to create paperclips. Let's further&nbsp;say that the market is filled only with rational speculators like Clippy.</div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">Well, based on the family of no-bet theorems, they will all expect that the&nbsp;best-informed actors in the market will make their money from the least-informed&nbsp;actors. Half of them will conclude that they are the least informed, and choose&nbsp;not to play. Eventually those speculators best equipped to find good information&nbsp;and make good speculations will be alone in the market, with no one to bet with,&nbsp;and no incentive to produce good information. Market volume drops, put/call&nbsp;spreads widen, everyone goes home, no one gets informed.</div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">Now let's bring lots of noise to the market: irrational actors convinced they&nbsp;know better than the going rates because their star sign told them so; agents of&nbsp;one political party or another intentionally manipulating the market to energize&nbsp;their base; rational insurance-buyers who (may) have good probabilities, but&nbsp;have skewed valuations on money and are willing to accept above-or-below optimal&nbsp;prices. Now there is money in the market. Now there are buy and sell orders from&nbsp;all sides. Now it&nbsp;<em> makes sense &nbsp;</em>for Clippy to spend&nbsp;<em> lots &nbsp;</em>of computing cycles&nbsp;running regression tests on past electoral outcomes, buying and selling whenever&nbsp;the advantage is to him, making lots of money, and &nbsp;<em>pricing the market&nbsp;accurately</em>. Volume shoots up. Spread goes down. Prices move near-instantly with good information. Everyone is informed.</div>\n<div style=\"border-collapse: collapse; font-family: arial, sans-serif; font-size: 13px;\">For all these reasons, gentle readers, I urge you to wire some money into an&nbsp;offshore account, log into intrade, find some outcome that would make you&nbsp;miserable, and bet heavily on it.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E8PHMuf7tsr8teXAe": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JDKfPsHvBwgq4Knn9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 42, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "4104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3nZMgRTfFEfHp34Gb", "3XgYbghWruBMrPTAL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-27T07:54:50.398Z", "modifiedAt": null, "url": null, "title": "I didn't want to have to do this but...", "slug": "i-didn-t-want-to-have-to-do-this-but", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:37.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f86kimGmPkAppXmBi/i-didn-t-want-to-have-to-do-this-but", "pageUrlRelative": "/posts/f86kimGmPkAppXmBi/i-didn-t-want-to-have-to-do-this-but", "linkUrl": "https://www.lesswrong.com/posts/f86kimGmPkAppXmBi/i-didn-t-want-to-have-to-do-this-but", "postedAtFormatted": "Saturday, November 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20didn't%20want%20to%20have%20to%20do%20this%20but...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20didn't%20want%20to%20have%20to%20do%20this%20but...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff86kimGmPkAppXmBi%2Fi-didn-t-want-to-have-to-do-this-but%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20didn't%20want%20to%20have%20to%20do%20this%20but...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff86kimGmPkAppXmBi%2Fi-didn-t-want-to-have-to-do-this-but", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff86kimGmPkAppXmBi%2Fi-didn-t-want-to-have-to-do-this-but", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>I don't want to become known on this website&nbsp;as the guy who is always asking for help with his personal problems (way too much status loss), but I'm still a novice at best as a rationalist and given others don't have my biases asking for advice is the best chance I've got at an objective solution.</p>\r\n<p>I've recently bought a game (with a few days left in which I can return it for a refund) called Dragon Ball Z: Burst Limit. Most of my play experience suggests Rubberband AI, but there aren't any references to it having such&nbsp;on the Internet and programmers have been known to conceal it. I really don't want the game to have it (as it provides a highly inconsistent challenge, and reduces the game to focusing on manipulating the A.I rather than winning), hence my bias, but my own play seems to suggest it does.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f86kimGmPkAppXmBi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -21, "extendedScore": null, "score": -3.9e-05, "legacy": true, "legacyId": "4106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-27T08:25:52.446Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 6", "slug": "harry-potter-and-the-methods-of-rationality-discussion-22", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.943Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y2Hszb4Dsm5FggnDC/harry-potter-and-the-methods-of-rationality-discussion-22", "pageUrlRelative": "/posts/y2Hszb4Dsm5FggnDC/harry-potter-and-the-methods-of-rationality-discussion-22", "linkUrl": "https://www.lesswrong.com/posts/y2Hszb4Dsm5FggnDC/harry-potter-and-the-methods-of-rationality-discussion-22", "postedAtFormatted": "Saturday, November 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%206&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%206%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2Hszb4Dsm5FggnDC%2Fharry-potter-and-the-methods-of-rationality-discussion-22%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%206%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2Hszb4Dsm5FggnDC%2Fharry-potter-and-the-methods-of-rationality-discussion-22", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2Hszb4Dsm5FggnDC%2Fharry-potter-and-the-methods-of-rationality-discussion-22", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p><strong>Update: Discussion has moved on to a <a href=\"/r/discussion/tag/harry_potter/\">new thread</a>.</strong></p>\n<p>After 61 chapters of <a href=\"http://www.fanfiction.net/s/5782108/1/\"><em>Harry Potter and the Methods of Rationality</em></a> and 5 discussion threads with over 500 comments each, HPMOR discussion has graduated from the main page and moved into the Less Wrong discussion section (which <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality/2w2z?c=1\">seems</a> <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality/303m?c=1\">like</a> a more appropriate location).&nbsp; You can post all of your insights, speculation, and, well, <em>discussion </em>about Eliezer Yudkowsky's Harry Potter fanfic here.</p>\n<p>Previous threads are available under the <a href=\"/tag/harry_potter/\">harry_potter tag</a> on the main page (or: <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality/\">one</a>, <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">four</a>, <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">five</a>); this and future threads will be found under the <a href=\"/r/discussion/tag/harry_potter/\">discussion section tag</a> (since there is a separate tag system for the discussion section).&nbsp; See also the <a href=\"http://www.fanfiction.net/u/2269863/Less_Wrong\">author page</a> for (almost) all things HPMOR, and AdeleneDawner's <a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">Author's Notes archive</a> for one thing that the author page is missing.<br /><br />As a reminder, it's useful to indicate at the start of your comment which chapter you are commenting on.&nbsp; Time passes but your comment stays the same.<br /><br /><strong>Spoiler Warning</strong>:&nbsp; this thread is full of spoilers.&nbsp; With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp; <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote>\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).<br /><br />If there is evidence for X in MOR and/or canon then it's fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that \"Eliezer said X is true\" unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y2Hszb4Dsm5FggnDC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 6.499924200852771e-07, "legacy": true, "legacyId": "4108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 549, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-27T16:28:24.651Z", "modifiedAt": null, "url": null, "title": "Link: Writing exercise closes the gender gap in university-level physics", "slug": "link-writing-exercise-closes-the-gender-gap-in-university", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:06.108Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Golovin", "createdAt": "2009-02-28T13:32:31.085Z", "isAdmin": false, "displayName": "Vladimir_Golovin"}, "userId": "Me2m84AhCn9H49riY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WYH9xdovnM6tigCsp/link-writing-exercise-closes-the-gender-gap-in-university", "pageUrlRelative": "/posts/WYH9xdovnM6tigCsp/link-writing-exercise-closes-the-gender-gap-in-university", "linkUrl": "https://www.lesswrong.com/posts/WYH9xdovnM6tigCsp/link-writing-exercise-closes-the-gender-gap-in-university", "postedAtFormatted": "Saturday, November 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Writing%20exercise%20closes%20the%20gender%20gap%20in%20university-level%20physics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Writing%20exercise%20closes%20the%20gender%20gap%20in%20university-level%20physics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWYH9xdovnM6tigCsp%2Flink-writing-exercise-closes-the-gender-gap-in-university%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Writing%20exercise%20closes%20the%20gender%20gap%20in%20university-level%20physics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWYH9xdovnM6tigCsp%2Flink-writing-exercise-closes-the-gender-gap-in-university", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWYH9xdovnM6tigCsp%2Flink-writing-exercise-closes-the-gender-gap-in-university", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p><a title=\"Permanent Link: 15-minute writing exercise closes the gender gap in university-level physics\" rel=\"bookmark\" href=\"http://blogs.discovermagazine.com/notrocketscience/2010/11/25/15-minute-writing-exercise-closes-the-gender-gap-in-university-level-physics/\">15-minute writing exercise closes the gender gap in university-level physics:</a></p>\n<blockquote>\n<p><em>Think about the things that are important to you. Perhaps you care about creativity, family relationships, your career, or having a sense of humour. Pick two or three of these values and write a few sentences about <em>why </em>they are important to you. You have fifteen minutes. It could change your life.</em></p>\n<p><em>This simple writing exercise may not seem like anything ground-breaking, but its effects speak for themselves. In a university physics class, <a onclick=\"javascript:urchinTracker('/outbound/psych.colorado.edu/_7Emiyake/?ref=http_//www.facebook.com/l.php?u=http_3A_2F_2Fblogs.discovermagazine.com_2Fnotrocketscience_2F2010_2F11_2F25_2F15-minute-writing-exercise-closes-the-gender-gap-in-university-level-physics_2F_h=05306');\" href=\"http://psych.colorado.edu/%7Emiyake/\">Akira Miyake</a> from the University of Colorado used it to close the gap between male and female performance. In the university&rsquo;s physics course, men typically do better than women but Miyake&rsquo;s study shows that this has nothing to do with innate ability. With nothing but his fifteen-minute exercise, performed twice at the beginning of the year, he virtually abolished the gender divide and allowed the female physicists to challenge their male peers.</em></p>\n<p><em>The exercise is designed to affirm a person&rsquo;s values, boosting their sense of self-worth and integrity, and reinforcing their belief in themselves. For people who suffer from negative stereotypes, this can make all the difference between success and failure.</em></p>\n</blockquote>\n<p>The article cites a paper, but it's behind a paywall:<br /><a href=\"http://www.sciencemag.org/content/330/6008/1234\">http://www.sciencemag.org/content/330/6008/1234</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 1, "fH8jPjHF2R27sRTTG": 1, "ksdiAMKfgSyEeKMo6": 1, "dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WYH9xdovnM6tigCsp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 27, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "4109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-27T21:44:06.794Z", "modifiedAt": null, "url": null, "title": "The Sin of Persuasion", "slug": "the-sin-of-persuasion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:54.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WmmL2S5pbdLAa2GR9/the-sin-of-persuasion", "pageUrlRelative": "/posts/WmmL2S5pbdLAa2GR9/the-sin-of-persuasion", "linkUrl": "https://www.lesswrong.com/posts/WmmL2S5pbdLAa2GR9/the-sin-of-persuasion", "postedAtFormatted": "Saturday, November 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Sin%20of%20Persuasion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Sin%20of%20Persuasion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmmL2S5pbdLAa2GR9%2Fthe-sin-of-persuasion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Sin%20of%20Persuasion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmmL2S5pbdLAa2GR9%2Fthe-sin-of-persuasion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmmL2S5pbdLAa2GR9%2Fthe-sin-of-persuasion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 596, "htmlBody": "<!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\"; mso-ansi-language:#0400; mso-fareast-language:#0400; mso-bidi-language:#0400;} -->\n<p>&nbsp;</p>\n<!--[endif]-->\n<p class=\"MsoNormal\">Related to <a href=\"/lw/hn/your_rationality_is_my_business/\" target=\"_self\">Your Rationality is My Business</a></p>\n<p class=\"MsoNormal\">Among religious believers in the developed world, there is something of a hierarchy in terms of social tolerability. Near the top are the liberal, nonjudgmental, frequently nondenominational believers, of whom it is highly unpopular to express disapproval. At the bottom you find people who picket funerals or bomb abortion clinics, the sort with whom even most vocally devout individuals are quick to deny association.</p>\n<p class=\"MsoNormal\">Slightly above these, but still very close to the bottom of the heap, are proselytizers and door to door evangelists. They may not be hateful about their beliefs, indeed many find that their local Jehovah&rsquo;s Witnesses are <em>exceptionally</em> nice people, but they&rsquo;re simply so annoying. How can they go around pressing their beliefs on others and judging people that way?</p>\n<p class=\"MsoNormal\">I have never known another person to criticize evangelists for not trying hard <em>enough</em> to change others&rsquo; beliefs.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<p class=\"MsoNormal\">And yet, when you think about it, these people are dealing with beliefs of tremendous scale. If the <a href=\"/lw/n3/circular_altruism/\" target=\"_self\">importance of saving a single human life</a> is worth so much more than our petty discomforts with defying social convention or our own cognitive biases, how much greater must be the weight of saving an immortal soul from an eternity of hell? Shouldn&rsquo;t they be doing everything in their power to change the minds of others, if that&rsquo;s what it takes to save them? Surely if there is a fault in their actions, it&rsquo;s that they&rsquo;re doing too <em>little</em> given the weight their beliefs should impose on them.</p>\n<p class=\"MsoNormal\">But even if you believe you believe this is a matter of eternity, of unimaginable degrees of utility, if you haven&rsquo;t internalized that belief, then it sure is annoying to be pestered about the state of your immortal soul.</p>\n<p class=\"MsoNormal\">This is by no means exclusive to religion. Proselytizing vegans, for instance, occupy a similar position on the scale of socially acceptable dietary positions. You might believe that nonhuman animals possess significant moral worth, and by raising them in oppressive conditions only to slaughter them en masse, humans are committing an enormous moral atrocity, but may heaven forgive you if you try to convince other people of this so that they can do their part in reversing the situation. Far more common are vegans who are adamantly non-condemnatory. They may abstain from using any sort of animal products on strictly moral grounds, but, they will defensively assert, they&rsquo;re not going to criticize anyone else for doing otherwise. Individuals like this are an object example that the disapproval of evangelism does not simply come down to distaste for the principles being preached. <span>&nbsp;&nbsp;</span></p>\n<p class=\"MsoNormal\">So why the taboo on trying to change others&rsquo; beliefs? Well, as a human universal, it&rsquo;s hard to change our minds. Having our beliefs confronted tends to make us anxious. It might feel nice to see someone strike a blow against the hated enemy, but it&rsquo;s safer and more comfortable to not have a war waged on your doorstep. And so, probably out of a shared desire not to have our own beliefs confronted, we&rsquo;ve developed a set of social norms where individuals have an expectation of being entitled to their own distinct factual beliefs about the universe. <span>&nbsp;</span></p>\n<p class=\"MsoNormal\">Of course, the very name of this blog derives from the conviction that that sort of thinking is not correct. But it&rsquo;s worth wondering, when we consider a society which upholds a free market of ideas which compete on their relative strength, whether we&rsquo;ve taken adequate precautions against the sheer <em>annoyingness</em> of a society where the taboo on actually trying to convince others of one&rsquo;s beliefs has been lifted.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "wzgcQCrwKfETcBpR9": 1, "NSMKfa8emSbGNXRKD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WmmL2S5pbdLAa2GR9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 37, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "4111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["anCubLdggTWjnEvBS", "4ZzefKQwAtMo5yp99"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-27T22:07:52.104Z", "modifiedAt": null, "url": null, "title": "META: Misleading error message on using wrong username", "slug": "meta-misleading-error-message-on-using-wrong-username", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:38.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PNqD5mm8LEPiYL2fc/meta-misleading-error-message-on-using-wrong-username", "pageUrlRelative": "/posts/PNqD5mm8LEPiYL2fc/meta-misleading-error-message-on-using-wrong-username", "linkUrl": "https://www.lesswrong.com/posts/PNqD5mm8LEPiYL2fc/meta-misleading-error-message-on-using-wrong-username", "postedAtFormatted": "Saturday, November 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20META%3A%20Misleading%20error%20message%20on%20using%20wrong%20username&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMETA%3A%20Misleading%20error%20message%20on%20using%20wrong%20username%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPNqD5mm8LEPiYL2fc%2Fmeta-misleading-error-message-on-using-wrong-username%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=META%3A%20Misleading%20error%20message%20on%20using%20wrong%20username%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPNqD5mm8LEPiYL2fc%2Fmeta-misleading-error-message-on-using-wrong-username", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPNqD5mm8LEPiYL2fc%2Fmeta-misleading-error-message-on-using-wrong-username", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p>I attempted to log in from a computer I don't usually use, and entered my username as \"Rolf Andreassen\", two words; in fact it's \"RolfAndreassen\", one word. The error message I got back was \"Incorrect password\", which is misleading. Not until I tried to recover my password did I realise my mistake. Clearly this is an unusual edge case, but I suggest updating the code to give back \"No such user\" when someone makes this mistake.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PNqD5mm8LEPiYL2fc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 6.501942180135703e-07, "legacy": true, "legacyId": "4112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-28T03:21:59.940Z", "modifiedAt": null, "url": null, "title": "Link: Compare your moral values to the general population", "slug": "link-compare-your-moral-values-to-the-general-population", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lunchbox", "createdAt": "2009-12-25T22:45:29.576Z", "isAdmin": false, "displayName": "lunchbox"}, "userId": "8JpWAz7htRGgQCpEr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yEwsuCTHGwwbvFPaD/link-compare-your-moral-values-to-the-general-population", "pageUrlRelative": "/posts/yEwsuCTHGwwbvFPaD/link-compare-your-moral-values-to-the-general-population", "linkUrl": "https://www.lesswrong.com/posts/yEwsuCTHGwwbvFPaD/link-compare-your-moral-values-to-the-general-population", "postedAtFormatted": "Sunday, November 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Compare%20your%20moral%20values%20to%20the%20general%20population&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Compare%20your%20moral%20values%20to%20the%20general%20population%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyEwsuCTHGwwbvFPaD%2Flink-compare-your-moral-values-to-the-general-population%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Compare%20your%20moral%20values%20to%20the%20general%20population%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyEwsuCTHGwwbvFPaD%2Flink-compare-your-moral-values-to-the-general-population", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyEwsuCTHGwwbvFPaD%2Flink-compare-your-moral-values-to-the-general-population", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Jonathan Haidt, a professor at UVA, runs an <a href=\"http://faculty.virginia.edu/haidtlab/mft/index.php\">online lab</a>&nbsp;with&nbsp;<a href=\"http://www.yourmorals.org/explore.php\">quizzes</a>&nbsp;that will compare your moral values to the rest of the population. I have found the test results useful for avoiding the <a href=\"/lw/dr/generalizing_from_one_example/\">typical mind fallacy</a>. When someone disagrees with me on a belief/opinion I feel certain about, it's often difficult to tease apart how much of this disagreement stems from them not \"getting it\", and how much stems from them having a different fundamental value system. One of the tests alerted me that I am an outlier in certain aspects of how I judge morality (green = me; blue = liberals; red = conservatives):</p>\n<p><img src=\"http://i.imgur.com/m4SGm.png\" alt=\"\" width=\"700\" height=\"300\" /></p>\n<p>Another benefit of these quizzes is that they can point out potential blind spots. For example, one quiz asks for opinions about punishment for crimes.&nbsp;If I discover I'm an outlier w.r.t. the population, I should reconsider whether my opinions are based on solid evidence (or did I see one study that found tit-for-tat punishment effective in a certain context, and take that as gospel?).</p>\n<p>Extra reading: Haidt wrote a <a href=\"http://online.wsj.com/article/SB10001424052748703673604575550243700895762.html\">WSJ article</a> last month that applied the learnings of these moral quizzes to better understanding the Tea Party.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yEwsuCTHGwwbvFPaD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "4113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-28T06:19:01.899Z", "modifiedAt": null, "url": null, "title": "We're all forgetting how to read analog clocks. Or are we?", "slug": "we-re-all-forgetting-how-to-read-analog-clocks-or-are-we", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.405Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Relsqui", "createdAt": "2010-09-14T04:44:29.083Z", "isAdmin": false, "displayName": "Relsqui"}, "userId": "itGQLX444X7ccy7vs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jhAWHZ3aZSPha46Wr/we-re-all-forgetting-how-to-read-analog-clocks-or-are-we", "pageUrlRelative": "/posts/jhAWHZ3aZSPha46Wr/we-re-all-forgetting-how-to-read-analog-clocks-or-are-we", "linkUrl": "https://www.lesswrong.com/posts/jhAWHZ3aZSPha46Wr/we-re-all-forgetting-how-to-read-analog-clocks-or-are-we", "postedAtFormatted": "Sunday, November 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We're%20all%20forgetting%20how%20to%20read%20analog%20clocks.%20Or%20are%20we%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe're%20all%20forgetting%20how%20to%20read%20analog%20clocks.%20Or%20are%20we%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjhAWHZ3aZSPha46Wr%2Fwe-re-all-forgetting-how-to-read-analog-clocks-or-are-we%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We're%20all%20forgetting%20how%20to%20read%20analog%20clocks.%20Or%20are%20we%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjhAWHZ3aZSPha46Wr%2Fwe-re-all-forgetting-how-to-read-analog-clocks-or-are-we", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjhAWHZ3aZSPha46Wr%2Fwe-re-all-forgetting-how-to-read-analog-clocks-or-are-we", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>I was thinking about this phenomenon today. Digital clocks are so common now that I don't often need to read an analog one, much less in a hurry. I worry that I'm losing the ability to do so. (The worry is a little bit because I might still need it at some point, and much more because being able to quickly read analog clocks makes me feel like a grown-up.) In particular, when I am called upon to read one, I'm embarrassed by how long it takes me to do so. It's only several seconds, but that's enough to make it clear to anyone watching that I had to stop and think about it.</p>\n<p>But then I caught myself, and thought, wait a moment. Am I actually much slower at this than I used to be? Or is reading an analog clock really just a noticeably slower action than reading a digital one? This is intuitively plausible; it has more mental steps. Rather than comparing my current analog-clock-reading speed with a previous one (which I don't really remember), I'm comparing it to my digital-clock-reading speed, which doesn't make sense. I was <em>going</em> to ask how you'd design an experiment to test this. Then I remembered that not everyone is young enough to have to speculate about what it's like not having mostly digital clocks around. :P So if you're old enough to have significantly more practice reading analog clocks than digital, how long does it take you to read one? Is it noticeably longer than reading a digital clock? If you aren't, and have a significantly different experience from mine, I'm interested in that too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jhAWHZ3aZSPha46Wr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 6.503147034618316e-07, "legacy": true, "legacyId": "4114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-28T11:51:50.475Z", "modifiedAt": null, "url": null, "title": "Superintelligent AI mentioned as a possible risk by Bill Gates", "slug": "superintelligent-ai-mentioned-as-a-possible-risk-by-bill", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:39.482Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SiReN2JHLwJJyPn5v/superintelligent-ai-mentioned-as-a-possible-risk-by-bill", "pageUrlRelative": "/posts/SiReN2JHLwJJyPn5v/superintelligent-ai-mentioned-as-a-possible-risk-by-bill", "linkUrl": "https://www.lesswrong.com/posts/SiReN2JHLwJJyPn5v/superintelligent-ai-mentioned-as-a-possible-risk-by-bill", "postedAtFormatted": "Sunday, November 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superintelligent%20AI%20mentioned%20as%20a%20possible%20risk%20by%20Bill%20Gates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperintelligent%20AI%20mentioned%20as%20a%20possible%20risk%20by%20Bill%20Gates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSiReN2JHLwJJyPn5v%2Fsuperintelligent-ai-mentioned-as-a-possible-risk-by-bill%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superintelligent%20AI%20mentioned%20as%20a%20possible%20risk%20by%20Bill%20Gates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSiReN2JHLwJJyPn5v%2Fsuperintelligent-ai-mentioned-as-a-possible-risk-by-bill", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSiReN2JHLwJJyPn5v%2Fsuperintelligent-ai-mentioned-as-a-possible-risk-by-bill", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><em>\"There are other potential problems in the future that Mr. Ridley could have addressed but did not. Some would put super-intelligent computers on that list. My own list would include large-scale bioterrorism or a pandemic ...&nbsp;</em></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><em>But bioterrorism and pandemics are the only threats I can foresee that could kill over a billion people.</em></span><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><em>\"</em></span></p>\n<p>- Bill Gates&nbsp;</p>\n<p>From</p>\n<p><a href=\"http://online.wsj.com/article/SB10001424052748704243904575630761699028330.html?mod=WSJEUROPE_newsreel_lifeStyle\">Africa Needs Aid, Not Flawed Theories</a></p>\n<p>One wonders where Bill Gates read that superintelligent AI could be (but in his estimation, in fact isn't) a GCR. It couldn't have been Kurzweil, because Kurzweil doesn't say that. The only realistic possibilities are that the influence came via Nick Bostrom, Stephen Hawking or Martin Rees or possibly Bill Joy<sup><strong>(See comments)</strong></sup>.&nbsp;</p>\n<p>It seems that Bill is also&nbsp;something&nbsp;of a Bayesian with respect to global catastrophic risk:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><em>\"Even though we can't compute the odds for threats like bioterrorism or a pandemic, it's important to have the right people worrying about them and taking steps to minimize their likelihood and potential impact. On these issues, I am not impressed right now with the work being done by the U.S. and other governments.\"</em></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SiReN2JHLwJJyPn5v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 6.503963662044307e-07, "legacy": true, "legacyId": "4115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-28T15:28:54.343Z", "modifiedAt": null, "url": null, "title": "[LINK] The Top Ten Daily Consequences of Having Evolved", "slug": "link-the-top-ten-daily-consequences-of-having-evolved", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:38.151Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2bezrW35QqWDQzGfD/link-the-top-ten-daily-consequences-of-having-evolved", "pageUrlRelative": "/posts/2bezrW35QqWDQzGfD/link-the-top-ten-daily-consequences-of-having-evolved", "linkUrl": "https://www.lesswrong.com/posts/2bezrW35QqWDQzGfD/link-the-top-ten-daily-consequences-of-having-evolved", "postedAtFormatted": "Sunday, November 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20Top%20Ten%20Daily%20Consequences%20of%20Having%20Evolved&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20Top%20Ten%20Daily%20Consequences%20of%20Having%20Evolved%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2bezrW35QqWDQzGfD%2Flink-the-top-ten-daily-consequences-of-having-evolved%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20Top%20Ten%20Daily%20Consequences%20of%20Having%20Evolved%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2bezrW35QqWDQzGfD%2Flink-the-top-ten-daily-consequences-of-having-evolved", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2bezrW35QqWDQzGfD%2Flink-the-top-ten-daily-consequences-of-having-evolved", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<blockquote>\n<p>From hiccups to wisdom teeth, the evolution of homo sapiens has left behind some glaring, yet innately human, imperfections</p>\n</blockquote>\n<p><a href=\"http://www.smithsonianmag.com/science-nature/The-Top-Ten-Daily-Consequences-of-Having-Evolved.html#ixzz16ai6M3eS\">Link</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2bezrW35QqWDQzGfD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-28T16:49:14.250Z", "modifiedAt": null, "url": null, "title": "Singularity Non-Fiction Compilation to be Written", "slug": "singularity-non-fiction-compilation-to-be-written", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:46.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9QNKGQjFwLB8GxHMS/singularity-non-fiction-compilation-to-be-written", "pageUrlRelative": "/posts/9QNKGQjFwLB8GxHMS/singularity-non-fiction-compilation-to-be-written", "linkUrl": "https://www.lesswrong.com/posts/9QNKGQjFwLB8GxHMS/singularity-non-fiction-compilation-to-be-written", "postedAtFormatted": "Sunday, November 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Non-Fiction%20Compilation%20to%20be%20Written&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Non-Fiction%20Compilation%20to%20be%20Written%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9QNKGQjFwLB8GxHMS%2Fsingularity-non-fiction-compilation-to-be-written%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Non-Fiction%20Compilation%20to%20be%20Written%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9QNKGQjFwLB8GxHMS%2Fsingularity-non-fiction-compilation-to-be-written", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9QNKGQjFwLB8GxHMS%2Fsingularity-non-fiction-compilation-to-be-written", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 452, "htmlBody": "<p>Call for Essays:&lt;<a href=\"http://singularityhypothesis.blogspot.com/p/submit.html\" target=\"_blank\">http://singularityhypothesis.blogspot.com/p/submit.html</a>&gt;<br /> The Singularity Hypothesis<br /> A Scientific and Philosophical Assessment<br /> Edited volume, to appear in The Frontiers Collection&lt;<a href=\"http://www.springer.com/series/5342\" target=\"_blank\">http://www.springer.com/series/5342</a>&gt;, Springer<br /> <br /> Does an intelligence explosion pose a genuine existential risk, or did  Alan Turing, Steven Hawking, and Alvin Toffler delude themselves with  visions 'straight from Cloud Cuckooland'? Should the notions of  superintelligent machines, brain emulations and transhumans be  ridiculed, or is it that skeptics are the ones who suffer from short  sightedness and 'carbon chauvinism'? These questions have remained open  because much of what we hear about the singularity originates from  popular depictions, fiction, artistic impressions, and apocalyptic  propaganda.<br /> <br /> Seeking to promote this debate, this edited, peer-reviewed volume shall  be concerned with scientific and philosophical analysis of the  conjectures related to a technological singularity. We solicit scholarly  essays offering a scientific and philosophical analysis of this  hypothesis, assess its empirical content, examine relevant evidence, or  explore its implications. &nbsp;Commentary offering a critical assessment of  selected essays may also be solicited.<br /> <br /> Important dates:<br /> <br /> &nbsp;* &nbsp; Extended abstracts (500&ndash;1,000 words): 15 January 2011<br /> &nbsp;* &nbsp; Full essays: (around 7,000 words): 30 September 2011<br /> &nbsp;* &nbsp; Notifications: 30 February 2012 (tentative)<br /> &nbsp;* &nbsp; Proofs: 30 April 2012 (tentative)<br /> We aim to get this volume published by the end of 2012.<br /> <br /> Purpose of this volume<br /> &middot; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Please read: Purpose of This Volume&lt;<a href=\"http://singularityhypothesis.blogspot.com/p/theme.html\" target=\"_blank\">http://singularityhypothesis.blogspot.com/p/theme.html</a>&gt;<br /> &nbsp;Central questions<br /> &middot; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Please read: Central Questions&lt;<a href=\"http://singularityhypothesis.blogspot.com/p/central-questions.html\" target=\"_blank\">http://singularityhypothesis.blogspot.com/p/central-questions.html</a>&gt;:<br /> Extended abstracts are ideally short (3 pages, 500 to 1000 words),  focused (!), relating directly to specific central questions&lt;<a href=\"http://singularityhypothesis.blogspot.com/p/central-questions.html\" target=\"_blank\">http://singularityhypothesis.blogspot.com/p/central-questions.html</a>&gt; and indicating how they will be treated in the full essay.<br /> <br /> Full essays are expected to be short (15 pages, around 7000 words) and  focused, relating directly to specific central questions&lt;<a href=\"http://singularityhypothesis.blogspot.com/p/central-questions.html\" target=\"_blank\">http://singularityhypothesis.blogspot.com/p/central-questions.html</a>&gt;.  Essays longer than 15 pages long will be proportionally more difficult  to fit into the volume. Essays that are three times this size or more  are unlikely to fit. &nbsp;Essays should address the scientifically-literate  non-specialist and written in a language that is divorced from  speculative and irrational line of argumentation. &nbsp;In addition, some  authors may be asked to make their submission available for commentary  (see below).<br /> <br /> (More details&lt;<a href=\"http://singularityhypothesis.blogspot.com/p/submit.html\" target=\"_blank\">http://singularityhypothesis.blogspot.com/p/submit.html</a>&gt;)<br /> <br /> Thank you for reading this call. Please forward it to individual who may wish to contribute.<br /> Amnon Eden, School of Computer Science and Electronic Engineering, University of Essex<br /> Johnny S&oslash;raker, Department of Philosophy, University of Twente<br /> Jim Moor, Department of Philosophy, Dartmouth College<br /> Eric Steinhart, Department of Philosophy, William Paterson University</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9QNKGQjFwLB8GxHMS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 23, "extendedScore": null, "score": 6.504693547978525e-07, "legacy": true, "legacyId": "4117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-28T18:47:52.503Z", "modifiedAt": null, "url": null, "title": "What would an ultra-intelligent machine make of the great filter?", "slug": "what-would-an-ultra-intelligent-machine-make-of-the-great", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:55.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7SS966KNXQQdu4AeC/what-would-an-ultra-intelligent-machine-make-of-the-great", "pageUrlRelative": "/posts/7SS966KNXQQdu4AeC/what-would-an-ultra-intelligent-machine-make-of-the-great", "linkUrl": "https://www.lesswrong.com/posts/7SS966KNXQQdu4AeC/what-would-an-ultra-intelligent-machine-make-of-the-great", "postedAtFormatted": "Sunday, November 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20would%20an%20ultra-intelligent%20machine%20make%20of%20the%20great%20filter%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20would%20an%20ultra-intelligent%20machine%20make%20of%20the%20great%20filter%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7SS966KNXQQdu4AeC%2Fwhat-would-an-ultra-intelligent-machine-make-of-the-great%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20would%20an%20ultra-intelligent%20machine%20make%20of%20the%20great%20filter%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7SS966KNXQQdu4AeC%2Fwhat-would-an-ultra-intelligent-machine-make-of-the-great", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7SS966KNXQQdu4AeC%2Fwhat-would-an-ultra-intelligent-machine-make-of-the-great", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>&nbsp;</p>\n<p>Imagine that an ultra-intelligent machine emerges from an <a href=\"http://yudkowsky.net/singularity/schools\">intelligence explosion</a>. &nbsp;The AI (a) finds no trace of extraterrestrial intelligence, (b) calculates that many star systems should have given birth to star faring civilizations so mankind hasn&rsquo;t pass through most of the&nbsp;<a href=\"http://hanson.gmu.edu/greatfilter.html\">Hanson</a>/<a href=\"http://meteuphoric.wordpress.com/2010/11/02/anthropic-principles-agree-on-bigger-future-filters/\">Grace </a>great filter, and (c) realizes that with trivial effort it could immediately send out some <a href=\"http://en.wikipedia.org/wiki/Self-replicating_machine\">self-replicating von Neumann machines</a> that could make the galaxy more to its liking. &nbsp;</p>\n<p>Based on my admittedly limited reasoning abilities and information set I would guess that the AI would conclude that the <a href=\"http://en.wikipedia.org/wiki/Zoo_hypothesis\">zoo hypothesis</a> is probably the solution to the Fermi paradox and because stars don&rsquo;t appear to have been &ldquo;turned off&rdquo; either free energy is not a limiting factor (so the Laws of Thermodynamics are incorrect) or we are being fooled into thinking that stars unnecessarily \"waste&rdquo; free energy (perhaps because we are in a computer simulation).</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7SS966KNXQQdu4AeC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -2, "extendedScore": null, "score": 6.504984755006293e-07, "legacy": true, "legacyId": "4118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-28T21:32:09.936Z", "modifiedAt": null, "url": null, "title": "\"Nahh, that wouldn't work\"", "slug": "nahh-that-wouldn-t-work", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:08.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/682i9R2oSRg7BG8yD/nahh-that-wouldn-t-work", "pageUrlRelative": "/posts/682i9R2oSRg7BG8yD/nahh-that-wouldn-t-work", "linkUrl": "https://www.lesswrong.com/posts/682i9R2oSRg7BG8yD/nahh-that-wouldn-t-work", "postedAtFormatted": "Sunday, November 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Nahh%2C%20that%20wouldn't%20work%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Nahh%2C%20that%20wouldn't%20work%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F682i9R2oSRg7BG8yD%2Fnahh-that-wouldn-t-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Nahh%2C%20that%20wouldn't%20work%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F682i9R2oSRg7BG8yD%2Fnahh-that-wouldn-t-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F682i9R2oSRg7BG8yD%2Fnahh-that-wouldn-t-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 486, "htmlBody": "<p>After having it recommended to me for the fifth time, I finally read through <a href=\"http://www.fanfiction.net/s/5782108/1/Harry_Potter_and_the_Methods_of_Rationality\">Harry Potter and the Methods of Rationality</a>. It didn't seem like it'd be interesting to me, but I was really mistaken. It's fantastic.</p>\n<p>One thing I noticed is that Harry threatens people a lot. My initial reaction was, \"Nahh, that wouldn't work.\"</p>\n<p>It wasn't to scrutinize my own experience. It wasn't to do a google search if there's literature available. It wasn't to ask a few friends what their experiences were like and compare them.</p>\n<p>After further thought, I came to realization - almost every time I've threatened someone (which is rarely), it's worked. Now, I'm kind of tempted to write that off as \"well, I had the moral high ground in each of those cases\" - but:</p>\n<p>1. Harry usually or always has the moral high ground when he threatens people in MOR.</p>\n<p>2. I don't have any personal anecdotes or data about threatening people from a non-moral high ground, but history provides a number of examples, and the threats often work.</p>\n<p>This gets me to thinking - \"Huh, why did I write that off so fast as not accurate?\" And I think the answer is because I don't want the world to work like that. I don't want threatening people to be an effective way of communicating.</p>\n<p>It's just... not a nice idea.</p>\n<p>And then I stop, and think. The world is as it is, not as I think it ought to be.</p>\n<p>And going further, this makes me consider all the times I've tried to explain something I understood to someone, but where they didn't like the answer. Saying things like, \"People don't care about your product features, they care about what benefit they'll derive in their own life... your engineering here is impressive, but 99% of people don't care that you just did an amazing engineering feat for the first time in history if you can't explain the benefit to them.\"</p>\n<p>Of course, highly technical people <em>hate</em>&nbsp;that, and tend not to adjust.</p>\n<p>Or explaining to someone how clothing is a tool that changes people's perceptions of you, and by studying the basics of fashion and aesthetics, you can achieve more of your aims in life. Yes, it <em>shouldn't</em>&nbsp;be like that in an ideal world. But we're not in that ideal world - fashion and aesthetics matter and people react to it.</p>\n<p>I used to rebel against that until I wizened up, studied a little fashion and aesthetics, and started dressing to produce outcomes. So I ask, what's my goal here? Okay, what kind of first impression furthers that goal? Okay, what kind of clothing helps make that first impression?</p>\n<p>Then I wear that clothing.</p>\n<p>And yet, when confronted with something I don't like - I dismiss it out of hand, without even considering my own past experiences. I think this is incredibly common. \"Nahh, that wouldn't work\" - because the person doesn't want to live in a world where it would work.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "5hpGj9nDLgokfghvR": 2, "Q6P8jLn8hH7kbuXRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "682i9R2oSRg7BG8yD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 88, "baseScore": 95, "extendedScore": null, "score": 0.000169, "legacy": true, "legacyId": "4119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 95, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-29T00:08:07.800Z", "modifiedAt": null, "url": null, "title": "Fine-Tuned Mind Projection", "slug": "fine-tuned-mind-projection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:36.648Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kGCzi3d8FtJyxvwXr/fine-tuned-mind-projection", "pageUrlRelative": "/posts/kGCzi3d8FtJyxvwXr/fine-tuned-mind-projection", "linkUrl": "https://www.lesswrong.com/posts/kGCzi3d8FtJyxvwXr/fine-tuned-mind-projection", "postedAtFormatted": "Monday, November 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fine-Tuned%20Mind%20Projection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFine-Tuned%20Mind%20Projection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkGCzi3d8FtJyxvwXr%2Ffine-tuned-mind-projection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fine-Tuned%20Mind%20Projection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkGCzi3d8FtJyxvwXr%2Ffine-tuned-mind-projection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkGCzi3d8FtJyxvwXr%2Ffine-tuned-mind-projection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 399, "htmlBody": "<p>The Fine-Tuning Argument (henceforth FTA) is the pet argument of many a religious apologist, allowing them as it does to build support for their theistic thesis on the findings of cosmology. The basic premise is this: The laws of nature appear to contain constants that if changed slightly would yield universes&nbsp;inhospitable&nbsp;to life. Even though a lot can be said about this premise, Let's assume it true for the purposes of this article.</p>\n<p>Luke Muehlhauser over at <a href=\"http://commonsenseatheism.com/\">Common Sense Atheism</a> recently wrote an <a href=\"http://commonsenseatheism.com/?p=11784\">article</a>&nbsp;pointing out what I think is a central flaw of the FTA. To summarise, he notes that there are multitudes of propositions that are true for this universe and would not be true in a different universe. For instance galaxies, or, Luke's tongue-in-cheek example: iPads. If you accept that the universe is fine-tuned for life, you also have to accept that it's fine-tuned for galaxies, and iPads, given that some changes in the fine-tuned constants would not produce galaxies, and certainly not iPads.&nbsp;</p>\n<p>So the question posed to defenders of the FTA is 'why life'? Why focus on this particular fact? What is it that sets life apart from all the other propositions true about our universe but not other the other possible universes? The usual answer is that life stands out, being valuable in ways that galaxies, iPads, and all the other true propositions are not. It seems that this is an unstated premise of the FTA. But where does that premise come from? Physics gives us no instrument to measure value, so how did this concept get in what was supposed to be a cosmology-based argument?</p>\n<p>I present the FTA here as an argument that while seemingly complex, simply evaporates in light of the <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\">Mind Projection Fallacy</a>. Knowing that humans tend to confuse 'I see X as valuable' with 'x is valuable', the provenance of the hidden premise 'life is valuable' is laid bare, as is the identity of the agent who is doing the valuing, and it is us. With the mystery solved, explaining why humans find life valuable does not require us to go to the extreme lengths of introducing a non-naturalistic cause for the universe.</p>\n<p>Without any support for life being special in some way, the FTA devolves into a straightforward case of <a href=\"http://en.wikipedia.org/wiki/Texas_sharpshooter_fallacy\">Texas Sharpshooter Fallacy</a>: There exists life, our god would have wanted to create life, therefore our god is real! Not quite as compelling.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kGCzi3d8FtJyxvwXr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 6.505770966300759e-07, "legacy": true, "legacyId": "4121", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-29T03:12:10.614Z", "modifiedAt": null, "url": null, "title": "Belief in Belief vs. Internalization", "slug": "belief-in-belief-vs-internalization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:32.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZQe33HWeFW3tSEQcx/belief-in-belief-vs-internalization", "pageUrlRelative": "/posts/ZQe33HWeFW3tSEQcx/belief-in-belief-vs-internalization", "linkUrl": "https://www.lesswrong.com/posts/ZQe33HWeFW3tSEQcx/belief-in-belief-vs-internalization", "postedAtFormatted": "Monday, November 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Belief%20in%20Belief%20vs.%20Internalization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABelief%20in%20Belief%20vs.%20Internalization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZQe33HWeFW3tSEQcx%2Fbelief-in-belief-vs-internalization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Belief%20in%20Belief%20vs.%20Internalization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZQe33HWeFW3tSEQcx%2Fbelief-in-belief-vs-internalization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZQe33HWeFW3tSEQcx%2Fbelief-in-belief-vs-internalization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 481, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\"; mso-ansi-language:#0400; mso-fareast-language:#0400; mso-bidi-language:#0400;} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">Related to <a href=\"/lw/i4/belief_in_belief/\" target=\"_self\">Belief In Belief</a></p>\n<p class=\"MsoNormal\">Suppose that a neighbor comes to you one day and tells you &ldquo;There&rsquo;s a dragon in my garage!&rdquo; Since all of us have been through this before at some point or another, you may be inclined to save time and ask &ldquo;Is the dragon by any chance invisible, inaudible, intangible, and does it convert oxygen to carbon dioxide when it breathes?&rdquo;</p>\n<p class=\"MsoNormal\">The neighbor, however, is a scientific minded fellow and responds &ldquo;Yes, yes, no, and maybe, I haven&rsquo;t checked. This is an idea with <em>testable consequences</em>. If I try to touch the dragon it gets out of the way, but it leaves footprints in flour when I sprinkle it on the garage floor, and whenever it gets hungry, it comes out of my garage and eats a nearby animal. It always chooses something weighing over thirty pounds, and you can see the animals get snatched up and mangled to a pulp in its invisible jaws. It&rsquo;s actually pretty horrible. You may have noticed that there have been fewer dogs around the neighborhood lately.&rdquo;</p>\n<p class=\"MsoNormal\">This triggers a tremendous number of your skepticism filters, and so the only thing you can think of to say is &ldquo;I think I&rsquo;m going to need to see this.&rdquo;</p>\n<p class=\"MsoNormal\">&ldquo;Of course,&rdquo; replies the neighbor, and he sets off across the street, opens the garage door, and is promptly eaten by the invisible dragon.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">Tragic though it is, his death provides a useful lesson. He clearly believed that there was an invisible dragon in his garage, and he was willing to stick his neck out and make predictions based on it. However, he hadn&rsquo;t internalized the idea that there was a dragon in his garage, otherwise he would have stayed the hell away to avoid being eaten. Humans have a fairly general weakness at internalizing beliefs when we don&rsquo;t have to come face to face with their immediate consequences on a regular basis.</p>\n<p class=\"MsoNormal\">You might believe, for example, that starvation is the single greatest burden on humanity, and that giving money to charities that aid starving children in underdeveloped countries has higher utility than any other use of your surplus funds. You might even be able to make predictions based on that belief. But if you see a shirt you really like that&rsquo;s on sale, you&rsquo;re almost certainly not going to think &ldquo;How many people will go hungry if I buy this who I could have fed?&rdquo; It&rsquo;s not a weakness of willpower that causes you to choose the shirt over the starving children, they simply don&rsquo;t impinge on your consciousness at that level.</p>\n<p class=\"MsoNormal\">When you consider if you really, properly hold a belief, it&rsquo;s worth asking not only how it controls your anticipations, but whether your actions make sense in light of a gut-level acceptance of its truth. Do you merely expect to see footprints in flour, or do you move out of the house to avoid being eaten?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YTCrHWYHAsAD74EHo": 1, "LDTSbmXtokYAsEq8e": 1, "Ng8Gice9KNkncxqcj": 1, "SJFsFfFhE6m2ThAYJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZQe33HWeFW3tSEQcx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 41, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "4122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CqyJzDZWvGhhFJ7dY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-29T08:20:17.002Z", "modifiedAt": null, "url": null, "title": "On Lottery Tickets", "slug": "on-lottery-tickets", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SeventhNadir", "createdAt": "2010-04-18T04:11:11.485Z", "isAdmin": false, "displayName": "SeventhNadir"}, "userId": "adAXuo6KKGxTap3SN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mZ5JtyDhJGj2B7E6b/on-lottery-tickets", "pageUrlRelative": "/posts/mZ5JtyDhJGj2B7E6b/on-lottery-tickets", "linkUrl": "https://www.lesswrong.com/posts/mZ5JtyDhJGj2B7E6b/on-lottery-tickets", "postedAtFormatted": "Monday, November 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Lottery%20Tickets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Lottery%20Tickets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZ5JtyDhJGj2B7E6b%2Fon-lottery-tickets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Lottery%20Tickets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZ5JtyDhJGj2B7E6b%2Fon-lottery-tickets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZ5JtyDhJGj2B7E6b%2Fon-lottery-tickets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 437, "htmlBody": "<p>I've often seen the issue of lottery tickets crop up on LessWrong and the [consensus](http://lesswrong.com/lw/hl/lotteries_a_waste_of_hope/) seems to be that the behaviour is irrational. It highlights for me a confusion that I've had about what it means for something to be \"rational\" and I'm seeking clarification. I think it might be useful to break the term down into the distinction I learnt about here, epistemic and instrumental rationality.</p>\n<p><em>Epistemic rationality - </em>This seems to be the most common failure of people who play the lottery. It might be an overt failure of probabilistic reasoning like someone believing their chances of winning to be 50-50 because they can imagine two potential outcome. Maybe they believe that they're \"due\" to win some money as they commit \"the gamblers fallacy\". Or it might be a more subtle failure resulting from correct knowledge of probability, but a fundamental inability to represent that number we call \"scope insensitivity\". I think in the cases where these errors are committed, no-one would argue that these people are being \"rational\".</p>\n<p>However, what if someone had a perfect knowledge of the probabilities involved? If this person bought a lottery ticket would we still consider this a failure of epistemic rationality? You might say that anyone with perfect information of these probabilities would know that lottery tickets are poor financial investments, but we're not talking about instrumental rationality just yet.</p>\n<p><em>Instrumental rationality</em> - Now we're talking about it. The criteria for rationality in this case is, acting in a way that achieves your goals. If your goals in buying a lottery ticket are as one dimensional as making money, then the lottery is a (very) poor investment and I don't think anyone else would disagree. Here is where I start getting confused though, because what happens when a lottery ticket satisfies goals other than financial gain? It is conceivable that I could get more than $5's worth (here meaning my subjective and relative sense of what money is worth) of entertainment out of a $5 lottery ticket. What happens here? I hope you can see the more general problem that arises if you'd answer \"It's still instrumentally irrational\".</p>\n<p>I'm <strong>not</strong> arguing that the lottery is a good idea or that it's socially desirable. I think that it does tend to drain capitol from the people that can least afford it. If you've argued the idea of the lottery to death, pick a different example, it's the underlying concept I'm trying to tease apart. I suppose it boils down to the idea that if an agent makes no instrumental or epistemic errors of rationality, and buys a lottery ticket, can that be irrational?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mZ5JtyDhJGj2B7E6b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-29T08:21:16.743Z", "modifiedAt": null, "url": null, "title": "On Lottery Tickets", "slug": "on-lottery-tickets-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:42.814Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SeventhNadir", "createdAt": "2010-04-18T04:11:11.485Z", "isAdmin": false, "displayName": "SeventhNadir"}, "userId": "adAXuo6KKGxTap3SN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XGqE3tG9SuCkavodJ/on-lottery-tickets-0", "pageUrlRelative": "/posts/XGqE3tG9SuCkavodJ/on-lottery-tickets-0", "linkUrl": "https://www.lesswrong.com/posts/XGqE3tG9SuCkavodJ/on-lottery-tickets-0", "postedAtFormatted": "Monday, November 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Lottery%20Tickets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Lottery%20Tickets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXGqE3tG9SuCkavodJ%2Fon-lottery-tickets-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Lottery%20Tickets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXGqE3tG9SuCkavodJ%2Fon-lottery-tickets-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXGqE3tG9SuCkavodJ%2Fon-lottery-tickets-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 437, "htmlBody": "<p>I've often seen the issue of lottery tickets crop up on LessWrong and the consensus seems to be that the behaviour is irrational. It highlights for me a confusion that I've had about what it means for something to be \"rational\" and I'm seeking clarification. I think it might be useful to break the term down into the distinction I learnt about here, epistemic and instrumental rationality.</p>\n<p><em>Epistemic rationality - </em>This seems to be the most common failure of people who play the lottery. It might be an overt failure of probabilistic reasoning like someone believing their chances of winning to be 50-50 because they can imagine two potential outcome. Maybe they believe that they're \"due\" to win some money as they commit \"the gamblers fallacy\". Or it might be a more subtle failure resulting from correct knowledge of probability, but a fundamental inability to represent that number we call \"scope insensitivity\". I think in the cases where these errors are committed, no-one would argue that these people are being \"rational\".</p>\n<p>However, what if someone had a perfect knowledge of the probabilities involved? If this person bought a lottery ticket would we still consider this a failure of epistemic rationality? You might say that anyone with perfect information of these probabilities would know that lottery tickets are poor financial investments, but we're not talking about instrumental rationality just yet.</p>\n<p><em>Instrumental rationality</em> - Now we're talking about it. The criteria for rationality in this case is, acting in a way that achieves your goals. If your goals in buying a lottery ticket are as one dimensional as making money, then the lottery is a (very) poor investment and I don't think anyone else would disagree. Here is where I start getting confused though, because what happens when a lottery ticket satisfies goals other than financial gain? It is conceivable that I could get more than $5's worth (here meaning my subjective and relative sense of what money is worth) of entertainment out of a $5 lottery ticket. What happens here? I hope you can see the more general problem that arises if you'd answer \"It's still instrumentally irrational\".</p>\n<p>I'm <strong>not</strong> arguing that the lottery is a good idea or that it's socially desirable. I think that it does tend to drain capitol from the people that can least afford it. If you've argued the idea of the lottery to death, pick a different example, it's the underlying concept I'm trying to tease apart. I suppose it boils down to the idea that if an agent makes no instrumental or epistemic errors of rationality, and buys a lottery ticket, can that be irrational?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XGqE3tG9SuCkavodJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": 6.506981953640912e-07, "legacy": true, "legacyId": "4128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-29T09:13:54.335Z", "modifiedAt": null, "url": null, "title": "Individuals angry with humanity as a possible existential risk?", "slug": "individuals-angry-with-humanity-as-a-possible-existential", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:51.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RAFsnW5qHbaWwZkys/individuals-angry-with-humanity-as-a-possible-existential", "pageUrlRelative": "/posts/RAFsnW5qHbaWwZkys/individuals-angry-with-humanity-as-a-possible-existential", "linkUrl": "https://www.lesswrong.com/posts/RAFsnW5qHbaWwZkys/individuals-angry-with-humanity-as-a-possible-existential", "postedAtFormatted": "Monday, November 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Individuals%20angry%20with%20humanity%20as%20a%20possible%20existential%20risk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIndividuals%20angry%20with%20humanity%20as%20a%20possible%20existential%20risk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAFsnW5qHbaWwZkys%2Findividuals-angry-with-humanity-as-a-possible-existential%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Individuals%20angry%20with%20humanity%20as%20a%20possible%20existential%20risk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAFsnW5qHbaWwZkys%2Findividuals-angry-with-humanity-as-a-possible-existential", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAFsnW5qHbaWwZkys%2Findividuals-angry-with-humanity-as-a-possible-existential", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<div class=\"mbl notesBlogText clearfix\">\n<div><span style=\"font-family: 'Helvetica Neue', Helvetica, Arial, default; color: #333333;\"><span style=\"line-height: 15px; white-space: pre-wrap;\">Basically, as technology improves, it will increase the ability of any individual human to change the world, and by extension, it will increase any individual's ability to inflict more significant damage on it, if they so desire. This could be significant in the case of individuals who are especially angry with the world, and who want to take others down with them (e.g. the Columbine shooters, the Unabomber to an extent)</span></span></div>\n<div><span style=\"font-family: 'Helvetica Neue', Helvetica, Arial, default; color: #333333;\"><span style=\"line-height: 15px; white-space: pre-wrap;\"> Now, the thing is this - what if someone angry at the world ultimately developed the means to annihilate the world at his own will? (or to cause massive destruction?) Certainly, this has not happened yet, but it's a possibility with improved technology (especially an improved ability to bioengineer viruses and various nanoparticles). Now, one of the biggest constraints to this is lack of resources (available to an individual). But of course, with the development of nanotechnology (and the use of fewer resources used to construct certain things, and other developments such as the substitution of carbon tubes for other materials), this may not be as much of a constraint as it is now. We could improve monitoring, but this would obviously present a threat to civil liberties.  (This is not an argument against technology - I'm a transhumanist after all, and I completely embrace technological developments. But this is something I've never seen a good solution to)  Of course, reducing the number of angry individuals would also reduce the probability of this happening. This demands an understanding of psychology (especially the psychologies of people who are self-centered, don't like it when they have to compromise, and who collect grudges very easily). And then a creative way to make them less angry (but this is quite difficult, which is why the creativity is needed). especially since many people get angry at the very thought of compromise.  So has anyone else thought of this? And of possible solutions?</span></span></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RAFsnW5qHbaWwZkys", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "4129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-29T14:12:15.156Z", "modifiedAt": null, "url": null, "title": "Blood Feud 2.0", "slug": "blood-feud-2-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:39.065Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Strange7", "createdAt": "2010-02-12T08:30:10.267Z", "isAdmin": false, "displayName": "Strange7"}, "userId": "hKxerxxgheQZCxHsR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SBCXNk2dvTcZFvzKN/blood-feud-2-0", "pageUrlRelative": "/posts/SBCXNk2dvTcZFvzKN/blood-feud-2-0", "linkUrl": "https://www.lesswrong.com/posts/SBCXNk2dvTcZFvzKN/blood-feud-2-0", "postedAtFormatted": "Monday, November 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Blood%20Feud%202.0&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABlood%20Feud%202.0%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBCXNk2dvTcZFvzKN%2Fblood-feud-2-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Blood%20Feud%202.0%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBCXNk2dvTcZFvzKN%2Fblood-feud-2-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBCXNk2dvTcZFvzKN%2Fblood-feud-2-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 604, "htmlBody": "<p>I've been thinking about the idea of culpability.</p>\n<p>What is it for, exactly? Why did societies that use the concept win out over those who stuck with the default response of not assigning any particular emotional significance to a given intangible abstraction?</p>\n<p>If I'm understanding correctly, a given person can be said to be responsible for a given event if and only if a different decision on the part of that person (at some point prior to the event) would be a necessary condition for the event to have not occurred. So, in a code of laws, statements along the lines of \"When X happens, find the person responsible and punish them\" act as an incentive to avoid becoming 'the person responsible,' that is, to put some effort into recognizing when a situation where your actions might lead to negative externalities, and to make the decision that won't result in someone, somewhere down the line, getting angry enough to hunt you down and burn you alive.</p>\n<p>A person cannot be said to be culpable if they had no choice in the matter, or if they had no way of knowing the full consequences of whatever choice they did have. Recklessness is punished less severely than premeditation, and being provably, irresistably coerced into something is hardly punished at all. The causal chain must be traced back to the most recent point where it was sensitive to a conscious decision in a mind capable of considering the law, because that's the only point where distant deterrence or encouragement could have an effect.</p>\n<p>\"Ignorance is no excuse\" because if it were, any halfway-competent criminal could cultivate scrupulous unawareness and be untouchable, but people think it <em>should</em> be an excuse because the law needs to be predictable to work. Same reason punishing someone for doing what was legal at the time doesn't make sense, except as a power-play.</p>\n<p>&nbsp;</p>\n<p>So, let's say you're a tribal hominid, having just figured out all the above in one of those incommunicable, unrepeatable flashes of brilliance. How do you go about implementing it? With limited resources, you can't implement it widely enough to benefit everyone in the known world, even if you wanted to. You can't lay down a written code of laws because standardized writing hasn't even been invented yet, and you can't trust the whole tribe to carry on an oral tradition because you can barely trust half of them not to stab you in the back when you catch something unusually juicy. You <em>can</em>, however, trust your immediate family and/or the spear-carriers you go hunting with to cooperate with you and suffer short-term disadvantages, even when you're not looking, so long as there's a big, plausible payoff within a month (for hunters) or a few years (for family).</p>\n<p>You offer them this: \"We tell the tribe about this idea of 'responsibility,' and then, whenever someone steals from one of us, we all get together and hurt the one responsible until they've lost more than they gained by stealing. When the rest of the tribe can see that stealing from any of us is pointless, we can just leave our stuff sitting out instead of having to worry about hiding it, and then we'll have more time for hunting and grooming.\"</p>\n<p>It works out well enough that soon everybody's doing the revenge thing. Causality and culpability are enough of a puzzle that specialization is necessary to do it right; the problem is, a judgement rendered by someone you don't trust is worse than useless, and the only people it's remotely safe to trust with life-changing decisions are kin.</p>\n<p>You ever notice how corrupt police act sorta like abusive parents?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SBCXNk2dvTcZFvzKN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -2, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "4130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-29T21:31:36.631Z", "modifiedAt": null, "url": null, "title": "Does TDT pay in Counterfactual Mugging?", "slug": "does-tdt-pay-in-counterfactual-mugging", "viewCount": null, "lastCommentedAt": "2018-07-24T00:19:03.790Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bongo", "createdAt": "2009-02-27T12:08:06.258Z", "isAdmin": false, "displayName": "Bongo"}, "userId": "mLnNK3xEMczLs8ind", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6eGw3CJDDqwrSYiHu/does-tdt-pay-in-counterfactual-mugging", "pageUrlRelative": "/posts/6eGw3CJDDqwrSYiHu/does-tdt-pay-in-counterfactual-mugging", "linkUrl": "https://www.lesswrong.com/posts/6eGw3CJDDqwrSYiHu/does-tdt-pay-in-counterfactual-mugging", "postedAtFormatted": "Monday, November 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20TDT%20pay%20in%20Counterfactual%20Mugging%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20TDT%20pay%20in%20Counterfactual%20Mugging%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6eGw3CJDDqwrSYiHu%2Fdoes-tdt-pay-in-counterfactual-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20TDT%20pay%20in%20Counterfactual%20Mugging%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6eGw3CJDDqwrSYiHu%2Fdoes-tdt-pay-in-counterfactual-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6eGw3CJDDqwrSYiHu%2Fdoes-tdt-pay-in-counterfactual-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p>On one hand, <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">this old article</a> said TDT doesn't pay. On the other hand, I imagine TDT not paying would be a slam-dunk argument for favoring UDT, which pays, and I haven't seen people make that argument. So I'm confused here. Thanks.</p>\n<p>Edit: <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">this wiki page</a> explains all the jargon</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1, "dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6eGw3CJDDqwrSYiHu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "4131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["c3wWnvgzdbRhNnNbQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-30T03:37:13.326Z", "modifiedAt": null, "url": null, "title": "The necklaces are known basically for their deep faith based connotations. ", "slug": "the-necklaces-are-known-basically-for-their-deep-faith-based", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beautifuljewelry", "createdAt": "2010-11-30T03:32:58.738Z", "isAdmin": false, "displayName": "beautifuljewelry"}, "userId": "Ridmt4FmL9zdEbpNP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BZAHM8fjJqbbofhuy/the-necklaces-are-known-basically-for-their-deep-faith-based", "pageUrlRelative": "/posts/BZAHM8fjJqbbofhuy/the-necklaces-are-known-basically-for-their-deep-faith-based", "linkUrl": "https://www.lesswrong.com/posts/BZAHM8fjJqbbofhuy/the-necklaces-are-known-basically-for-their-deep-faith-based", "postedAtFormatted": "Tuesday, November 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20necklaces%20are%20known%20basically%20for%20their%20deep%20faith%20based%20connotations.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20necklaces%20are%20known%20basically%20for%20their%20deep%20faith%20based%20connotations.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZAHM8fjJqbbofhuy%2Fthe-necklaces-are-known-basically-for-their-deep-faith-based%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20necklaces%20are%20known%20basically%20for%20their%20deep%20faith%20based%20connotations.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZAHM8fjJqbbofhuy%2Fthe-necklaces-are-known-basically-for-their-deep-faith-based", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBZAHM8fjJqbbofhuy%2Fthe-necklaces-are-known-basically-for-their-deep-faith-based", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>The jewelry are known generally for their deep faith based connotations. They are put on around the neck while protective <a href=\"http://www.pandorajewelrysky.com/\"><strong>pandora 2010</strong></a> versus evil powers and many types of dangers. They are also extremely necessary for fashion make-up. Nonetheless, most women use them either for fashion so that as charms. The pendants are also known as things of attraction.<br /><br /> They might attract good fortune, success,Tiffany Pendants, luck and also other goodies to you with them regularly. Underneath normal circumstances, you will find proper behavioral habits expected from those who wear the. For example, the <a href=\"http://www.pandorajewelrysky.com/\"><strong>pandora sale</strong></a> are certainly not to be worn with a clothing piece and also attire. You can\uffe5t stash them under your outfit rather; you have to keep the exposed around your neck especially when you wear any of them a quality Buddha necklace. Again, you\uffe5re not expected to visit inappropriate locations whilst wearing the chains especially the ones that will bear the Buddha image.<br /><br /> In all, Buddha pendants are generally unique <a href=\"http://www.pandorajewelrysky.com/pandora-bangles-sale\"><strong>Pandora Bangles</strong></a> items you can always go for. These people not only boost your style appearance; rather,Tiffany Necklace sale, they also serve you as protective amulets.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BZAHM8fjJqbbofhuy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4132", "legacySpam": true, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-30T08:56:51.279Z", "modifiedAt": null, "url": null, "title": "Unsolved Problems in Philosophy Part 1: The Liar's Paradox", "slug": "unsolved-problems-in-philosophy-part-1-the-liar-s-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:05.667Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2PdR5oXCAoNjpnSSd/unsolved-problems-in-philosophy-part-1-the-liar-s-paradox", "pageUrlRelative": "/posts/2PdR5oXCAoNjpnSSd/unsolved-problems-in-philosophy-part-1-the-liar-s-paradox", "linkUrl": "https://www.lesswrong.com/posts/2PdR5oXCAoNjpnSSd/unsolved-problems-in-philosophy-part-1-the-liar-s-paradox", "postedAtFormatted": "Tuesday, November 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unsolved%20Problems%20in%20Philosophy%20Part%201%3A%20The%20Liar's%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnsolved%20Problems%20in%20Philosophy%20Part%201%3A%20The%20Liar's%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PdR5oXCAoNjpnSSd%2Funsolved-problems-in-philosophy-part-1-the-liar-s-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unsolved%20Problems%20in%20Philosophy%20Part%201%3A%20The%20Liar's%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PdR5oXCAoNjpnSSd%2Funsolved-problems-in-philosophy-part-1-the-liar-s-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2PdR5oXCAoNjpnSSd%2Funsolved-problems-in-philosophy-part-1-the-liar-s-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>Graham Priest <a href=\"http://opinionator.blogs.nytimes.com/2010/11/28/paradoxical-truth/\">discusses The Liar's Paradox</a> for a NY Times blog. It seems that one way of solving the Liar's Paradox is defining <em><a href=\"http://plato.stanford.edu/entries/dialetheism/\">dialethei</a></em>, a true contradiction. Less Wrong, can you do what modern philosophers have failed to do and solve or successfully dissolve the Liar's Paradox? This doesn't seem nearly as hard as solving <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">free will</a>.</p>\n<p>This post is a practice problem for what may become a sequence on unsolved problems in philosophy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2PdR5oXCAoNjpnSSd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 6.51060779126888e-07, "legacy": true, "legacyId": "4135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-30T09:55:27.904Z", "modifiedAt": null, "url": null, "title": "Where are other places to meet rationality-minded people?", "slug": "where-are-other-places-to-meet-rationality-minded-people", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:45.599Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nCpDyQxr4QfQFjLuX/where-are-other-places-to-meet-rationality-minded-people", "pageUrlRelative": "/posts/nCpDyQxr4QfQFjLuX/where-are-other-places-to-meet-rationality-minded-people", "linkUrl": "https://www.lesswrong.com/posts/nCpDyQxr4QfQFjLuX/where-are-other-places-to-meet-rationality-minded-people", "postedAtFormatted": "Tuesday, November 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20are%20other%20places%20to%20meet%20rationality-minded%20people%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20are%20other%20places%20to%20meet%20rationality-minded%20people%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCpDyQxr4QfQFjLuX%2Fwhere-are-other-places-to-meet-rationality-minded-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20are%20other%20places%20to%20meet%20rationality-minded%20people%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCpDyQxr4QfQFjLuX%2Fwhere-are-other-places-to-meet-rationality-minded-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCpDyQxr4QfQFjLuX%2Fwhere-are-other-places-to-meet-rationality-minded-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1291, "htmlBody": "<p>This thread can be seen as a continuation of a previous thread (http://lesswrong.com/r/discussion/lw/33u/theoretical_target_audience_size_of_less_wrong/). I felt the need to make a new thread since this post is long and also since it is more targeted. This may help us further achieve our goals as well. EDIT: http://www.google.com/search?q=related%3Alesswrong.com is actually one of the best places to find people in the LW demographic.</p>\n<p>Now, I'll say this - I much prefer LessWrong to other hangouts for those who are focused on rationality. But of course, posts here must be of fairly high quality (for good reasons), and some rationality-minded people may want to continue talking with other rationality-minded people in less formal settings (such as chatroom or forum settings). So that's a good reason to think about other communities too (and in addition, it can attract new visitors to our community). These communities do not necessarily have to have a focus on rationality - rather - they should just have a population of rationality-focused individuals who are not afraid to speak out.</p>\n<p>Now, part of the strategy that we often have to use is to find the rationality-focused people in a sea of people who don't care much about rationality. So I've been always thought a lot about the demographics of each community I've joined. Most people in local gifted programs, for example, don't seem to be very interested in rationality (although it seems that interest in rationality does seem to increase with age).There are some real-life venues for meeting similar people, but it's often a lot harder since almost all rationality groups are online (and it's far easier to attract attention to low-popularity topics online). College, for example, seems to be a difficult place to locate rationality-minded individuals, unless the college has an online discussion with a significant portion of the population participating. Sometimes, facebook groups have been an excellent substitute for college, as long as the facebook groups have an audience of a large portion of the college population. Normally, this only works for smaller populations since Facebook's interface is not good for handling large groups (I've seen the Caltech freshman facebook groups, which I've really liked, but unfortunately, I go to a state university rather than Caltech).</p>\n<p>There are numerous online venues for finding people: IRC, forums, and various other Internet communities (comment sections on blogospheres, IRC, chatrooms, reddit, Facebook, Meetup, Google Reader/Buzz (shared items), Google Groups/Yahoo Groups/Usenet, and other social networking websites), and mailing lists. Some of them are better than others for finding rationality-minded people (in particular, people who use certain services are generally more \"intelligent/educated\"+NT than people who use other services [see facebook vs. myspace, reddit vs digg, and gmail vs yahoo mail/hotmail]).</p>\n<p>The venues that are easiest to find are obviously the ones that are not in the deep web. This is mostly just inclusive of the blogosphere, some forums, reddit, and a small fraction of items on Google groups. Things in facebook are also easy to find, since there's only one facebook (as opposed to numerous IRC channels/groups on google/mailing lists) and you can use facebook's internal search engine to find groups (which many people do). But there haven't been many deep discussions on facebook (whose format isn't very good for deep discussions) - and it's now harder to have these discussions than before since Facebook has recently made it difficult to find groups other people are in (as it is now making pages more prominent than groups). My impression is that the aggregated blogosphere has the largest populations of rationality-minded readers (and of quality discussions), but current blog interfaces make it difficult to view people's profiles or to follow people's posts (both features which I think are necessary for faciliating social bonding). And I always wonder - where else do these blog commenters hang out?</p>\n<p>With forums, we can find a nice list of them at http://www.big-boards.com (which is far from comprehensive, but better than nothing)</p>\n<p>Some forums may ostensibly seem good for discussions, but end up not being good for them. Asperger's Syndrome forums, for example, may have some rationality-minded people. But most people will \"tl;dr\" (or ignore) thoughtful posts (and I know this as a fact for the two most popular Asperger's Syndrome forums - both which don't seem to have particularly intelligent readers). Physics Forums also seemed attractive, but unfortunately people there don't seem to care much about rationality (and the debates are full of emotions rather than rationality). Some atheist forums also might have high populations of rational people (but richarddawkins.net forums died, and I don't know much about infidels.org). One of the main problems, anyways, is that it's a lot easier to complain about a post than to make a \"good post\" comment (which might be interpreted as a \"+1 post count\" post). There are then some forums that have very high populations of very intelligent+NT people (College Confidential and phdcomics forums [which have crashed for several weeks by now] come to mind in particular), but most \"rationality\" posts suited for places like this still end up completely ignored. Philosophy forums might have some rationality-minded people, but I don't know the place very well. The same applies to programming forums (it seems that many programmers are NT, but engineering majors, including computer science ones, tend towards ST styles of thinking)</p>\n<p>I've been very surprised by the quality of some discussions that are in the deep web, but they are very difficult to locate (especially since most discussion forums are essentially dead). The calorie restriction mailing list, for example, has surprisingly good discussions (even better than many of the discussions I've seen on Imminst), and people there are more receptive to my emails than the people on Imminst (Imminst just doesn't seem to be that great of a place to discuss rationality topics - although others may disagree with me and I can be convinced).</p>\n<p>It's possible that some foreign language sites may also have high populations of rationality-minded people. There may be a time when Google Translate may finally make people produce mutually intelligible conversations. If this happens, we may be able to increase our space of potentially rational individuals (especially from countries like Japan, Germany, and France, and countries with high %s of German speakers, like the Czech Republic). Many European countries seem to have more \"rational\" belief systems than the U.S., and this may apply to East Asian countries as well. Many Scandinavians+Dutch+Estonians do understand English, though, and even preferentially associate with English-speaking communities (perhaps since they have small populations)</p>\n<p>I know that I haven't found all possible online venues for meeting rationality minded people. My criteria may be different from the criteria of most readers here, as I only feel comfortable discussing personal/daily issues with other late-teenagers (so groups like meetup don't really work for me). In any case, it may be wise to discuss options for rationality-minded teenagers, since it is from teenagers where we can expect to make the biggest increases in the rationality-minded audience (especially from this generation, which seems to have more more rational religious+social attitudes than previous generations).</p>\n<p>Since the vast majority of available groups are unspecialized, this is where we will have to do much of our searching. I've noticed that unspecialized groups with very high average intelligence are probably the best groups to find such individuals. Unfortunately, I hang out in many of these groups, and few people in them seem to be curious enough to develop a deep desire to talk to others about rationality-minded topics. I would like to encourage a discussion of the venues that readers here have liked the most (with respect to finding other rationality-minded individuals), especially with a focus on discussion venues on the deep web (which many people here may not be aware of).</p>\n<p>As suggested by another poster, there's a LessWrong IRC chatroom:&nbsp;http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nCpDyQxr4QfQFjLuX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 6.510751884650486e-07, "legacy": true, "legacyId": "4136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-30T20:34:56.861Z", "modifiedAt": null, "url": null, "title": "New Diplomacy Game in need of two more.", "slug": "new-diplomacy-game-in-need-of-two-more", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:45.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "saliency", "createdAt": "2009-10-25T03:59:58.587Z", "isAdmin": false, "displayName": "saliency"}, "userId": "RNx6ydjKM2J3Heae3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kks2ZeknMT9SbuXFu/new-diplomacy-game-in-need-of-two-more", "pageUrlRelative": "/posts/kks2ZeknMT9SbuXFu/new-diplomacy-game-in-need-of-two-more", "linkUrl": "https://www.lesswrong.com/posts/kks2ZeknMT9SbuXFu/new-diplomacy-game-in-need-of-two-more", "postedAtFormatted": "Tuesday, November 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Diplomacy%20Game%20in%20need%20of%20two%20more.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Diplomacy%20Game%20in%20need%20of%20two%20more.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkks2ZeknMT9SbuXFu%2Fnew-diplomacy-game-in-need-of-two-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Diplomacy%20Game%20in%20need%20of%20two%20more.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkks2ZeknMT9SbuXFu%2Fnew-diplomacy-game-in-need-of-two-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkks2ZeknMT9SbuXFu%2Fnew-diplomacy-game-in-need-of-two-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>We have five people from the NYC division of LW. &nbsp;We need two more players</p>\n<p><a href=\"http://webdiplomacy.net/board.php?gameID=42765\">http://webdiplomacy.net/board.php?gameID=42765</a></p>\n<p>&nbsp;</p>\n<p>passcode:&nbsp;<span style=\"font-family: arial, sans-serif; font-size: 12px;\">streetlight</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kks2ZeknMT9SbuXFu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 6.512324416591934e-07, "legacy": true, "legacyId": "4137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-30T22:08:39.540Z", "modifiedAt": null, "url": null, "title": "Convincing An Illogical Being", "slug": "convincing-an-illogical-being", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "falenas108", "createdAt": "2010-10-28T17:32:39.696Z", "isAdmin": false, "displayName": "falenas108"}, "userId": "BCX7q7NMQphQiXc8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bkaPisTjcYSHDRTWw/convincing-an-illogical-being", "pageUrlRelative": "/posts/bkaPisTjcYSHDRTWw/convincing-an-illogical-being", "linkUrl": "https://www.lesswrong.com/posts/bkaPisTjcYSHDRTWw/convincing-an-illogical-being", "postedAtFormatted": "Tuesday, November 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Convincing%20An%20Illogical%20Being&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConvincing%20An%20Illogical%20Being%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkaPisTjcYSHDRTWw%2Fconvincing-an-illogical-being%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Convincing%20An%20Illogical%20Being%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkaPisTjcYSHDRTWw%2Fconvincing-an-illogical-being", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkaPisTjcYSHDRTWw%2Fconvincing-an-illogical-being", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p class=\"MsoNormal\">There is a person who rejects logic as a valid way of making decisions.&nbsp; This person is perfectly capable of understanding logical arguments, but chooses to reject them.</p>\n<p class=\"MsoNormal\">Is there any way to convince this person that logic is not necessarily a bad thing?</p>\n<p class=\"MsoNormal\">The reason I bring this up is that there is a significant percentage of the population who thinks that using logic is bad, and thinks that they will make better decisions simply by relying on emotions.&nbsp; I think we should clarify if there is anything we as a community could say to these people to help them accept the idea of logic.</p>\n<p class=\"MsoNormal\">Trying to convince them using purely logical methods would not work, as they reject logic.&nbsp; The idea is similar to that of trying to prove that a system of mathematics works without using math.&nbsp; I believe that the proof for this is looking at the real world, and seeing the results math predicts.</p>\n<p class=\"MsoNormal\">The obvious application would be to show how using logic has better results than going by pure emotions. However, doing that would also seem like a logical argument, and would most likely also get rejected.&nbsp; Is succeeding at swaying such an individual even possible?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bkaPisTjcYSHDRTWw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-30T22:50:51.939Z", "modifiedAt": null, "url": null, "title": "Public rationality", "slug": "public-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:44.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GgCwkeAnFHXPkNjik/public-rationality", "pageUrlRelative": "/posts/GgCwkeAnFHXPkNjik/public-rationality", "linkUrl": "https://www.lesswrong.com/posts/GgCwkeAnFHXPkNjik/public-rationality", "postedAtFormatted": "Tuesday, November 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Public%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APublic%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGgCwkeAnFHXPkNjik%2Fpublic-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Public%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGgCwkeAnFHXPkNjik%2Fpublic-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGgCwkeAnFHXPkNjik%2Fpublic-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>I'm going to list some moderately well-known people who strike me as unusually rational. They aren't \"rationalists\" in the sense that they don't generally explicitly talk about rationality.</p>\n<p><a href=\"http://www.cartalk.com\">Tom and Ray Magliozzi</a> run a web site and talk radio show about car repair. They have a repetitious sense of humor, but if you look past that, you see that they have a very wide body of knowledge (and sometime, we should talk about how much detailed knowledge is worth acquiring so that you have something to be rational with), publicly display the process of testing hypotheses, and get in touch with people they've given advice to later to find out whether the advice worked. Sometimes it does, sometimes it doesn't.</p>\n<p><a href=\"http://www.theatlantic.com/ta-nehisi-coates/\">Ta Nehisi Coates</a> writes a politics and culture blog for The Atlantic. He's notable for trying to see how everyone is doing what makes sense to them-- rather a difficult thing when you're taking on the mind-killer subjects.</p>\n<p><a href=\"http://gawande.com/\">Atul Gawande</a> writes books and articles about the practice of medicine. It was particularly striking in his recent <em>The Checklist Manifesto</em> that when his checklists seemed to produce notable improvements in surgical outcomes, his first reaction was concern that there was something wrong with the experiment rather than delight that he'd been proven correct.</p>\n<p>Any other recommendations?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GgCwkeAnFHXPkNjik", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 6.512658734791244e-07, "legacy": true, "legacyId": "4139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T00:43:02.154Z", "modifiedAt": null, "url": null, "title": "The Boundaries of Biases", "slug": "the-boundaries-of-biases", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:04.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yoMdLrEPEAk7noQw9/the-boundaries-of-biases", "pageUrlRelative": "/posts/yoMdLrEPEAk7noQw9/the-boundaries-of-biases", "linkUrl": "https://www.lesswrong.com/posts/yoMdLrEPEAk7noQw9/the-boundaries-of-biases", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Boundaries%20of%20Biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Boundaries%20of%20Biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyoMdLrEPEAk7noQw9%2Fthe-boundaries-of-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Boundaries%20of%20Biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyoMdLrEPEAk7noQw9%2Fthe-boundaries-of-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyoMdLrEPEAk7noQw9%2Fthe-boundaries-of-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1547, "htmlBody": "<p>Thinking about <a href=\"/lw/2pv/intellectual_hipsters_and_metacontrarianism/\">meta-contrarianism</a> and biases (among other things), I came to the following question:</p>\n<p>When are biases a good thing?</p>\n<p>Since the caution too important to leave to the conclusion, I'm going to put it <strong>before</strong> I give an answer, even though the flow will be fudged as a result. In <a href=\"http://www.amazon.com/Epistemology-Psychology-Judgment-Michael-Bishop/dp/0195162307/ref=sr_1_1?ie=UTF8&amp;qid=1291160069&amp;sr=8-1\">Epistemology and the Psychology of Human Judgment</a> (a book I strongly recommend), Bishop and Trout talk a lot about statistical prediction rules, where linear regressions on data often outperform human experts. One of the findings they discussed was that not only did experts have lower accuracy than the statistically generated rules, when given the result of the rule and the option to defect from its prediction they were much more likely to chose to defect when <em>the rule was right</em> and they were wrong than the other way around. So, for almost all of the experts, the best choice was \"stick to the rule, even if you think it's wrong.\" Likewise, even if you've got a long explanation as to why your action isn't biased and how this is a good idea <em>just this once</em>, you should <strong>stick to the rule</strong>.</p>\n<p>But there's still something very important to keep in mind: rules have domains in which they apply. \"Do as the Romans do\" has the domain of \"when in Rome.\" Even if the rule is a black box, such that you do not understand how it created its outputs given its inputs, you can trust the outputs so long as you know which inputs are appropriate for that box. Sticking to the behavior of other Romans will make you fit in better, even if you're not socially aware enough to notice differences in how much you fit in. But if you're in Japan, \"do as the Romans do\" is bad advice.</p>\n<p>When you know how the rule works- when you're at <a href=\"/lw/1yq/understanding_your_understanding\">level 2 or 3 understanding</a>- then you can probably look at a rule and decide if it's appropriate in that circumstance, because you can see what determines the domain of the rule. When you understand the proverb, you know it <em>really</em> means \"When in X, do as the residents of X do\", and then you can pretty easily figure out that \"do as the Romans do\" only fits when you're in Rome. But, as we can see with this toy example, pushing these boundaries helps you understand the rule- \"why is it a bad idea to do what the Romans do in Japan?\"</p>\n<p><a id=\"more\"></a></p>\n<p>On that note, let's return to our original question. When are biases a good thing?</p>\n<p>Someone who likes to capitalize the word Truth would probably instantly reply \"never!\". But the fact that their reply was instant and their capitalization is odd should give us pause. That sounds a lot like a bias, and we're trying to evaluate those. It could be that adjusting the value of truth upwards in all situations is a good bias to have, but we've got to establish that on a level that's more fundamental.</p>\n<p>And so after a bit of thought, we come up with a better way to redefine the original problem. \"If a bias is a decision-making heuristic that has negative instrumental value, are there ranges where that same decision-making heuristic has positive instrumental value?\"</p>\n<p>Three results immediately pop out: the first is that we've constructed a tautological answer to our initial question- never, since we've defined them as bad.</p>\n<p>The second result is still somewhat uninteresting- as your thoughts take time and energy, decision-making heuristics have a natural cost involved. Meticulous but expensive heuristics can be negative value compared to sloppy but cheap heuristics for many applications; you might be better off making a <a href=\"http://en.wikipedia.org/wiki/Brand_loyalty\">biased decision about laundry detergent</a> since you can use that time and energy to better effect elsewhere. But sometimes it's worth expunging low-damage biases at moderate cost because bias-expunging experience is a good in its own right; then, when it comes to make a big decision about buying a house or a car you can use your skills at unbiased purchasing developed by treating laundry detergent as a tough choice.</p>\n<p>The more interesting third result is a distinction between bias and bigotry. Consider a bigoted employer and a biased employer: the bigoted employer doesn't like members of a particular group and the biased employer misjudges members of a particular group. The bigoted employer would only hire people they dislike if the potential hire's superiority to the next best candidate is high enough to overcome the employer's dislike of the potential hire, and the biased employer wants to hire the best candidate but is unconsciously misjudging the quality of the potential hire. Both will only choose the disliked/misjudged potential hire in the same situation- where the innate quality difference is higher than degree of dislike/misjudging- but if you introduce a blind system that masks the disliked characteristic of potential hires, they have opposite responses. The bigoted employer is made worse off- now when choosing between the top set of candidates he might accidentally choose a candidate that would satisfy him less than one he chose with perfect information- but the biased employer is made better off- now, instead of having imperfect information he has perfect information, and will never accidentally choose a candidate that would satisfy him less than the one he chose otherwise. Notice that <em>subtracting</em> data made the biased employer's available information <em>more perfect</em>.</p>\n<p>That distinction is incredibly valuable for anti-discrimination thinkers and anyone who talks with them; much of the pushback against anti-discrimination measures seems to be because they're not equipped to think about and easily discuss the difference between bigotry and bias.</p>\n<p>This is a question that we can ask about every bias. Is racism ever appropriate? Yes, if you're casting for a movie where the character's race is relevant. Is sexism ever appropriate? Yes, if you're looking to hire a surrogate mother (or, for many of us, a mate). But for other biases the question becomes more interesting.</p>\n<p>For example, when does loss aversion represent a decision-making heuristic with positive instrumental value?</p>\n<p>First, we have to identify the decision-making heuristic. A typical experiment that demonstrates loss aversion presents the subject with a gamble: 50% chance of gaining X, 50% chance of losing a pre-determined number between 0 and X. That range has a positive expected value, so a quick calculation suggests that taking the gamble is a good plan. But until the loss is small enough (typical numeric values for the loss aversion effect are about twice, so until the loss is less than X/2), subjects don't take the bet, even though the expected value is positive. That looks an awful lot like \"doublecount your losses when comparing them to gains.\"</p>\n<p>When is that a good heuristic? Well, it is if utility is nonlinear; but the coefficient of 2 for losses seems pretty durable, suggesting it's not people doing marginal benefit / marginal loss calculations in their head. The heuristic seems well-suited to an iterated zero-sum game where your losses benefit the person you lose to, but your opponent's losses aren't significant enough to enter your calculations. If you're playing a game against one other person, then if they lose you win. But if you're in a tournament with 100 entrants, the benefit to you from your opponent's loss is almost zero, while the loss to you from your loss is still doubled- you've fallen down, and in doing so you've lifted a competitor up.</p>\n<p>An example of a bias false positive (calling a decision biased when the decision was outside of the bias's domain) for loss aversion is <a href=\"/r/discussion/lw/32z/winter_1902_france_gets_a_mastectomy/2zm7?c=1\">here</a>, from our first Diplomacy game in the Less Wrong Diplomacy Lab. Germany had no piece in Munich, which was threatened by an Italian piece with other options, and Germany could move his piece in Kiel to Munich (preventing Italian theft) or to Holland (gaining an independent center but risking Munich). If Germany made decisions based only on the number of supply centers it would control after 1901, he would prefer Kie-Hol to Kie-Mun at P(Tyr-Mun)&lt;1, and only be indifferent at P(Tyr-Mun)=1. If Germany made decisions based on the number of supply centers it would control after 1901 minus the number of supply centers the other players controlled divided by 6 (Why 6? Because each player has 6 opponents, and this makes Germany indifferent to a plan that increases or decreases the number of centers each player controls by the same amount), he would be indifferent at P(Tyr-Mun)=5/6. If Germany didn't discount the gains of other countries, he would be indifferent at P(Tyr-Mun)=1/2. If Germany takes into account board position, the number drops even lower- and if Germany has a utility function over supply centers that <em>drops</em> from 5 to 6, as suggested by Douglas Knight, then Germany might never be indifferent between Hol and Mun (but in such a situation, Germany would be unlikely to subscribe to loss aversion).</p>\n<p>&nbsp;</p>\n<p>A final note: we're talking about heuristics, here- the ideal plan in every case is to correctly predict utility in all possible outcomes and maximize predicted utility. But we have to deal with real plans, which almost always involves applying rules to situations and pattern-matching. I've just talked about one bias in one situation here- which biases have you internalized more deeply by exploring their boundaries? Which biases do you think have interesting boundaries?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yoMdLrEPEAk7noQw9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 12, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "4120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9kcTNWopvXFncXgPy", "4gevjbK77NQS6hybY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T04:48:12.618Z", "modifiedAt": null, "url": null, "title": "Broken window fallacy and economic illiteracy.", "slug": "broken-window-fallacy-and-economic-illiteracy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:45.957Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lxppy7MmN2y7aaHvH/broken-window-fallacy-and-economic-illiteracy", "pageUrlRelative": "/posts/Lxppy7MmN2y7aaHvH/broken-window-fallacy-and-economic-illiteracy", "linkUrl": "https://www.lesswrong.com/posts/Lxppy7MmN2y7aaHvH/broken-window-fallacy-and-economic-illiteracy", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Broken%20window%20fallacy%20and%20economic%20illiteracy.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABroken%20window%20fallacy%20and%20economic%20illiteracy.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLxppy7MmN2y7aaHvH%2Fbroken-window-fallacy-and-economic-illiteracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Broken%20window%20fallacy%20and%20economic%20illiteracy.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLxppy7MmN2y7aaHvH%2Fbroken-window-fallacy-and-economic-illiteracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLxppy7MmN2y7aaHvH%2Fbroken-window-fallacy-and-economic-illiteracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 259, "htmlBody": "<p>Some time ago, I had a talk with my father where I explained to him the concept of the <a href=\"http://en.wikipedia.org/wiki/Broken_window_fallacy\" target=\"_self\">broken window fallacy</a>. The idea was completely novel to him, and while it didn't take long for him to grasp the principles, he still needed my help in coming up with examples of ways that it applies to the market in the real world.</p>\n<p><em>My father has an MBA from Columbia University and has held VP positions at multiple marketing firms.</em></p>\n<p>I am not remotely expert on economics; I do not even consider myself an aficionado. But it has frequently been my observation that not just average citizens, but people whose positions have given them every reason to learn and use the information, are <em>critically</em> ignorant of basic economic principles. It feels like watching engineers try to produce functional designs based on Aristotelian physics. You cannot rationally pursue self interest when your map does not correspond to the territory.</p>\n<p>I suppose the <em>worst</em> thing for me to hear at this point is that there is some reason with which I am not yet familiar which prevents this from having grand scale detrimental effects on the economy, since it would imply that businesses <em>cannot</em> be made more sane by the increased dissemination of basic economic information. Otherwise, this seems like a fairly important avenue to address, since the basic standards for economic education, in educated businesspeople and the general public, are so low that I doubt the educational system has even begun to climb the slope of diminishing returns on effort invested into it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lxppy7MmN2y7aaHvH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 6.513537842438055e-07, "legacy": true, "legacyId": "4140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T07:18:52.929Z", "modifiedAt": null, "url": null, "title": "Aspie toy: the Neocube", "slug": "aspie-toy-the-neocube", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nFhrLBEvtDTnZPRac/aspie-toy-the-neocube", "pageUrlRelative": "/posts/nFhrLBEvtDTnZPRac/aspie-toy-the-neocube", "linkUrl": "https://www.lesswrong.com/posts/nFhrLBEvtDTnZPRac/aspie-toy-the-neocube", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aspie%20toy%3A%20the%20Neocube&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAspie%20toy%3A%20the%20Neocube%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFhrLBEvtDTnZPRac%2Faspie-toy-the-neocube%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aspie%20toy%3A%20the%20Neocube%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFhrLBEvtDTnZPRac%2Faspie-toy-the-neocube", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnFhrLBEvtDTnZPRac%2Faspie-toy-the-neocube", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>This post is going to sound like an ad. Sorry about that. I'm not affiliated, etc, etc.</p>\n<p>Last Friday I bought a very simple toy: a set of 216 little magnetic metal balls, about the size of ball bearings. Since then I've been completely entranced by it and unable to put the thing down.&nbsp;Here's a&nbsp;<a href=\"http://www.flickr.com/groups/magnetspheres/pool/\">Flickr group</a>&nbsp;to show what I mean. The little balls seem to&nbsp;<em>want</em> to come together in symmetrical patterns: you can make square and hexagonal flat patches, curved patches with 3/4/5/6-fold symmetry, stable 3D cubic lattices, <a href=\"http://en.wikipedia.org/wiki/Close_packing\">fcc and hcp</a> lattices and many hollow and solid polyhedra. So far I've managed to make a <a href=\"http://my.execpc.com/~rhoadley/images/tetra6.jpg\">tetrahedron</a>, two varieties of cube (<a href=\"http://www.flickr.com/photos/individual8/2615896565/\">1</a>, <a href=\"http://www.flickr.com/photos/29050552@N04/4523894987/\">2</a>), an <a href=\"http://www.ict.griffith.edu.au/~anthony/hobbies/magnets/chain_octa_md.jpg\">octahedron</a>, an <a href=\"http://www.ict.griffith.edu.au/~anthony/hobbies/magnets/chain_icosa_2_md.jpg\">icosahedron</a>, and other stuff (my current favorite shape is the solid truncated octahedron). It's like crack for the right type of person.</p>\n<p>And there's the rub. Carrying this toy around and showing it to my friends has made me realize with forgotten clarity that I'm <em>special</em>. Practically <em>no one</em> reacts to it the same way as me. The word \"aspie\" has been uttered, half in jest, half seriously. Even though my intelligence may be pretty average (judging by online tests I have lower IQ than most LW regulars), I seem to have this rare natural ability to get deeply interested in things that \"normal\" people find boring.</p>\n<p>This ability... this <em>instinctive desire to tinker with&nbsp;symmetrical patterns</em>... has shaped my entire life by now, because it's what first attracted me to math and then programming. But how could it ever be environmental, if I remember having it since my earliest childhood? Is it genetic? Is math success genetic, then? What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nFhrLBEvtDTnZPRac", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 6.513908574910883e-07, "legacy": true, "legacyId": "4141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T08:25:47.450Z", "modifiedAt": null, "url": null, "title": "Defecting by Accident - A Flaw Common to Analytical People", "slug": "defecting-by-accident-a-flaw-common-to-analytical-people", "viewCount": null, "lastCommentedAt": "2022-03-02T20:44:23.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "lionhearted", "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GG2rtBReAm6o3mrtn/defecting-by-accident-a-flaw-common-to-analytical-people", "pageUrlRelative": "/posts/GG2rtBReAm6o3mrtn/defecting-by-accident-a-flaw-common-to-analytical-people", "linkUrl": "https://www.lesswrong.com/posts/GG2rtBReAm6o3mrtn/defecting-by-accident-a-flaw-common-to-analytical-people", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Defecting%20by%20Accident%20-%20A%20Flaw%20Common%20to%20Analytical%20People&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefecting%20by%20Accident%20-%20A%20Flaw%20Common%20to%20Analytical%20People%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGG2rtBReAm6o3mrtn%2Fdefecting-by-accident-a-flaw-common-to-analytical-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Defecting%20by%20Accident%20-%20A%20Flaw%20Common%20to%20Analytical%20People%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGG2rtBReAm6o3mrtn%2Fdefecting-by-accident-a-flaw-common-to-analytical-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGG2rtBReAm6o3mrtn%2Fdefecting-by-accident-a-flaw-common-to-analytical-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4529, "htmlBody": "<p>Related to: <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">Rationalists Should Win</a>, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">Why Our Kind Can&#x27;t Cooperate,</a> <a href=\"/lw/5t/can_humanism_match_religions_output/ \">Can Humanism Match Religion&#x27;s Output?</a>, <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>, Paul Graham&#x27;s &quot;<a href=\"http://www.paulgraham.com/nerds.html\">Why Nerds Are Unpopular</a>&quot;</p><p>The &quot;Prisoner&#x27;s Dilemma&quot; refers to a game theory problem developed in the 1950&#x27;s. Two prisoners are taken and interrogated separately. If either of them confesses and betrays the other person - &quot;defecting&quot; - they&#x27;ll receive a reduced sentence, and their partner will get a greater sentence. However, if <em>both</em> defect, then they&#x27;ll both receive higher sentences than if neither of them confessed.</p><p>This brings the prisoner to a strange problem. The best solution individually is to defect. But if both take the individually best solution, then they&#x27;ll be worst off overall. This has wide ranging implications for international relations, negotiation, politics, and many other fields.</p><p>Members of LessWrong are incredibly smart people who tend to like game theory, and debate and explore and try to understand problems like this. But, does knowing game theory actually make you more effective in real life?</p><p>I think the answer is yes, with a caveat - you need the basic social skills to implement your game theory solution. The worst-case scenario in an interrogation would be to &quot;defect by accident&quot; - meaning that you&#x27;d just blurt out something stupidly because you didn&#x27;t think it through before speaking. This might result in you and your partner both receiving higher sentences... a very bad situation. Game theory doesn&#x27;t take over until basic skill conditions are met, so that you could actually execute any plan you come up with.</p><p><strong>The Purpose of This Post</strong>: I think many smart people &quot;defect&quot; by accident. I don&#x27;t mean in serious situations like a police investigation. I mean in casual, everyday situations, where they tweak and upset people around them by accident, due to a lack of reflection of desired outcomes.</p><p>Rationalists should win. Defecting by accident frequently results in losing. Let&#x27;s examine this phenomenon, and ideally work to improve it.</p><p>Contents Of This Post</p><ul><li>I&#x27;ll define &quot;defecting by accident.&quot;</li><li>I&#x27;ll explain a common outcome of defecting by accident.</li><li>I&#x27;ll give some recent, mild examples of accidental defections.</li><li>I&#x27;ll give examples of how to turn accidental defections into cooperation.</li><li>I&#x27;ll give some examples of how this can make you more successful at your goals.</li><li>I&#x27;ll list some books I recommend if you decide to learn more on the topic.</li></ul><p><strong>Background - On Analytical Skills and Rhetoric</strong></p><p>From Paul Graham&#x27;s &quot;<a href=\"http://www.paulgraham.com/nerds.html\">Why Nerds Are Unpopular</a>&quot; -</p><blockquote>I know a lot of people who were nerds in school, and they all tell the same story: there is a strong correlation between being smart and being a nerd, and an even stronger inverse correlation between being a nerd and being popular. Being smart seems to make you unpopular.<br/> </blockquote><blockquote>[...]<br/> </blockquote><blockquote>The key to this mystery is to rephrase the question slightly. Why don&#x27;t smart kids make themselves popular? If they&#x27;re so smart, why don&#x27;t they figure out how popularity works and beat the system, just as they do for standardized tests?<br/> </blockquote><blockquote>[...]<br/> </blockquote><blockquote>So if intelligence in itself is not a factor in popularity, why are smart kids so consistently unpopular? The answer, I think, is that they don&#x27;t really want to be popular.<br/> </blockquote><blockquote>If someone had told me that at the time, I would have laughed at him. Being unpopular in school makes kids miserable, some of them so miserable that they commit suicide. Telling me that I didn&#x27;t want to be popular would have seemed like telling someone dying of thirst in a desert that he didn&#x27;t want a glass of water. Of course I wanted to be popular.<br/> </blockquote><blockquote>But in fact I didn&#x27;t, not enough. There was something else I wanted more: to be smart.</blockquote><p>I believe that &quot;defecting by accident&quot; is a result of not learning how different phrasing of words and language can dramatically effect how well your point is taken. It&#x27;s been a general observation of mine that a lot of people in highly intellectual disciplines like mathematics, physics, robotics, engineering, and computer science/programming look down on social skills.</p><p>Of course, they wouldn&#x27;t phrase it that way. They&#x27;d say they don&#x27;t have time for it - they don&#x27;t have time for gossip, or politics, or sugarcoating. They might say, &quot;I&#x27;m a realist&quot; or &quot;I say it like it is.&quot;</p><p>I believe this is a result of not realizing how big the difference in your effectiveness will be depending on how you phrase things, in what order, how well you appeal to another person&#x27;s emotions. People in highly analytical disciplines often care about &quot;just the facts&quot; - but, let&#x27;s face it, we highly analytical people are a great minority of the population.</p><p>Sooner or later, you&#x27;re going to have something you care about and you&#x27;re going to need to persuade someone who is not highly analytical. At that point, you run some serious risks of failure if you don&#x27;t understand basic social skills.</p><p>Now, most people would claim that they have basic social skills. But I&#x27;m not sure this is borne out by observation. This used to be a very key part of any educated person&#x27;s studies: rhetoric. From <a href=\"http://en.wikipedia.org/wiki/Rhetoric\">Wikiedpia</a>: &quot;<strong>Rhetoric</strong> is the art of using language to communicate effectively and persuasively. ... From ancient Greece to the late 19th Century, it was a central part of Western education, filling the need to train public speakers and writers to move audiences to action with arguments.&quot;</p><p>Rhetoric is now frequently looked down upon by highly intelligent and analytical people. Like Paul Graham says, it&#x27;s not that intellectuals can&#x27;t learn it. It&#x27;s that they think it&#x27;s not a good use of their time, that they&#x27;d rather be smart instead.</p><p><strong>Defecting by Accident</strong></p><p>Thus, you see highly intelligent people do what I now term &quot;defecting by accident&quot; - meaning, in the process of trying to have a discussion, they insult, belittle, or offend their conversational partner. They commit obvious, blatant social faux pases, <em>not as a conscious decision of the tradeoffs, but by accident because they don&#x27;t know better. </em></p><p>Sometimes defecting is the right course of action. Sometimes you need to break from whoever you&#x27;re negotiating with, insist that things are done your way, even at their expense, and take the consequences that may arise from that.</p><p>But it&#x27;s rarely something you should do by accident.</p><p>I&#x27;ll give specific, clear examples in a moment, but before I do so, let&#x27;s look at a general example of how this can happen.</p><p>If you&#x27;re at a meeting and someone gives a presentation and asks if anyone has questions, and you ask point-blank, &quot;But we don&#x27;t have the budget or skills to do that, how would we overcome that?&quot; - then, that seems like a highly reasonable question. It&#x27;s probably very intelligent.</p><p>What normal people would consider, though, is how this affects the perception of everyone in the room. To put it bluntly - <em>it makes the presenter look very bad.</em></p><p>That&#x27;s okay, if you decide that that&#x27;s an acceptable part of what you&#x27;re doing. But you now have someone who is likely to actively work to undermine you going forwards. A minor enemy. Just because you asked a question casually without thinking about it.</p><p>Interestingly, there&#x27;s about a thousand ways you could be diplomatic and tactful to address the key issue you have - budgeting/staffing - without embarrassing the presenter. You could take them aside quietly later and express your concern. You could phrase it as, &quot;This seems like an amazing idea and a great presentation. I wonder how we could secure the budgeting and get the team for it, because it seems like it&#x27;d be a profitable if we do, and it&#x27;d be a shame to miss this opportunity.&quot;</p><p>Just by phrasing it that way, you make the presenter look good even if the option can&#x27;t be funded or staffed. Instead of expressing your concern as a hole in their presentation, you express it as a challenge to be overcome by everyone in the room. Instead of your underlying point coming across as &quot;your idea is unfeasible,&quot; it comes across as, &quot;You&#x27;ve brought this good idea to us, and I hope we&#x27;re smart enough to make it work.&quot;</p><p>If the real goal is just to make sure budgeting and funding is taken care of, there&#x27;s many ways to do that without embarrassing and making an enemy out of the presenter. </p><p>Defecting by accident is lacking the awareness, tact, and skill to realize what the secondary effects of your actions are and act accordingly to win. </p><p>This is a relatively basic problem that the majority of &quot;normal&quot; people understand, at least on a subconscious level. Most people realize that you can&#x27;t just show up a presenter and make them look bad. Or at least, you should expect them to be hostile to you if you do. But many intelligent people say, &quot;What the hell is his problem? I just asked a question.&quot;</p><p>This is due to a lack of understanding of social skills, diplomacy, tact, and yes, perhaps &quot;politics&quot; - which are unfortunately a reality of the world. And again, rationalists should win. If your actions are leading to hostility and defection against you, then you need to consider if your actions are the best possible.</p><p><strong>&quot;Why Our Kind Can&#x27;t Cooperate&quot;</strong></p><p>Eliezer&#x27;s &quot;<a href=\"/lw/3h/why_our_kind_cant_cooperate/\">Why Our Kind Can&#x27;t Cooperate</a>&quot; is a masterpiece. I&#x27;m only going to excerpt three parts, but I&#x27;d recommend the whole article.</p><p> </p><blockquote>From when I was still forced to attend, I remember our synagogue&#x27;s annual fundraising appeal.  It was a simple enough format, if I recall correctly.  The rabbi and the treasurer talked about the shul&#x27;s expenses and how vital this annual fundraise was, and then the synagogue&#x27;s members called out their pledges from their seats.</blockquote><blockquote><br/>Straightforward, yes?<br/> </blockquote><blockquote>Let me tell you about a different annual fundraising appeal.  One that I ran, in fact; during the early years of a nonprofit organization that <u><a href=\"http://wiki.lesswrong.com/wiki/Topic_that_must_not_be_named\">may not be named</a></u>.  One difference was that the appeal was conducted over the Internet.  And another difference was that the audience was largely drawn from the atheist/libertarian/technophile/sf-fan/early-adopter/programmer/etc crowd.  (To point in the rough direction of an empirical cluster in personspace.  If you understood the phrase &quot;empirical cluster in personspace&quot; then you know who I&#x27;m talking about.)<br/> </blockquote><blockquote>I crafted the fundraising appeal with care.  By my nature I&#x27;m too proud to ask other people for help; but I&#x27;ve gotten over around 60% of that reluctance over the years.  The nonprofit needed money and was growing too slowly, so I put some force and poetry into that year&#x27;s annual appeal.  I sent it out to several mailing lists that covered most of our potential support base.<br/> </blockquote><blockquote>And almost immediately, people started posting to the mailing lists about why they weren&#x27;t going to donate.  Some of them raised basic questions about the nonprofit&#x27;s philosophy and mission.  Others talked about their brilliant ideas for all the <em>other</em> sources that the nonprofit could get funding from, instead of them.  (They didn&#x27;t volunteer to contact any of those sources <em>themselves</em>, they just had ideas for how <em>we</em> could do it.)<br/> </blockquote><blockquote>Now you might say, &quot;Well, maybe your mission and philosophy <em>did</em> have basic problems - you wouldn&#x27;t want to<em>censor </em>that discussion, would you?&quot;<br/> </blockquote><blockquote>Hold on to that thought.<br/> </blockquote><blockquote>Because people <em>were</em> donating.  We started getting donations right away, via Paypal.  We even got congratulatory notes saying how the appeal had finally gotten them to start moving.  A donation of $111.11 was accompanied by a message saying, &quot;I decided to give **** a little bit more.  One more hundred, one more ten, one more single, one more dime, and one more penny.  All may not be for one, but this one is trying to be for all.&quot;<br/> </blockquote><blockquote>But none of those donors posted their agreement to the mailing list.  Not one.<br/> <br/>So far as any of those donors knew, they were alone.  And when they tuned in the next day, they discovered not thanks, but arguments for why they <em>shouldn&#x27;t</em> have donated.  The criticisms, the justifications for not donating - <em>only those</em> were displayed proudly in the open.<br/> </blockquote><blockquote>As though the treasurer had finished his annual appeal, and everyone <em>not</em> making a pledge had proudly stood up to call out justifications for refusing; while those making pledges whispered them quietly, so that no one could hear.</blockquote><p> </p><p>Indeed, that&#x27;s a problem. Eliezer continues:</p><blockquote> &quot;It is dangerous to be half a rationalist.&quot;</blockquote><p> </p><p>And finally, this point, which is magnificent -</p><blockquote>Our culture puts all the emphasis on heroic disagreement and <u><a href=\"http://www.overcomingbias.com/2007/12/lonely-dissent.html\">heroic defiance</a></u>, and none on heroic agreement or heroic group consensus.  We signal our superior intelligence and our <em>membership in the nonconformist community</em> by inventing clever objections to others&#x27; arguments.  Perhaps <em>that </em>is why the atheist/libertarian/technophile/sf-fan/Silicon-Valley/programmer/early-adopter crowd stays marginalized, losing battles with less nonconformist factions in larger society.  No, we&#x27;re not losing because we&#x27;re so superior, we&#x27;re losing because our exclusively individualist traditions sabotage our ability to cooperate.</blockquote><p><strong>On Being Pedantic, Sarcastic, Disagreeable, Non-Complimentary, and Otherwise Defecting by Accident</strong></p><p>You might not realize it, but in almost all of human civilization it&#x27;s considered insulting to just point out something wrong someone is doing without any preface, softening, or making it clear why you&#x27;re doing it.</p><p>It&#x27;s taken for granted in some blunt, &quot;say it like it is&quot; communities, but it&#x27;s usually taken as a personal attack and a sign of animosity in, oh, 90%+ of the rest of civilization.</p><p>In these so-called &quot;normal people&#x27;s societies,&quot; correcting them in front of their peers will be perceived as trying to lower them and make them look stupid. <em>Thus, they&#x27;ll likely want to retaliate against you, or at least not cooperate with you.</em></p><p>Now, there&#x27;s a time and place to do this anyways. Sometimes there&#x27;s an emergency, and you don&#x27;t have time to take care of people&#x27;s feelings, and just need to get something done. But surfing the internet is not that time.</p><p>I&#x27;m going to take some example replies from a recent post I made to illustrate this. There&#x27;s always a risk in doing this of not being objective, but I think it&#x27;s worth it because (1) I tend to read every reply to me and carefully reflect on it for a moment, (2) I understand exactly my first reactions to these comments, and (3) I won&#x27;t have to rehash criticisms of another person. Take a grain of salt with you since I&#x27;m looking at replies to myself originally, but I think I can give you some good examples.</p><p>The first thing I want to do is take a second to mention that almost everyone in the entire world gets emotionally invested in things they create, and are also a little insecure about their creations. It&#x27;s extraordinarily rare that people don&#x27;t care what others&#x27; think of their writing, science, or art. </p><p>Criticism has good and bad points. Great critics are rare, but they actually make works of creation even in critique. A great critic can give background, context, and highlight a number of relevant mainstream and obscure works through history that the piece they&#x27;re critiquing reminds them of.</p><p>Good critique is an art of creation in and of itself. But bad critique - just blind &quot;that&#x27;s wrong&quot; without explaining why - tends to be construed as a hostile action and not accomplish much, other than signalling that &quot;heroic disagreement&quot; that Eliezer talks about.</p><p>I recently wrote a post titled, <a href=\"/lw/36f/nahh_that_wouldnt_work\">&quot;Nahh, that wouldn&#x27;t work&quot;</a>. I thought about it for around a week, then it took me about two hours to think it through, draw up key examples on paper, choose the most suitable, edit, and post it. It was generally well-received here on LW and on my blog.</p><p>I&#x27;ll show you three comments on there, and how I believe they could be subtly tweaked.</p><p>1.</p><p> </p><blockquote> &gt; I wizened up,<br/> I don&#x27;t think that&#x27;s the word you want to use, unless you&#x27;re talking about how you finally lost those 20 pounds by not drinking anymore.</blockquote><p> </p><p>2.</p><blockquote>FWIW, I think posts like this are more valuable the more they include real-world examples; it&#x27;s kind of odd to read a post which says I had theory A of the world but now I hold theory B, without reading about the actual observations. It would be like reading a history of quantum mechanics or relativity with all mentions of things like the laser or double-slit experiment or Edding or Michelson-Morley removed.</blockquote><p>3.</p><blockquote>An interesting start, but I would rather see this in Discussion -- it&#x27;s not fully adapted yet, I think...</blockquote><p>Now, I spend a lot of time around analytical people, so I take no offense at this. But I believe these are good examples of what I&#x27;d call &quot;accidental defection&quot; - this is the kind of thing that produces a negative reaction in the person you&#x27;re talking to, perhaps without you even noticing.</p><p>#1 is kind of clever pointing out a spelling error. But you have to realize, in normal society that&#x27;s going to upset and make hostile the person you&#x27;re addressing. Whether you mean to or not, it comes across as, &quot;I&#x27;m demonstrating that I&#x27;m more clever than you.&quot;</p><p>There&#x27;s a few ways it could be done differently. For instance, an email that says, &quot;Hey Sebastian, I wanted to give you a heads up. I saw your recent post, but you spelled &quot;wisen&quot; as &quot;wizen&quot; - easy spelling error to make, since they&#x27;re uncommonly used words, but I thought you should know. &quot;Wizen&quot; means for things to dry up and lose water. Cheers and best wishes.&quot;</p><p>That would point out the error (if that&#x27;s the main goal), and also engender a feeling of gratitude in whoever received it (me, in this case). Then I would have written back, &quot;Hey, thanks... I don&#x27;t worry about spelling too much, but yeah that one&#x27;s embarrassing, I&#x27;ll fix it. Much appreciated. Anyways, what are you working on? How can I help?&quot;</p><p>I know that&#x27;s how I&#x27;d have written back, because that&#x27;s how I generally write back to someone who tries to help me out. Mutual goodwill, it&#x27;s a virtuous cycle.</p><p>Just pointing out someone is wrong in a clever way usually engenders bad will and makes them dislike you. The thing is, I know that&#x27;s not the intention of anyone here - hence, &quot;defecting by accident.&quot; Analytical people often <em>don&#x27;t even realize they&#x27;re showing someone up </em>when they do it.</p><p>I&#x27;m not particularly bothered. I get the intent behind it. But normal people are going to be ultra-hostile if you do it to them. There&#x27;s other ways, if you feel the need to point it out publicly. You could &quot;soften&quot; it by praising first - &quot;Hey, some interesting points in this one... I&#x27;ve thought about a similar bias of not considering outcomes if I don&#x27;t like what it&#x27;d mean by the world. By the way, you probably didn&#x27;t mean wizen there...&quot; - or even just saying, &quot;I think you meant &#x27;wisen&#x27; instead of &#x27;wizen&#x27;&quot; - with links to the dictionary, maybe. Any of those would go over better with the original author/presenter whom you&#x27;re pointing out the error to.</p><p>Let&#x27;s look at point #2. &quot;FWIW, I think posts like this are more valuable the more they include real-world examples; it&#x27;s kind of odd to read a post which says I had theory A of the world but now I hold theory B, without reading about the actual observations.&quot;</p><p>This is something which makes people trying to help or create shake their head. See, it&#x27;s potentially a good point. But after someone takes some time to create something and give it away for free, then hearing, &quot;Your work would be more valuable if you did (xyz) instead. Your way is kind of odd.&quot;</p><p>People generally don&#x27;t like that.</p><p>Again, it&#x27;s trivially easy to write that differently. Something like, &quot;Thanks for the post. I was wondering, you mentioned (claim X), but I wonder if you have any examples of claim X so I can understand it better?&quot;</p><p>That one has - gratitude, no unnecessary criticism, explains your motivation. All of which are good social skill points, especially the last one as written about in Cialdini&#x27;s &quot;Influence&quot; - give a reason why.</p><p>#3 - &quot;An interesting start, but I would rather see this in Discussion -- it&#x27;s not fully adapted yet, I think...&quot;</p><p>Okay. Why?</p><p>The difference between complaining and constructive work is looking for solutions. So, &quot;There&#x27;s some good stuff in here, but I think we could adapt it more. One thing I was thinking is (main point).&quot;</p><p><strong>Becoming More Self-Aware and Strategic; Some Practical Social Guidelines</strong></p><p>From Anna Salamon&#x27;s &quot;<a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>&quot; - </p><blockquote> But there are clearly also heuristics that would be useful to goal-achievement (or that would be part of what it means to \u201chave goals\u201d at all) that we do <em>not</em> automatically carry out.  We do <em>not</em> automatically:</blockquote><ul><li>(a) Ask ourselves what we\u2019re trying to achieve; </li><li>(b) Ask ourselves how we could tell if we achieved it (\u201cwhat does it look like to be a good comedian?\u201d) and how we can track progress; </li><li>(c) Find ourselves strongly, intrinsically curious about information that would help us achieve our goal; </li></ul><p>Anna points out that people don&#x27;t automatically ask what they&#x27;re trying to achieve. You don&#x27;t, necessarily, ask what you&#x27;re trying to achieve.</p><p>But I would recommend you do ask that before speaking up socially. At least for a while, until you&#x27;ve got the general patterns figured out.</p><p>If you don&#x27;t, you run the risk of antagonizing and making people hostile to you who would otherwise cooperate and work with you. </p><p>Now, I&#x27;ve heard smart people say, &quot;I don&#x27;t have time for that.&quot; This is akin to saying, &quot;I don&#x27;t have time to achieve what I want to achieve.&quot;</p><p>Because it doesn&#x27;t take much time, and it makes you <em>much </em>more effective. Asking, &quot;What am I trying to achieve here?&quot; goes a long way.</p><p>When commenting on a discussion site, who are you writing for? For the author? For the regular readers? What&#x27;s your point in replying? If your main point is just to &quot;get to truth and understanding,&quot; then what should your secondary considerations be? If there&#x27;s a conflict between the two, would you prefer to encourage the author to write more, or to look clever by pointing out a pedantic point?</p><p>I understand where you&#x27;re coming from, because I used to come from the same place. I was the kid who argued with teachers when they were wrong, not realizing the long term ramifications of that. People matter, and people&#x27;s feelings matter, <em>especially</em> if they have sway over your life, but even if they don&#x27;t have sway over your life.</p><p>To that, here&#x27;s some suggestions I think would make you more effective:</p><ul><li>Generally, be gracious and thankful. This goes immensely far. Things like starting a reply with, &quot;Thanks for this&quot; or &quot;Thanks for sharing these insights.&quot;</li><li>Praising someone makes it more likely they&#x27;ll accept your criticisms. &quot;I thought your point A was excellent, however point B...&quot;</li><li>If you&#x27;re going to disagree, summarize the person&#x27;s main argument beforehand - this has a few positive effects. First, it forces you make sure you actually understand. Second, if the author has a different main point and wasn&#x27;t clear, that comes out. Third, it shows some respect that you actually took the time to read and understand the post. So you could write, &quot;I know your main argument is A, but I wanted to explore your minor point X.&quot;</li><li>If you think something is wrong, give an explanation of what would be correct and better. &quot;I enjoyed this post a lot - thanks for that - but one thing that&#x27;s tough for me is that all the examples are about martial arts, and I don&#x27;t really understand martial arts so much. Maybe next time you could provide some examples from other fields? For instance, I remember reading you&#x27;re an accountant and you write poetry, maybe some examples from there?&quot;</li><li>If you point out something is wrong, do your best to make the mistake-maker not feel stupid. This makes them massively appreciate that. &quot;Hey, you got your math on example X wrong... I think it actually works to 11.7. Anyways, I only recognize that because I made that mistake dozens of times myself, it&#x27;s a common one to make, just wanted to point it out.&quot;</li><li>Explain why you care about a point. This has a few positive effects. First, it lets the author cater a reply to exactly what you want. Second, you&#x27;d be amazed at how many people assume evil intent and worst-possible motives - it neutralizes that. Third, it forces you to think through how you&#x27;d like things to be, which is again good. &quot;Hey man, I really liked this post, but I wonder if you could have split it into pieces and made it a three-parter? I ask because I surf the web from work, and I can only read in 10 minute chunks... longer posts are harder for me to get through, and I like reading your writing.&quot; </li><li>Consider correcting someone privately while praising them publicly. This combination has been observed to engender loyalty and good feelings throughout history. I recently read an example of a samurai encouraging lords to do this from the early 1700&#x27;s book &quot;Hagakure.&quot; It works.</li><li>Consider dropping it altogether if it&#x27;s not a big deal. This about learning to prioritize - I had someone comment on my site thinking mistakenly that The Richest Man in Babylon and The Greatest Salesman in the World were by the same author. It wasn&#x27;t, but who cares? It makes no difference. It&#x27;s not worth pointing it out - almost everyone has an aversion to being corrected, so only do it if there&#x27;s actually tangible gain. Otherwise, go do something more important and not engender the potential bad will.</li></ul><p>Following some of these simple points will make you much more effective socially. I feel like a lot of times analytical and intelligent people study really hard, difficult problems, while ignoring basic considerations that have much more immediate and larger impact.<br/> </p><p><strong>Further reading:</strong></p><ul><li><a href=\"http://www.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/0688128165\">Influence: The Psychology of Persuasion</a></li><li><a href=\"http://www.amazon.com/How-Win-Friends-Influence-People/dp/0671723650\">How to Win Friends and Influence People</a></li><li><a href=\"http://www.amazon.com/Crucial-Conversations-Tools-Talking-Stakes/dp/0071401946\">Crucial Conversations: Tools For Talking When the Stakes Are High</a></li><li>Any and all books on negotiation you can find (I&#x27;d recommend you read at least five highly-reviewed books on negotiation to get different points of you) </li></ul><p></p><p>Edit: Lots of comments on this. 130 and counting. The most common criticism seems to be that adding fluff is a waste of time, insincere, and reduces signal:noise ratio. I&#x27;d encourage you to actually try it instead of just guessing - a quick word of thanks or encouragement before criticizing creates a more friendly, cooperative environment and works well. It doesn&#x27;t take very long, and it doesn&#x27;t detract from S:N ratio much, if at all.</p><p>Don&#x27;t just guess here. Try it out for a month. I think you&#x27;ll be amazed at how differently people react to you, and the uptake on your suggestions and feedback and ability to convince and teach people. Of course, you can construct examples of going overboard and it being silly. But that&#x27;s not required - just try to make everything 10% more gracious, and watch how much your effectiveness increases.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"chuP2QqQycjD8qakL": 1, "MXcpQvaPGtXpB6vkM": 1, "9YFoDPFwMoWthzgkY": 1, "SEuoBQeHLYd9dtqpK": 1, "uL87Bw3TKzsYFMpZp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GG2rtBReAm6o3mrtn", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 143, "baseScore": 115, "extendedScore": null, "score": 0.000204, "legacy": true, "legacyId": "4142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 115, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to: <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">Rationalists Should Win</a>, <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">Why Our Kind Can't Cooperate,</a> <a href=\"/lw/5t/can_humanism_match_religions_output/ \">Can Humanism Match Religion's Output?</a>, <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>, Paul Graham's \"<a href=\"http://www.paulgraham.com/nerds.html\">Why Nerds Are Unpopular</a>\"</p><p>The \"Prisoner's Dilemma\" refers to a game theory problem developed in the 1950's. Two prisoners are taken and interrogated separately. If either of them confesses and betrays the other person - \"defecting\" - they'll receive a reduced sentence, and their partner will get a greater sentence. However, if <em>both</em> defect, then they'll both receive higher sentences than if neither of them confessed.</p><p>This brings the prisoner to a strange problem. The best solution individually is to defect. But if both take the individually best solution, then they'll be worst off overall. This has wide ranging implications for international relations, negotiation, politics, and many other fields.</p><p>Members of LessWrong are incredibly smart people who tend to like game theory, and debate and explore and try to understand problems like this. But, does knowing game theory actually make you more effective in real life?</p><p>I think the answer is yes, with a caveat - you need the basic social skills to implement your game theory solution. The worst-case scenario in an interrogation would be to \"defect by accident\" - meaning that you'd just blurt out something stupidly because you didn't think it through before speaking. This might result in you and your partner both receiving higher sentences... a very bad situation. Game theory doesn't take over until basic skill conditions are met, so that you could actually execute any plan you come up with.</p><p><strong>The Purpose of This Post</strong>: I think many smart people \"defect\" by accident. I don't mean in serious situations like a police investigation. I mean in casual, everyday situations, where they tweak and upset people around them by accident, due to a lack of reflection of desired outcomes.</p><p>Rationalists should win. Defecting by accident frequently results in losing. Let's examine this phenomenon, and ideally work to improve it.</p><p>Contents Of This Post</p><ul><li>I'll define \"defecting by accident.\"</li><li>I'll explain a common outcome of defecting by accident.</li><li>I'll give some recent, mild examples of accidental defections.</li><li>I'll give examples of how to turn accidental defections into cooperation.</li><li>I'll give some examples of how this can make you more successful at your goals.</li><li>I'll list some books I recommend if you decide to learn more on the topic.</li></ul><p><strong id=\"Background___On_Analytical_Skills_and_Rhetoric\">Background - On Analytical Skills and Rhetoric</strong></p><p>From Paul Graham's \"<a href=\"http://www.paulgraham.com/nerds.html\">Why Nerds Are Unpopular</a>\" -</p><blockquote>I know a lot of people who were nerds in school, and they all tell the same story: there is a strong correlation between being smart and being a nerd, and an even stronger inverse correlation between being a nerd and being popular. Being smart seems to make you unpopular.<br> </blockquote><blockquote>[...]<br> </blockquote><blockquote>The key to this mystery is to rephrase the question slightly. Why don't smart kids make themselves popular? If they're so smart, why don't they figure out how popularity works and beat the system, just as they do for standardized tests?<br> </blockquote><blockquote>[...]<br> </blockquote><blockquote>So if intelligence in itself is not a factor in popularity, why are smart kids so consistently unpopular? The answer, I think, is that they don't really want to be popular.<br> </blockquote><blockquote>If someone had told me that at the time, I would have laughed at him. Being unpopular in school makes kids miserable, some of them so miserable that they commit suicide. Telling me that I didn't want to be popular would have seemed like telling someone dying of thirst in a desert that he didn't want a glass of water. Of course I wanted to be popular.<br> </blockquote><blockquote>But in fact I didn't, not enough. There was something else I wanted more: to be smart.</blockquote><p>I believe that \"defecting by accident\" is a result of not learning how different phrasing of words and language can dramatically effect how well your point is taken. It's been a general observation of mine that a lot of people in highly intellectual disciplines like mathematics, physics, robotics, engineering, and computer science/programming look down on social skills.</p><p>Of course, they wouldn't phrase it that way. They'd say they don't have time for it - they don't have time for gossip, or politics, or sugarcoating. They might say, \"I'm a realist\" or \"I say it like it is.\"</p><p>I believe this is a result of not realizing how big the difference in your effectiveness will be depending on how you phrase things, in what order, how well you appeal to another person's emotions. People in highly analytical disciplines often care about \"just the facts\" - but, let's face it, we highly analytical people are a great minority of the population.</p><p>Sooner or later, you're going to have something you care about and you're going to need to persuade someone who is not highly analytical. At that point, you run some serious risks of failure if you don't understand basic social skills.</p><p>Now, most people would claim that they have basic social skills. But I'm not sure this is borne out by observation. This used to be a very key part of any educated person's studies: rhetoric. From <a href=\"http://en.wikipedia.org/wiki/Rhetoric\">Wikiedpia</a>: \"<strong>Rhetoric</strong> is the art of using language to communicate effectively and persuasively. ... From ancient Greece to the late 19th Century, it was a central part of Western education, filling the need to train public speakers and writers to move audiences to action with arguments.\"</p><p>Rhetoric is now frequently looked down upon by highly intelligent and analytical people. Like Paul Graham says, it's not that intellectuals can't learn it. It's that they think it's not a good use of their time, that they'd rather be smart instead.</p><p><strong id=\"Defecting_by_Accident\">Defecting by Accident</strong></p><p>Thus, you see highly intelligent people do what I now term \"defecting by accident\" - meaning, in the process of trying to have a discussion, they insult, belittle, or offend their conversational partner. They commit obvious, blatant social faux pases, <em>not as a conscious decision of the tradeoffs, but by accident because they don't know better. </em></p><p>Sometimes defecting is the right course of action. Sometimes you need to break from whoever you're negotiating with, insist that things are done your way, even at their expense, and take the consequences that may arise from that.</p><p>But it's rarely something you should do by accident.</p><p>I'll give specific, clear examples in a moment, but before I do so, let's look at a general example of how this can happen.</p><p>If you're at a meeting and someone gives a presentation and asks if anyone has questions, and you ask point-blank, \"But we don't have the budget or skills to do that, how would we overcome that?\" - then, that seems like a highly reasonable question. It's probably very intelligent.</p><p>What normal people would consider, though, is how this affects the perception of everyone in the room. To put it bluntly - <em>it makes the presenter look very bad.</em></p><p>That's okay, if you decide that that's an acceptable part of what you're doing. But you now have someone who is likely to actively work to undermine you going forwards. A minor enemy. Just because you asked a question casually without thinking about it.</p><p>Interestingly, there's about a thousand ways you could be diplomatic and tactful to address the key issue you have - budgeting/staffing - without embarrassing the presenter. You could take them aside quietly later and express your concern. You could phrase it as, \"This seems like an amazing idea and a great presentation. I wonder how we could secure the budgeting and get the team for it, because it seems like it'd be a profitable if we do, and it'd be a shame to miss this opportunity.\"</p><p>Just by phrasing it that way, you make the presenter look good even if the option can't be funded or staffed. Instead of expressing your concern as a hole in their presentation, you express it as a challenge to be overcome by everyone in the room. Instead of your underlying point coming across as \"your idea is unfeasible,\" it comes across as, \"You've brought this good idea to us, and I hope we're smart enough to make it work.\"</p><p>If the real goal is just to make sure budgeting and funding is taken care of, there's many ways to do that without embarrassing and making an enemy out of the presenter. </p><p>Defecting by accident is lacking the awareness, tact, and skill to realize what the secondary effects of your actions are and act accordingly to win. </p><p>This is a relatively basic problem that the majority of \"normal\" people understand, at least on a subconscious level. Most people realize that you can't just show up a presenter and make them look bad. Or at least, you should expect them to be hostile to you if you do. But many intelligent people say, \"What the hell is his problem? I just asked a question.\"</p><p>This is due to a lack of understanding of social skills, diplomacy, tact, and yes, perhaps \"politics\" - which are unfortunately a reality of the world. And again, rationalists should win. If your actions are leading to hostility and defection against you, then you need to consider if your actions are the best possible.</p><p><strong id=\"_Why_Our_Kind_Can_t_Cooperate_\">\"Why Our Kind Can't Cooperate\"</strong></p><p>Eliezer's \"<a href=\"/lw/3h/why_our_kind_cant_cooperate/\">Why Our Kind Can't Cooperate</a>\" is a masterpiece. I'm only going to excerpt three parts, but I'd recommend the whole article.</p><p> </p><blockquote>From when I was still forced to attend, I remember our synagogue's annual fundraising appeal.  It was a simple enough format, if I recall correctly.  The rabbi and the treasurer talked about the shul's expenses and how vital this annual fundraise was, and then the synagogue's members called out their pledges from their seats.</blockquote><blockquote><br>Straightforward, yes?<br> </blockquote><blockquote>Let me tell you about a different annual fundraising appeal.  One that I ran, in fact; during the early years of a nonprofit organization that <u><a href=\"http://wiki.lesswrong.com/wiki/Topic_that_must_not_be_named\">may not be named</a></u>.  One difference was that the appeal was conducted over the Internet.  And another difference was that the audience was largely drawn from the atheist/libertarian/technophile/sf-fan/early-adopter/programmer/etc crowd.  (To point in the rough direction of an empirical cluster in personspace.  If you understood the phrase \"empirical cluster in personspace\" then you know who I'm talking about.)<br> </blockquote><blockquote>I crafted the fundraising appeal with care.  By my nature I'm too proud to ask other people for help; but I've gotten over around 60% of that reluctance over the years.  The nonprofit needed money and was growing too slowly, so I put some force and poetry into that year's annual appeal.  I sent it out to several mailing lists that covered most of our potential support base.<br> </blockquote><blockquote>And almost immediately, people started posting to the mailing lists about why they weren't going to donate.  Some of them raised basic questions about the nonprofit's philosophy and mission.  Others talked about their brilliant ideas for all the <em>other</em> sources that the nonprofit could get funding from, instead of them.  (They didn't volunteer to contact any of those sources <em>themselves</em>, they just had ideas for how <em>we</em> could do it.)<br> </blockquote><blockquote>Now you might say, \"Well, maybe your mission and philosophy <em>did</em> have basic problems - you wouldn't want to<em>censor </em>that discussion, would you?\"<br> </blockquote><blockquote>Hold on to that thought.<br> </blockquote><blockquote>Because people <em>were</em> donating.  We started getting donations right away, via Paypal.  We even got congratulatory notes saying how the appeal had finally gotten them to start moving.  A donation of $111.11 was accompanied by a message saying, \"I decided to give **** a little bit more.  One more hundred, one more ten, one more single, one more dime, and one more penny.  All may not be for one, but this one is trying to be for all.\"<br> </blockquote><blockquote>But none of those donors posted their agreement to the mailing list.  Not one.<br> <br>So far as any of those donors knew, they were alone.  And when they tuned in the next day, they discovered not thanks, but arguments for why they <em>shouldn't</em> have donated.  The criticisms, the justifications for not donating - <em>only those</em> were displayed proudly in the open.<br> </blockquote><blockquote>As though the treasurer had finished his annual appeal, and everyone <em>not</em> making a pledge had proudly stood up to call out justifications for refusing; while those making pledges whispered them quietly, so that no one could hear.</blockquote><p> </p><p>Indeed, that's a problem. Eliezer continues:</p><blockquote> \"It is dangerous to be half a rationalist.\"</blockquote><p> </p><p>And finally, this point, which is magnificent -</p><blockquote>Our culture puts all the emphasis on heroic disagreement and <u><a href=\"http://www.overcomingbias.com/2007/12/lonely-dissent.html\">heroic defiance</a></u>, and none on heroic agreement or heroic group consensus.  We signal our superior intelligence and our <em>membership in the nonconformist community</em> by inventing clever objections to others' arguments.  Perhaps <em>that </em>is why the atheist/libertarian/technophile/sf-fan/Silicon-Valley/programmer/early-adopter crowd stays marginalized, losing battles with less nonconformist factions in larger society.  No, we're not losing because we're so superior, we're losing because our exclusively individualist traditions sabotage our ability to cooperate.</blockquote><p><strong id=\"On_Being_Pedantic__Sarcastic__Disagreeable__Non_Complimentary__and_Otherwise_Defecting_by_Accident\">On Being Pedantic, Sarcastic, Disagreeable, Non-Complimentary, and Otherwise Defecting by Accident</strong></p><p>You might not realize it, but in almost all of human civilization it's considered insulting to just point out something wrong someone is doing without any preface, softening, or making it clear why you're doing it.</p><p>It's taken for granted in some blunt, \"say it like it is\" communities, but it's usually taken as a personal attack and a sign of animosity in, oh, 90%+ of the rest of civilization.</p><p>In these so-called \"normal people's societies,\" correcting them in front of their peers will be perceived as trying to lower them and make them look stupid. <em>Thus, they'll likely want to retaliate against you, or at least not cooperate with you.</em></p><p>Now, there's a time and place to do this anyways. Sometimes there's an emergency, and you don't have time to take care of people's feelings, and just need to get something done. But surfing the internet is not that time.</p><p>I'm going to take some example replies from a recent post I made to illustrate this. There's always a risk in doing this of not being objective, but I think it's worth it because (1) I tend to read every reply to me and carefully reflect on it for a moment, (2) I understand exactly my first reactions to these comments, and (3) I won't have to rehash criticisms of another person. Take a grain of salt with you since I'm looking at replies to myself originally, but I think I can give you some good examples.</p><p>The first thing I want to do is take a second to mention that almost everyone in the entire world gets emotionally invested in things they create, and are also a little insecure about their creations. It's extraordinarily rare that people don't care what others' think of their writing, science, or art. </p><p>Criticism has good and bad points. Great critics are rare, but they actually make works of creation even in critique. A great critic can give background, context, and highlight a number of relevant mainstream and obscure works through history that the piece they're critiquing reminds them of.</p><p>Good critique is an art of creation in and of itself. But bad critique - just blind \"that's wrong\" without explaining why - tends to be construed as a hostile action and not accomplish much, other than signalling that \"heroic disagreement\" that Eliezer talks about.</p><p>I recently wrote a post titled, <a href=\"/lw/36f/nahh_that_wouldnt_work\">\"Nahh, that wouldn't work\"</a>. I thought about it for around a week, then it took me about two hours to think it through, draw up key examples on paper, choose the most suitable, edit, and post it. It was generally well-received here on LW and on my blog.</p><p>I'll show you three comments on there, and how I believe they could be subtly tweaked.</p><p>1.</p><p> </p><blockquote> &gt; I wizened up,<br> I don't think that's the word you want to use, unless you're talking about how you finally lost those 20 pounds by not drinking anymore.</blockquote><p> </p><p>2.</p><blockquote>FWIW, I think posts like this are more valuable the more they include real-world examples; it's kind of odd to read a post which says I had theory A of the world but now I hold theory B, without reading about the actual observations. It would be like reading a history of quantum mechanics or relativity with all mentions of things like the laser or double-slit experiment or Edding or Michelson-Morley removed.</blockquote><p>3.</p><blockquote>An interesting start, but I would rather see this in Discussion -- it's not fully adapted yet, I think...</blockquote><p>Now, I spend a lot of time around analytical people, so I take no offense at this. But I believe these are good examples of what I'd call \"accidental defection\" - this is the kind of thing that produces a negative reaction in the person you're talking to, perhaps without you even noticing.</p><p>#1 is kind of clever pointing out a spelling error. But you have to realize, in normal society that's going to upset and make hostile the person you're addressing. Whether you mean to or not, it comes across as, \"I'm demonstrating that I'm more clever than you.\"</p><p>There's a few ways it could be done differently. For instance, an email that says, \"Hey Sebastian, I wanted to give you a heads up. I saw your recent post, but you spelled \"wisen\" as \"wizen\" - easy spelling error to make, since they're uncommonly used words, but I thought you should know. \"Wizen\" means for things to dry up and lose water. Cheers and best wishes.\"</p><p>That would point out the error (if that's the main goal), and also engender a feeling of gratitude in whoever received it (me, in this case). Then I would have written back, \"Hey, thanks... I don't worry about spelling too much, but yeah that one's embarrassing, I'll fix it. Much appreciated. Anyways, what are you working on? How can I help?\"</p><p>I know that's how I'd have written back, because that's how I generally write back to someone who tries to help me out. Mutual goodwill, it's a virtuous cycle.</p><p>Just pointing out someone is wrong in a clever way usually engenders bad will and makes them dislike you. The thing is, I know that's not the intention of anyone here - hence, \"defecting by accident.\" Analytical people often <em>don't even realize they're showing someone up </em>when they do it.</p><p>I'm not particularly bothered. I get the intent behind it. But normal people are going to be ultra-hostile if you do it to them. There's other ways, if you feel the need to point it out publicly. You could \"soften\" it by praising first - \"Hey, some interesting points in this one... I've thought about a similar bias of not considering outcomes if I don't like what it'd mean by the world. By the way, you probably didn't mean wizen there...\" - or even just saying, \"I think you meant 'wisen' instead of 'wizen'\" - with links to the dictionary, maybe. Any of those would go over better with the original author/presenter whom you're pointing out the error to.</p><p>Let's look at point #2. \"FWIW, I think posts like this are more valuable the more they include real-world examples; it's kind of odd to read a post which says I had theory A of the world but now I hold theory B, without reading about the actual observations.\"</p><p>This is something which makes people trying to help or create shake their head. See, it's potentially a good point. But after someone takes some time to create something and give it away for free, then hearing, \"Your work would be more valuable if you did (xyz) instead. Your way is kind of odd.\"</p><p>People generally don't like that.</p><p>Again, it's trivially easy to write that differently. Something like, \"Thanks for the post. I was wondering, you mentioned (claim X), but I wonder if you have any examples of claim X so I can understand it better?\"</p><p>That one has - gratitude, no unnecessary criticism, explains your motivation. All of which are good social skill points, especially the last one as written about in Cialdini's \"Influence\" - give a reason why.</p><p>#3 - \"An interesting start, but I would rather see this in Discussion -- it's not fully adapted yet, I think...\"</p><p>Okay. Why?</p><p>The difference between complaining and constructive work is looking for solutions. So, \"There's some good stuff in here, but I think we could adapt it more. One thing I was thinking is (main point).\"</p><p><strong id=\"Becoming_More_Self_Aware_and_Strategic__Some_Practical_Social_Guidelines\">Becoming More Self-Aware and Strategic; Some Practical Social Guidelines</strong></p><p>From Anna Salamon's \"<a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans Are Not Automatically Strategic</a>\" - </p><blockquote> But there are clearly also heuristics that would be useful to goal-achievement (or that would be part of what it means to \u201chave goals\u201d at all) that we do <em>not</em> automatically carry out.  We do <em>not</em> automatically:</blockquote><ul><li>(a) Ask ourselves what we\u2019re trying to achieve; </li><li>(b) Ask ourselves how we could tell if we achieved it (\u201cwhat does it look like to be a good comedian?\u201d) and how we can track progress; </li><li>(c) Find ourselves strongly, intrinsically curious about information that would help us achieve our goal; </li></ul><p>Anna points out that people don't automatically ask what they're trying to achieve. You don't, necessarily, ask what you're trying to achieve.</p><p>But I would recommend you do ask that before speaking up socially. At least for a while, until you've got the general patterns figured out.</p><p>If you don't, you run the risk of antagonizing and making people hostile to you who would otherwise cooperate and work with you. </p><p>Now, I've heard smart people say, \"I don't have time for that.\" This is akin to saying, \"I don't have time to achieve what I want to achieve.\"</p><p>Because it doesn't take much time, and it makes you <em>much </em>more effective. Asking, \"What am I trying to achieve here?\" goes a long way.</p><p>When commenting on a discussion site, who are you writing for? For the author? For the regular readers? What's your point in replying? If your main point is just to \"get to truth and understanding,\" then what should your secondary considerations be? If there's a conflict between the two, would you prefer to encourage the author to write more, or to look clever by pointing out a pedantic point?</p><p>I understand where you're coming from, because I used to come from the same place. I was the kid who argued with teachers when they were wrong, not realizing the long term ramifications of that. People matter, and people's feelings matter, <em>especially</em> if they have sway over your life, but even if they don't have sway over your life.</p><p>To that, here's some suggestions I think would make you more effective:</p><ul><li>Generally, be gracious and thankful. This goes immensely far. Things like starting a reply with, \"Thanks for this\" or \"Thanks for sharing these insights.\"</li><li>Praising someone makes it more likely they'll accept your criticisms. \"I thought your point A was excellent, however point B...\"</li><li>If you're going to disagree, summarize the person's main argument beforehand - this has a few positive effects. First, it forces you make sure you actually understand. Second, if the author has a different main point and wasn't clear, that comes out. Third, it shows some respect that you actually took the time to read and understand the post. So you could write, \"I know your main argument is A, but I wanted to explore your minor point X.\"</li><li>If you think something is wrong, give an explanation of what would be correct and better. \"I enjoyed this post a lot - thanks for that - but one thing that's tough for me is that all the examples are about martial arts, and I don't really understand martial arts so much. Maybe next time you could provide some examples from other fields? For instance, I remember reading you're an accountant and you write poetry, maybe some examples from there?\"</li><li>If you point out something is wrong, do your best to make the mistake-maker not feel stupid. This makes them massively appreciate that. \"Hey, you got your math on example X wrong... I think it actually works to 11.7. Anyways, I only recognize that because I made that mistake dozens of times myself, it's a common one to make, just wanted to point it out.\"</li><li>Explain why you care about a point. This has a few positive effects. First, it lets the author cater a reply to exactly what you want. Second, you'd be amazed at how many people assume evil intent and worst-possible motives - it neutralizes that. Third, it forces you to think through how you'd like things to be, which is again good. \"Hey man, I really liked this post, but I wonder if you could have split it into pieces and made it a three-parter? I ask because I surf the web from work, and I can only read in 10 minute chunks... longer posts are harder for me to get through, and I like reading your writing.\" </li><li>Consider correcting someone privately while praising them publicly. This combination has been observed to engender loyalty and good feelings throughout history. I recently read an example of a samurai encouraging lords to do this from the early 1700's book \"Hagakure.\" It works.</li><li>Consider dropping it altogether if it's not a big deal. This about learning to prioritize - I had someone comment on my site thinking mistakenly that The Richest Man in Babylon and The Greatest Salesman in the World were by the same author. It wasn't, but who cares? It makes no difference. It's not worth pointing it out - almost everyone has an aversion to being corrected, so only do it if there's actually tangible gain. Otherwise, go do something more important and not engender the potential bad will.</li></ul><p>Following some of these simple points will make you much more effective socially. I feel like a lot of times analytical and intelligent people study really hard, difficult problems, while ignoring basic considerations that have much more immediate and larger impact.<br> </p><p><strong id=\"Further_reading_\">Further reading:</strong></p><ul><li><a href=\"http://www.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/0688128165\">Influence: The Psychology of Persuasion</a></li><li><a href=\"http://www.amazon.com/How-Win-Friends-Influence-People/dp/0671723650\">How to Win Friends and Influence People</a></li><li><a href=\"http://www.amazon.com/Crucial-Conversations-Tools-Talking-Stakes/dp/0071401946\">Crucial Conversations: Tools For Talking When the Stakes Are High</a></li><li>Any and all books on negotiation you can find (I'd recommend you read at least five highly-reviewed books on negotiation to get different points of you) </li></ul><p></p><p>Edit: Lots of comments on this. 130 and counting. The most common criticism seems to be that adding fluff is a waste of time, insincere, and reduces signal:noise ratio. I'd encourage you to actually try it instead of just guessing - a quick word of thanks or encouragement before criticizing creates a more friendly, cooperative environment and works well. It doesn't take very long, and it doesn't detract from S:N ratio much, if at all.</p><p>Don't just guess here. Try it out for a month. I think you'll be amazed at how differently people react to you, and the uptake on your suggestions and feedback and ability to convince and teach people. Of course, you can construct examples of going overboard and it being silly. But that's not required - just try to make everything 10% more gracious, and watch how much your effectiveness increases.</p>", "sections": [{"title": "Background - On Analytical Skills and Rhetoric", "anchor": "Background___On_Analytical_Skills_and_Rhetoric", "level": 1}, {"title": "Defecting by Accident", "anchor": "Defecting_by_Accident", "level": 1}, {"title": "\"Why Our Kind Can't Cooperate\"", "anchor": "_Why_Our_Kind_Can_t_Cooperate_", "level": 1}, {"title": "On Being Pedantic, Sarcastic, Disagreeable, Non-Complimentary, and Otherwise Defecting by Accident", "anchor": "On_Being_Pedantic__Sarcastic__Disagreeable__Non_Complimentary__and_Otherwise_Defecting_by_Accident", "level": 1}, {"title": "Becoming More Self-Aware and Strategic; Some Practical Social Guidelines", "anchor": "Becoming_More_Self_Aware_and_Strategic__Some_Practical_Social_Guidelines", "level": 1}, {"title": "Further reading:", "anchor": "Further_reading_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "432 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 432, "af": false, "version": "1.2.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC", "3fNL2ssfvRzpApvdN", "PBRWb2Em5SNeWYwwB", "682i9R2oSRg7BG8yD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T16:32:25.785Z", "modifiedAt": null, "url": null, "title": "Gender Identity and Rationality", "slug": "gender-identity-and-rationality", "viewCount": null, "lastCommentedAt": "2019-06-27T20:05:11.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lucidfox", "createdAt": "2010-11-22T06:58:06.993Z", "isAdmin": false, "displayName": "lucidfox"}, "userId": "hNnKSqvajCMRj9eKK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/axgfcSEQwaaAn4d3Z/gender-identity-and-rationality", "pageUrlRelative": "/posts/axgfcSEQwaaAn4d3Z/gender-identity-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/axgfcSEQwaaAn4d3Z/gender-identity-and-rationality", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gender%20Identity%20and%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGender%20Identity%20and%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaxgfcSEQwaaAn4d3Z%2Fgender-identity-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gender%20Identity%20and%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaxgfcSEQwaaAn4d3Z%2Fgender-identity-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaxgfcSEQwaaAn4d3Z%2Fgender-identity-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1753, "htmlBody": "<p>Not sure if I would be better off posting this on the main page instead, but since it's almost entirely about my personal experiences, here it goes.</p>\n<p>Two years ago, I underwent a radical change in my worldview. A series of events caused me to completely re-evaluate my beliefs in everything related to gender, sexuality, tolerance, and diversity -- which in turn caused a cascade that made me rethink my stance on many other topics.</p>\n<p>Coincidentally, the same events caused me to also rethink the way I thought of myself. This was, as it turned out, not very good. It still makes it difficult for me to untangle various consequences, correlated but potentially not directly bound by a cause-effect relation.</p>\n<p>To be more blunt: being biologically male, I confessed to someone online about things that things that \"men weren't supposed to do\": my dissatisfaction with my body, my wish to have a female body, persistent fantasies of a sex change, desires to shave my body, grow long hair and wear women's clothes, and so on and so forth. She listened, and then asked, \"Maybe you're transsexual?\"</p>\n<p>Back then, it would never even occur to me to think of that -- and my first gut response, which I'm not proud of, was denying association with \"those freaks\". As I understand now, I was relying on a cached thought, and it limited the scope of my reasoning. She used simple intuitive reasoning to arrive at the hypothesis based on what I revealed to her; I didn't know the hypothesis was even there, as I knew nothing about gender identity.</p>\n<p>In the events that unfolded, I integrated myself into some LGBT communities and learned about all kinds of people, including those who didn't fit into notions of the gender binary at all. I've learned to view gender as a multidimensional space with two big clusters, rather than as a boolean flag. It felt incredibly heartwarming to be able to mentally call myself by a female name, to go by it on the Internet, to talk to like-minded people who had similar experiences and feelings, and to be referred by the pronoun \"she\" -- which at first bugged me, because I somehow felt I had \"no moral right\" or had to \"earn that privilege\", but quickly I got at ease with it, and soon it just felt ordinary, and like the only acceptable thing to do, the only way of presentation that felt right.</p>\n<p>(I'm compressing and simplifying here for the sake of readability -- I'm skipping over the brief period after that conversation when I thought of myself as genderless, not yet ready to accept a fully female gender identity, and carried out thought experiments with imaginary conversations between my \"male\" and \"female selves\", before deciding that there was no male self to begin with after all.)</p>\n<p>Nowadays, gender-wise, I address people the way they wish to be address. I also have some pretty strong opinions on the legal concept of gender, which I won't voice here. And I've learned a lot, and was able to drive my introspection deeper than I ever managed before... But that's not really relevant.</p>\n<p>And yet... And yet.</p>\n<p>As gleefully as I embraced a female role, feeling on the way to fulfilling my dream, I couldn't get out the nagging feeling of being somehow \"fake\". I kept thinking that I don't always \"think like a real woman would\", and I've had days of odd apathy when I didn't care about anything, including my gender presentation. Some cases happened even before my gender \"awakening\", and at those days, I felt empty and genderless, a drained shell of a person.</p>\n<p>How, in all honesty, can I know if I'm \"really a woman on the inside\"? What does that even mean? I can speak in terms of desired behavior, in terms of the way I'm seen socially, from the outside. But how can I compare my subjective experience to those of different men and women, without getting into their heads? All I have is empathic inference, which works by building crude, approximate models of other people inside my head, and is so full of ill-defined biases that I have a suspicion I shouldn't rely on it at all and don't say things like \"well, a man's subjective experience is way off for me, but a woman's subjective experience only weakly fits\".</p>\n<p>And yet... transpeople report \"feeling like\" their claimed gender. I prefer to work with more unambiguous subjective feelings -- like feeling I have a wrong body -- but I have caught myself thinking at different times, \"This day I felt like a woman, and that day I didn't feel like a woman, but more like... nothing at all. And that other day my mind was occupied with completely different matters, like writing a Less Wrong post.\" It helps sometmes to visualize my brain as a system of connected logical components, with an \"introspection center\" as a separate component, but that doesn't bring me close to solving the mystery.</p>\n<p>I want to be seen as a woman, and nothing else. I take steps to ensure that it happens. If I could start from a clean slate, magically get an unambiguously female body, and live somewhere where nobody would know about my past male life, perhaps that would be the end of it -- there would be no need for me to worry about it anymore. But as things stand, my introspection center keeps generating those nagging thoughts: \"What if I'm merely a pretender, a man who merely thinks he's a woman, but isn't?\" One friend of mine postulated that \"wanting to be a gender is the same as being it\"; but is it really that simple?</p>\n<p>The sheer number of converging testimonies between myself and transpeople I've met and talked to would seem to rule that out. \"If I'm fake, then they're fake too, and surely that sounds extremely unlikely.\" But while discovering similarities makes me generically happy, every deviation from the mean -- for example, I consciously discovered my gender identity at 21, a relatively late age -- stings painfully and brings up the uncertainty again. Could this be a case of failing to properly assign Bayesian weights, of giving evidence less significance than counterevidence? But every time I discovered a piece of counterevidence, my mind interpreted it as a breach of my mental defenses and tried to route around it, in other words, rationalize it away.</p>\n<p>Maybe I could just tell myself, \"Shut up and live the way you want to.\"</p>\n<p>And yet...</p>\n<p>I caught myself in thinking that I really, deeply didn't want to go back, to the point that I didn't want to accept the conclusion \"I'm really a man and an impostor\",  even that time when it looked like evidence weighted that way. (It's no longer the case now that I've learned more facts, but the point still stands.) It was an unthinkable thought, and still is. Even now, I fail to apply the Litany of Tarski. \"If I'm really a man, then I desire to bel--\" Wait, doesn't compute. If that were true, it would cause my whole system of values to collapse, and it feels like stating an incoherent statement, like \"If sexism is morally and scientifically justified, then...\" It feels like it would cause my entire system of values to collapse, and I can't bring myself to think that -- but isn't that the danger of \"already knowing the answer\", rationalizing, etc.?</p>\n<p>It also bugs me, I guess, that despite relying on rational reasoning in so many aspects of my daily life, with this one case, about an aspect of myself, I'm relying on some subjective, vague \"gut feeling\". Granted, I try to approach it in a rational way: someone used my revelations to locate a hypothesis, I found it likely based on the evidence and accepted it, then started updating... or did I? Would I really be able to change my belief even in principle? And even then, the root cause, the very root cause, comes from feelings of uneasiness with my assigned gender role that I cannot rationally explain -- they're just there, in the same way that my consciousness is \"just there\".</p>\n<p>So...</p>\n<p>When I heard about p-zombies, I immediately drew parallels. I asked myself if \"fake transpeople\" were even a coherent concept. Would it be possible to imagine two people who behave identically (and true to themselves, not acting), except one has \"real\" subjective feelings of gender and the other doesn't? After applying an appropriately tweaked anti-zombie argument, it seems to me that the answer is no, but it's also prossible that the question is too ill-defined for any answer to make sense.</p>\n<p>The way it stands now, the so-called gender identity disorder isn't really something that is truly diagnosed, because it's based on self-reporting; you cannot look into someone's head and say \"you're definitely transsexual\" without their conscious understanding of themselves and their consent. So it seems to me outside the domain of psychiatry in the first place. I've heard some transpeople voice hope that there could be a device that could scan the part of the brain responsible for gender identity and say \"yes, this one is definitely trans\" and \"no, this one definitely isn't\". But to me, the prospect of such a device horrifies me even in principle. What if the device conflicts their self-reporting? (I suspect I'm anxious about the possibility of it filtering me, specifically.) What should we consider more reliable -- the machine or self-reporting? On one hand, we know how filled human brains are with cognitive biases, but on the other hand, it seems to me like a truism that \"you are the final authority in your own self-identification.\"</p>\n<p>Maybe it's a question of definitions, like the question about a tree making a sound, and the final answer depends on how exactly we define \"gender identity\". Or maybe -- this thought occurred to me right now -- my decision agent has a gender identity while my introspection center (which operates entirely on abstract knowledge rather than social conventions) doesn't, and that's the cause of the confusion that I get from looking at things in both a gendered and genderless way, in the same way as if I would be able to switch at will between a timed view from inside the timeline and a timeless view of the entire 4D spacetime at once. In any case, so far, for those two years since the realization I've stuck with the identity and role that I at least believe is the only one I won't regret assuming.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 1, "W9aNkPwtPhMrcfgj7": 1, "HXA9WxPpzZCCEwXHT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "axgfcSEQwaaAn4d3Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 54, "extendedScore": null, "score": 0.000106, "legacy": true, "legacyId": "4144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T17:17:48.713Z", "modifiedAt": null, "url": null, "title": "How to Save the World", "slug": "how-to-save-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.765Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TrmMcujGZt5JAtMGg/how-to-save-the-world", "pageUrlRelative": "/posts/TrmMcujGZt5JAtMGg/how-to-save-the-world", "linkUrl": "https://www.lesswrong.com/posts/TrmMcujGZt5JAtMGg/how-to-save-the-world", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Save%20the%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Save%20the%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrmMcujGZt5JAtMGg%2Fhow-to-save-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Save%20the%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrmMcujGZt5JAtMGg%2Fhow-to-save-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrmMcujGZt5JAtMGg%2Fhow-to-save-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1999, "htmlBody": "<p>Most of us want to make the world a better place. But what should we do if we want to generate the most positive impact possible? It&rsquo;s definitely not an easy problem. Lots of smart, talented people with the best of intentions have tried to <a rel=\"nofollow\" href=\"http://www.codepink4peace.org/\">end war</a>, <a rel=\"nofollow\" href=\"http://www.one.org/\">eliminate poverty</a>, <a rel=\"nofollow\" href=\"http://www.redcross.org/\">cure disease</a>, <a rel=\"nofollow\" href=\"http://www.oxfam.org/\">stop hunger</a>, <a rel=\"nofollow\" href=\"http://www.peta.org/\">prevent animal suffering</a>, and <a rel=\"nofollow\" href=\"http://www.greenpeace.org/\">save the environment</a>. As you may have noticed, we&rsquo;re still working on all of those. So the track record of people trying to permanently solve the world's biggest problems isn&rsquo;t that spectacular. This isn&rsquo;t just a &ldquo;look to your left, look to your right, one of you won&rsquo;t be here next year&rdquo;-kind of thing, this is more like &ldquo;behold the trail of dead and dying who line the path before you, and despair&rdquo;. So how can you make your attempt to save the world turn out significantly better than the generations of others who've tried this already?<br /><br />It turns out there actually are a number of things we can do to substantially increase our odds of doing the most good. Here's a brief summary of some on the most crucial considerations that one needs to take into account when soberly approaching the task of doing the most good possible (aka \"saving the world\").<br /><br /><strong>1. <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Patch your moral intuition</a> </strong>(<strong><a href=\"/lw/n3/circular_altruism/\">with math!</a></strong>) - Human moral intuition is really useful. But it tends to fail us at precisely the wrong times -- like <a href=\"/lw/hw/scope_insensitivity/\">when a problem gets too big</a> [&ldquo;millions of people dying? *yawn*&rdquo;] or <a href=\"/lw/mz/zut_allais/\">when it involves uncertainty</a> [&ldquo;you can only save 60% of them? call me when you can save <em>everyone</em>!&rdquo;]. Unfortunately, these happen to be the defining characteristics of the world&rsquo;s most difficult problems. Think about it. If your standard moral intuition were enough to confront the world&rsquo;s biggest challenges, they wouldn&rsquo;t be the world&rsquo;s biggest challenges anymore... they&rsquo;d be &ldquo;those problems we solved already cause they were natural for us to understand&rdquo;. If you&rsquo;re trying to do things that have never been done before, use all the tools available to you. That means setting aside your emotional numbness by <a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">using math to feel</a> what <a href=\"http://wiki.lesswrong.com/wiki/Scope_insensitivity\">your moral intuition can&rsquo;t</a>. You can also do better by acquainting yourself with <a href=\"/lw/jg/planning_fallacy/\">some</a> <a href=\"/lw/m9/aschs_conformity_experiment/\">of</a> <a href=\"/lw/my/the_allais_paradox/\">the</a> <a href=\"/lw/ml/but_theres_still_a_chance_right/\">more</a> <a href=\"/lw/hz/correspondence_bias/\">common</a> <a href=\"http://wiki.lesswrong.com/wiki/Biases\">human biases</a>. It turns out <a href=\"/lw/34m/what_ive_learned_from_less_wrong/\">your brain isn't always right</a>. Yes, even <em>your</em> brain. So knowing the ways in which it systematically gets things wrong is a good way to avoid making the most obvious errors when setting out to help save the world.<br /><br /><strong>2. <a href=\"/lw/37f/efficient_charity/\">Identify a cause with lots of leverage</a></strong> - It&rsquo;s noble to try and save the world, but it&rsquo;s ineffective and unrealistic to try and do it <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">all on your own</a>. So let&rsquo;s start out by <a href=\"/lw/5j/your_price_for_joining/\">joining forces</a> with an established organization who&rsquo;s already working on what you care about. Seriously, unless you&rsquo;re already <a href=\"http://www.gatesfoundation.org/\">ridiculously rich + brilliant</a> or <a href=\"http://www.clintonglobalinitiative.org/\">ludicrously influential</a>, going solo or further fragmenting the philanthropic world by <a href=\"http://philanthropy.com/blogs/government-and-politics/number-of-charitiesfoundations-passes-12-million/21832\">creating US-Charity#1,238,202</a> is almost certainly a mistake. Now that we&rsquo;re <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">all working together here</a>, let's keep in mind that only a few charitable organizations are <a href=\"http://www.givewell.org/\">truly great investments</a> -- and the <a rel=\"nofollow\" href=\"http://www.charitynavigator.org/index.cfm?bay=search.summary&amp;orgid=6488\">vast</a> <a rel=\"nofollow\" href=\"http://www.charitynavigator.org/index.cfm?bay=search.summary&amp;orgid=8195\">majority</a> <a rel=\"nofollow\" href=\"http://www.charitynavigator.org/index.cfm?bay=search.summary&amp;orgid=11776\">just</a> <a rel=\"nofollow\" href=\"http://www.charitynavigator.org/index.cfm?bay=search.summary&amp;orgid=8747\">aren&rsquo;t</a>. So maximize your leverage by investing your time and money into supporting <a href=\"http://reg-charity.org/effective-giving/\">the best non-profits</a> with the <a href=\"http://reg-charity.org/effective-giving/\">largest expected pay-offs</a>. <a id=\"more\"></a><br /><br /> <strong>3. <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Don&rsquo;t confuse what &ldquo;feels good&rdquo; with what actually helps the most</a> </strong>- Wanna know something that feels good? <a href=\"http://www.kiva.org/lender/louiehelm\">I fund micro-loans</a> on Kiva. It&rsquo;s a ridiculously cheap way to feel good about helping people. It totally plays into this romantic story I have in my mind about helping business owners help themselves. And there&rsquo;s lots of shiny pictures of people I can identify with. But does loaning $25 to someone on the other side of the planet really make the biggest impact possible? Definitely not. So I fund a few Kiva loans a month because it fulfills a deep-seated psychological need of mine -- a need that doesn&rsquo;t go away by ignoring it or pretending it doesn&rsquo;t exist. But once that&rsquo;s out of the way, I devote the vast majority of my time and resources to contributing to <a href=\"http://reg-charity.org/\">other non-profits</a> with <a href=\"http://reg-charity.org/effective-giving/\">staggeringly higher pay-offs</a>.<br /><br /> <strong>4. <a href=\"http://eztravelpad.typepad.com/eztravelpad/2010/01/happy-new-year-and-some-interesting-thoughts-on-giving-back.html\">Don&rsquo;t be a &ldquo;cause snob&rdquo;</a></strong> - This one's tough. The <a href=\"/lw/lg/the_affect_heuristic/\">more you begin to care</a> about a cause, <a href=\"/lw/lj/the_halo_effect/\">the more difficult it becomes</a> not to be self-righteous about it.&nbsp; The problem doesn&rsquo;t go away just because <em>you really <strong>do</strong> have a great cause</em>... <a href=\"/lw/lm/affective_death_spirals/\">it only gets worse</a>. Resist the temptation to kick dirt in the faces of others who are doing something different. There are always <a href=\"http://www.fhi.ox.ac.uk/\">other ways to help</a> no matter what philanthropic cause you're involved with. And everyone starts out somewhere. 15 years ago, I was <a href=\"http://www.scribd.com/doc/3511074/Anarchy-Cookbook-Version-2000\">optimizing for anarchy</a>. Things change. And even if they don't, people deserve your respect regardless of whether they want to help save the world or not. We're entitled to nothing and no one. Our fortunes will rise and fall based on our abilities, including <a href=\"/lw/372/defecting_by_accident_a_flaw_common_to_analytical/\">the ability</a> to <a href=\"/lw/1ln/a_suite_of_pragmatic_considerations_in_favor_of/\">be nice</a> -- not the intrinsic goodness of our causes.<br /><br /> <strong>5. <a href=\"http://louie.divide0.net/being-generally-capable.pdf\">Be more effective</a></strong> - You know how sometimes you get <a href=\"/lw/1sm/akrasia_tactics_review/\">stuck in motivational holes</a>, end up <a href=\"/lw/2as/diseased_thinking_dissolving_questions_about/\">sick all the time</a>, and have trouble <a href=\"http://www.amazon.com/gp/product/0142000280/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0142000280&amp;linkCode=as2&amp;tag=lesswrong-20\">getting things done</a>? That&rsquo;s gonna happen to everyone, every now and then. But if it&rsquo;s an everyday kind of thing for you, check out <a href=\"http://www.amazon.com/gp/product/0307465357/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0307465357&amp;linkCode=as2&amp;tag=lesswrong-20\">some helpful</a> <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">resources</a> <a href=\"http://dirtsimple.org/\">that can</a> <a href=\"http://thinkingthingsdone.com/\">get you</a> <a href=\"http://www.stevepavlina.com/\">unstuck</a>. This is incredibly important because the steps up until now only depended on what you believed and what your priorities were. But your beliefs and priorities won&rsquo;t even get you through the day, much less help you save the world. You're gonna need to <a href=\"http://www.43things.com/\">formulate goals</a> and be able to act on them. Becoming <a href=\"http://louie.divide0.net/being-generally-capable.pdf\">more capable</a>, <a href=\"http://www.activeinboxhq.com/\">more organized</a>, <a href=\"http://www.amazon.com/gp/product/0671027034/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0671027034&amp;linkCode=as2&amp;tag=lesswrong-20\">more well-connected</a>, and <a href=\"http://www.amazon.com/gp/product/1451639619/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=1451639619&amp;linkCode=as2&amp;tag=lesswrong-20\">more motivated</a> is an essential part of saving the world. Your goals aren&rsquo;t going to just accomplish themselves <a href=\"/lw/uh/trying_to_try/\">the first time you &ldquo;try&rdquo;</a>. If you want to succeed, you&rsquo;ll likely have to fail a bunch first, and then <a href=\"/lw/ui/use_the_try_harder_luke/\">try harder</a>.<br /><br /> <strong>6. <a href=\"http://www.youtube.com/watch?v=Yd9cf_vLviI\">Spread awareness</a></strong> - This is a necessary meta-strategy no matter what you&rsquo;re trying to accomplish. Remember, deep down, most people really <em>do</em> want to find a way to help others or save the world. They just might not be looking for it all the time. So tell people what you&rsquo;re up to and if they want to know more, tell them that too. You shouldn&rsquo;t expect everyone to join you, but you should at least give people a chance to surprise you. And there are other less obvious things you can do, like join <a href=\"http://www.xrisknetwork.com/\">networking groups</a> for your cause or link to the website of your favorite cause a lot from your blog and other sites where they might not be mentioned quite so much. That way, they can consistently <a href=\"/lw/2ag/on_less_wrong_traffic_and_new_users_and_how_you/\">turn up higher in Google searches</a>. Or post this article on Facebook. Some of your friends will be happy you shared it with them. Just saying.<br /><br /> <strong>7. <a href=\"/lw/65/money_the_unit_of_caring\">Give money</a></strong> - Spreading awareness can only accomplish so much. Money is still the <a href=\"/lw/65/money_the_unit_of_caring\">ultimate meta-tool for accomplishing everything</a>. There are millions of excuses not to give, but at the end of the day, this is the highest-leverage way for you to contribute to that already high-leverage cause that you identified. And don&rsquo;t feel like you&rsquo;re alone in finding it difficult to give. Most people find it incredibly difficult to give money -- even to a cause they deeply support. But even if it&rsquo;s a heroically difficult task, we should still aspire to achieve it... we&rsquo;re trying to save the world here, remember? If this were easy, someone else (<a href=\"/lw/jq/926_is_petrov_day/\">besides Petrov</a>) would have done it already.<br /><br /> <strong>8. Give now </strong>(<strong>rather than later</strong>) - I&rsquo;ve seen <a href=\"http://www.utilitarian-essays.com/donatable-money.pdf\">fascinating arguments</a> that it might be possible to do more good by investing your money in the stock market for a long time and then giving all the proceeds to charity later. It&rsquo;s an interesting strategy but it has a number of limitations. To name just two: 1) Not contributing to charity each year prevents you from taking advantage of the best tax planning strategy available to you. That tax-break is free money. You should take free money. Not taking the free money is implicitly agreeing that your government knows how to spend your money better than you do. Do you think your government&rsquo;s judgment and preferences are superior to yours? and; 2) Non-profit organizations can have endowments and those endowments <a href=\"http://www.avvo.com/legal-answers/can-a-non-profit-organization-invest-it-s-funds-in-351932.html\">can invest in securities</a> just like individuals. So if long term-investment in the stock market were really a superior strategy, the charity you&rsquo;re intending to give your money to could do the exact same thing. They could tuck all your annual contributions away in a big fat, tax-free fund to earn market returns until they were ready to unleash a massive bundle of money just like you would have. If they aren&rsquo;t doing this already, it&rsquo;s probably because the problem they&rsquo;re trying to solve is compounding faster than the stock market compounds interest. <a href=\"http://en.wikipedia.org/wiki/HIV/AIDS_in_Africa\">Diseases spread</a>, <a href=\"http://www.ehow.com/about_4613929_effects-poverty.html\">poverty is passed down</a>, <a href=\"http://www.nickbostrom.com/existential/risks.html\">existential risk increases</a>. At the very least, don&rsquo;t try to out-think the non-profit you support without talking to them - they probably wish you were donating now, not just later.<br /><br /> <strong>9. <a href=\"http://www.utilitarian-essays.com/make-money.html\">Optimize your income</a></strong> - Do you know how much you should be earning? <a href=\"http://www.glassdoor.com/\">Information on salaries</a> in your industry / job market could help you <a href=\"http://www.forbes.com/2006/01/04/careers-work-employment-cx_sr_0105bizbasics.html\">negotiate a pay raise</a>. And if you&rsquo;re still in school, why not spend 2 hours to compare the salaries of the different careers you&rsquo;re interested in? Careers can last decades. Degrees take 4-6 years to complete. Make sure you really want <a href=\"http://www.glassdoor.com/\">the kind of salaries you&rsquo;ll be getting</a> and you know <a href=\"http://www.vault.com/\">what it will be like to work in your chosen industry</a>. Even if you&rsquo;re a few years into a degree program, changing course now is still better than regretting not having <a href=\"http://www.wetfeet.com/\">explored other options</a> later. Saving the world is hard enough. Don&rsquo;t make it harder on yourself by earning below market wages or choosing the wrong career to begin with.<br /><br /> <strong>10. <a href=\"http://www.mint.com/\">Optimize your outlays</a></strong> - Cost of living can vary drastically across different <a href=\"http://www.statemaster.com/red/graph/eco_tot_tax_bur-total-tax-burden-per-capita&amp;b_map=1\">tax districts</a>, <a href=\"http://www.scribd.com/doc/17162291/CoL-2009-Ranking-and-Price-Comparison\">real estate markets</a>, <a href=\"http://myfamilysmoney.com/blog/commuting-cost-analysis-bus-vs-bike-vs-car/\">commuting methods</a>, and <a href=\"http://www.ehow.com/how_7524383_save-money-everyday-living.html\">other daily spending habits</a>. It&rsquo;s unlikely you ended up with an optimal configuration. For starters, if you don&rsquo;t currently track your spending, I highly recommend you at least try out something light-weight like <a href=\"http://www.mint.com/\">Mint.com</a> so you can figure out where all your money is going. Remember, you don&rsquo;t have to scrimp and sacrifice your quality of life to save money -- a lot of things can be less expensive just by planning ahead a little and <a href=\"http://www.amazon.com/gp/product/0345496132/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0345496132&amp;linkCode=as2&amp;tag=lesswrong-20\">avoiding those unnecessary &ldquo;gotcha&rdquo; fees</a>. No matter what you want to do to improve the world, having more money to do it makes things easier.<br /><br /> <strong>11. <a href=\"http://www.matchinggifts.com/citadel/\">Look into matching donations</a></strong> - If you&rsquo;re gonna give money to charity anyway, you should see if you can get your employer to match your gift. I've done this before and know others who have too. Thousands of employers will match donations to qualified non-profits. When you get free money -- you should take it.<br /><br /> <strong>12. <a href=\"/lw/xy/the_fun_theory_sequence/\">Have fun!</a></strong> - Don&rsquo;t get so wrapped up trying to save the world that you sacrifice your own humanity. Having a rich, fulfilling personal life is a well-spring of passion that will only boost your ability to contribute -- not distract you. Trust me: you won&rsquo;t be sucked into <a href=\"http://en.wikipedia.org/wiki/Maya_%28illusion%29\">the veil of Maya</a> and forget about your <a href=\"http://en.wikipedia.org/wiki/Bodhisattva_vows\">vow to save the world</a>. So have a beer. Call up your best friend. Watch a movie that has absolutely no world-saving side-benefits whatsoever! You should do whatever it is that connects to that essential joy of being human and you should do it as often as you need; without apologies. Enough people <a href=\"/lw/2et/what_cost_for_irrationality/\">sacrifice their lives without even realizing it</a> -- don&rsquo;t sacrifice your own on purpose.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 2, "qAvbtzdG2A2RBn7in": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TrmMcujGZt5JAtMGg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 96, "baseScore": 97, "extendedScore": null, "score": 0.000195, "legacy": true, "legacyId": "4143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 97, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 135, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ZzefKQwAtMo5yp99", "2ftJ38y9SRBCBsCzy", "zNcLnqHF5rvrTsQJx", "CPm5LTwHrvBJCa9h5", "WHK94zXkQm7qm7wXk", "zJZvoiwydJ5zvzTHK", "q7Me34xvSG3Wm97As", "DB6wbyrMugYMK5o6a", "qGEqpy7J78bZh3awf", "FCxHgPsDScx4C3H8n", "4PPE6D635iBcGPGRy", "Q8evewZW5SeidLdbA", "7FzD7pNm9X68Gp5ZC", "3p3CYauiX8oLjmwRF", "Kow8xRzpfkoY7pa69", "ACGeaAk6KButv2xwQ", "XrzQW69HpidzvBxGr", "GG2rtBReAm6o3mrtn", "w8g7AkSbyApokD3dH", "rRmisKb45dN7DK4BW", "895quRDaK6gR2rM82", "PBRWb2Em5SNeWYwwB", "WLJwTJ7uGPA5Qphbp", "fhEPnveFhb9tmd7Pe", "bbf4ZWwcPQkRijEpt", "ZpDnRCeef2CLEFeKM", "QtyKq4BDyuJ3tysoK", "K4aGvLnHvYgX9pZHS", "ujTE9FLWveYz9WTxZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T17:54:10.469Z", "modifiedAt": null, "url": null, "title": "Ask and Guess", "slug": "ask-and-guess", "viewCount": null, "lastCommentedAt": "2019-06-22T18:08:34.334Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess", "pageUrlRelative": "/posts/vs3kzjLhbdKsndnBy/ask-and-guess", "linkUrl": "https://www.lesswrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20and%20Guess&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20and%20Guess%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvs3kzjLhbdKsndnBy%2Fask-and-guess%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20and%20Guess%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvs3kzjLhbdKsndnBy%2Fask-and-guess", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvs3kzjLhbdKsndnBy%2Fask-and-guess", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 565, "htmlBody": "<p>There's a concept (inspired by a Metafilter blog post) of <a href=\"http://www.theatlanticwire.com/features/view/feature/Askers-vs-Guessers-1230\" target=\"_blank\">ask culture vs. guess culture.</a>&nbsp;&nbsp;In \"ask culture,\" it's socially acceptable to ask for a favor -- staying over at a friend's house, requesting a raise or a letter of recommendation -- and equally acceptable to refuse a favor. &nbsp;Asking is literally just inquiring if the request will be granted, and it's never wrong to ask, provided you know you might be refused. &nbsp;In \"guess culture,\" however, you're expected to guess if your request is appropriate, and you are rude if you accidentally make a request that's judged excessive or inappropriate. &nbsp;You can develop a reputation as greedy or thoughtless if you make inappropriate requests.</p>\n<p>When an asker and a guesser collide, the results are awful. &nbsp;I've seen it in marriages, for example.</p>\n<p>Husband: \"Could you iron my shirt? &nbsp;I have a meeting today.\"</p>\n<p>Wife: \"Can't you see I'm packing lunches and I'm not even dressed yet? &nbsp;You're so insensitive!\"</p>\n<p>Husband: \"But I just <em>asked</em>. &nbsp;You could have just said no if you were too busy -- you don't have to yell at me!\"</p>\n<p>Wife: \"But you should pay enough attention to me to know when you <em>shouldn't ask!\"</em></p>\n<p>It's not clear how how the asking vs. guessing divide works. &nbsp;Some <em>individual people</em>&nbsp;are more comfortable asking than guessing, and vice versa. &nbsp;It's also possible that some families, and some cultures, are more \"ask-based\" than \"guess-based.\" &nbsp;(Apparently East Asia is more \"guess-based\" than the US.) &nbsp;It also varies from situation to situation: \"Will you marry me?\" is a question you should only ask if you know the answer is yes, but \"Would you like to get coffee with me?\" is the kind of question you should ask freely and not worry too much about rejection. &nbsp;</p>\n<p>There's a lot of scope for rationality in deciding when to ask and when to guess. &nbsp;I'm a guesser, myself. &nbsp;But that means I often pass up the opportunity to get what I want, because I'm afraid of being judged as \"greedy\" if I make an inappropriate request. &nbsp;If you're a systematic \"asker\" or a systematic \"guesser,\" then you're systematically biased, liable to guess when you should ask and vice versa. &nbsp;</p>\n<p>In my experience, there are a few situations in which you should experiment with asking even if you're a guesser: in a situation where failure/rejection is so common as to not be shameful (i.e. dating), in a situation where it's someone's job to handle requests, and requests are common (e.g. applying for jobs or awards, academic administration), in a situation where granting or refusing a request is ridiculously easy (most internet communication.) &nbsp;Most of the time when I've tried this out I've gotten my requests granted. I'm still much more afraid of being judged as greedy than I am of not getting what I want, so I'll probably always stay on the \"guessing\" end of the spectrum, but I'd like to get more flexible about it, and more willing to ask when I'm in situations that call for it.</p>\n<p>Anyone else have a systematic bias, one way or another? &nbsp;Anybody trying to overcome it?</p>\n<p>(relevant: <a href=\"http://thedailyasker.blogspot.com/search/label/How%20To\" target=\"_blank\">The Daily Ask</a>, a website full of examples of ways you can make requests. &nbsp;Some of these shock me -- I wouldn't believe it's acceptable to bargain over store prices like that. But, then again, I'm running on corrupted hardware and I wouldn't know what works and what doesn't until I make the experiment.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AADZcNS24mmSfPp2w": 9, "AHK82ypfxF45rqh9D": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vs3kzjLhbdKsndnBy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 100, "baseScore": 116, "extendedScore": null, "score": 0.000208, "legacy": true, "legacyId": "4145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 116, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T18:54:54.639Z", "modifiedAt": null, "url": null, "title": "Is ambition rational?", "slug": "is-ambition-rational", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:57.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sliverlake", "createdAt": "2010-12-01T18:34:06.391Z", "isAdmin": false, "displayName": "sliverlake"}, "userId": "KeoQq7374jsZTH5cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JRpSoarmeY4S7LrKM/is-ambition-rational", "pageUrlRelative": "/posts/JRpSoarmeY4S7LrKM/is-ambition-rational", "linkUrl": "https://www.lesswrong.com/posts/JRpSoarmeY4S7LrKM/is-ambition-rational", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20ambition%20rational%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20ambition%20rational%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJRpSoarmeY4S7LrKM%2Fis-ambition-rational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20ambition%20rational%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJRpSoarmeY4S7LrKM%2Fis-ambition-rational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJRpSoarmeY4S7LrKM%2Fis-ambition-rational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>I don't understand the people around me who are working so very hard to succeed. It strikes me as irrational. Why do <strong>you </strong>do it?</p>\n<p>A long time ago I reasoned that it is more efficient to strive for a 90% rather than 100% on a test because both yield the same \"A\". This morphed into a way of life. I barely got past grad school to earn my PhD, and now I'm a \"Dr.\" just like anyone else. I worked in corporate research where promotions are largely determined by time served. So I aimed to do a good job, but I didn't put in extra effort. Recently, I do just enough consulting to get by and spend the rest of my time as a lazy hipster. :-)&nbsp;</p>\n<p>The emotional half of my brain would like to be more successful, but the \"logical\" part of my brain explains (condescendingly) that the poor odds don't justify the extra effort. Which is right?&nbsp;</p>\n<p>Here's a practical example: My friend is a senior manager at an investment bank. If she works extremely hard for a few more years, she has a small chance (1 in 50?) at being promoted to managing director (2x income). On the other hand, she could scale back her responsibilities and coast for a few years on her already outrageous salary. She has not decided what to do.&nbsp;</p>\n<p>I'm new. If this has already been discussed please post links. Searching didn't yield anything relevant. Thanks.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JRpSoarmeY4S7LrKM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 6.51562042483554e-07, "legacy": true, "legacyId": "4146", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T19:33:04.193Z", "modifiedAt": null, "url": null, "title": "A  possible example of failure to apply lessons from Less Wrong", "slug": "a-possible-example-of-failure-to-apply-lessons-from-less", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:57.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/duHdjMH369xBKvEB9/a-possible-example-of-failure-to-apply-lessons-from-less", "pageUrlRelative": "/posts/duHdjMH369xBKvEB9/a-possible-example-of-failure-to-apply-lessons-from-less", "linkUrl": "https://www.lesswrong.com/posts/duHdjMH369xBKvEB9/a-possible-example-of-failure-to-apply-lessons-from-less", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20%20possible%20example%20of%20failure%20to%20apply%20lessons%20from%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20%20possible%20example%20of%20failure%20to%20apply%20lessons%20from%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FduHdjMH369xBKvEB9%2Fa-possible-example-of-failure-to-apply-lessons-from-less%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20%20possible%20example%20of%20failure%20to%20apply%20lessons%20from%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FduHdjMH369xBKvEB9%2Fa-possible-example-of-failure-to-apply-lessons-from-less", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FduHdjMH369xBKvEB9%2Fa-possible-example-of-failure-to-apply-lessons-from-less", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 260, "htmlBody": "<p>One issue that has been discussed here before is<a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\"> whether Less Wrong is causing readers and participants to behave more rationally or is primarily a time-sink.</a> I recently encountered an example that seemed worth pointing out to the community that suggested mixed results. The entry for <a href=\"http://rationalwiki.org/wiki/Less_Wrong\">Less Wrong on RationalkWiki</a> says \" In the outside world, the ugly manifests itself as LessWrong acolytes, minds freshly blown, metastasising to other sites, bringing the Good News for Modern Rationalists, without clearing their local jargon cache.\" RationalWiki has a variety of issues that I'm not going to discuss in detail here (such as a healthy of dose of motivated cognition pervading the entire project and having serious mind-killing problems) but this sentence should be a cause for concern. What they are essentially talking about is LWians not realizing (or not internalizing) that there's a serious problem of <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distance</a> between people who are familiar with many of the ideas here and people who are not. Since inferential distance is an issue that has been discussed here a lot, this suggests that some people who have read a lot here are not applying the lessons even when they are consciously talking about material related to those lessons. Of course, there's no easy way to tell how representative a sample this is, how common it is, and given RW's inclination to list every possible thing they don't like about something, no matter how small, this may not be a serious issue at all. But it did seem to be serious enough to point out here.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "duHdjMH369xBKvEB9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "4147", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uFYQaGCRwt3wKtyZP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T20:24:53.952Z", "modifiedAt": null, "url": null, "title": "Philadelphia Meetup: Dec 5 or 6", "slug": "philadelphia-meetup-dec-5-or-6", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:48.791Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RitnzDJdSePjT9JXC/philadelphia-meetup-dec-5-or-6", "pageUrlRelative": "/posts/RitnzDJdSePjT9JXC/philadelphia-meetup-dec-5-or-6", "linkUrl": "https://www.lesswrong.com/posts/RitnzDJdSePjT9JXC/philadelphia-meetup-dec-5-or-6", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philadelphia%20Meetup%3A%20Dec%205%20or%206&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhiladelphia%20Meetup%3A%20Dec%205%20or%206%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRitnzDJdSePjT9JXC%2Fphiladelphia-meetup-dec-5-or-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philadelphia%20Meetup%3A%20Dec%205%20or%206%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRitnzDJdSePjT9JXC%2Fphiladelphia-meetup-dec-5-or-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRitnzDJdSePjT9JXC%2Fphiladelphia-meetup-dec-5-or-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>I'll be in Philadelphia on Dec. 6-7 for the Systems Biology of Human Aging conference.&nbsp; If there are LWers in Philadelphia who'd like to meet, we can meet at my hotel, and either stay there or go out.&nbsp; This would be either Sunday Dec. 5 at 6pm-11pm, or Monday Dec. 6 at 9pm-midnight. If interested, please respond here, with date preference, and also email my username at gmail.</p>\n<p>Notice the different hours.&nbsp; I really don't have much time on Monday night.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RitnzDJdSePjT9JXC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 6.515843198067173e-07, "legacy": true, "legacyId": "4148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T20:45:30.123Z", "modifiedAt": null, "url": null, "title": "Smart people who are usually wrong", "slug": "smart-people-who-are-usually-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:37.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3hyKHCcWK5h5X2Jdi/smart-people-who-are-usually-wrong", "pageUrlRelative": "/posts/3hyKHCcWK5h5X2Jdi/smart-people-who-are-usually-wrong", "linkUrl": "https://www.lesswrong.com/posts/3hyKHCcWK5h5X2Jdi/smart-people-who-are-usually-wrong", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Smart%20people%20who%20are%20usually%20wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASmart%20people%20who%20are%20usually%20wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3hyKHCcWK5h5X2Jdi%2Fsmart-people-who-are-usually-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Smart%20people%20who%20are%20usually%20wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3hyKHCcWK5h5X2Jdi%2Fsmart-people-who-are-usually-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3hyKHCcWK5h5X2Jdi%2Fsmart-people-who-are-usually-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 385, "htmlBody": "<p>There are several posters on Less Wrong whom I</p>\n<ul>\n<li>think are unusually smart, and would probably test within the top 2% in the US on standardized tests, and</li>\n<li>think are usually wrong when they post or comment here.</li>\n</ul>\n<p>So I think they are exceptionally smart people whose judgement is consistently worse than if they flipped a coin.</p>\n<p>How probable is this?</p>\n<p>Some theories:</p>\n<ul>\n<li>This is a statistical anomaly.&nbsp; With enough posters, some will by chance always disagree with me.&nbsp; To test this hypothesis, I can write down my list of smart people who are usually wrong, then make a new list based on the next six months of LessWrong, and see how well the lists agree.&nbsp; I have already done this.&nbsp; They agree above chance.</li>\n<li>I remember times when people disagree with me better than times when they agree with me.&nbsp; To test this, I should make a list of smart people, and count the number of times I votes their comments up and down.&nbsp; (It would be really nice if the website could report this to me.)</li>\n<li>This is a bias learned from an initial statistical anomaly.&nbsp; The first time I made my list, I became prejudiced against everything those people wrote.&nbsp; This could be tested using the anti-kibitzer.</li>\n<li>I have poor judgement on who is smart, and they are actually stupid people.&nbsp; I'm not interested in testing this hypothesis.</li>\n<li>What I am actually detecting is smart people who have strong opinions on, and are likely to comment on, areas where I am either wrong, or have a minority opinon.</li>\n<li>These people comment only on difficult, controversial issues which are selected as issues where people perform worse than random.</li>\n<li>Many of these comments are in response to comments or posts I made, which I made only because I thought they were interesting because I already disagreed with smart people about the answers.</li>\n<li>Intelligence does not correlate highly with judgement.&nbsp; Opinions are primarily formed to be compatible with pre-existing opinions, and therefore historical accident is more significant than intelligence in forming opinons.&nbsp; The distribution of historical accidents is probably such that some number of smart people will have opinions based on an entire worldview that is largely wrong.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3hyKHCcWK5h5X2Jdi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 8, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "4150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-01T21:19:39.547Z", "modifiedAt": null, "url": null, "title": "Cheat codes", "slug": "cheat-codes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:38.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sketerpot", "createdAt": "2009-03-22T16:57:11.829Z", "isAdmin": false, "displayName": "sketerpot"}, "userId": "N3Ap8bfskR8WabLmr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5eRnAtwuirHC3uhua/cheat-codes", "pageUrlRelative": "/posts/5eRnAtwuirHC3uhua/cheat-codes", "linkUrl": "https://www.lesswrong.com/posts/5eRnAtwuirHC3uhua/cheat-codes", "postedAtFormatted": "Wednesday, December 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cheat%20codes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACheat%20codes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5eRnAtwuirHC3uhua%2Fcheat-codes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cheat%20codes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5eRnAtwuirHC3uhua%2Fcheat-codes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5eRnAtwuirHC3uhua%2Fcheat-codes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p>Most things worth doing take serious, sustained effort. If you want to become an expert violinist, you're going to have to spend a lot of time practicing. If you want to write a good book, there really is no quick-and-dirty way to do it. But sustained effort is hard<em>,</em>&nbsp;and can be difficult to get rolling<em>.</em>&nbsp;Maybe there are some easier gains to be had with simple, local optimizations. Contrary to oft-repeated <a href=\"/lw/k8/how_to_seem_and_be_deep/\">cached wisdom</a>, not <em>everything</em>&nbsp;worth doing is hard. Some little things you can do are like cheat codes for the real world.</p>\n<p>Take habits, for example: your habits are not fixed. My diet got dramatically better once I figured out how to change my own habits, and actually applied that knowledge. The general trick was to figure out a new, <em>stable</em>&nbsp;state to change my habits to, then use willpower for a week or two until I settle into that stable state. In the case of diet, a stable state was one where junk food was replaced with fruit, tea, or having a slightly more substantial meal beforehand so I wouldn't feel hungry for snacks. That's an equilibrium I can live with, long-term, without needing to worry about \"falling off the wagon.\" Once I figured out the pattern -- work out a stable state, and force myself into it over 1-2 weeks -- I was able to improve several habits, permanently. It was amazing. Why didn't anybody <em>tell </em>me about this?</p>\n<p>In education, there are similar easy wins. If you're trying to commit a lot of things to memory, there's solid evidence that <a href=\"http://en.wikipedia.org/wiki/Spaced_repetition\">spaced repetition</a> works. If you're trying to learn from a difficult textbook, reading in multiple overlapping passes is often more time-efficient than reading through linearly. And I've personally witnessed several people academically un-cripple themselves by learning to reflexively look everything up on Wikipedia. None of this stuff is particularly hard. The problem is just that a lot of people don't know about it.</p>\n<p>What other easy things have a high marginal return-on-effort? Feel free to include speculative ones, if they're testable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "5Whwix4cZ3p5otshm": 1, "H2q58pKG6xFrv8bPz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5eRnAtwuirHC3uhua", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 50, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "4149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aSQy7yHj6nPD44RNo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-02T03:54:40.520Z", "modifiedAt": null, "url": null, "title": "Hard To See What", "slug": "hard-to-see-what", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ferd", "createdAt": "2010-11-01T00:26:27.354Z", "isAdmin": false, "displayName": "Ferd"}, "userId": "CieNYTnjGGS9fs4hn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aC8A3BWFbpyoR953t/hard-to-see-what", "pageUrlRelative": "/posts/aC8A3BWFbpyoR953t/hard-to-see-what", "linkUrl": "https://www.lesswrong.com/posts/aC8A3BWFbpyoR953t/hard-to-see-what", "postedAtFormatted": "Thursday, December 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hard%20To%20See%20What&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHard%20To%20See%20What%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaC8A3BWFbpyoR953t%2Fhard-to-see-what%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hard%20To%20See%20What%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaC8A3BWFbpyoR953t%2Fhard-to-see-what", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaC8A3BWFbpyoR953t%2Fhard-to-see-what", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": null, "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aC8A3BWFbpyoR953t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4153", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": null, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-02T13:25:39.742Z", "modifiedAt": null, "url": null, "title": "Social Presuppositions", "slug": "social-presuppositions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:37.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lucidfox", "createdAt": "2010-11-22T06:58:06.993Z", "isAdmin": false, "displayName": "lucidfox"}, "userId": "hNnKSqvajCMRj9eKK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vYrKWXuQuAYsNtBHC/social-presuppositions", "pageUrlRelative": "/posts/vYrKWXuQuAYsNtBHC/social-presuppositions", "linkUrl": "https://www.lesswrong.com/posts/vYrKWXuQuAYsNtBHC/social-presuppositions", "postedAtFormatted": "Thursday, December 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Social%20Presuppositions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASocial%20Presuppositions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYrKWXuQuAYsNtBHC%2Fsocial-presuppositions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Social%20Presuppositions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYrKWXuQuAYsNtBHC%2Fsocial-presuppositions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvYrKWXuQuAYsNtBHC%2Fsocial-presuppositions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 291, "htmlBody": "<p>During discussion in my previous post, when we touched the subject of human statistical majorities, I had a side-thought. If taking the Less Wrong audience as an example, <a href=\"/lw/fk/survey_results/\">the statistics</a>&nbsp;say that any given participant is strongly likely to be white, male, atheist, and well, just going by general human statistics, probably heterosexual.</p>\n<p>But in my actual interaction, I've taken as a rule not to make any assumptions about the other person. Does it mean, I thought, that I reset my prior probabilities, and consciously choose to discard information? Not relying on implicit assumptions seems the socially right thing to do, I thought; but is it <em>rational</em>?</p>\n<p>When I discussed it on IRC, this quote by sh struck me as insightful:</p>\n<blockquote>\n<p>I.e. making the guess incorrectly probably causes far more friction than deliberately not making a correct guess you could make.</p>\n</blockquote>\n<p>I came up with the following payoff matrix:</p>\n<table border=\"+1&quot;\">\n<tbody>\n<tr>\n<td style=\"font-weight: bold\" colspan=\"2\" rowspan=\"2\">&nbsp;</td>\n<td style=\"font-weight: bold; text-align: center\" colspan=\"2\">Bob</td>\n</tr>\n<tr>\n<td>Has trait X (p = 0.95)</td>\n<td>Doesn't have trait X (p = 0.05)</td>\n</tr>\n<tr>\n<td style=\"font-weight: bold\" rowspan=\"2\">Alice</td>\n<td>Acts as if Bob has trait X</td>\n<td>+1</td>\n<td>-100</td>\n</tr>\n<tr>\n<td>Acts without assumptions about Bob</td>\n<td>0</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>In this case, the second option is strictly preferable. In other words, I don't discard the information, but the repercussions to our social interaction in case of an incorrect guess outweigh the benefit from guessing correctly. And it also matters whether either Alice or Bob is <a href=\"/r/discussion/lw/375/ask_and_guess/\">an Asker or a Guesser</a>.</p>\n<p>One consequence I can think of is that with a sufficiently low p, or if Bob wouldn't be particularly offended by Alice's incorrect guess, taking the guess would be preferable. Now I wonder if we do that a lot in daily life with issues we don't consider controversial (\"hmm, are you from my country/state too?\"), and if all the \"you're overreacting/too sensitive\" complaints come from Alice incorrectly assessing a too low-by-absolute-value negative payoff in (0, 1).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vYrKWXuQuAYsNtBHC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "4156", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZWC3n9c6v4s35rrZ3", "vs3kzjLhbdKsndnBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-02T20:57:31.410Z", "modifiedAt": null, "url": null, "title": "$100 for the best article on efficient charity -- Submit your articles", "slug": "usd100-for-the-best-article-on-efficient-charity-submit-your", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:47.168Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hD7MQfR92zFyDHiv8/usd100-for-the-best-article-on-efficient-charity-submit-your", "pageUrlRelative": "/posts/hD7MQfR92zFyDHiv8/usd100-for-the-best-article-on-efficient-charity-submit-your", "linkUrl": "https://www.lesswrong.com/posts/hD7MQfR92zFyDHiv8/usd100-for-the-best-article-on-efficient-charity-submit-your", "postedAtFormatted": "Thursday, December 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%24100%20for%20the%20best%20article%20on%20efficient%20charity%20--%20Submit%20your%20articles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%24100%20for%20the%20best%20article%20on%20efficient%20charity%20--%20Submit%20your%20articles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhD7MQfR92zFyDHiv8%2Fusd100-for-the-best-article-on-efficient-charity-submit-your%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%24100%20for%20the%20best%20article%20on%20efficient%20charity%20--%20Submit%20your%20articles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhD7MQfR92zFyDHiv8%2Fusd100-for-the-best-article-on-efficient-charity-submit-your", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhD7MQfR92zFyDHiv8%2Fusd100-for-the-best-article-on-efficient-charity-submit-your", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<p>Several people have written articles on efficient charity -- throwawayaccount_1 has an excellent article <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/30hx?c=1\">hidden away in a comment</a>, as does <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/31bi?c=1\">waitingforgodel</a>. Multifoliaterose promises to write an article \"at some point soon\" ..., and louie has actually submitted an <a href=\"/lw/373/how_to_save_the_world/\">article</a> to the main LW page.</p>\r\n<p>What I'd like is for throwawayaccount_1, waitingforgodel and multifoliaterose <em><strong>to submit to the main LW articles page</strong></em>. People will read the articles, and hopefully vote more for better articles. Srticles not submitted to the main LW articles page are <em>not eligible for the prize.</em></p>\r\n<p>Note that it is hard for me to judge which article(s) will actually have the best effect in terms of causing people to make better decisions, so at least some empiricism is desirable. Yes, it isn't perfect, but if anyone has a better suggestion, I am all ears.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hD7MQfR92zFyDHiv8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 6.519470505109481e-07, "legacy": true, "legacyId": "4157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TrmMcujGZt5JAtMGg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T03:18:07.947Z", "modifiedAt": null, "url": null, "title": "Definitions, characterizations, and hard-to-ground variables", "slug": "definitions-characterizations-and-hard-to-ground-variables", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:46.800Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sniffnoy", "createdAt": "2009-10-25T00:27:41.113Z", "isAdmin": false, "displayName": "Sniffnoy"}, "userId": "66EwcncPSoZ25StpW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z6jqEoZ55WvfZeZZt/definitions-characterizations-and-hard-to-ground-variables", "pageUrlRelative": "/posts/z6jqEoZ55WvfZeZZt/definitions-characterizations-and-hard-to-ground-variables", "linkUrl": "https://www.lesswrong.com/posts/z6jqEoZ55WvfZeZZt/definitions-characterizations-and-hard-to-ground-variables", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Definitions%2C%20characterizations%2C%20and%20hard-to-ground%20variables&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefinitions%2C%20characterizations%2C%20and%20hard-to-ground%20variables%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz6jqEoZ55WvfZeZZt%2Fdefinitions-characterizations-and-hard-to-ground-variables%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Definitions%2C%20characterizations%2C%20and%20hard-to-ground%20variables%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz6jqEoZ55WvfZeZZt%2Fdefinitions-characterizations-and-hard-to-ground-variables", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz6jqEoZ55WvfZeZZt%2Fdefinitions-characterizations-and-hard-to-ground-variables", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1228, "htmlBody": "<p>[I am hoping this post is not too repetitive, does not spend too much time rehashing basics... also: What should this be tagged with?]</p>\n<p>Systems are not always made to be understandable - especially if they were not designed in the first place, like the human brain. Thus, they can often contain variables that are hard to ground in an outside meaning (e.g. \"status\", \"gender\"...).&nbsp; In this case, it may often be more appropriate to simply characterize how the variable behaves, rather than worry about attempting to see what it \"represents\" and \"define\" it thus.&nbsp; Ultimately, the variable is grounded in the effects it has on the outside world via the rest of the system.&nbsp; Meanwhile it may not represent anything more than \"a flag I needed to make this hack work\".</p>\n<p>I will refer to this as <em>characterizing</em> the object in question rather than <em>defining</em> it.&nbsp; Rather than say what something \"is\", we simply specify how it behaves.&nbsp; Strictly speaking, characterization is of course a form of definition[0] - indeed, strictly speaking, nearly all definitions are of this form - but I expect you will forgive me if for now I allow a fuzzy notion of \"characterization vs. definition\" scale.</p>\n<p><a id=\"more\"></a>Let us consider a simple example where this is appropriate; consider the notion of \"flying\" in Magic: the Gathering.&nbsp; In this game, a player may have his creatures attack another player, who can then block them with creatures of his own.&nbsp; Some of these creatures have printed on them the text \"flying\", which is defined by the game rules to expand to a larger block of explanatory text. What does \"flying\" mean? It means \"this creature can't be blocked except by creatures with flying\"[1].&nbsp; So the creature can only be blocked by creatures that can only be blocked by creatures that can only be blocked by... well, you see the problem.&nbsp; This is not the real definition at all; if you took a card with \"flying\" and instead actually replaced it with the text \"this creature can't be blocked except by creatures with flying\", you'd get a weaker card.&nbsp; (Such cards <a href=\"http://magiccards.info/gp/en/94.html\">actually</a> <a href=\"http://magiccards.info/us/en/279.html\">exist</a>, <a href=\"http://magiccards.info/sc/en/130.html\">too</a>.)</p>\n<p>The real definition isn't what it nominally expands out to; really, it's just a flag.&nbsp; Meanwhile it's characterized by an external rule that says that creatures with the flag can only be blocked by other creatures with the flag.&nbsp; It may represent the ability to fly but that's just a helpful reminder.</p>\n<p>Now a description of anything is of no use unless it actually bottoms out somewhere, so such self-referential \"definitions\" will not typically occur by themselves in nature[2].&nbsp; Once we establish our primitive physical notions by characterization, our more complex ones we should typically be able to describe by definition.&nbsp; And yet the fact remains that the notion of \"flying\" in Magic <em>is</em> meaningful, and <em>does</em> bottom out; it just didn't appear to at first because I didn't define it correctly.&nbsp; Once we have a substrate that allows us to add variables (like whether or not a creature has flying) and use these to control the actions of the system, these variables automatically obtain meaning from how they control the system.&nbsp; However, an attempt to <em>define</em> such a variable and simply state what it \"is\" may run into the problem of self-reference.</p>\n<p>Typically we expect that the variables in a program correspond to some specific outside concept, that they each \"represent\" something.&nbsp; But how do you make this notion work when the program you're analyzing has 5 distinct states, each with very distinct but seemingly arbitrary behavior?&nbsp; Then the central state variable represents... well, what state it's in, and more than that is hard to say.&nbsp; A definition is the wrong notion to apply here.</p>\n<p>Now perhaps that sort of thing shouldn't occur in a well-written program, but if you're reading an entry in an <a href=\"http://www.ioccc.org/\">obfuscated code contest</a>, it'll be commonplace.&nbsp; And the human brain is a system that wasn't written by an intelligent mind in the first place.&nbsp; So it shouldn't be surprising that it is a mistake to attempt to <em>define</em> something like \"status\" by identifying with some outside phenomenon.&nbsp; Obviously this is technically possible - a meaningful notion must be grounded - but it is better to describe it in a way that takes account of the fact that status is a variable that is instantiated in human brains.&nbsp; As Vladimir_Nesov pointed out, status is not power, it is some sort of <a href=\"/lw/l3/thou_art_godshatter/\">godshatter</a> proxy for power.&nbsp; So we have to be willing to say: \"Status is a variable kept track of in the human brain; it is read on the following occasions with the following effects; it is written to on the following occasions; it satisfies the following properties and invariants...\"</p>\n<p>(This is common in mathematics - what's the tensor product of M and N?&nbsp; Well, it's the thing such that a homomorphism from it is the same as a bilinear map from M&times;N.&nbsp; OK, but what <em>is</em> it?&nbsp; The answer to that question is rarely relevant.)</p>\n<p>This is speculative, but given how transsexual people seem to talk about it I suspect something similar is true of \"gender\".&nbsp; People report knowing from a young age that they were the \"wrong\" gender, they naturally imitated more closely those of this gender (the same way anyone naturally imitates more closely those of their own gender)... what does it mean that this person feels like a \"female\" despite being male in sex? I suspect the answer is: Nothing, it just means that the \"gender\" flag in her head has been set to female!&nbsp; A primitive \"gender\" flag exists, and has no intrinsic meaning except for how it influences our actions, such as by directing us to imitate more closely those who we perceive to have that flag in the same state as we do.&nbsp; A male imitates other males the same way a flying creature blocks other flying creatures, because there's a computational substrate that allows us to turn these informal self-referential definitions into properly grounded characterizations.&nbsp; (Though obviously the statement about males should just be taken as one example statement, not a complete attempt at a characterization!)&nbsp; Note that this would mean that the answer to Eliezer's question <a href=\"/r/discussion/lw/374/gender_identity_and_rationality/31mo?c=1\">\"If you know who you are apart from categorizations, why does it make so much difference whether it fits into a particular category?\"</a> is, \"Well, if you've really pinned down everything downstream of it (which you probably haven't), it doesn't - but I've <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">got this hanging node in my brain</a>...\"</p>\n<p>&nbsp;</p>\n<hr />\n<p>[0]Usually; there is an exception. Whatever notions we take as primitive cannot be defined, and must be characterized instead. However these characterizations (\"axiomatizations\") are not definitions because there is nothing for them to be defined on top of.</p>\n<p>[1]Before anyone else points it out: Yes, I realize that as of Future Sight, it's \"this creature can't be blocked except by creatures with flying or reach\". I'm keeping things simple here.</p>\n<p>[2]Except, of course, at the level of the primitive laws of physics; at the primitive level, only characterizations can occur. What does it mean for a particle to be positively charged? It means it repels other positively charged particles and attracts negatively charged particles. You see where this is going.&nbsp; For this reason, ultimately we have to ground things in what we can detect and predict, rather than the fundamental laws of physics - after all, we don't even know the latter yet!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z6jqEoZ55WvfZeZZt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 6.520408586387797e-07, "legacy": true, "legacyId": "4158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cSXZpvqpa9vbGGLtG", "yA4gF5KrboK2m2Xu7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T03:23:07.900Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes: December 2010", "slug": "rationality-quotes-december-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:27.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tiiba", "createdAt": "2009-02-27T06:55:57.544Z", "isAdmin": false, "displayName": "Tiiba"}, "userId": "FngsS7fwH2r3ikxTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jvi9LLcvZxm529496/rationality-quotes-december-2010", "pageUrlRelative": "/posts/Jvi9LLcvZxm529496/rationality-quotes-december-2010", "linkUrl": "https://www.lesswrong.com/posts/Jvi9LLcvZxm529496/rationality-quotes-december-2010", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%3A%20December%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%3A%20December%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJvi9LLcvZxm529496%2Frationality-quotes-december-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%3A%20December%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJvi9LLcvZxm529496%2Frationality-quotes-december-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJvi9LLcvZxm529496%2Frationality-quotes-december-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>Every month on the month, Less Wrong has a thread where we post Deep Wisdom from the Masters. I saw that nobody did this yet for December for some reason, so I figured I could do it myself.</p>\r\n<p>* Please post all quotes separately, so that they can be voted up/down separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</p>\r\n<p>* \"Do not quote yourself.\" --Tiiba</p>\r\n<p>* Do not quote comments/posts on LW/OB. That's like shooting fish in a barrel. :)</p>\r\n<p>* No more than 5 quotes per person per monthly thread, please.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jvi9LLcvZxm529496", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 6.520418437605576e-07, "legacy": true, "legacyId": "4160", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 342, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T06:20:38.961Z", "modifiedAt": null, "url": null, "title": "How Greedy Bastards Have Saved More Lives Than Mother Theresa Ever Did", "slug": "how-greedy-bastards-have-saved-more-lives-than-mother", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.740Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "waitingforgodel", "createdAt": "2010-09-13T09:13:00.018Z", "isAdmin": false, "displayName": "waitingforgodel"}, "userId": "SbWBTEXPYb5H4AbnY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xg5KCY4FYrxEcCifa/how-greedy-bastards-have-saved-more-lives-than-mother", "pageUrlRelative": "/posts/Xg5KCY4FYrxEcCifa/how-greedy-bastards-have-saved-more-lives-than-mother", "linkUrl": "https://www.lesswrong.com/posts/Xg5KCY4FYrxEcCifa/how-greedy-bastards-have-saved-more-lives-than-mother", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20Greedy%20Bastards%20Have%20Saved%20More%20Lives%20Than%20Mother%20Theresa%20Ever%20Did&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20Greedy%20Bastards%20Have%20Saved%20More%20Lives%20Than%20Mother%20Theresa%20Ever%20Did%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXg5KCY4FYrxEcCifa%2Fhow-greedy-bastards-have-saved-more-lives-than-mother%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20Greedy%20Bastards%20Have%20Saved%20More%20Lives%20Than%20Mother%20Theresa%20Ever%20Did%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXg5KCY4FYrxEcCifa%2Fhow-greedy-bastards-have-saved-more-lives-than-mother", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXg5KCY4FYrxEcCifa%2Fhow-greedy-bastards-have-saved-more-lives-than-mother", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 864, "htmlBody": "<p>And how you can use the same techniques to save a stranger's life for under $600</p>\n<hr />\n<p>It's a strange world we live in.</p>\n<p>When I first heard of Optimal Philanthropy, it was in a news article about Bill Gates's plan for retirement. He'd decided to donate tens of billions of dollars to charity, but had decided that <strong>no existing charity was worth donating to.</strong></p>\n<p>Gates felt they weren't run properly.</p>\n<p>You see, at the time most people thought that \"efficient charities\" were those that had little or no overhead. Everyone wanted as much money to go to the front lines as possible, with little or none for administration.</p>\n<p>Gates didn't care about any of that.</p>\n<p>No, <strong>what Gates wanted was </strong><em><strong>measurable results...</strong></em>&nbsp;and if more administration would get better results, he was all for it.</p>\n<p>In business, it all comes down to return on investment. How much money did you use (to rent buildings, buy supplies, hire employees), and how much money did you earn in return.</p>\n<p>Gates felt that something similar was needed for charity.</p>\n<p>If the charity saved lives, Gates reasoned, then it should be judged by how much money it used to save that life. If a charity could save twice as many lives on the same budget by using more administrators, they by all means they should do that.</p>\n<p>As you may have heard, Bill Gates was appalled that he couldn't find a charity he could measure.</p>\n<p>Here he was, trying to selflessly give away over <em>ten billion dollars</em> to any charity that could <em>prove</em> it would have the highest impact.... and finding a bunch of nonsense answers about how that's not the way charity works... or how little overhead there was.</p>\n<p>And as you may have also heard, Mr. Gates turned that frustration into a revolution in the world of charity -- and inspired others to follow him. His foundation -- the Bill and Melinda Gates Foundation -- is now the biggest in the world, and makes a difference everyday in the areas of world education, malaria, and sustainable energy.</p>\n<p>&nbsp;</p>\n<h2 style=\"padding-left: 90px; \">But Enough About All That! This Isn't About Bill Gates, This Is About You</h2>\n<p>Although the billionaires of the world have gotten their heads screwed on right about charity (and saving hundreds of millions more lives as a result), us non-billionaires didn't seem to get the memo.</p>\n<p>And that means, if you are the sort of person who donates, you're not doing nearly the amount of good you could.</p>\n<p>Here are 3 simple steps you can use right away that will <strong>at least double</strong> the impact your donations have.</p>\n<p>Pause a second to think about what that would mean.</p>\n<p>Why do you donate?</p>\n<p>How would it feel to know that those donations now to twice as much good in this world? To know that at least twice as many people were helped?</p>\n<p>Ready to hear the steps? Great!</p>\n<p>&nbsp;</p>\n<p><strong>Step 1:</strong> Make your reason for donating CONCRETE!</p>\n<p>This step requires being very honest with yourself. It means <strong>not</strong> donating to the Haiti relief fund just because it was tragic (or because Bill Clinton said you should), but instead thinking about what that donation to Haiti would accomplish.</p>\n<p>Something along the lines of: save lives and put good people back into homes. Whatever you hope your donation will accomplish.</p>\n<p>What we're doing is moving from causes and goals (global warming, world peace, freedom from dictators), to <strong>concrete outcomes</strong> (reducing or negating carbon emissions, preventing wars, saving solders lives, educating people about the benefits of democracy).</p>\n<p>Once you've got a concrete outcome you'd like to see in the world, it's time to find out the best way to accomplish that goal.</p>\n<p>&nbsp;</p>\n<p><strong>Step 2:</strong> Use 3rd party charity evaluations that focus on <em>outcomes</em>, and donate where it will do the most good.</p>\n<p>Go to givewell.com and see if your current charity is listed, and what kinds of results they can get per donated dollar.</p>\n<p>Also, don't forget to look at similar outcomes your donation money can accomplish. It's not uncommon to find out that, for example, the cost of giving a blind child a seeing eye dog is <strong>three times</strong> more than the cost of preventing childhood blindness in the first place.</p>\n<p>Yes it might seem tragic to think of a little blind girl without a dog to guide her, but it's even worse to think that we'd give that girl a seeing eye dog <em>at the expense</em>&nbsp;of three other children going blind.</p>\n<p><strong>If nothing else, visit givewell.com</strong>, it will change the way you think about donating for the rest of your life.</p>\n<p>&nbsp;</p>\n<p><strong>Step 3:</strong> Donate what you can, but don't donate time unless you earn less than $10 an hour.</p>\n<p>The strange truth of the matter is that, unless you're donating your time as a professional (Doctor's Without Borders, Pro-Bono Legal Aid), it's often more cost effective to simply work an extra hour and donate the money.</p>\n<p>If you make $25/hr, your cause can probably can get 150 minutes of work for every hour of income you donate.</p>\n<p>&nbsp;</p>\n<hr />\n<p>Okay! If you do those three steps you will get more good from your donation money than 90% of all the donors out there.</p>\n<p>If you felt that this letter helped you, please consider forwarding it to your friends and family, or at least talking about these important issues with them.</p>\n<p>Together, we can make a difference.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xg5KCY4FYrxEcCifa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 21, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "4161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T14:53:47.932Z", "modifiedAt": null, "url": null, "title": "Aieee! The stupid! it burns!", "slug": "aieee-the-stupid-it-burns", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:51.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oRKY7Wd4EwL4wrPfw/aieee-the-stupid-it-burns", "pageUrlRelative": "/posts/oRKY7Wd4EwL4wrPfw/aieee-the-stupid-it-burns", "linkUrl": "https://www.lesswrong.com/posts/oRKY7Wd4EwL4wrPfw/aieee-the-stupid-it-burns", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aieee!%20The%20stupid!%20it%20burns!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAieee!%20The%20stupid!%20it%20burns!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRKY7Wd4EwL4wrPfw%2Faieee-the-stupid-it-burns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aieee!%20The%20stupid!%20it%20burns!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRKY7Wd4EwL4wrPfw%2Faieee-the-stupid-it-burns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoRKY7Wd4EwL4wrPfw%2Faieee-the-stupid-it-burns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p>Last Wednesday (2010 Dec 01), BBC Radio 4 broadcast a studio discussion on the question: \"should we actively try to extend life itself?\" The programme can be listened to from the BBC&nbsp;<a href=\"http://www.bbc.co.uk/programmes/b00w7ccn\">here</a>&nbsp;for one week from broadcast, and is also being repeated tomorrow (Saturday Dec 04) at 22:15 BST. <strong>(ETA: not BST, GMT.)</strong></p>\n<p>All of the dreadful arguments for why death is good came out. For uninteresting reasons I missed a few minutes here and there, but in what I heard, not one of the speakers on any side of the question said anything like, \"This is a <em>no-brainer</em>! Death is <em>evil</em>. Disease is <em>evil</em>. The less of both we have, the <em>better</em>. There is <em>nothing</em> good about death, <em>at all</em>, and all the arguments to the contrary are <em>moral imbecility.</em>\"</p>\n<p>Instead, I heard people saying that work on life extension is disrespectful to the old, that to prolong life would be like prolonging an opera, which has a certain natural size and shape, that the old are wise, so if we make them physically young then old people won't be old, so they won't be wise. Whatever cockeyed argument you can construct by scattering into a Deeply Wise template the words \"old\", \"young\", \"wise\", \"decrepit\", \"healthy\", \"natural\", \"unnatural\", \"boredom\", \"inevitable\", \"denial\", I heard worse.</p>\n<p>If I can bear to listen again to the whole thing just to check I didn't miss anything important, I may write something on their discussion board.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oRKY7Wd4EwL4wrPfw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 6.522123802234317e-07, "legacy": true, "legacyId": "4164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T15:34:05.359Z", "modifiedAt": null, "url": null, "title": "One argument in favor of limited life spans", "slug": "one-argument-in-favor-of-limited-life-spans", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:49.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NWb8KNodKgJJP37Jc/one-argument-in-favor-of-limited-life-spans", "pageUrlRelative": "/posts/NWb8KNodKgJJP37Jc/one-argument-in-favor-of-limited-life-spans", "linkUrl": "https://www.lesswrong.com/posts/NWb8KNodKgJJP37Jc/one-argument-in-favor-of-limited-life-spans", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20argument%20in%20favor%20of%20limited%20life%20spans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20argument%20in%20favor%20of%20limited%20life%20spans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNWb8KNodKgJJP37Jc%2Fone-argument-in-favor-of-limited-life-spans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20argument%20in%20favor%20of%20limited%20life%20spans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNWb8KNodKgJJP37Jc%2Fone-argument-in-favor-of-limited-life-spans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNWb8KNodKgJJP37Jc%2Fone-argument-in-favor-of-limited-life-spans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p>It's the only absolutely reliable way of getting rid of bad leaders.</p>\n<p>This might not be a good enough reason to oppose longevity tech, but I don't think it's easily disposed of.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NWb8KNodKgJJP37Jc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 7, "extendedScore": null, "score": 6.522223165641154e-07, "legacy": true, "legacyId": "4165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T15:41:47.163Z", "modifiedAt": null, "url": null, "title": "Two publicity ideas", "slug": "two-publicity-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:48.239Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RrHxgWpvxHT9w6RYy/two-publicity-ideas", "pageUrlRelative": "/posts/RrHxgWpvxHT9w6RYy/two-publicity-ideas", "linkUrl": "https://www.lesswrong.com/posts/RrHxgWpvxHT9w6RYy/two-publicity-ideas", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20publicity%20ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20publicity%20ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrHxgWpvxHT9w6RYy%2Ftwo-publicity-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20publicity%20ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrHxgWpvxHT9w6RYy%2Ftwo-publicity-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrHxgWpvxHT9w6RYy%2Ftwo-publicity-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<p>Here's the easy idea-- how about a brochure about LW and SIAI? I was just at a couple of science fiction conventions, and it occurred to me that if there were a brochure, I could have printed it out and put it on the freebie table.</p>\n<p>The hard idea is Sesame Street for rationalism-- entertaining video that dramatizes rationality. This would require money and possibly talents which aren't currently in the community, but I think a good bit of rationality could be dramatized, and it would be a big win for raising the rationality waterline.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RrHxgWpvxHT9w6RYy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 6.522242147308763e-07, "legacy": true, "legacyId": "4166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T18:47:22.335Z", "modifiedAt": null, "url": null, "title": "Longterm/Difficult to measure charities", "slug": "longterm-difficult-to-measure-charities", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:46.995Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tmL9Km7eHmZ2tF4Rm/longterm-difficult-to-measure-charities", "pageUrlRelative": "/posts/tmL9Km7eHmZ2tF4Rm/longterm-difficult-to-measure-charities", "linkUrl": "https://www.lesswrong.com/posts/tmL9Km7eHmZ2tF4Rm/longterm-difficult-to-measure-charities", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Longterm%2FDifficult%20to%20measure%20charities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALongterm%2FDifficult%20to%20measure%20charities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmL9Km7eHmZ2tF4Rm%2Flongterm-difficult-to-measure-charities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Longterm%2FDifficult%20to%20measure%20charities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmL9Km7eHmZ2tF4Rm%2Flongterm-difficult-to-measure-charities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmL9Km7eHmZ2tF4Rm%2Flongterm-difficult-to-measure-charities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p>I'm not sure if this necessarily warrants a new discussion, or if there's an existing article/thread that addresses this topic.</p>\n<p>There's a lot of discussion recently about charity, and how to give effectively. I've been looking over givewell.org and it definitely is the single most important thing I've found on lesswrong. But one discouraging thing is that by focusing on easy to measure charities, there's not a lot of info on charities that are trying to accomplish long term less measurable goals. The best charity there that matches my priorities was an educational agency in India that put a lot of emphasis on self improvement.</p>\n<p>My *think* my ideal charity would be something similar to Heifer International, but which also focuses on reproductive health and/or women's rights. Feeding people fish for a day means you just need to feed them again tomorrow, and if they have a bunch of kids you haven't necessarily accomplished anything. From what I've read, in places where the standard of living improves and women get more equality, overpopulation becomes less of an issue. So it seems to me that addressing those issues together in particular regions would produce sustainable longterm benefit. But Givewell doesn't seem to have a lot of information on those types of charities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tmL9Km7eHmZ2tF4Rm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 6.522699873081108e-07, "legacy": true, "legacyId": "4167", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T20:20:03.484Z", "modifiedAt": null, "url": null, "title": "Starting point for calculating inferential distance?", "slug": "starting-point-for-calculating-inferential-distance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:56.248Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JenniferRM", "createdAt": "2009-03-06T17:16:50.600Z", "isAdmin": false, "displayName": "JenniferRM"}, "userId": "g8JkZfL8PTqAefpvx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FDNoTdWH8oaojztN3/starting-point-for-calculating-inferential-distance", "pageUrlRelative": "/posts/FDNoTdWH8oaojztN3/starting-point-for-calculating-inferential-distance", "linkUrl": "https://www.lesswrong.com/posts/FDNoTdWH8oaojztN3/starting-point-for-calculating-inferential-distance", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Starting%20point%20for%20calculating%20inferential%20distance%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStarting%20point%20for%20calculating%20inferential%20distance%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDNoTdWH8oaojztN3%2Fstarting-point-for-calculating-inferential-distance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Starting%20point%20for%20calculating%20inferential%20distance%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDNoTdWH8oaojztN3%2Fstarting-point-for-calculating-inferential-distance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDNoTdWH8oaojztN3%2Fstarting-point-for-calculating-inferential-distance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 485, "htmlBody": "<p>One of the shiniest ideas I picked up from LW is <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distance</a>.&nbsp; I say \"shiny\" because the term, so far as I'm aware, has no clear mathematical or pragmatic definition, no substantive use in peer reviewed science, but was novel to me and appeared to make a lot of stuff about the world suddenly make sense.&nbsp; In my head it is marked as \"<em>super neat</em>... but possibly a convenient falsehood\".&nbsp; I ran across something yesterday that struck me a beautifully succinct and helpful towards resolving the epistemic status of the concept of \"inferential distance\".<a id=\"more\"></a></p>\n<p>While surfing the language log archives I ran across a <a href=\"http://itre.cis.upenn.edu/~myl/languagelog/archives/005530.html\">mailbox response to correspondence about comparative communication efficiency</a>.&nbsp; The author, Mark Liberman, was interested in calculating the amount of information in text and was surprised to find that something about the texts, or the subjects, or his calculation lead to estimating different amounts of information in different translations of the same text (with English requiring 20%-40% more bits than Chinese to say the things in his example text).</p>\n<p>Mr. Liberman was helped by Bob Moore who, among other things, noted:</p>\n<blockquote>\n<p>...why should we expect two languages to use the same number of bits to convey the same thoughts? I believe that when we speak or write we always simplify the complexity of what is actually in our heads, and different languages might implicitly do this more than others. Applying Shannon's source/channel model, suppose that when we have a thought T that we want to convey with an utterance U, we act as if our hearer has a prior P(T) over the possible thoughts we may be conveying and estimates a probability P(U|T) that we will have used U to express T. As you well know, according to Shannon, the hearer should find the T that maximizes P(U|T)*P(T) in order to decide what we meant. But the amount of effort that the speaker puts into U will determine the probability that the hearer will get the message T correctly. If the speaker thinks the prior on T is high, then he may choose a shorter U that has a less peaked probability of only coming from T. If I say to my wife \"I got it,\" I can get by with this short cryptic message, if I think there is a very high probability that she will know what \"it\" is, but I am taking a risk.<br /><br />My conjecture is that the acceptable trade-off between linguistic effort and risk of being misunderstood is socially determined over time by each language community and embodied in the language itself. If the probability of being misunderstood varies smoothly with linguistic effort (i.e., bits) without any sharp discontinuities, then there is no reason to suppose that different linguistic communities would end up at exactly the same place on this curve.</p>\n</blockquote>\n<p>Application to inferential distance is left as an exercise for the reader :-)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FDNoTdWH8oaojztN3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 22, "extendedScore": null, "score": 6.522928492645886e-07, "legacy": true, "legacyId": "4168", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-03T23:45:26.814Z", "modifiedAt": null, "url": null, "title": "Are stereotypes ever irrational?", "slug": "are-stereotypes-ever-irrational", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:01.945Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZXgSMwyHHce837ecn/are-stereotypes-ever-irrational", "pageUrlRelative": "/posts/ZXgSMwyHHce837ecn/are-stereotypes-ever-irrational", "linkUrl": "https://www.lesswrong.com/posts/ZXgSMwyHHce837ecn/are-stereotypes-ever-irrational", "postedAtFormatted": "Friday, December 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20stereotypes%20ever%20irrational%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20stereotypes%20ever%20irrational%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXgSMwyHHce837ecn%2Fare-stereotypes-ever-irrational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20stereotypes%20ever%20irrational%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXgSMwyHHce837ecn%2Fare-stereotypes-ever-irrational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXgSMwyHHce837ecn%2Fare-stereotypes-ever-irrational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 919, "htmlBody": "<p>Harvard's undergraduate admission office will tell you \"There is no typical Harvard student.\" &nbsp;This platitude ticks me off. &nbsp;Of course there's such a thing as a typical Harvard student! &nbsp;Harvard students aren't magically exceptions to the laws of probability. &nbsp;Just as a robin is a more typical bird than an ostrich, some Harvard students are especially typical. &nbsp;Let's say (I'm not actually looking at data here) that most Harvard students are rich, most have high SAT scores, most are white, and most are snobbish. &nbsp;Note: I am not a Harvard student. :)</p>\n<p>Now, a very typical Harvard student would be rich AND white AND smart AND snobbish. &nbsp;But observe that a given student has a smaller probability of being <em>all</em>&nbsp;of these than of just, say, being rich. &nbsp;If you add enough majority characteristics, eventually the \"typical\" student will become very rare. &nbsp;Even if there's a 99% probability of having any one of these characteristics, 0.99<sup>n</sup>&nbsp;-&gt;0 as n goes to infinity. &nbsp;Some Harvard students are typical; but <em>extremely </em>typical Harvard students are rare. If you encountered a random Harvard student, and expected her to have <em>all</em>&nbsp;the majority characteristics of Harvard students, you could very well be wrong.</p>\n<p>So far, so obvious. &nbsp;But who would make that mistake? &nbsp;</p>\n<p>You, that's who. &nbsp;The <a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\" target=\"_blank\">conjunction fallacy</a>&nbsp;is the tendency of humans to think specific conditions are more probable than general conditions. &nbsp;People are more likely to believe that a smart, single, politically active woman is a feminist bank teller than just a bank teller. &nbsp;Policy experts (in the 1980's) were more likely to think that the USSR would invade Poland <em>and </em>that the US would break off relations with the USSR, than either one of these events alone. &nbsp;Of course, this is mistaken: the probability of A and B is always less than or equal to the probability of A alone. &nbsp;The reason we make this mistake is the <a href=\"http://en.wikipedia.org/wiki/Representativeness_heuristic\" target=\"_blank\">representativeness heuristic</a>: a specific, compelling story that resembles available data is judged as more probable than general (but more likely) data. &nbsp;Judging by this evidence, I'd hypothesize that most people will overestimate the probability of a random Harvard student matching the profile of a \"very typical\" Harvard student. &nbsp;The conjunction fallacy says something even stronger: the <em>more</em>&nbsp;information you add to the profile of the \"very typical\" Harvard student, the more specific the portrait you paint (add a popped collar, for instance) the <em>more likely</em>&nbsp;people will think it is. &nbsp;Even though in fact the \"typical student\" is getting less and less likely as you add more information.</p>\n<p>Now, let's talk about stereotypes. &nbsp;Stereotypes -- at least the kind that offend some people -- have their apologists. &nbsp;Some people say, \"They're offensive because they're <em>true</em>. &nbsp;Of <em>course</em>&nbsp;some traits are more common in some populations than others. That's just having accurate priors.\" &nbsp;This is worth taking seriously. &nbsp;The mere act of making assumptions based on statistics is <em>not </em>irrational. &nbsp;In fact, that's the only way we can go about our daily lives; we make estimates based on what we think is likely. &nbsp;There's nothing wrong with stating \"Most birds can fly,\" even if some can't. &nbsp;And exhortations not to stereotype people are often blatantly irrational. &nbsp;\"There is no typical Harvard student\" -- well, yes, there is. &nbsp;\"You can't make assumptions about people\" -- well, yes, you can, and you'd be pathologically helpless if you never made <em>any</em>. &nbsp;You can assume people don't like rotten meat, for instance. &nbsp;If stereotyping is just <em>making inferences</em>, then stereotyping is not just morally acceptable, it's<em>&nbsp;</em>absolutely necessary. &nbsp;And, though it may be true that some people are offended by some accurate priors and rational inferences, it is not generally<em>&nbsp;</em>good for people to be thus offended; any more than it is good for people to want to be wrong about anything.</p>\n<p>But there is a kind of \"stereotyping\" that really is a logical fallacy. &nbsp;The picture of the \"very typical\" Harvard student is a stereotype. &nbsp;If people overestimate the probability of that representative-looking <em>picture</em>, then they are stereotyping Harvard students in an irrational way. &nbsp;An irrational stereotype is a \"typical\" or \"representative\" picture that isn't actually all that common. &nbsp;Because the human mind likes stories, because we like completing patterns, we'll think it's more likely that someone <em>matches a pattern or story completely</em>&nbsp;than that she matches only part of the story. &nbsp;</p>\n<p>There's a line in the movie <em>Annie Hall </em>that illustrates this. &nbsp;(This is within five minutes of Alvy meeting Allison.)</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; color: #333333; line-height: 17px;\">Alvy Singer: You, you, you're like New York, Jewish, left-wing, liberal, intellectual, Central Park West, Brandeis University, the socialist summer camps and the, the father with the Ben Shahn drawings, right, and the really, y'know, strike-oriented kind of, red diaper, stop me before I make a complete imbecile of myself.&nbsp;</span></p>\n</blockquote>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; color: #333333; line-height: 17px;\">Allison: No, that was wonderful. I love being reduced to a cultural stereotype.</span></p>\n</blockquote>\n<p>If Alvy had only stopped with \"New York, Jewish, left-wing,\" he'd probably be right. &nbsp;But he <em>kept going</em>. &nbsp;He had to complete the pattern. &nbsp;By the time he's got to the Ben Shahn drawings, it's just getting fanciful. &nbsp;If you build up too detailed a story, you'll find it irresistible to believe, but it's getting less and less likely all the time.&nbsp;</p>\n<p>Stereotypes <em>can </em>&nbsp;be irrational. &nbsp;Not every inference or assumption about people is irrational, of course, but our tendency to find specific stories more believable than broader qualities is irrational. &nbsp;Our tendency to think that most people resemble the \"most typical\" members of a class is irrational. &nbsp;Mistaken stereotypes are what happen when people are more attracted to complete stories than to actual probability distributions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZXgSMwyHHce837ecn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 28, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "4169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-04T00:11:27.912Z", "modifiedAt": null, "url": null, "title": "Cambridge Sunday meetup: New time and location", "slug": "cambridge-sunday-meetup-new-time-and-location", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:29.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JqsyrjZzaHYaXWuLT/cambridge-sunday-meetup-new-time-and-location", "pageUrlRelative": "/posts/JqsyrjZzaHYaXWuLT/cambridge-sunday-meetup-new-time-and-location", "linkUrl": "https://www.lesswrong.com/posts/JqsyrjZzaHYaXWuLT/cambridge-sunday-meetup-new-time-and-location", "postedAtFormatted": "Saturday, December 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20Sunday%20meetup%3A%20New%20time%20and%20location&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20Sunday%20meetup%3A%20New%20time%20and%20location%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJqsyrjZzaHYaXWuLT%2Fcambridge-sunday-meetup-new-time-and-location%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20Sunday%20meetup%3A%20New%20time%20and%20location%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJqsyrjZzaHYaXWuLT%2Fcambridge-sunday-meetup-new-time-and-location", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJqsyrjZzaHYaXWuLT%2Fcambridge-sunday-meetup-new-time-and-location", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>The Sunday Cambridge meetups, which happen on the first and third Sunday of each month, are now at 2:00 pm at <a href=\"http://maps.google.com/maps/place?cid=2537582743464648203&amp;q=Cosi+near+Kendall+Sq+Cambridge,+MIT&amp;hl=en&amp;ved=0CBUQ-QswAA&amp;ei=z4P5TNa8Ip_wyAWo9ayGBA&amp;sll=42.366388,-71.091778&amp;sspn=0.019786,0.038418&amp;ie=UTF8&amp;ll=42.381531,-71.117935&amp;spn=0,0&amp;t=h&amp;z=15\">Cosi</a>, near Kendall Square. As discussed at the last meetup, we've changed the location because the old location was becoming too crowded, and the time to allow the meetup to go on longer before people have to leave.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JqsyrjZzaHYaXWuLT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 6.523499345127942e-07, "legacy": true, "legacyId": "4170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-04T01:40:40.102Z", "modifiedAt": null, "url": null, "title": "Sequences in Alternative Formats", "slug": "sequences-in-alternative-formats", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OneWhoFrogs", "createdAt": "2010-04-19T01:17:25.787Z", "isAdmin": false, "displayName": "OneWhoFrogs"}, "userId": "JkEZmsNpFhvhLvJuC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6BHkQoijdYymu2HBr/sequences-in-alternative-formats", "pageUrlRelative": "/posts/6BHkQoijdYymu2HBr/sequences-in-alternative-formats", "linkUrl": "https://www.lesswrong.com/posts/6BHkQoijdYymu2HBr/sequences-in-alternative-formats", "postedAtFormatted": "Saturday, December 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sequences%20in%20Alternative%20Formats&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASequences%20in%20Alternative%20Formats%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6BHkQoijdYymu2HBr%2Fsequences-in-alternative-formats%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sequences%20in%20Alternative%20Formats%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6BHkQoijdYymu2HBr%2Fsequences-in-alternative-formats", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6BHkQoijdYymu2HBr%2Fsequences-in-alternative-formats", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>In the past weeks, there have been a few projects aiming to convert the sequences into more eye-friendly formats than the computer screen.&nbsp; There's no collection of these projects yet, so I'm posting here and in the wiki to get the word out.</p>\n<ul>\n<li><a href=\"http://jb55.com/lesswrong/\">Print ready versions</a> by jb55 (<a href=\"https://github.com/jb55/lesswrong-print\">GitHub</a>.)&nbsp; Has versions in Markdown, PDF, and ePub.&nbsp; ePubs can be converted to nearly any other format with <a href=\"http://calibre-ebook.com/\">calibre</a>.</li>\n<li><a href=\"http://173.255.238.116/ebooks\">lw2ebook</a> by me (<a href=\"https://github.com/OneWhoFrogs/lw2ebook\">GitHub</a>.)&nbsp; Includes all sequences in ePub and mobi format.</li>\n<li><a href=\"/lw/319/print_ready_version_of_the_sequences/\">Print ready versions</a> by Jordan.&nbsp; Contains all posts of the sequence in one HTML file.</li>\n</ul>\n<p>Some of the sequences can be rather lengthy -- the one on quantum physics is around 118,000 words.&nbsp; I know that there are some people that don't mind reading off LCD screens, but hopefully these tools will prove useful to the rest.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6BHkQoijdYymu2HBr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 22, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "4171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["j6byboWcPASu5cYk7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-04T09:12:04.436Z", "modifiedAt": null, "url": null, "title": "How to Live on 24 Hours a Day", "slug": "how-to-live-on-24-hours-a-day", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:02.786Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6yW7hGvr5k33gxgkB/how-to-live-on-24-hours-a-day", "pageUrlRelative": "/posts/6yW7hGvr5k33gxgkB/how-to-live-on-24-hours-a-day", "linkUrl": "https://www.lesswrong.com/posts/6yW7hGvr5k33gxgkB/how-to-live-on-24-hours-a-day", "postedAtFormatted": "Saturday, December 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Live%20on%2024%20Hours%20a%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Live%20on%2024%20Hours%20a%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6yW7hGvr5k33gxgkB%2Fhow-to-live-on-24-hours-a-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Live%20on%2024%20Hours%20a%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6yW7hGvr5k33gxgkB%2Fhow-to-live-on-24-hours-a-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6yW7hGvr5k33gxgkB%2Fhow-to-live-on-24-hours-a-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>I can think of no better way to spend my karma than on encouraging people to read this 19th century self-help book. It's free and online in full.</p>\n<p>The guidelines on what makes an appropriate front-page article be damned, or, if necessary, enforced by official censorship.</p>\n<p>Thanks to User:sfb for the quote that led me here, although the decision to post is entirely my own.</p>\n<p>http://www.gutenberg.org/files/2274/2274-h/2274-h.htm</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6yW7hGvr5k33gxgkB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 16, "extendedScore": null, "score": 6.524832067509306e-07, "legacy": true, "legacyId": "4174", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-04T10:27:58.909Z", "modifiedAt": null, "url": null, "title": "Efficient Charity", "slug": "efficient-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FCxHgPsDScx4C3H8n/efficient-charity", "pageUrlRelative": "/posts/FCxHgPsDScx4C3H8n/efficient-charity", "linkUrl": "https://www.lesswrong.com/posts/FCxHgPsDScx4C3H8n/efficient-charity", "postedAtFormatted": "Saturday, December 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Efficient%20Charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEfficient%20Charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCxHgPsDScx4C3H8n%2Fefficient-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Efficient%20Charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCxHgPsDScx4C3H8n%2Fefficient-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCxHgPsDScx4C3H8n%2Fefficient-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2552, "htmlBody": "<p><em>I wrote this article in response to <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/\">Roko's request</a> for an article about efficient charity. As a disclosure of a possible conflict of interest I'll note that I have <a href=\"http://www.givewell.org/about/self-evaluation/external-reviews#NarrowingtheFieldassignment\">served as a volunteer</a> for GiveWell. Last edited 12/06/10.<br /></em></p>\n<p>Charitable giving is widely considered to be virtuous and admirable. If statistical behavior is any guide, most people regard charitable donations to be worthwhile expenditures. In 2001 a full 89% of American households donated money to charity and during 2009 Americans donated $303.75 billion to charity [1].&nbsp;</p>\n<p>A heart-breaking fact about modern human experience is that there's little connection between such generosity and positive social impact. The reason why humans evolved charitable tendencies is because such tendencies served as marker to nearby humans that a given individual is a dependable ally. Those who expend their resources to help others are more likely than others to care about people in general and are therefore more likely than others to care about their companions. But one can tell that people care based exclusively on their willingness to make sacrifices independently of whether these sacrifices actually help anybody.</p>\n<p>Modern human society is very far removed from our ancestral environment. Technological and social innovations have made it possible for us to influence people on the other side of the globe and potentially to have a profound impact on the long term survival of the human race. The current population of New York is ten times the human population of the entire world in our ancestral environment. In view of these radical changes it should be no surprise that the impact of a typical charitable donation falls staggeringly short of the impact of donation optimized to help people as much as possible.</p>\n<p>While this may not be a problem for donors who are unconcerned about their donations helping people, it's a huge problem for donors who want their donations to help people as much as possible and it's a huge problem for the people who lose out on assistance because of inefficiency in the philanthropic world. Picking out charities that have high positive impact per dollar is a task no less difficult than picking good financial investments and one that requires heavy use of critical and quantitative reasoning. Donors who wish for their donations to help people as much as possible should engage in such reasoning and/or rely on the recommendations of trusted parties who have done so.</p>\n<p><a id=\"more\"></a><strong>The Overhead Ratio: Not a Good Metric</strong></p>\n<p>A commonly used statistic for charity evaluation which has a thin veneer of analytical rigor is a charity's &ldquo;overhead ratio&rdquo;: that is, the relative amounts of money spent on programs vs. administration. According to a <a href=\"http://www.philanthropyaction.com/documents/Worst_Way_to_Pick_A_Charity_Dec_1_2009.pdf\">press release</a> issued in December 2009 by Philanthropy Action, Charity Navigator, GiveWell, Great Nonprofits, Guidestar and Philanthropedia :</p>\n<blockquote>\n<p>For years, people have turned to the overhead ratio&mdash;a measure of how much of each donation is spent on &ldquo;programs&rdquo; versus administrative and fundraising costs&mdash;to guide their choice of charity. But overhead ratios and executive salaries are useless for evaluating a nonprofit&rsquo;s impact.</p>\n<p>While the idea of sending money &ldquo;straight to the beneficiaries&rdquo; is tempting, nonprofit experts agree that judging charities by how much of their money goes to &ldquo;programs&rdquo; is counterproductive. &ldquo;Achieving a low overhead ratio drives many charities to behaviors that make them less effective and means more, not less, wasted dollars,&rdquo; says Paul Brest, President of the Hewlett Foundation, and co-author of <em>Money Well Spent</em>.</p>\n</blockquote>\n<p>The common focus on low overhead ratio has produced perverse incentives; pressuring some charities to skimp on administrative costs that would improve the efficacy of their programs. More importantly, cost-effectiveness of different charities' activities varies so dramatically as to totally eclipse any usefulness that the overhead ratio might have in a world of charities performing homogeneous activities.</p>\n<p><strong>A Comparison of Cost-Effectiveness</strong></p>\n<p>A well-known and well-funded charity is the <a href=\"http://en.wikipedia.org/wiki/Make-A-Wish_Foundation\">Make-A-Wish Foundation</a>, &ldquo;a 501(c)(3) non-profit organization in the United States that grants wishes to children (2.5 years to 18 years old) who have life-threatening medical conditions.&rdquo; According to the website's <a href=\"http://www.wish.org/about/managing_our_funds\">Managing Our Funds</a> page:</p>\n<blockquote>\n<p>The Make-A-Wish Foundation<sup>&reg;</sup> is proud of the way it manages and safeguards the generous contributions it receives from individual donors, corporations and other organizations.</p>\n<p>Seventy-six percent of the revenue the Make-A-Wish Foundation receives is allotted to program services. This percentage well exceeds the standard upheld by organizations that monitor the work of charities.</p>\n</blockquote>\n<p>And indeed, the percentage allotted to program services is sufficiently high in juxtaposition with other financial statistics so that Charity Navigator <a href=\"http://www.charitynavigator.org/index.cfm?bay=search.summary&amp;orgid=4038\">grants the Make-A-Wish Foundation its highest rating</a>. But how cost-effective are the charity's programs?</p>\n<p>The Make-A-Wish Foundation <a href=\"http://www.wish.org/content/download/9869/81525/version/1/file/MAWF_09_AnnualReport.pdf\">2009 Annual Report</a> states that &ldquo;A record-breaking 13,471 children had their wishes come true in FY09.&rdquo; The annual report gives a break down of wishes by type: for example, 40.3% of the wishes were trips to the Walt Disney World Resort, 11.7% of them were shopping sprees, 7.1% of them were celebrity meetings and 5.5% of them were cruises.</p>\n<p>The annual report claims that in 2009 the charity's &ldquo;total program and support services&rdquo; amounted a figure of $203,865,550. Thus, the Make-A-Wish Foundation implicitly reports to spending an average of $15,134 for each wish that it grants.</p>\n<p>A charity that helps children in the United States far more efficiently is <a href=\"http://www.nursefamilypartnership.org/\">Nurse-Family Partnership</a> which provides an approximately three year long program of weekly nurse visits to inexperienced expectant and early mothers for at a cost of $11,200 yielding <a href=\"http://www.nursefamilypartnership.org/proven-results\">improved prenatal health, fewer childhood injuries and improved school readiness</a>. A deeper appreciation of how little good per dollar the Make-A-Wish Foundation does relative to what is possible requires a digression.</p>\n<hr />\n<p>In November 2010 the United Nations released its <a href=\"http://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index\">2010 Human Development Report</a> ranking the world's countries according to a \"Human Development Index\" based on data concerning life expectancy, education and per-capita GDP. One of the lowest ranked countries on this list is Mozambique which has an <a href=\"http://www.google.com/publicdata?ds=wb-wdi&amp;met=sp_dyn_imrt_in&amp;idim=country:MOZ&amp;dl=en&amp;hl=en&amp;q=infant+mortality+rate+mozambique\">infant mortality rate around 10%</a>. This contrasts dramatically with the <a href=\"http://www.google.com/publicdata?ds=wb-wdi&amp;met=sp_dyn_imrt_in&amp;idim=country:USA&amp;dl=en&amp;hl=en&amp;q=infant+mortality+rate+united+states\">infant mortality rate in the United States</a> which is less than 1%. Every tenth pregnancy in Mozambique is followed by the grief of losing a child within several years. A child in sub-Saharan Africa who survives past the age of five is more likely than not to live a full life extending past the age of 60 [2].</p>\n<p>Why is the infant mortality rate in Mozambique so high? A major cause of death is infectious disease. Around a third of infants in Mozambique do not have the opportunity to receive the standard vaccinations for polio, measles, tentanus, tuberculosis, diphtheria and other fatal diseases because of the poverty of their surroundings and some of them will die as a result.</p>\n<p>An organization called <a href=\"http://villagereach.org/\">VillageReach</a> is working to improve Mozambique's health logistics. Between 2002 and 2008 VillageReach ran a <a href=\"http://www.givewell.org/international/top-charities/villagereach#Pilotproject\">pilot program in the Mozambique province of Cabo Delgado</a> designed to improve the province's health logistics. This program was dramatically successful. One tangible indicator of impact is that VillageReach increased the percentage of Cabo Delgado infants who received the third and final dose of the diphtheria-tetanus-pertussis vaccine from 68.9% to 95.4%, yielding a final percentage higher than that of the average in any sub-Saharan African country. When one looks at the available evidence in juxtaposition with the cost of the program and runs through cost-effectiveness calculations one finds that under conservative assumptions <a href=\"http://www.givewell.org/international/top-charities/villagereach#Pastcosteffectivenesspilotprogram\">VillageReach saved an infant's life for every $545 donated to VillageReach</a>.</p>\n<p>Now VillageReach is in the process of expanding its operations to more provinces of Mozambique, hoping to expand its pilot project into seven more of Mozambique's eleven provinces over the next six years. VillageReach requires an additional ~ $1.5 million [3] to implement its proposal as fast as possible. In light of the fact that VillageReach has so far received only about 20-25% of this funding, it's plausible that additional donations will have a cost-effectiveness similar to that of those used for the pilot project.</p>\n<hr />\n<p>Thus we see that while a $15,134 donation to the Make-A-Wish Foundation can be expected to grant an average of one wish to an ill child<em> </em>(a good thing all else being equal), a donation to VillageReach can 27 infants lives! With this framing it becomes clear that the amount of good per dollar that the Make-A-Wish Foundation is doing is negligible relative to that of VillageReach . No parent would prefer to send a child to Disney World over preventing even a single one of his or her children from contracting a life threatening illness!</p>\n<p>Nor is this phenomenon of badly suboptimal giving specific to Make-A-Wish Foundation donors. Even if one restricts one's attention to the cause of health in the developing world [4], many donors donate to charities pursuing health interventions in the developing world that do&nbsp;<a href=\"http://blog.givewell.org/2010/01/28/can-choosing-the-right-charity-double-your-impact/\"><em>a thousand times less good </em>per dollar than the most cost-effective health interventions</a>.</p>\n<p>A hypothetical charity running programs like VillageReach's which embezzled 95% of its budget and had correspondingly greatly reduced cost-effectiveness would <em>still</em> be doing far more good per dollar than the Make-A-Wish Foundation or the least effective developing world charities do. This example makes it clear how profoundly useless the overhead ratio is for assessing the relative quality of a charity.</p>\n<p><strong>Holding Charities Accountable</strong><strong><br /></strong></p>\n<p>Donors should be aware that <a href=\"http://www.givewell.org/international/technical/criteria/cost-effectiveness#Charitiesfrequentlycitemisleadingcosteffectivenessfigures\">charities frequently cite misleading cost-effectiveness figures</a> in their promotional materials. And just because a charity claims to be performing activities of very high value doesn't mean that the charity is performing the activity as reported. William Easterly recently commented on Peter Singer's <a href=\"http://www.utilitarian.net/singer/by/199704--.htm\">child in a pond</a> metaphor [5] saying:</p>\n<blockquote>\n<p>In our situation trying to help a poor person, what we're actually doing is we're not physically able to rush in ourselves and save the child. In fact, we are not even able to observe whether the child is saved or not. What we are doing is we're sending money off to someone else on the other side of the world...and we're counting on them to save the child. And so I guess to put the metaphor another way, if your person who was saving a child was in a situation where they were physically unable to help and they knew they had to delegate it to someone else, then it would also be morally reprehensible if they did not find a person who was reliable who they were sure was going to save the child. And it would be morally reprehensible if they did not in fact check up to make sure that the child was saved. That would be just as morally objectionable as your situation of yourself directly failing to rush to the aid of the child.</p>\n</blockquote>\n<p>Of course, for a donor with limited time and energy it is frequently not possible to personally check that a charity is performing its stated function. As such, it is useful to have independent charity evaluators that evaluate charities for impact. The only such organization that I'm familiar with is <a href=\"http://www.givewell.org/\">GiveWell</a> which has reviewed 409 charities working in the areas of equality of opportunity in the United States, health in the developing world, and economic empowerment in the developing world and has highlighted those charities with the strongest evidence of positive impact. VillageReach is currently GiveWell's top ranked charity in the cause of health in the developing world.</p>\n<p>There are many causes that GiveWell has not yet covered and there may be charities working in them that absorb donations substantially more cost-effectively than VillageReach does. GiveWell has prepared a <a href=\"http://www.givewell.org/your-charity\">Do-it-Yourself Charity Evaluation Guide</a> as an aid to donors who are interested in personally investigating charities working in causes that GiveWell has not yet covered.</p>\n<p><strong>Volunteering, Nonprofit Work and Cost-Effectiveness<br /></strong></p>\n<p>So far I've restricted my discussion to charitable giving. Giving is not the only philanthropic activity that people engage in;&nbsp; some people volunteer their time to benefit others and some people choose to forgo income to work at a lower paying nonprofit job that they deem to have greater social value than the job that they would otherwise take. There are many instances in which such philanthropic activities are the best way to help people, but one should consider such activities against the backdrop of there being huge variability in the cost-effectiveness of philanthropic activities. GiveWell's recommended charities have set a concrete minimal standard for optimizing cost-effectiveness of philanthropic activities.</p>\n<p>To determine whether or not volunteering or taking a nonprofit job is a good way of helping people, one should compare additional positive impact that one would have by switching jobs with the positive impact that one would have by donating all of one's forgone income to the most efficient charity that one can find. For those with low earning potential and skills that are useful and rare in the philanthropic world, the most efficient way of helping people will typically be volunteering and/or non-profit work. For those who have high earning potential and lack skills that are especially rare in the philanthropic world the most efficient way of helping people will typically be taking a high paying job and donating one's income to an efficient charity. [6]</p>\n<p>Of course, many people who volunteer or forgo income to work at a non-profit do so not only with a view toward helping people but also because they want to experience the visceral sense of helping people directly or of working directly on a cause that they feel passionate about. This latter factor can be a good reason to engage in such activities. Humans are not automatons capable of persistently adopting the most efficient course possible. Fulfilling our own very substantial personal needs and desires is important to maintaining good health and energy. At the same time, in view of the great variability of cost-effectiveness of various philanthropic activities, if one doesn't devote <em>some</em> resources toward helping people as efficiently as possible, one will probably accomplish very little of one's potential capacity to make the world a better place. [7]</p>\n<p><strong>Conclusion</strong></p>\n<p>People often have good intentions and frequently fail to direct them to create the substantial positive impact that they could if they thought carefully about how to do as much good as possible. I have already mentioned <a href=\"http://www.givewell.org/\">GiveWell</a> as a useful resource for donors who interested in accomplishing the most good for their dollar. Such donors may also find it useful to visit <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> which is a society whose members pledge to donate a portion of their income &ldquo;to whichever organizations can most effectively use it to fight poverty in developing countries&rdquo; and whose members &ldquo;share advice on the most effective ways to give.&rdquo; By thinking critically and making use of available resources, one can reasonably expect to be able to have a much greater positive social impact than one otherwise would be able to.</p>\n<hr />\n<p><strong>Footnotes</strong></p>\n<p>[1] Figures taken from a <a href=\"http://www.independentsector.org/uploads/Resources/GV01keyfind.pdf\">survey by Independent Sector</a> and <a href=\"http://www.cfbroward.org/cfbroward/media/Documents/Sidebar%20Documents/GivingUSA_2010_ExecSummary_Print.pdf\">The Annual Report on Philanthropy for the Year 2009</a>.</p>\n<p>[2] According to <a href=\"http://www.givewell.org/international/technical/additional/Standard-of-Living#MortalityandMorbiditywhatarethemajorhealthproblemsinthedevelopingworld\">calculations by GiveWell using data from the World Health Organization.</a></p>\n<p>[3] See the section of GiveWell's review of VillageReach titled <a href=\"http://www.givewell.org/international/top-charities/villagereach#Roomformorefunds\">Room For More Funds?</a></p>\n<p>[4] For an indication of the relative cost-effectiveness of health interventions in the U.S. refer to a 1995 academic journal article from titled <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.1995.tb00330.x/abstract\">Five-Hundred Life-Saving Interventions and Their Cost-Effectiveness</a>.</p>\n<p>[5] In a December 2009 <a href=\"http://bloggingheads.tv/diavlogs/24804\">BloggingHeads Diavlog</a> with Peter Singer. William Easterly is an economist at NYU and author of the <a href=\"http://aidwatchers.com/\">Aid Watch blog</a></p>\n<p>[6] Alan Dawrst's essay titled <a href=\"http://www.utilitarian-essays.com/make-money.html\">Why Activists Should Consider Making Lots of Money</a> gives more on this topic.</p>\n<p>[7] Eliezer Yudkowsky's <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Purchase Fuzzies and Utilons Separately</a> gives a nice discussion of this theme.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "EeSkeTcT4wtW2fWsL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FCxHgPsDScx4C3H8n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 42, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "4155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>I wrote this article in response to <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/\">Roko's request</a> for an article about efficient charity. As a disclosure of a possible conflict of interest I'll note that I have <a href=\"http://www.givewell.org/about/self-evaluation/external-reviews#NarrowingtheFieldassignment\">served as a volunteer</a> for GiveWell. Last edited 12/06/10.<br></em></p>\n<p>Charitable giving is widely considered to be virtuous and admirable. If statistical behavior is any guide, most people regard charitable donations to be worthwhile expenditures. In 2001 a full 89% of American households donated money to charity and during 2009 Americans donated $303.75 billion to charity [1].&nbsp;</p>\n<p>A heart-breaking fact about modern human experience is that there's little connection between such generosity and positive social impact. The reason why humans evolved charitable tendencies is because such tendencies served as marker to nearby humans that a given individual is a dependable ally. Those who expend their resources to help others are more likely than others to care about people in general and are therefore more likely than others to care about their companions. But one can tell that people care based exclusively on their willingness to make sacrifices independently of whether these sacrifices actually help anybody.</p>\n<p>Modern human society is very far removed from our ancestral environment. Technological and social innovations have made it possible for us to influence people on the other side of the globe and potentially to have a profound impact on the long term survival of the human race. The current population of New York is ten times the human population of the entire world in our ancestral environment. In view of these radical changes it should be no surprise that the impact of a typical charitable donation falls staggeringly short of the impact of donation optimized to help people as much as possible.</p>\n<p>While this may not be a problem for donors who are unconcerned about their donations helping people, it's a huge problem for donors who want their donations to help people as much as possible and it's a huge problem for the people who lose out on assistance because of inefficiency in the philanthropic world. Picking out charities that have high positive impact per dollar is a task no less difficult than picking good financial investments and one that requires heavy use of critical and quantitative reasoning. Donors who wish for their donations to help people as much as possible should engage in such reasoning and/or rely on the recommendations of trusted parties who have done so.</p>\n<p><a id=\"more\"></a><strong>The Overhead Ratio: Not a Good Metric</strong></p>\n<p>A commonly used statistic for charity evaluation which has a thin veneer of analytical rigor is a charity's \u201coverhead ratio\u201d: that is, the relative amounts of money spent on programs vs. administration. According to a <a href=\"http://www.philanthropyaction.com/documents/Worst_Way_to_Pick_A_Charity_Dec_1_2009.pdf\">press release</a> issued in December 2009 by Philanthropy Action, Charity Navigator, GiveWell, Great Nonprofits, Guidestar and Philanthropedia :</p>\n<blockquote>\n<p>For years, people have turned to the overhead ratio\u2014a measure of how much of each donation is spent on \u201cprograms\u201d versus administrative and fundraising costs\u2014to guide their choice of charity. But overhead ratios and executive salaries are useless for evaluating a nonprofit\u2019s impact.</p>\n<p>While the idea of sending money \u201cstraight to the beneficiaries\u201d is tempting, nonprofit experts agree that judging charities by how much of their money goes to \u201cprograms\u201d is counterproductive. \u201cAchieving a low overhead ratio drives many charities to behaviors that make them less effective and means more, not less, wasted dollars,\u201d says Paul Brest, President of the Hewlett Foundation, and co-author of <em>Money Well Spent</em>.</p>\n</blockquote>\n<p>The common focus on low overhead ratio has produced perverse incentives; pressuring some charities to skimp on administrative costs that would improve the efficacy of their programs. More importantly, cost-effectiveness of different charities' activities varies so dramatically as to totally eclipse any usefulness that the overhead ratio might have in a world of charities performing homogeneous activities.</p>\n<p><strong id=\"A_Comparison_of_Cost_Effectiveness\">A Comparison of Cost-Effectiveness</strong></p>\n<p>A well-known and well-funded charity is the <a href=\"http://en.wikipedia.org/wiki/Make-A-Wish_Foundation\">Make-A-Wish Foundation</a>, \u201ca 501(c)(3) non-profit organization in the United States that grants wishes to children (2.5 years to 18 years old) who have life-threatening medical conditions.\u201d According to the website's <a href=\"http://www.wish.org/about/managing_our_funds\">Managing Our Funds</a> page:</p>\n<blockquote>\n<p>The Make-A-Wish Foundation<sup>\u00ae</sup> is proud of the way it manages and safeguards the generous contributions it receives from individual donors, corporations and other organizations.</p>\n<p>Seventy-six percent of the revenue the Make-A-Wish Foundation receives is allotted to program services. This percentage well exceeds the standard upheld by organizations that monitor the work of charities.</p>\n</blockquote>\n<p>And indeed, the percentage allotted to program services is sufficiently high in juxtaposition with other financial statistics so that Charity Navigator <a href=\"http://www.charitynavigator.org/index.cfm?bay=search.summary&amp;orgid=4038\">grants the Make-A-Wish Foundation its highest rating</a>. But how cost-effective are the charity's programs?</p>\n<p>The Make-A-Wish Foundation <a href=\"http://www.wish.org/content/download/9869/81525/version/1/file/MAWF_09_AnnualReport.pdf\">2009 Annual Report</a> states that \u201cA record-breaking 13,471 children had their wishes come true in FY09.\u201d The annual report gives a break down of wishes by type: for example, 40.3% of the wishes were trips to the Walt Disney World Resort, 11.7% of them were shopping sprees, 7.1% of them were celebrity meetings and 5.5% of them were cruises.</p>\n<p>The annual report claims that in 2009 the charity's \u201ctotal program and support services\u201d amounted a figure of $203,865,550. Thus, the Make-A-Wish Foundation implicitly reports to spending an average of $15,134 for each wish that it grants.</p>\n<p>A charity that helps children in the United States far more efficiently is <a href=\"http://www.nursefamilypartnership.org/\">Nurse-Family Partnership</a> which provides an approximately three year long program of weekly nurse visits to inexperienced expectant and early mothers for at a cost of $11,200 yielding <a href=\"http://www.nursefamilypartnership.org/proven-results\">improved prenatal health, fewer childhood injuries and improved school readiness</a>. A deeper appreciation of how little good per dollar the Make-A-Wish Foundation does relative to what is possible requires a digression.</p>\n<hr>\n<p>In November 2010 the United Nations released its <a href=\"http://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index\">2010 Human Development Report</a> ranking the world's countries according to a \"Human Development Index\" based on data concerning life expectancy, education and per-capita GDP. One of the lowest ranked countries on this list is Mozambique which has an <a href=\"http://www.google.com/publicdata?ds=wb-wdi&amp;met=sp_dyn_imrt_in&amp;idim=country:MOZ&amp;dl=en&amp;hl=en&amp;q=infant+mortality+rate+mozambique\">infant mortality rate around 10%</a>. This contrasts dramatically with the <a href=\"http://www.google.com/publicdata?ds=wb-wdi&amp;met=sp_dyn_imrt_in&amp;idim=country:USA&amp;dl=en&amp;hl=en&amp;q=infant+mortality+rate+united+states\">infant mortality rate in the United States</a> which is less than 1%. Every tenth pregnancy in Mozambique is followed by the grief of losing a child within several years. A child in sub-Saharan Africa who survives past the age of five is more likely than not to live a full life extending past the age of 60 [2].</p>\n<p>Why is the infant mortality rate in Mozambique so high? A major cause of death is infectious disease. Around a third of infants in Mozambique do not have the opportunity to receive the standard vaccinations for polio, measles, tentanus, tuberculosis, diphtheria and other fatal diseases because of the poverty of their surroundings and some of them will die as a result.</p>\n<p>An organization called <a href=\"http://villagereach.org/\">VillageReach</a> is working to improve Mozambique's health logistics. Between 2002 and 2008 VillageReach ran a <a href=\"http://www.givewell.org/international/top-charities/villagereach#Pilotproject\">pilot program in the Mozambique province of Cabo Delgado</a> designed to improve the province's health logistics. This program was dramatically successful. One tangible indicator of impact is that VillageReach increased the percentage of Cabo Delgado infants who received the third and final dose of the diphtheria-tetanus-pertussis vaccine from 68.9% to 95.4%, yielding a final percentage higher than that of the average in any sub-Saharan African country. When one looks at the available evidence in juxtaposition with the cost of the program and runs through cost-effectiveness calculations one finds that under conservative assumptions <a href=\"http://www.givewell.org/international/top-charities/villagereach#Pastcosteffectivenesspilotprogram\">VillageReach saved an infant's life for every $545 donated to VillageReach</a>.</p>\n<p>Now VillageReach is in the process of expanding its operations to more provinces of Mozambique, hoping to expand its pilot project into seven more of Mozambique's eleven provinces over the next six years. VillageReach requires an additional ~ $1.5 million [3] to implement its proposal as fast as possible. In light of the fact that VillageReach has so far received only about 20-25% of this funding, it's plausible that additional donations will have a cost-effectiveness similar to that of those used for the pilot project.</p>\n<hr>\n<p>Thus we see that while a $15,134 donation to the Make-A-Wish Foundation can be expected to grant an average of one wish to an ill child<em> </em>(a good thing all else being equal), a donation to VillageReach can 27 infants lives! With this framing it becomes clear that the amount of good per dollar that the Make-A-Wish Foundation is doing is negligible relative to that of VillageReach . No parent would prefer to send a child to Disney World over preventing even a single one of his or her children from contracting a life threatening illness!</p>\n<p>Nor is this phenomenon of badly suboptimal giving specific to Make-A-Wish Foundation donors. Even if one restricts one's attention to the cause of health in the developing world [4], many donors donate to charities pursuing health interventions in the developing world that do&nbsp;<a href=\"http://blog.givewell.org/2010/01/28/can-choosing-the-right-charity-double-your-impact/\"><em>a thousand times less good </em>per dollar than the most cost-effective health interventions</a>.</p>\n<p>A hypothetical charity running programs like VillageReach's which embezzled 95% of its budget and had correspondingly greatly reduced cost-effectiveness would <em>still</em> be doing far more good per dollar than the Make-A-Wish Foundation or the least effective developing world charities do. This example makes it clear how profoundly useless the overhead ratio is for assessing the relative quality of a charity.</p>\n<p><strong>Holding Charities Accountable</strong><strong><br></strong></p>\n<p>Donors should be aware that <a href=\"http://www.givewell.org/international/technical/criteria/cost-effectiveness#Charitiesfrequentlycitemisleadingcosteffectivenessfigures\">charities frequently cite misleading cost-effectiveness figures</a> in their promotional materials. And just because a charity claims to be performing activities of very high value doesn't mean that the charity is performing the activity as reported. William Easterly recently commented on Peter Singer's <a href=\"http://www.utilitarian.net/singer/by/199704--.htm\">child in a pond</a> metaphor [5] saying:</p>\n<blockquote>\n<p>In our situation trying to help a poor person, what we're actually doing is we're not physically able to rush in ourselves and save the child. In fact, we are not even able to observe whether the child is saved or not. What we are doing is we're sending money off to someone else on the other side of the world...and we're counting on them to save the child. And so I guess to put the metaphor another way, if your person who was saving a child was in a situation where they were physically unable to help and they knew they had to delegate it to someone else, then it would also be morally reprehensible if they did not find a person who was reliable who they were sure was going to save the child. And it would be morally reprehensible if they did not in fact check up to make sure that the child was saved. That would be just as morally objectionable as your situation of yourself directly failing to rush to the aid of the child.</p>\n</blockquote>\n<p>Of course, for a donor with limited time and energy it is frequently not possible to personally check that a charity is performing its stated function. As such, it is useful to have independent charity evaluators that evaluate charities for impact. The only such organization that I'm familiar with is <a href=\"http://www.givewell.org/\">GiveWell</a> which has reviewed 409 charities working in the areas of equality of opportunity in the United States, health in the developing world, and economic empowerment in the developing world and has highlighted those charities with the strongest evidence of positive impact. VillageReach is currently GiveWell's top ranked charity in the cause of health in the developing world.</p>\n<p>There are many causes that GiveWell has not yet covered and there may be charities working in them that absorb donations substantially more cost-effectively than VillageReach does. GiveWell has prepared a <a href=\"http://www.givewell.org/your-charity\">Do-it-Yourself Charity Evaluation Guide</a> as an aid to donors who are interested in personally investigating charities working in causes that GiveWell has not yet covered.</p>\n<p><strong id=\"Volunteering__Nonprofit_Work_and_Cost_Effectiveness\">Volunteering, Nonprofit Work and Cost-Effectiveness<br></strong></p>\n<p>So far I've restricted my discussion to charitable giving. Giving is not the only philanthropic activity that people engage in;&nbsp; some people volunteer their time to benefit others and some people choose to forgo income to work at a lower paying nonprofit job that they deem to have greater social value than the job that they would otherwise take. There are many instances in which such philanthropic activities are the best way to help people, but one should consider such activities against the backdrop of there being huge variability in the cost-effectiveness of philanthropic activities. GiveWell's recommended charities have set a concrete minimal standard for optimizing cost-effectiveness of philanthropic activities.</p>\n<p>To determine whether or not volunteering or taking a nonprofit job is a good way of helping people, one should compare additional positive impact that one would have by switching jobs with the positive impact that one would have by donating all of one's forgone income to the most efficient charity that one can find. For those with low earning potential and skills that are useful and rare in the philanthropic world, the most efficient way of helping people will typically be volunteering and/or non-profit work. For those who have high earning potential and lack skills that are especially rare in the philanthropic world the most efficient way of helping people will typically be taking a high paying job and donating one's income to an efficient charity. [6]</p>\n<p>Of course, many people who volunteer or forgo income to work at a non-profit do so not only with a view toward helping people but also because they want to experience the visceral sense of helping people directly or of working directly on a cause that they feel passionate about. This latter factor can be a good reason to engage in such activities. Humans are not automatons capable of persistently adopting the most efficient course possible. Fulfilling our own very substantial personal needs and desires is important to maintaining good health and energy. At the same time, in view of the great variability of cost-effectiveness of various philanthropic activities, if one doesn't devote <em>some</em> resources toward helping people as efficiently as possible, one will probably accomplish very little of one's potential capacity to make the world a better place. [7]</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>People often have good intentions and frequently fail to direct them to create the substantial positive impact that they could if they thought carefully about how to do as much good as possible. I have already mentioned <a href=\"http://www.givewell.org/\">GiveWell</a> as a useful resource for donors who interested in accomplishing the most good for their dollar. Such donors may also find it useful to visit <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> which is a society whose members pledge to donate a portion of their income \u201cto whichever organizations can most effectively use it to fight poverty in developing countries\u201d and whose members \u201cshare advice on the most effective ways to give.\u201d By thinking critically and making use of available resources, one can reasonably expect to be able to have a much greater positive social impact than one otherwise would be able to.</p>\n<hr>\n<p><strong id=\"Footnotes\">Footnotes</strong></p>\n<p>[1] Figures taken from a <a href=\"http://www.independentsector.org/uploads/Resources/GV01keyfind.pdf\">survey by Independent Sector</a> and <a href=\"http://www.cfbroward.org/cfbroward/media/Documents/Sidebar%20Documents/GivingUSA_2010_ExecSummary_Print.pdf\">The Annual Report on Philanthropy for the Year 2009</a>.</p>\n<p>[2] According to <a href=\"http://www.givewell.org/international/technical/additional/Standard-of-Living#MortalityandMorbiditywhatarethemajorhealthproblemsinthedevelopingworld\">calculations by GiveWell using data from the World Health Organization.</a></p>\n<p>[3] See the section of GiveWell's review of VillageReach titled <a href=\"http://www.givewell.org/international/top-charities/villagereach#Roomformorefunds\">Room For More Funds?</a></p>\n<p>[4] For an indication of the relative cost-effectiveness of health interventions in the U.S. refer to a 1995 academic journal article from titled <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.1995.tb00330.x/abstract\">Five-Hundred Life-Saving Interventions and Their Cost-Effectiveness</a>.</p>\n<p>[5] In a December 2009 <a href=\"http://bloggingheads.tv/diavlogs/24804\">BloggingHeads Diavlog</a> with Peter Singer. William Easterly is an economist at NYU and author of the <a href=\"http://aidwatchers.com/\">Aid Watch blog</a></p>\n<p>[6] Alan Dawrst's essay titled <a href=\"http://www.utilitarian-essays.com/make-money.html\">Why Activists Should Consider Making Lots of Money</a> gives more on this topic.</p>\n<p>[7] Eliezer Yudkowsky's <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Purchase Fuzzies and Utilons Separately</a> gives a nice discussion of this theme.</p>", "sections": [{"title": "A Comparison of Cost-Effectiveness", "anchor": "A_Comparison_of_Cost_Effectiveness", "level": 1}, {"title": "Volunteering, Nonprofit Work and Cost-Effectiveness", "anchor": "Volunteering__Nonprofit_Work_and_Cost_Effectiveness", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "185 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 185, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4amcyxad5bnBR9Afm", "3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-04T11:15:48.594Z", "modifiedAt": "2020-08-19T23:09:24.879Z", "url": null, "title": "Applied cognitive science: learning from a faux pas", "slug": "applied-cognitive-science-learning-from-a-faux-pas", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:49.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Kaj_Sotala", "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ti2SW9GoZLCq36zEz/applied-cognitive-science-learning-from-a-faux-pas", "pageUrlRelative": "/posts/Ti2SW9GoZLCq36zEz/applied-cognitive-science-learning-from-a-faux-pas", "linkUrl": "https://www.lesswrong.com/posts/Ti2SW9GoZLCq36zEz/applied-cognitive-science-learning-from-a-faux-pas", "postedAtFormatted": "Saturday, December 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20cognitive%20science%3A%20learning%20from%20a%20faux%20pas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20cognitive%20science%3A%20learning%20from%20a%20faux%20pas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTi2SW9GoZLCq36zEz%2Fapplied-cognitive-science-learning-from-a-faux-pas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20cognitive%20science%3A%20learning%20from%20a%20faux%20pas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTi2SW9GoZLCq36zEz%2Fapplied-cognitive-science-learning-from-a-faux-pas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTi2SW9GoZLCq36zEz%2Fapplied-cognitive-science-learning-from-a-faux-pas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1059, "htmlBody": "<p><a href=\"http://xuenay.livejournal.com/336968.html\">Cross-posted from my LiveJournal</a>:</p><blockquote><p>Yesterday evening, I pasted to two IRC channels an excerpt of what someone had written. In the context of the original text, that excerpt had seemed to me like harmless if somewhat raunchy humor. What I didn't realize at the time was that by removing the context, the person writing it came off looking like a jerk, and by laughing at it I came off looking as something of a jerk as well.<br><br>Two people, both of whom I have known for many years now and whose opinions I value, approached me by private message and pointed out that that may not have been the smartest thing to do. My initial reaction was defensive, but I soon realized that they were right and thanked them for pointing it out to me. Putting on a positive <a href=\"http://xuenay.livejournal.com/329071.html\">growth mindset</a>, I decided to treat this event as a positive one, as in the future I'd know better.<br><br>Later that evening, as I lay in bed waiting to fall asleep, the episode replayed itself in my mind. I learnt long ago that trying to push such replays out of my mind would just make them take longer and make them feel worse. So I settled back to just observing the replay and waiting for it to go away. As I waited, I started thinking about what kind of lower-level neural process this feeling might be a sign of.<br><br>Artificial neural networks use what is called a <a href=\"http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html\">backpropagation algorithm</a> to learn from mistakes. First the network is provided some input, then it computes some value, and then the obtained value is compared to the expected value. The difference between the obtained and expected value is the error, which is then propagated back from the end of the network to the input layer. As the error signal works it way through the network, neural weights are adjusted in such a fashion to produce a different output the next time.<br><br>Backprop is known to be biologically unrealistic, but there are more realistic algorithms that work in a roughly similar manner. The human brain seems to be using something called <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Temporal_difference_learning\">temporal difference learning</a>. As <a href=\"lw/21b/ugh_fields/\">Roko described it</a>: \"Your brain propagates the psychological pain 'back to the earliest reliable stimulus for the punishment'. If you fail or are punished sufficiently many times in some problem area, and acting in that area is always preceeded by [doing something], your brain will propagate the psychological pain right back to the moment you first begin to [do that something]\".<br><br>As I lay there in bed, I couldn't help the feeling that something similar to those two algorithms was going on. The main thing that kept repeating itself was not the actual action of pasting the quote to the channel or laughing about it, but the admonishments from my friends. Being independently rebuked for something by two people I considered important: a powerful error signal that had to be taken into account. Their reactions filling my mind: an attempt to re-set the network to the state it was in soon after the event. The uncomfortable feeling of thinking about that: negative affect flooding the network as it was in that state, acting as a signal to re-adjust the neural weights that had caused that kind of an outcome.<br><br>After those feelings had passed, I thought about the episode again. Now I felt silly for committing that faux pas, for now it felt <strong>obvious</strong> that the quote would come across badly. For a moment I wondered if I had just been unusually tired, or distracted, or otherwise out of my normal mode of thought to not have seen that. But then it occurred to me - the judgment of this being obviously a bad idea was produced <i>by the network that had just been rewired</i> in response to social feedback. The pain of the feedback had been propagated back to the action that caused it, so just thinking about doing that (or thinking about having done that) made me feel stupid. I have no way of knowing whether the \"don't do that, idiot\" judgment is something that would actually have been produced had I been paying more attention, or if it's a genuinely new judgment that wouldn't have been produced by the old network.<br><br>I tend to be somewhat amused by the people who go about claiming that computers can never be truly intelligent, because a computer doesn't genuinely understand the information it's processing. I think they're vastly overestimating how smart we are, and that a lot of our thinking is just relatively crude pattern-matching, with various patterns (including behavioral ones) being labeled as good or bad after the fact, as we try out various things.<br><br>On the other hand, there would probably have been <i>one</i> way to avoid that incident. We do have the capacity for reflective thought, which allows us to simulate various events in our heads without needing to actually undergo them. Had I actually imagined the various ways in which people could interpret that quote, I would probably have relatively quickly reached the conclusion that yes, it might easily be taken as jerk-ish. Simply imagining that reaction might then have provided the decision-making network with a similar, albeit weaker, error signal and taught it not to do that.<br><br>However, there's the question of combinatorial explosions: any decision could potentially have countless of consequences, and we can't simulate them all. (See <a href=\"http://plato.stanford.edu/entries/frame-problem/#EpiFraPro\">the epistemological frame problem</a>.) So in the end, knowing the answer to the question of \"which actions are such that we should pause to reflect upon their potential consequences\" is something we need to learn by trial and error as well.<br><br>So I guess the lesson here is that you shouldn't blame yourself too much if you've done something that feels obviously wrong in retrospect. That decision was made by an earlier version of you. Although it feels obvious now, that version of you might literally have had no way of knowing that it was making a mistake, as it hadn't been properly trained yet.</p></blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "mip7tdAN87Jarkcew": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ti2SW9GoZLCq36zEz", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 41, "extendedScore": null, "score": 6.525136217725025e-07, "legacy": true, "legacyId": "4175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-04T17:35:53.137Z", "modifiedAt": null, "url": null, "title": "\"Behind the Power Curve\" by Simon Funk", "slug": "behind-the-power-curve-by-simon-funk", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:48.141Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dYokqRNkN3vXskvQy/behind-the-power-curve-by-simon-funk", "pageUrlRelative": "/posts/dYokqRNkN3vXskvQy/behind-the-power-curve-by-simon-funk", "linkUrl": "https://www.lesswrong.com/posts/dYokqRNkN3vXskvQy/behind-the-power-curve-by-simon-funk", "postedAtFormatted": "Saturday, December 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Behind%20the%20Power%20Curve%22%20by%20Simon%20Funk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Behind%20the%20Power%20Curve%22%20by%20Simon%20Funk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYokqRNkN3vXskvQy%2Fbehind-the-power-curve-by-simon-funk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Behind%20the%20Power%20Curve%22%20by%20Simon%20Funk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYokqRNkN3vXskvQy%2Fbehind-the-power-curve-by-simon-funk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYokqRNkN3vXskvQy%2Fbehind-the-power-curve-by-simon-funk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>Brandyn Webb (Simon Funk) of <a href=\"http://sifter.org/~simon/AfterLife/\">After Life</a> fame is an impressively rational person. One of his essays, <a href=\"http://sifter.org/~simon/journal/20100718.h.html\">Behind the Power Curve</a>, resonated with me so much that I need to share it with you. Quotes don't do it justice, just read the whole thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dYokqRNkN3vXskvQy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "4176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T01:17:49.434Z", "modifiedAt": null, "url": null, "title": "A Catalog of Confusions", "slug": "a-catalog-of-confusions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.777Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FaZH325yEhCzQtrEf/a-catalog-of-confusions", "pageUrlRelative": "/posts/FaZH325yEhCzQtrEf/a-catalog-of-confusions", "linkUrl": "https://www.lesswrong.com/posts/FaZH325yEhCzQtrEf/a-catalog-of-confusions", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Catalog%20of%20Confusions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Catalog%20of%20Confusions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFaZH325yEhCzQtrEf%2Fa-catalog-of-confusions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Catalog%20of%20Confusions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFaZH325yEhCzQtrEf%2Fa-catalog-of-confusions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFaZH325yEhCzQtrEf%2Fa-catalog-of-confusions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 814, "htmlBody": "<p><em>tl;dr - can we categorise confusing events&nbsp;by skills required to deal with them?&nbsp; What are those skills?</em></p>\r\n<p>I am sometimes haunted by things I read online.&nbsp; It's probably a couple of years since I first read <a href=\"http://I am sometimes haunted by things I read online.  It's probably a couple of years since I first read [Your Strength as a Rationalist](http//lesswrong.com/lw/if/your_strength_as_a_rationalist/), but over the past month or two I've had a number of \">Your Strength as a Rationalist</a>, but over the past month or two I've been reminded of it a surprising number of times in different circumstances.&nbsp; It's led me to wonder whether the idea of being \"confused by fiction\" can be helpfully broken down into categories, with each of those categories having certain skills that can be worked on to help notice them.</p>\r\n<p>I'm going to&nbsp;describe two such categories I think I've identified, and invite your criticism, or suggestions of other similar categories.&nbsp; In both cases, I believe there to be some instinct, acquired skill, or some combination&nbsp;thereof&nbsp;that draws it to my attention.&nbsp; I could just be making this up, though, so criticism is also welcome on this front.</p>\r\n<p><strong>Absence of Salient Information</strong></p>\r\n<p>I believe tech support is like a magic trick in reverse.&nbsp; With a magic trick, the magician hides a crucial fact which he then distracts you from.&nbsp; He provides a false narrative of what's going on while confusing the sequence of events, culminating in the impossible,&nbsp;and relies on your own fear of appearing foolish to make you falsely report the conditions of the trick to both yourself and other spectators.</p>\r\n<p>In tech support,&nbsp;you are often presented with an&nbsp;impossible sequence of events; the customer's&nbsp;fear of appearing foolish makes them falsely report the conditions of the fault to both themselves and you, concealing a crucial fact which the rest of the narrative distracts you from.&nbsp; You then have to figure out how it was done.</p>\r\n<p>I recently asked a girl from my dance class out for a drink, and proceeded to receive the most shocking litany of mixed signals I could ever imagine receiving, drink not forthcoming.&nbsp; I boiled it down to three possibilities: either she was interested but incredibly shy, uninterested but just really friendly; or she had a completely different set of standards when it came to signaling romantic interest or lack thereof.&nbsp; I remember thinking how none of these possibilities made sense in context, and was reminded quite specifically of the idea of being more confused by fiction than by reality.&nbsp; It was driving my problem-solving faculties to distraction, and I have never been so&nbsp;relieved to discover a woman I was interested in already had a boyfriend.</p>\r\n<p>The phenomenon wasn't unlike a film with a massive plot-integral spoiler.&nbsp; There's this nagging feeling that the whole thing doesn't quite make sense, until the spoiler is revealed, at which point you suddenly see the whole of the preceeding sequence of events in a new revelatory light.&nbsp; I've often noticed with such films that when people know there's a big spoiler, they're more likely to spot it early on because they start groping around for plausible plot twists.&nbsp; I'm not sure if this is the best way to go about fishing for information you know is absent, though.</p>\r\n<p><strong>Having One's Head Messed With</strong></p>\r\n<p>I've read a few books on hypnosis, NLP and persuasion techniques, and I'm at least as well-versed on cognitive biases as most LW readers, but a couple of weeks ago someone fucked with my head.</p>\r\n<p>I was in East London (never a good start), fairly late at night with food in my hand.&nbsp; Beggars <em>always</em> seem to approach me when I have food in my hand.&nbsp; I don't think this is coincidence.&nbsp; This particular beggar, a woman in her twenties, spun a very quick story which I can't even begin to remember all the details of.&nbsp; Something about desperately needing bus fare to escape her abusive boyfriend and having just been released from hospital.&nbsp; Just thinking about it, two weeks later, makes me confused and disorientated.</p>\r\n<p>In retrospect, the story made no sense whatsoever, she was far too aggressive to be a downtrodden out-patient abuse victim, and far too <em>good</em> at making me feel like the only way I could possibly get out of this horrible distressing&nbsp;situation was to give her my small change, which I did.&nbsp; Afterwards I felt violated.</p>\r\n<p>The experience itself has probably armed me against it happening again to a certain degree, but I'm now worried about what I'm <em>not</em> armed against.&nbsp; There is a feeling of having your head messed with, but I only ever seem to experience it retrospectively.&nbsp; Can I train myself to spot it as it's happening?&nbsp; Is it related to the feeling I get when I recognise I'm being manipulated by advertising?&nbsp; Is there a how-to&nbsp;body of knowledge that can be assembled to defend against manipulation in general?</p>\r\n<p>This probably could have been more coherent, but it was surprisingly cathartic to write.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FaZH325yEhCzQtrEf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 9, "extendedScore": null, "score": 6.527217505939463e-07, "legacy": true, "legacyId": "4177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>tl;dr - can we categorise confusing events&nbsp;by skills required to deal with them?&nbsp; What are those skills?</em></p>\n<p>I am sometimes haunted by things I read online.&nbsp; It's probably a couple of years since I first read <a href=\"http://I am sometimes haunted by things I read online.  It's probably a couple of years since I first read [Your Strength as a Rationalist](http//lesswrong.com/lw/if/your_strength_as_a_rationalist/), but over the past month or two I've had a number of \">Your Strength as a Rationalist</a>, but over the past month or two I've been reminded of it a surprising number of times in different circumstances.&nbsp; It's led me to wonder whether the idea of being \"confused by fiction\" can be helpfully broken down into categories, with each of those categories having certain skills that can be worked on to help notice them.</p>\n<p>I'm going to&nbsp;describe two such categories I think I've identified, and invite your criticism, or suggestions of other similar categories.&nbsp; In both cases, I believe there to be some instinct, acquired skill, or some combination&nbsp;thereof&nbsp;that draws it to my attention.&nbsp; I could just be making this up, though, so criticism is also welcome on this front.</p>\n<p><strong id=\"Absence_of_Salient_Information\">Absence of Salient Information</strong></p>\n<p>I believe tech support is like a magic trick in reverse.&nbsp; With a magic trick, the magician hides a crucial fact which he then distracts you from.&nbsp; He provides a false narrative of what's going on while confusing the sequence of events, culminating in the impossible,&nbsp;and relies on your own fear of appearing foolish to make you falsely report the conditions of the trick to both yourself and other spectators.</p>\n<p>In tech support,&nbsp;you are often presented with an&nbsp;impossible sequence of events; the customer's&nbsp;fear of appearing foolish makes them falsely report the conditions of the fault to both themselves and you, concealing a crucial fact which the rest of the narrative distracts you from.&nbsp; You then have to figure out how it was done.</p>\n<p>I recently asked a girl from my dance class out for a drink, and proceeded to receive the most shocking litany of mixed signals I could ever imagine receiving, drink not forthcoming.&nbsp; I boiled it down to three possibilities: either she was interested but incredibly shy, uninterested but just really friendly; or she had a completely different set of standards when it came to signaling romantic interest or lack thereof.&nbsp; I remember thinking how none of these possibilities made sense in context, and was reminded quite specifically of the idea of being more confused by fiction than by reality.&nbsp; It was driving my problem-solving faculties to distraction, and I have never been so&nbsp;relieved to discover a woman I was interested in already had a boyfriend.</p>\n<p>The phenomenon wasn't unlike a film with a massive plot-integral spoiler.&nbsp; There's this nagging feeling that the whole thing doesn't quite make sense, until the spoiler is revealed, at which point you suddenly see the whole of the preceeding sequence of events in a new revelatory light.&nbsp; I've often noticed with such films that when people know there's a big spoiler, they're more likely to spot it early on because they start groping around for plausible plot twists.&nbsp; I'm not sure if this is the best way to go about fishing for information you know is absent, though.</p>\n<p><strong id=\"Having_One_s_Head_Messed_With\">Having One's Head Messed With</strong></p>\n<p>I've read a few books on hypnosis, NLP and persuasion techniques, and I'm at least as well-versed on cognitive biases as most LW readers, but a couple of weeks ago someone fucked with my head.</p>\n<p>I was in East London (never a good start), fairly late at night with food in my hand.&nbsp; Beggars <em>always</em> seem to approach me when I have food in my hand.&nbsp; I don't think this is coincidence.&nbsp; This particular beggar, a woman in her twenties, spun a very quick story which I can't even begin to remember all the details of.&nbsp; Something about desperately needing bus fare to escape her abusive boyfriend and having just been released from hospital.&nbsp; Just thinking about it, two weeks later, makes me confused and disorientated.</p>\n<p>In retrospect, the story made no sense whatsoever, she was far too aggressive to be a downtrodden out-patient abuse victim, and far too <em>good</em> at making me feel like the only way I could possibly get out of this horrible distressing&nbsp;situation was to give her my small change, which I did.&nbsp; Afterwards I felt violated.</p>\n<p>The experience itself has probably armed me against it happening again to a certain degree, but I'm now worried about what I'm <em>not</em> armed against.&nbsp; There is a feeling of having your head messed with, but I only ever seem to experience it retrospectively.&nbsp; Can I train myself to spot it as it's happening?&nbsp; Is it related to the feeling I get when I recognise I'm being manipulated by advertising?&nbsp; Is there a how-to&nbsp;body of knowledge that can be assembled to defend against manipulation in general?</p>\n<p>This probably could have been more coherent, but it was surprisingly cathartic to write.</p>", "sections": [{"title": "Absence of Salient Information", "anchor": "Absence_of_Salient_Information", "level": 1}, {"title": "Having One's Head Messed With", "anchor": "Having_One_s_Head_Messed_With", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T01:44:02.521Z", "modifiedAt": null, "url": null, "title": "A neural correlate of certainty", "slug": "a-neural-correlate-of-certainty", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:50.838Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sGjYMpx5rKLaig7FX/a-neural-correlate-of-certainty", "pageUrlRelative": "/posts/sGjYMpx5rKLaig7FX/a-neural-correlate-of-certainty", "linkUrl": "https://www.lesswrong.com/posts/sGjYMpx5rKLaig7FX/a-neural-correlate-of-certainty", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20neural%20correlate%20of%20certainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20neural%20correlate%20of%20certainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsGjYMpx5rKLaig7FX%2Fa-neural-correlate-of-certainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20neural%20correlate%20of%20certainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsGjYMpx5rKLaig7FX%2Fa-neural-correlate-of-certainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsGjYMpx5rKLaig7FX%2Fa-neural-correlate-of-certainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>Adam Kepecs' <a href=\"http://www.sciencemag.org/site/feature/data/prizes/eppendorf/2010/kepecs.xhtml\">Eppendorf essay</a>, hosted at science's website (but not printed in the magazine), is about some neurons in the orbitofrontal cortex of rats that appear to represent uncertainty in an odor-recognition task by firing more often, at a rate roughly linearly proportional to the error rate.</p>\n<p>The involvement of OFC in decision-making isn't new, but the <a href=\"http://www.sciencemag.org.mutex.gmu.edu/site/feature/data/prizes/eppendorf/2009/images/McleanFig-med.gif\">graphs</a> are nice and quantitative.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sGjYMpx5rKLaig7FX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 6.527282253508017e-07, "legacy": true, "legacyId": "4178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T04:58:34.599Z", "modifiedAt": null, "url": null, "title": "The Trolley Problem: Dodging moral questions", "slug": "the-trolley-problem-dodging-moral-questions", "viewCount": null, "lastCommentedAt": "2018-05-08T12:19:40.215Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5YuQAj63CkcDLewbW/the-trolley-problem-dodging-moral-questions", "pageUrlRelative": "/posts/5YuQAj63CkcDLewbW/the-trolley-problem-dodging-moral-questions", "linkUrl": "https://www.lesswrong.com/posts/5YuQAj63CkcDLewbW/the-trolley-problem-dodging-moral-questions", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Trolley%20Problem%3A%20Dodging%20moral%20questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Trolley%20Problem%3A%20Dodging%20moral%20questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YuQAj63CkcDLewbW%2Fthe-trolley-problem-dodging-moral-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Trolley%20Problem%3A%20Dodging%20moral%20questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YuQAj63CkcDLewbW%2Fthe-trolley-problem-dodging-moral-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5YuQAj63CkcDLewbW%2Fthe-trolley-problem-dodging-moral-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 528, "htmlBody": "<p>The trolley problem is one of the more famous thought experiments in moral philosophy, and studies by psychologists and anthropologists suggest that the response distributions to its major permutations remain roughly the same throughout all human cultures. Most people will permit pulling the lever to redirect the trolley so that it will kill one person rather than five, but will balk at pushing one fat person in front of the trolley to save the five if that is the only available option of stopping it.</p>\n<p>However, in informal settings, where the dilemma is posed by a peer rather than a teacher or researcher, it has been my observation that there is another major category which accounts for a significant proportion of respondents' answers. Rather than choosing to flip the switch, push the fat man, or remain passive, many people will reject the question outright. They will attack the improbability of the premise, attempt to invent <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/TakeAThirdOption\" target=\"_self\">third options</a>, or appeal to their emotional state in the provided scenario (\"I would be too panicked to do anything\",) or some combination of the above, in order to opt out of answering the question on its own terms.</p>\n<p><a id=\"more\"></a></p>\n<p>However, in most cases, these excuses are not their <a href=\"/lw/wj/is_that_your_true_rejection/\" target=\"_self\">true rejection</a>. Those who tried to find third options or appeal to their emotional state will continue to reject the dilemma even when it is posed in its <a href=\"/lw/2k/the_least_convenient_possible_world/\" target=\"_self\">most inconvenient possible forms</a>, where they have the time to collect themselves and make a reasoned choice, but no possibility of implementing alternative solutions.</p>\n<p>Those who appealed to the unlikelihood of the scenario might appear to have the stronger objection; after all, the trolley dilemma <em>is</em> extremely improbable, and more inconvenient permutations of the problem might appear even less probable. However, trolleylike dilemmas are actually quite common in real life, when you take the scenario not as a case where only two options are available, but as a metaphor for any situation where <em>all</em> the available choices have negative repercussions, and attempting to optimize the outcome demands increased complicity in the dilemma. This method of framing the problem also tends not to cause people to reverse their rejections.&nbsp;</p>\n<p>Ultimately, when provided with optimally inconvenient and general forms of the dilemma, most of those who rejected the question will continue to make excuses to avoid answering the question on its own terms. They will insist that there <em>must</em> be superior alternatives, that external circumstances will absolve them from having to make a choice, or simply that they have no responsibility to address an artificial moral dilemma.</p>\n<p>When the respondents feel that they can possibly opt out of answering the question, the implications of the trolley problem become even more unnerving than the results from past studies suggest. It appears that we live in a world where not only will most people refuse complicity in a disaster in order to save more lives, but where many people reject outright the idea that they should have any considered set of moral standards for making hard choices <em>at all</em>. They have placed themselves in a reality <a href=\"/lw/36f/nahh_that_wouldnt_work/\" target=\"_self\">too accommodating of their preferences</a> to force them to have a system for dealing with situations with no ideal outcomes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LMFBzsJaCRADQqw3F": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5YuQAj63CkcDLewbW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 17, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "4179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 131, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TGux5Fhcd7GmTfNGC", "neQ7eXuaXpiYw7SBy", "682i9R2oSRg7BG8yD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T10:19:29.237Z", "modifiedAt": null, "url": null, "title": "Genetically Engineered Intelligence", "slug": "genetically-engineered-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:26.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jordan", "createdAt": "2009-04-01T03:52:25.470Z", "isAdmin": false, "displayName": "Jordan"}, "userId": "Za3R2v3y6Dn27G4ey", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dc9ehbHh6YA63ZyeS/genetically-engineered-intelligence", "pageUrlRelative": "/posts/dc9ehbHh6YA63ZyeS/genetically-engineered-intelligence", "linkUrl": "https://www.lesswrong.com/posts/dc9ehbHh6YA63ZyeS/genetically-engineered-intelligence", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Genetically%20Engineered%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGenetically%20Engineered%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdc9ehbHh6YA63ZyeS%2Fgenetically-engineered-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Genetically%20Engineered%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdc9ehbHh6YA63ZyeS%2Fgenetically-engineered-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdc9ehbHh6YA63ZyeS%2Fgenetically-engineered-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 336, "htmlBody": "<p>There are a lot of unknowns about the future of intelligence: artificial intelligence, uploading, augmentation, and so on. Most of these technologies are likely a ways off, or at least far enough away to confound predictions. Genetic engineering, however, presents a very near term and well understood possibility for developing greater intelligence.</p>\n<p>A recent news story published in <a href=\"http://www.scmp.com/portal/site/SCMP/menuitem.2c913216495213d5df646910cba0a0a0/?vgnextoid=663ae01bd1dac210VgnVCM100000360a0a0aRCRD&amp;vgnextfmt=teaser&amp;ss=hong+kong&amp;s=news\">South China Morning</a>&nbsp;and discussed on <a href=\"http://infoproc.blogspot.com/2010/12/supercomputers-and-mystery-of-iq.html\">Steve Hsu's blog</a>&nbsp;highlights China's push to understand the genetic underpinnings of intelligence. China is planning to sequence the full genome of 1000 of its brightest kids, in the hopes of locating key genes responsible for higher intelligence. Behind the current project is&nbsp;<a href=\"http://www.genomics.cn/en/index.php\">BGI</a>, which&nbsp;is aiming to be (or already is) <a href=\"http://www.bio-itworld.com/2010/08/11/BGI-exclusive.html\">the largest DNA sequencing center in the world</a>.</p>\n<p>Suppose that intelligence has a large genetic component (reasonable, considering&nbsp;<a href=\"http://en.wikipedia.org/wiki/Heritability_of_IQ#Estimates_and_caveats_to_them\">estimates for heritability</a>). Suppose that the current study unveils those components (if not this study, then likely another study soon,&nbsp;<a href=\"http://singularityhub.com/2010/01/26/exclusive-complete-genomics-to-sequence-1-million-genomes-interview-with-ceo/\">perhaps with millions of genomes</a>). Then with some advances in genetic engineering China could quickly raise a huge population of incredibly intelligent people.</p>\n<p>Such an&nbsp;endeavor could never be carried out on a large, public scale in the West, but it seems China has fewer qualms.</p>\n<p>The timescales here are on the order of 20 years, which are relevant compared to most estimates for AI and WBE. More, genetic engineering human intelligence&nbsp;seems to be on a much more predictable path than other intelligence technologies. For both these reasons I think understanding, discussing, and keeping an eye on this issue is important.</p>\n<p>What are the ramifications for</p>\n<ul>\n<li>AI research? FAI? In particular relating to enhanced humans speeding further development</li>\n<li>Whole Brain Emulation research?</li>\n<li>Other technologies that may pose existential risks (nanotech, biotech, etc, especially in light of the fact that it may be China leading the way)?</li>\n<li>The potential for recursive feedback? (Smarter scientists engineering smarter scientists. Less worrisome due to timescales)</li>\n</ul>\n<p>Of course, there are a host of other interesting questions relating to societal impact, both near and long term. Feel free to discuss these as well.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dc9ehbHh6YA63ZyeS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 28, "extendedScore": null, "score": 6.528555415872859e-07, "legacy": true, "legacyId": "4180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T10:23:55.626Z", "modifiedAt": null, "url": null, "title": "Help request: What is the Kolmogorov complexity of computable approximations to AIXI?", "slug": "help-request-what-is-the-kolmogorov-complexity-of-computable", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:12.223Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f6b8ESmTYZPHgFWWg/help-request-what-is-the-kolmogorov-complexity-of-computable", "pageUrlRelative": "/posts/f6b8ESmTYZPHgFWWg/help-request-what-is-the-kolmogorov-complexity-of-computable", "linkUrl": "https://www.lesswrong.com/posts/f6b8ESmTYZPHgFWWg/help-request-what-is-the-kolmogorov-complexity-of-computable", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20request%3A%20What%20is%20the%20Kolmogorov%20complexity%20of%20computable%20approximations%20to%20AIXI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20request%3A%20What%20is%20the%20Kolmogorov%20complexity%20of%20computable%20approximations%20to%20AIXI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff6b8ESmTYZPHgFWWg%2Fhelp-request-what-is-the-kolmogorov-complexity-of-computable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20request%3A%20What%20is%20the%20Kolmogorov%20complexity%20of%20computable%20approximations%20to%20AIXI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff6b8ESmTYZPHgFWWg%2Fhelp-request-what-is-the-kolmogorov-complexity-of-computable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff6b8ESmTYZPHgFWWg%2Fhelp-request-what-is-the-kolmogorov-complexity-of-computable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Does anyone happen to know the Komogorov complexity (in some suitable, standard UTM -- or, failing that, in lines of Python or something) of computable approximations of AIXI?</p>\n<p>I'm writing a paper on how simple or complicated intelligence is, and what implications that has for AI forecasting. &nbsp;In that context: adopt Shane Legg's measure of intelligence (i.e., let \"intelligence\" measure a system's average goal-achievement across the different \"universe\" programs that might be causing it to win or not win reward at each time step, weighted according to the universe program's simplicity).</p>\n<p>Let k(x, y) denote the Kolmogorov complexity of the shortest program that attains an intelligence of at least x, when allowed an amount y of computation (i.e., of steps it gets to run our canonical UTM). &nbsp;Then, granting certain caveats, AIXI and approximations thereto tell us that the limit as y approaches infinity of k(x,y) is pretty small for any computably attainable value of x. &nbsp;(Right?)</p>\n<p>What I'd like is to stick an actual number, or at least an upper bound, on \"pretty small\".</p>\n<p>If someone could help me out, I'd be much obliged.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 2, "sYm3HiWcfZvrGu3ui": 2, "5f5c37ee1b5cdee568cfb25c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f6b8ESmTYZPHgFWWg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 6.528566384155034e-07, "legacy": true, "legacyId": "4181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T10:44:19.060Z", "modifiedAt": null, "url": null, "title": "Some ideas on communicating risks to the general public", "slug": "some-ideas-on-communicating-risks-to-the-general-public", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:48.267Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lightwave", "createdAt": "2009-03-02T00:10:45.771Z", "isAdmin": false, "displayName": "Lightwave"}, "userId": "wmf7PMjRsYvMAcQHg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4HkMtfb5gzTyzgzSm/some-ideas-on-communicating-risks-to-the-general-public", "pageUrlRelative": "/posts/4HkMtfb5gzTyzgzSm/some-ideas-on-communicating-risks-to-the-general-public", "linkUrl": "https://www.lesswrong.com/posts/4HkMtfb5gzTyzgzSm/some-ideas-on-communicating-risks-to-the-general-public", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20ideas%20on%20communicating%20risks%20to%20the%20general%20public&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20ideas%20on%20communicating%20risks%20to%20the%20general%20public%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4HkMtfb5gzTyzgzSm%2Fsome-ideas-on-communicating-risks-to-the-general-public%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20ideas%20on%20communicating%20risks%20to%20the%20general%20public%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4HkMtfb5gzTyzgzSm%2Fsome-ideas-on-communicating-risks-to-the-general-public", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4HkMtfb5gzTyzgzSm%2Fsome-ideas-on-communicating-risks-to-the-general-public", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p><a href=\"http://www.decisionsciencenews.com/2010/12/03/some-ideas-on-communicating-risks-to-the-general-public/\" target=\"_blank\">http://www.decisionsciencenews.com/2010/12/03/some-ideas-on-communicating-risks-to-the-general-public/</a></p>\n<p>Got this off from Reddit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4HkMtfb5gzTyzgzSm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 6.528616758456091e-07, "legacy": true, "legacyId": "4183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T18:08:26.618Z", "modifiedAt": null, "url": null, "title": "LINK: BBC News on living forever, cryonics", "slug": "link-bbc-news-on-living-forever-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:51.518Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oFp6JLn8z9uxgdPp8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8bCg5GcJMArkZ2dhh/link-bbc-news-on-living-forever-cryonics", "pageUrlRelative": "/posts/8bCg5GcJMArkZ2dhh/link-bbc-news-on-living-forever-cryonics", "linkUrl": "https://www.lesswrong.com/posts/8bCg5GcJMArkZ2dhh/link-bbc-news-on-living-forever-cryonics", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20BBC%20News%20on%20living%20forever%2C%20cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20BBC%20News%20on%20living%20forever%2C%20cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bCg5GcJMArkZ2dhh%2Flink-bbc-news-on-living-forever-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20BBC%20News%20on%20living%20forever%2C%20cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bCg5GcJMArkZ2dhh%2Flink-bbc-news-on-living-forever-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bCg5GcJMArkZ2dhh%2Flink-bbc-news-on-living-forever-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p>The BBC News recently ran an <a href=\"http://www.bbc.co.uk/news/magazine-11911065\">interesting piece</a> on living forever. They discuss some of the standard arguments against cryonics and transhumanism; overall, the article is pretty critical of both. I suspect most LessWrong readers won't find it convincing, but it's still worth a quick read.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8bCg5GcJMArkZ2dhh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 6.529714131066769e-07, "legacy": true, "legacyId": "4184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T21:49:06.384Z", "modifiedAt": null, "url": null, "title": "The Truth about Scotsmen, or: Dissolving Fallacies", "slug": "the-truth-about-scotsmen-or-dissolving-fallacies-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oNiAySMEJ5Ep84nMa/the-truth-about-scotsmen-or-dissolving-fallacies-0", "pageUrlRelative": "/posts/oNiAySMEJ5Ep84nMa/the-truth-about-scotsmen-or-dissolving-fallacies-0", "linkUrl": "https://www.lesswrong.com/posts/oNiAySMEJ5Ep84nMa/the-truth-about-scotsmen-or-dissolving-fallacies-0", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Truth%20about%20Scotsmen%2C%20or%3A%20Dissolving%20Fallacies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Truth%20about%20Scotsmen%2C%20or%3A%20Dissolving%20Fallacies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNiAySMEJ5Ep84nMa%2Fthe-truth-about-scotsmen-or-dissolving-fallacies-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Truth%20about%20Scotsmen%2C%20or%3A%20Dissolving%20Fallacies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNiAySMEJ5Ep84nMa%2Fthe-truth-about-scotsmen-or-dissolving-fallacies-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNiAySMEJ5Ep84nMa%2Fthe-truth-about-scotsmen-or-dissolving-fallacies-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 886, "htmlBody": "<p>@font-face {   font-family: \"Cambria\"; }p.MsoNormal, li.MsoNormal, div.MsoNormal { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }div.Section1 { page: Section1; }\n<p class=\"MsoNormal\">One unfortunate feature I&rsquo;ve noticed in arguments between logically well-trained people and the untrained is a tendency for members of the former group to point out logical errors as if they were counterarguments. This is almost totally ineffective either in changing the mind of your opponent or in convincing neutral observers. There are two main reasons for this failure.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong>1. Pointing out fallacies is not the same thing as urging someone to reconsider their viewpoint. </strong></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Fallacies are problematic because they&rsquo;re errors in the line of reasoning that one uses to arrive at or support a conclusion &ndash; and in the same way that taking the wrong route to the movie theater is bad because you won&rsquo;t get there, committing a fallacy is bad because you&rsquo;ll be led to the wrong conclusions.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">But all that isn&rsquo;t inherent in the word &lsquo;fallacy&rsquo;: the vast majority of human beings don&rsquo;t understand the statement &ldquo;that&rsquo;s a fallacy&rdquo; as &ldquo;you seem to have been misled by this particular logical error &ndash; you should reevaluate your thought process and see if you arrive at the same conclusions without it.&rdquo; Rather, most people will regard it as an <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">enemy attack</a>,with the result that they will either reject the existence of the fallacy or simply ignore it. If, by some chance, they do acknowledge the error, they&rsquo;ll usually interpret it as &ldquo;your argument for that conclusion is wrong &ndash; you should argue for the same conclusion in a different way.&rdquo;</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">If you&rsquo;re actually trying to convince someone (as opposed to, say, arguing to <a href=\"http://en.wikipedia.org/wiki/Eristic\">appease the goddess Eris</a>) by showing them that the chain of logic they base their current belief on is unsound, you have to say so explicitly. Otherwise saying &ldquo;fallacy&rdquo; is about as effective as just telling them that they&rsquo;re wrong.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong>2. Pointing out the obvious logical errors that fallacies characterize often obscures the deeper errors that generate the fallacies.</strong></p>\n<p class=\"MsoNormal\"><strong>&nbsp;</strong></p>\n<p class=\"MsoNormal\"><span>&nbsp;</span>Take as an example the <a href=\"http://en.wikipedia.org/wiki/No_true_Scotsman\">No True Scotsman</a> fallacy. In the canonical example, the Scotsman, having seen a report of a crime, claims that no Scotsman would do such a thing. When presented with evidence of just such a Scottish criminal, he qualifies his claim, saying that no <em>true </em>Scotsman would do such a thing.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">The obvious response to such a statement is &ldquo;Ah, but you&rsquo;re committing <a href=\"http://en.wikipedia.org/wiki/Self-reference\">the No True Scotsman fallacy</a>! By excluding any Scotsman who <em>would </em>do such a thing from your reference class, you&rsquo;re making your statement tautologically true!&rdquo;</p>\n<p class=\"MsoListParagraph\">&nbsp;</p>\n<p class=\"MsoNormal\">While this is a valid argument, it&rsquo;s not an effective one. The Scotsman, rather than changing his beliefs about the inherent goodness of all Scots, is likely to just look at you sulkily. That&rsquo;s because all you&rsquo;ve done is deprive him of evidence for his belief, not make him disbelieve it &ndash; wiped out one of his squadrons, so to speak, rather than making him switch sides in the war. If you were actually trying to make him change his mind, you&rsquo;d have to have a better model of how it works.</p>\n<p class=\"MsoListParagraph\">&nbsp;</p>\n<p class=\"MsoNormal\">No one is legitimately entranced by a fallacy like No True Scotsman &ndash; it&rsquo;s used strictly as rationalization, not as a faulty but appealing reason to <em>create </em>a belief. Therefore the reason for his belief must lie deeper. In this case, you can find it by looking at what counts for him as evidence. To the Scotsman, the crime committed by the Englishman is an indictment of the English national character, not just the action of an individual. Likewise, a similar crime committed by a Scotsman would be evidence against the goodness of the Scottish character. Since he already believes deeply in the goodness of the Scottish character, he has only two choices: acknowledge that he was wrong about a deeply-felt belief, or decide that the criminal was not <em>really</em> Scottish.</p>\n<p class=\"MsoListParagraph\">&nbsp;</p>\n<p class=\"MsoNormal\">The error at the deepest level is that the Scotman possesses an unreasoned belief in the superiority of Scottish character, but it would be impractical at best to argue that point. The intermediate and more important error is that he views national character as monolithic &ndash; if Scottish character is better than English character, it must be better across all individuals &ndash; and therefore counts the actions of one individual as non-negligible evidence against the goodness of Scotland. If you&rsquo;re trying to convince him that yes, that criminal really <em>can </em>be a Scotsman, the best way to do so would not be to tell him that he&rsquo;s comitting a fallacy, but to argue directly against the underlying rationale connecting the individual&rsquo;s crime and his nationalism. If national character is determined by, say, the ratio of good men to bad men in each nation, then bad men can exist in both England and Scotland without impinging on Scotland&rsquo;s superiority &ndash; and suddenly there&rsquo;s no reason for the fallacy at all. You&rsquo;ve disproved his belief and changed his mind, without the word &lsquo;fallacy&rsquo; once passing your lips.</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oNiAySMEJ5Ep84nMa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>@font-face {   font-family: \"Cambria\"; }p.MsoNormal, li.MsoNormal, div.MsoNormal { margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast { margin: 0in 0in 0.0001pt 0.5in; font-size: 12pt; font-family: \"Times New Roman\"; }div.Section1 { page: Section1; }\n</p><p class=\"MsoNormal\">One unfortunate feature I\u2019ve noticed in arguments between logically well-trained people and the untrained is a tendency for members of the former group to point out logical errors as if they were counterarguments. This is almost totally ineffective either in changing the mind of your opponent or in convincing neutral observers. There are two main reasons for this failure.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong id=\"1__Pointing_out_fallacies_is_not_the_same_thing_as_urging_someone_to_reconsider_their_viewpoint__\">1. Pointing out fallacies is not the same thing as urging someone to reconsider their viewpoint. </strong></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Fallacies are problematic because they\u2019re errors in the line of reasoning that one uses to arrive at or support a conclusion \u2013 and in the same way that taking the wrong route to the movie theater is bad because you won\u2019t get there, committing a fallacy is bad because you\u2019ll be led to the wrong conclusions.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">But all that isn\u2019t inherent in the word \u2018fallacy\u2019: the vast majority of human beings don\u2019t understand the statement \u201cthat\u2019s a fallacy\u201d as \u201cyou seem to have been misled by this particular logical error \u2013 you should reevaluate your thought process and see if you arrive at the same conclusions without it.\u201d Rather, most people will regard it as an <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">enemy attack</a>,with the result that they will either reject the existence of the fallacy or simply ignore it. If, by some chance, they do acknowledge the error, they\u2019ll usually interpret it as \u201cyour argument for that conclusion is wrong \u2013 you should argue for the same conclusion in a different way.\u201d</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">If you\u2019re actually trying to convince someone (as opposed to, say, arguing to <a href=\"http://en.wikipedia.org/wiki/Eristic\">appease the goddess Eris</a>) by showing them that the chain of logic they base their current belief on is unsound, you have to say so explicitly. Otherwise saying \u201cfallacy\u201d is about as effective as just telling them that they\u2019re wrong.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong id=\"2__Pointing_out_the_obvious_logical_errors_that_fallacies_characterize_often_obscures_the_deeper_errors_that_generate_the_fallacies_\">2. Pointing out the obvious logical errors that fallacies characterize often obscures the deeper errors that generate the fallacies.</strong></p>\n<p class=\"MsoNormal\"><strong>&nbsp;</strong></p>\n<p class=\"MsoNormal\"><span>&nbsp;</span>Take as an example the <a href=\"http://en.wikipedia.org/wiki/No_true_Scotsman\">No True Scotsman</a> fallacy. In the canonical example, the Scotsman, having seen a report of a crime, claims that no Scotsman would do such a thing. When presented with evidence of just such a Scottish criminal, he qualifies his claim, saying that no <em>true </em>Scotsman would do such a thing.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">The obvious response to such a statement is \u201cAh, but you\u2019re committing <a href=\"http://en.wikipedia.org/wiki/Self-reference\">the No True Scotsman fallacy</a>! By excluding any Scotsman who <em>would </em>do such a thing from your reference class, you\u2019re making your statement tautologically true!\u201d</p>\n<p class=\"MsoListParagraph\">&nbsp;</p>\n<p class=\"MsoNormal\">While this is a valid argument, it\u2019s not an effective one. The Scotsman, rather than changing his beliefs about the inherent goodness of all Scots, is likely to just look at you sulkily. That\u2019s because all you\u2019ve done is deprive him of evidence for his belief, not make him disbelieve it \u2013 wiped out one of his squadrons, so to speak, rather than making him switch sides in the war. If you were actually trying to make him change his mind, you\u2019d have to have a better model of how it works.</p>\n<p class=\"MsoListParagraph\">&nbsp;</p>\n<p class=\"MsoNormal\">No one is legitimately entranced by a fallacy like No True Scotsman \u2013 it\u2019s used strictly as rationalization, not as a faulty but appealing reason to <em>create </em>a belief. Therefore the reason for his belief must lie deeper. In this case, you can find it by looking at what counts for him as evidence. To the Scotsman, the crime committed by the Englishman is an indictment of the English national character, not just the action of an individual. Likewise, a similar crime committed by a Scotsman would be evidence against the goodness of the Scottish character. Since he already believes deeply in the goodness of the Scottish character, he has only two choices: acknowledge that he was wrong about a deeply-felt belief, or decide that the criminal was not <em>really</em> Scottish.</p>\n<p class=\"MsoListParagraph\">&nbsp;</p>\n<p class=\"MsoNormal\">The error at the deepest level is that the Scotman possesses an unreasoned belief in the superiority of Scottish character, but it would be impractical at best to argue that point. The intermediate and more important error is that he views national character as monolithic \u2013 if Scottish character is better than English character, it must be better across all individuals \u2013 and therefore counts the actions of one individual as non-negligible evidence against the goodness of Scotland. If you\u2019re trying to convince him that yes, that criminal really <em>can </em>be a Scotsman, the best way to do so would not be to tell him that he\u2019s comitting a fallacy, but to argue directly against the underlying rationale connecting the individual\u2019s crime and his nationalism. If national character is determined by, say, the ratio of good men to bad men in each nation, then bad men can exist in both England and Scotland without impinging on Scotland\u2019s superiority \u2013 and suddenly there\u2019s no reason for the fallacy at all. You\u2019ve disproved his belief and changed his mind, without the word \u2018fallacy\u2019 once passing your lips.</p>\n<p></p>", "sections": [{"title": "1. Pointing out fallacies is not the same thing as urging someone to reconsider their viewpoint. ", "anchor": "1__Pointing_out_fallacies_is_not_the_same_thing_as_urging_someone_to_reconsider_their_viewpoint__", "level": 1}, {"title": "2. Pointing out the obvious logical errors that fallacies characterize often obscures the deeper errors that generate the fallacies.", "anchor": "2__Pointing_out_the_obvious_logical_errors_that_fallacies_characterize_often_obscures_the_deeper_errors_that_generate_the_fallacies_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-05T21:57:07.976Z", "modifiedAt": null, "url": null, "title": "The Truth about Scotsmen, or: Dissolving Fallacies", "slug": "the-truth-about-scotsmen-or-dissolving-fallacies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:51.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3MAAm5iByjQP9LZKf/the-truth-about-scotsmen-or-dissolving-fallacies", "pageUrlRelative": "/posts/3MAAm5iByjQP9LZKf/the-truth-about-scotsmen-or-dissolving-fallacies", "linkUrl": "https://www.lesswrong.com/posts/3MAAm5iByjQP9LZKf/the-truth-about-scotsmen-or-dissolving-fallacies", "postedAtFormatted": "Sunday, December 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Truth%20about%20Scotsmen%2C%20or%3A%20Dissolving%20Fallacies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Truth%20about%20Scotsmen%2C%20or%3A%20Dissolving%20Fallacies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3MAAm5iByjQP9LZKf%2Fthe-truth-about-scotsmen-or-dissolving-fallacies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Truth%20about%20Scotsmen%2C%20or%3A%20Dissolving%20Fallacies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3MAAm5iByjQP9LZKf%2Fthe-truth-about-scotsmen-or-dissolving-fallacies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3MAAm5iByjQP9LZKf%2Fthe-truth-about-scotsmen-or-dissolving-fallacies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 803, "htmlBody": "<div id=\"entry_t3_389\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p class=\"MsoNormal\">One unfortunate feature I&rsquo;ve noticed in arguments between logically well-trained people and the untrained is a tendency for members of the former group to point out logical errors as if they were counterarguments. This is almost totally ineffective either in changing the mind of your opponent or in convincing neutral observers. There are two main reasons for this failure.</p>\n<p class=\"MsoNormal\"><strong>1. Pointing out fallacies is not the same thing as urging someone to reconsider their viewpoint. </strong></p>\n<p class=\"MsoNormal\">Fallacies are problematic because they&rsquo;re errors in the line of reasoning that one uses to arrive at or support a conclusion. In the same way that taking the wrong route to the movie theater is bad because you won&rsquo;t get there, committing a fallacy is bad because you&rsquo;ll be led to the wrong conclusions.</p>\n<p class=\"MsoNormal\">But all that isn&rsquo;t inherent in the word &lsquo;fallacy&rsquo;: the vast majority of human beings don&rsquo;t understand the statement &ldquo;that&rsquo;s a fallacy&rdquo; as &ldquo;you seem to have been misled by this particular logical error &ndash; you should reevaluate your thought process and see if you arrive at the same conclusions without it.&rdquo; Rather, most people will regard it as an <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">enemy attack</a>,with the result that they will either reject the existence of the fallacy or simply ignore it. If, by some chance, they do acknowledge the error, they&rsquo;ll usually interpret it as &ldquo;your argument for that conclusion is wrong &ndash; you should argue for the same conclusion in a different way.&rdquo;</p>\n<p class=\"MsoNormal\">If you&rsquo;re actually trying to convince someone (as opposed to, say, arguing to <a href=\"http://en.wikipedia.org/wiki/Eristic\">appease the goddess Eris</a>) by showing them that the chain of logic they base their current belief on is unsound, you have to say so explicitly. Otherwise saying &ldquo;fallacy&rdquo; is about as effective as just telling them that they&rsquo;re wrong.</p>\n<p class=\"MsoNormal\"><strong>2. Pointing out the obvious logical errors that fallacies characterize often obscures the deeper errors that generate the fallacies.</strong></p>\n<p class=\"MsoNormal\">Take as an example the <a href=\"http://en.wikipedia.org/wiki/No_true_Scotsman\">No True Scotsman</a> fallacy. In the canonical example, the Scotsman, having seen a report of a crime, claims that no Scotsman would do such a thing. When presented with evidence of just such a Scottish criminal, he qualifies his claim, saying that no <em>true </em>Scotsman would do such a thing.</p>\n<p class=\"MsoNormal\">The obvious response to such a statement is &ldquo;Ah, but you&rsquo;re committing <a href=\"http://en.wikipedia.org/wiki/Self-reference\">the No True Scotsman fallacy</a>! By excluding any Scotsman who <em>would </em>do such a thing from your reference class, you&rsquo;re making your statement tautologically true!&rdquo;</p>\n<p class=\"MsoNormal\">While this is a valid argument, it&rsquo;s not an effective one. The Scotsman, rather than changing his beliefs about the inherent goodness of all Scots, is likely to just look at you sulkily. That&rsquo;s because all you&rsquo;ve done is deprive him of evidence for his belief, not make him disbelieve it &ndash; wiped out one of his squadrons, so to speak, rather than making him switch sides in the war. If you were actually trying to make him change his mind, you&rsquo;d have to have a better model of how it works.</p>\n<p class=\"MsoNormal\">No one is legitimately entranced by a fallacy like No True Scotsman &ndash; it&rsquo;s used strictly as rationalization, not as a faulty but appealing reason to <em>create </em>a belief. Therefore the reason for his belief must lie deeper. In this case, you can find it by looking at what counts for him as evidence. To the Scotsman, the crime committed by the Englishman is an indictment of the English national character, not just the action of an individual. Likewise, a similar crime committed by a Scotsman would be evidence against the goodness of the Scottish character. Since he already believes deeply in the goodness of the Scottish character, he has only two choices: acknowledge that he was wrong about a deeply felt belief, or decide that the criminal was not <em>really</em> Scottish.</p>\n<p class=\"MsoNormal\">The error at the deepest level is that the Scotman possesses an unreasoned belief in the superiority of Scottish character, but it would be impractical at best to argue that point. The intermediate and more important error is that he views national character as monolithic &ndash; if Scottish character is better than English character, it must be better across all individuals &ndash; and therefore counts the actions of one individual as non-negligible evidence against the goodness of Scotland. If you&rsquo;re trying to convince him that yes, that criminal really <em>can </em>be a Scotsman, the best way to do so would not be to tell him that he&rsquo;s comitting a fallacy, but to argue directly against the underlying rationale connecting the individual&rsquo;s crime and his nationalism. If national character is determined by, say, the ratio of good men to bad men in each nation, then bad men can exist in both England and Scotland without impinging on Scotland&rsquo;s superiority &ndash; and suddenly there&rsquo;s no reason for the fallacy at all. You&rsquo;ve disproved his belief and changed his mind, without the word &lsquo;fallacy&rsquo; once passing your lips.</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 3, "ZXFpyQWPB5ideFbEG": 1, "LDTSbmXtokYAsEq8e": 1, "RMtdp6eGNjTZcmwJ6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3MAAm5iByjQP9LZKf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 37, "extendedScore": null, "score": 6.530279317357865e-07, "legacy": true, "legacyId": "4186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"entry_t3_389\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p class=\"MsoNormal\">One unfortunate feature I\u2019ve noticed in arguments between logically well-trained people and the untrained is a tendency for members of the former group to point out logical errors as if they were counterarguments. This is almost totally ineffective either in changing the mind of your opponent or in convincing neutral observers. There are two main reasons for this failure.</p>\n<p class=\"MsoNormal\"><strong id=\"1__Pointing_out_fallacies_is_not_the_same_thing_as_urging_someone_to_reconsider_their_viewpoint__\">1. Pointing out fallacies is not the same thing as urging someone to reconsider their viewpoint. </strong></p>\n<p class=\"MsoNormal\">Fallacies are problematic because they\u2019re errors in the line of reasoning that one uses to arrive at or support a conclusion. In the same way that taking the wrong route to the movie theater is bad because you won\u2019t get there, committing a fallacy is bad because you\u2019ll be led to the wrong conclusions.</p>\n<p class=\"MsoNormal\">But all that isn\u2019t inherent in the word \u2018fallacy\u2019: the vast majority of human beings don\u2019t understand the statement \u201cthat\u2019s a fallacy\u201d as \u201cyou seem to have been misled by this particular logical error \u2013 you should reevaluate your thought process and see if you arrive at the same conclusions without it.\u201d Rather, most people will regard it as an <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">enemy attack</a>,with the result that they will either reject the existence of the fallacy or simply ignore it. If, by some chance, they do acknowledge the error, they\u2019ll usually interpret it as \u201cyour argument for that conclusion is wrong \u2013 you should argue for the same conclusion in a different way.\u201d</p>\n<p class=\"MsoNormal\">If you\u2019re actually trying to convince someone (as opposed to, say, arguing to <a href=\"http://en.wikipedia.org/wiki/Eristic\">appease the goddess Eris</a>) by showing them that the chain of logic they base their current belief on is unsound, you have to say so explicitly. Otherwise saying \u201cfallacy\u201d is about as effective as just telling them that they\u2019re wrong.</p>\n<p class=\"MsoNormal\"><strong id=\"2__Pointing_out_the_obvious_logical_errors_that_fallacies_characterize_often_obscures_the_deeper_errors_that_generate_the_fallacies_\">2. Pointing out the obvious logical errors that fallacies characterize often obscures the deeper errors that generate the fallacies.</strong></p>\n<p class=\"MsoNormal\">Take as an example the <a href=\"http://en.wikipedia.org/wiki/No_true_Scotsman\">No True Scotsman</a> fallacy. In the canonical example, the Scotsman, having seen a report of a crime, claims that no Scotsman would do such a thing. When presented with evidence of just such a Scottish criminal, he qualifies his claim, saying that no <em>true </em>Scotsman would do such a thing.</p>\n<p class=\"MsoNormal\">The obvious response to such a statement is \u201cAh, but you\u2019re committing <a href=\"http://en.wikipedia.org/wiki/Self-reference\">the No True Scotsman fallacy</a>! By excluding any Scotsman who <em>would </em>do such a thing from your reference class, you\u2019re making your statement tautologically true!\u201d</p>\n<p class=\"MsoNormal\">While this is a valid argument, it\u2019s not an effective one. The Scotsman, rather than changing his beliefs about the inherent goodness of all Scots, is likely to just look at you sulkily. That\u2019s because all you\u2019ve done is deprive him of evidence for his belief, not make him disbelieve it \u2013 wiped out one of his squadrons, so to speak, rather than making him switch sides in the war. If you were actually trying to make him change his mind, you\u2019d have to have a better model of how it works.</p>\n<p class=\"MsoNormal\">No one is legitimately entranced by a fallacy like No True Scotsman \u2013 it\u2019s used strictly as rationalization, not as a faulty but appealing reason to <em>create </em>a belief. Therefore the reason for his belief must lie deeper. In this case, you can find it by looking at what counts for him as evidence. To the Scotsman, the crime committed by the Englishman is an indictment of the English national character, not just the action of an individual. Likewise, a similar crime committed by a Scotsman would be evidence against the goodness of the Scottish character. Since he already believes deeply in the goodness of the Scottish character, he has only two choices: acknowledge that he was wrong about a deeply felt belief, or decide that the criminal was not <em>really</em> Scottish.</p>\n<p class=\"MsoNormal\">The error at the deepest level is that the Scotman possesses an unreasoned belief in the superiority of Scottish character, but it would be impractical at best to argue that point. The intermediate and more important error is that he views national character as monolithic \u2013 if Scottish character is better than English character, it must be better across all individuals \u2013 and therefore counts the actions of one individual as non-negligible evidence against the goodness of Scotland. If you\u2019re trying to convince him that yes, that criminal really <em>can </em>be a Scotsman, the best way to do so would not be to tell him that he\u2019s comitting a fallacy, but to argue directly against the underlying rationale connecting the individual\u2019s crime and his nationalism. If national character is determined by, say, the ratio of good men to bad men in each nation, then bad men can exist in both England and Scotland without impinging on Scotland\u2019s superiority \u2013 and suddenly there\u2019s no reason for the fallacy at all. You\u2019ve disproved his belief and changed his mind, without the word \u2018fallacy\u2019 once passing your lips.</p>\n</div>\n</div>\n</div>", "sections": [{"title": "1. Pointing out fallacies is not the same thing as urging someone to reconsider their viewpoint. ", "anchor": "1__Pointing_out_fallacies_is_not_the_same_thing_as_urging_someone_to_reconsider_their_viewpoint__", "level": 1}, {"title": "2. Pointing out the obvious logical errors that fallacies characterize often obscures the deeper errors that generate the fallacies.", "anchor": "2__Pointing_out_the_obvious_logical_errors_that_fallacies_characterize_often_obscures_the_deeper_errors_that_generate_the_fallacies_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-06T02:04:43.359Z", "modifiedAt": null, "url": null, "title": "How seriously should I take the supposed problems with Cox's theorem?", "slug": "how-seriously-should-i-take-the-supposed-problems-with-cox-s", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:51.021Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gasTtZvvbXRZgJQey/how-seriously-should-i-take-the-supposed-problems-with-cox-s", "pageUrlRelative": "/posts/gasTtZvvbXRZgJQey/how-seriously-should-i-take-the-supposed-problems-with-cox-s", "linkUrl": "https://www.lesswrong.com/posts/gasTtZvvbXRZgJQey/how-seriously-should-i-take-the-supposed-problems-with-cox-s", "postedAtFormatted": "Monday, December 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20seriously%20should%20I%20take%20the%20supposed%20problems%20with%20Cox's%20theorem%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20seriously%20should%20I%20take%20the%20supposed%20problems%20with%20Cox's%20theorem%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgasTtZvvbXRZgJQey%2Fhow-seriously-should-i-take-the-supposed-problems-with-cox-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20seriously%20should%20I%20take%20the%20supposed%20problems%20with%20Cox's%20theorem%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgasTtZvvbXRZgJQey%2Fhow-seriously-should-i-take-the-supposed-problems-with-cox-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgasTtZvvbXRZgJQey%2Fhow-seriously-should-i-take-the-supposed-problems-with-cox-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>I had been under the impression that <a href=\"http://en.wikipedia.org/wiki/Cox%27s_theorem\">Cox's theorem</a> said something pretty strong about the consistent ways to represent uncertainty, relying on very plausible assumptions. However, I recently found <a href=\"http://arxiv.org/abs/cs/9911012\">this 1999 paper</a>, which claims that Cox's result actually requires some stronger assumptions. I am curious what people here think of this. Has there been subsequent work which relaxes the stronger assumptions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gasTtZvvbXRZgJQey", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 6.530890073293276e-07, "legacy": true, "legacyId": "4188", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-06T04:12:41.198Z", "modifiedAt": null, "url": null, "title": "It's Not About Efficiency", "slug": "it-s-not-about-efficiency", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:51.907Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jbBdRijbRfWhk92Mu/it-s-not-about-efficiency", "pageUrlRelative": "/posts/jbBdRijbRfWhk92Mu/it-s-not-about-efficiency", "linkUrl": "https://www.lesswrong.com/posts/jbBdRijbRfWhk92Mu/it-s-not-about-efficiency", "postedAtFormatted": "Monday, December 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20It's%20Not%20About%20Efficiency&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIt's%20Not%20About%20Efficiency%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjbBdRijbRfWhk92Mu%2Fit-s-not-about-efficiency%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=It's%20Not%20About%20Efficiency%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjbBdRijbRfWhk92Mu%2Fit-s-not-about-efficiency", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjbBdRijbRfWhk92Mu%2Fit-s-not-about-efficiency", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>When I explain the importance of donating only to the right charity, I've been told that it's not about efficiency. This is completely correct.</p>\n<p>Imagine a paperclip company. They care only about making paperclips. They will do anything within their power to improve efficiency, but they don't care about efficiency. They care about making paperclips. Efficiency is just a measure of how well they're accomplishing their goal. You don't try to be efficient because you want to be efficient. You try to be efficient because you want something.</p>\n<p>When I try to help people, the same principle applies. I couldn't care less about a charity's efficiency. I care about how much they help people. Efficiency is just a measure of how well they accomplish that goal.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jbBdRijbRfWhk92Mu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 6.531207654390825e-07, "legacy": true, "legacyId": "4189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-06T06:11:54.173Z", "modifiedAt": null, "url": null, "title": "Berkeley LW Meet-up Friday December 10", "slug": "berkeley-lw-meet-up-friday-december-10", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:53.786Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LucasSloan", "createdAt": "2009-05-28T05:04:38.345Z", "isAdmin": false, "displayName": "LucasSloan"}, "userId": "ouo6Fqn5kTNY7LvqM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/56zW6CNEkGgo2YrFy/berkeley-lw-meet-up-friday-december-10", "pageUrlRelative": "/posts/56zW6CNEkGgo2YrFy/berkeley-lw-meet-up-friday-december-10", "linkUrl": "https://www.lesswrong.com/posts/56zW6CNEkGgo2YrFy/berkeley-lw-meet-up-friday-december-10", "postedAtFormatted": "Monday, December 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Berkeley%20LW%20Meet-up%20Friday%20December%2010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABerkeley%20LW%20Meet-up%20Friday%20December%2010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56zW6CNEkGgo2YrFy%2Fberkeley-lw-meet-up-friday-december-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Berkeley%20LW%20Meet-up%20Friday%20December%2010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56zW6CNEkGgo2YrFy%2Fberkeley-lw-meet-up-friday-december-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56zW6CNEkGgo2YrFy%2Fberkeley-lw-meet-up-friday-december-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<p>Last month, about 20 people showed up to the <a href=\"/lw/2zp/berkeley_lw_meetup_saturday_november_6/\">Berkeley LW</a> meet-up.&nbsp; To continue the tradition of Berkeley Meetups, we will be meeting on <del>Saturday, December 11</del>&nbsp; Friday, December 10 at 7 PM at the Starbucks <span><span dir=\"ltr\">at </span></span><a href=\"http://maps.google.com/maps?f=d&amp;source=s_d&amp;saddr=Downtown+Berkeley+BART&amp;daddr=2128+Oxford+St,+Berkeley,+CA+94704-1311+%28Starbucks%29&amp;geocode=FSzZQQIdbVa2-Ck7jvjKnX6FgDG3LOQ7rN5ZmA%3BFW7bQQIdQl62-CEGuQ_bapaUqCmz2L6SnX6FgDHtCjCMFeuX-A&amp;hl=en&amp;mra=ltm&amp;dirflg=w&amp;sll=37.86999,-122.26696&amp;sspn=0.001931,0.005284&amp;ie=UTF8&amp;ll=37.870225,-122.266577&amp;spn=0.001931,0.005284&amp;z=18\"><span dir=\"ltr\">2128 Oxford Street</span></a><span><span dir=\"ltr\">.&nbsp; Last time, we chatted at the Starbucks for about 45 minutes, then went to get dinner and ate and talked under a T-Rex skeleton - we'll probably do something similar, so don't feel like you have to eat before you come.&nbsp; Hope to see you there!</span></span></p>\n<p>&nbsp;</p>\n<p><span><span dir=\"ltr\">ETA:&nbsp;<del> Some <a href=\"/lw/38f/berkeley_lw_meetup_saturday_december_11/32r9?c=1\">people are unavailable</a> on Saturday, do people have a strong preference for Saturday?&nbsp; If no one does, I'll move it to Friday.</del>&nbsp; Due to two votes for Friday and none for Saturday, I have changed the date to Friday.<br /></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "56zW6CNEkGgo2YrFy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 6.531502395703438e-07, "legacy": true, "legacyId": "4191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["faYaa4ry7M7buSP9L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-06T14:29:34.755Z", "modifiedAt": null, "url": null, "title": "Less Wrong: Open Thread, December 2010", "slug": "less-wrong-open-thread-december-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:29.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZyxMdPTJszi5a2jwq/less-wrong-open-thread-december-2010", "pageUrlRelative": "/posts/ZyxMdPTJszi5a2jwq/less-wrong-open-thread-december-2010", "linkUrl": "https://www.lesswrong.com/posts/ZyxMdPTJszi5a2jwq/less-wrong-open-thread-december-2010", "postedAtFormatted": "Monday, December 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%3A%20Open%20Thread%2C%20December%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%3A%20Open%20Thread%2C%20December%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyxMdPTJszi5a2jwq%2Fless-wrong-open-thread-december-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%3A%20Open%20Thread%2C%20December%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyxMdPTJszi5a2jwq%2Fless-wrong-open-thread-december-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyxMdPTJszi5a2jwq%2Fless-wrong-open-thread-december-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>Even with the discussion section, there are ideas or questions too short or inchoate to be worth a post.</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZyxMdPTJszi5a2jwq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "4195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-06T15:15:45.524Z", "modifiedAt": null, "url": null, "title": "New Haven Meetup, Saturday, Dec. 11", "slug": "new-haven-meetup-saturday-dec-11", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x8KEDhBc3cniyjZn6/new-haven-meetup-saturday-dec-11", "pageUrlRelative": "/posts/x8KEDhBc3cniyjZn6/new-haven-meetup-saturday-dec-11", "linkUrl": "https://www.lesswrong.com/posts/x8KEDhBc3cniyjZn6/new-haven-meetup-saturday-dec-11", "postedAtFormatted": "Monday, December 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Haven%20Meetup%2C%20Saturday%2C%20Dec.%2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Haven%20Meetup%2C%20Saturday%2C%20Dec.%2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx8KEDhBc3cniyjZn6%2Fnew-haven-meetup-saturday-dec-11%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Haven%20Meetup%2C%20Saturday%2C%20Dec.%2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx8KEDhBc3cniyjZn6%2Fnew-haven-meetup-saturday-dec-11", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx8KEDhBc3cniyjZn6%2Fnew-haven-meetup-saturday-dec-11", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>On Saturday, New Haven residents, people who live in other nearby assorted Havens, and anybody else who would like to trek out here are welcomed to a Less Wrong meetup in <a href=\"/user/thomblake\">thomblake</a>'s and my home at noon(ish).&nbsp; The address is thus:</p>\n<p>173 Russo Ave Unit 406<br />East Haven, CT 06513</p>\n<p>I will make food; if you plan to come and wish to submit food-related requests, say so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x8KEDhBc3cniyjZn6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 12, "extendedScore": null, "score": 6.532847282564591e-07, "legacy": true, "legacyId": "4196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-06T19:43:11.014Z", "modifiedAt": null, "url": null, "title": "It's not about truth", "slug": "it-s-not-about-truth", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:51.105Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CFk65m88nTNjmRNxk/it-s-not-about-truth", "pageUrlRelative": "/posts/CFk65m88nTNjmRNxk/it-s-not-about-truth", "linkUrl": "https://www.lesswrong.com/posts/CFk65m88nTNjmRNxk/it-s-not-about-truth", "postedAtFormatted": "Monday, December 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20It's%20not%20about%20truth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIt's%20not%20about%20truth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFk65m88nTNjmRNxk%2Fit-s-not-about-truth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=It's%20not%20about%20truth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFk65m88nTNjmRNxk%2Fit-s-not-about-truth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCFk65m88nTNjmRNxk%2Fit-s-not-about-truth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p>A rather sane article about the actual purpose of religions and why they persist among the rational &amp; intelligent.</p>\n<p><a href=\"http://blog.evangelicalrealism.com/2010/12/04/getting-religion/\">http://blog.evangelicalrealism.com/2010/12/04/getting-religion/</a></p>\n<p>A bit of a Hansonian bent as well. \"<span style=\"font-size: 12px; color: #302753; line-height: 20px;\">Genuine objective truths can be complicated and uncomfortable, but what&rsquo;s worse, they confer no particular social advantage</span><span style=\"font-size: 12px; color: #302753; line-height: 20px;\">&nbsp;\"</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CFk65m88nTNjmRNxk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 6.53350877023907e-07, "legacy": true, "legacyId": "4197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T03:14:24.123Z", "modifiedAt": null, "url": null, "title": "Wonder who is going to be there...", "slug": "wonder-who-is-going-to-be-there", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:52.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v2uPKpWTvZjWyQfZf/wonder-who-is-going-to-be-there", "pageUrlRelative": "/posts/v2uPKpWTvZjWyQfZf/wonder-who-is-going-to-be-there", "linkUrl": "https://www.lesswrong.com/posts/v2uPKpWTvZjWyQfZf/wonder-who-is-going-to-be-there", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wonder%20who%20is%20going%20to%20be%20there...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWonder%20who%20is%20going%20to%20be%20there...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv2uPKpWTvZjWyQfZf%2Fwonder-who-is-going-to-be-there%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wonder%20who%20is%20going%20to%20be%20there...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv2uPKpWTvZjWyQfZf%2Fwonder-who-is-going-to-be-there", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv2uPKpWTvZjWyQfZf%2Fwonder-who-is-going-to-be-there", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4, "htmlBody": "<p>http://www.mercurynews.com/business/ci_16792615?nclick_check=1</p>\n<p>(Betting on Michael Vassar)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v2uPKpWTvZjWyQfZf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 6.534625148671333e-07, "legacy": true, "legacyId": "4199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T12:01:46.091Z", "modifiedAt": null, "url": null, "title": "Writing and enthusiasm", "slug": "writing-and-enthusiasm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:51.652Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z8xEsyHwHpsgrhJe8/writing-and-enthusiasm", "pageUrlRelative": "/posts/z8xEsyHwHpsgrhJe8/writing-and-enthusiasm", "linkUrl": "https://www.lesswrong.com/posts/z8xEsyHwHpsgrhJe8/writing-and-enthusiasm", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Writing%20and%20enthusiasm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWriting%20and%20enthusiasm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz8xEsyHwHpsgrhJe8%2Fwriting-and-enthusiasm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Writing%20and%20enthusiasm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz8xEsyHwHpsgrhJe8%2Fwriting-and-enthusiasm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz8xEsyHwHpsgrhJe8%2Fwriting-and-enthusiasm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p><a href=\"http://www.stevepavlina.com/blog/2010/12/how-i-write/\">Steve Pavlina</a> explains that the method he'd been taught in school-- a highly structured writing process of organizing what to say before it's written-- tends to produce dull writing, but starting from enthusiasm results in articles which are a pleasure to write and are apt to be more fun and memorable to read.</p>\n<blockquote>Inspirational energy has a half life of about 24 hours. If I act on an idea immediately (or at least within the first few hours), I feel optimally motivated, and I can surf that wave of energy all the way to clicking &ldquo;Publish.&rdquo; If I sit on an idea for one day, I feel only half as inspired by it, and I have to paddle a lot more to get it done. If I sit on it for 2 days, the inspiration level has dropped by 75%, and for all practical purposes, the idea is dead. If I try to write it at that point, it feels like pulling teeth. It&rsquo;s much better for me to let it go and wait for a fresh wave. There will always be another wave, so there&rsquo;s no need to chase the ones I missed.</blockquote>\n<p>This looks like <a href=\"/user/pjeby\">PJ Eby</a> territory-- it's about the importance of pleasure as a motivator.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z8xEsyHwHpsgrhJe8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 6.535930354982873e-07, "legacy": true, "legacyId": "4201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T13:31:51.919Z", "modifiedAt": null, "url": null, "title": "An uneducated thought on the irreality of reality", "slug": "an-uneducated-thought-on-the-irreality-of-reality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:50.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "righthereonthisrock", "createdAt": "2010-11-29T18:34:39.829Z", "isAdmin": false, "displayName": "righthereonthisrock"}, "userId": "EQXqzBs3YSYz9jnra", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AYskpLbLTcD5Xnuuq/an-uneducated-thought-on-the-irreality-of-reality", "pageUrlRelative": "/posts/AYskpLbLTcD5Xnuuq/an-uneducated-thought-on-the-irreality-of-reality", "linkUrl": "https://www.lesswrong.com/posts/AYskpLbLTcD5Xnuuq/an-uneducated-thought-on-the-irreality-of-reality", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20uneducated%20thought%20on%20the%20irreality%20of%20reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20uneducated%20thought%20on%20the%20irreality%20of%20reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAYskpLbLTcD5Xnuuq%2Fan-uneducated-thought-on-the-irreality-of-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20uneducated%20thought%20on%20the%20irreality%20of%20reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAYskpLbLTcD5Xnuuq%2Fan-uneducated-thought-on-the-irreality-of-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAYskpLbLTcD5Xnuuq%2Fan-uneducated-thought-on-the-irreality-of-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 923, "htmlBody": "<p>I realize that this is not standard fare on lesswrong. I have as yet found no community other than this which I'd expect to receive valuable feedback and discussion from regarding the pedantic bologna that my mind spews. I am learning to communicate more effectively but find the encyclopedias of experiential relevance behind words themselves as they pertain to a unique mind to be immense roadblocks towards constructing my own understanding of life or communicating my own to others.</p>\n<p>Anybody else have this issue?</p>\n<p>I see where this connects to philosophies which stand debunked in lieu of the \"current state\" of philosophy. I do not know exactly how to communicate that to myself in a way which makes fluid sense.</p>\n<p>Please keep in mind that I've only just begun digging through this site! I'm posting this in hopes of having my ambiguity ripped to shreds.</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>I preach and fade</p>\n<p>into a monologue so strong</p>\n<p>that it usurps the ungraspable nature of reality.</p>\n<p>&nbsp;</p>\n<p>Day by day, I preach and fade,</p>\n<p>just like you.</p>\n<p>&nbsp;</p>\n<p>Sit in the shade of my monstrous thoughts.</p>\n<p>When I feel like shouting out,</p>\n<p>\"Come sit, let's share\",</p>\n<p>I do so with nil hesitation.</p>\n<p>And yet I know that to me love is love,</p>\n<p>but to you love is love,</p>\n<p>and our flavors will likely never coincide.</p>\n<p>&nbsp;</p>\n<p>I can sift through feelings, use big words to relate,</p>\n<p>day after day, preach and then fade,</p>\n<p>talking my way through facade, facade.</p>\n<p>&nbsp;</p>\n<p>My play's a bit different than yours.</p>\n<p>My music, so alien, though from the same score.</p>\n<p>&nbsp;</p>\n<p><br />This is why man needs God;</p>\n<p>Man is God. Only God IS man, can feel the same sense of language, memory, sensory input and emotional language, response...</p>\n<p>&nbsp;</p>\n<p>God is comfort with one's own insanity.</p>\n<p>All it takes is two to make any song sane.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>The specifics are unnecessary, the concepts of love and God chosen as experiences tied to words immeasurably unique to each individual. I see the connections coming to light even as I type this up as to why this post is a futile relic of thought processes of my past, but there is still a lot of this that sticks with me that would be interesting to have dissected, as cognition does not always present itself clearly!</p>\n<p>How many people do you meet out there who actually ask people \"are you okay?\" ...and actually mean it? They seem to generally be people caught up in an inner monologue in which they are caring because that's the narrative they choose to follow. Not to say that there is anything inherintly wrong with caring because of an ethical concern; it doesn't always have to be about intimacy.</p>\n<p>But the care is expressed in a paradigm where once it's said their focus is averted because it's satisfied the need to 'care'. Like... they care, but they are speaking to themselves.</p>\n<p>The whole idea of the internal monologue that I see people maintaining... hmm. Not sure where to start. So language is an organizational and communicative tool, right? It allows communication of abstract thought from person to person. So language only holds levity if it is common - if everybody uses the same collection of letters/symbols/sounds/patterns/vocal inflections/facial expression and whatnot. But even though we all use the same words, something like \"red\" is going to mean something completely different to me than it will to you, because our realities, the spaces within which our minds construct the 'external', must never be the same. These words evoke an emotional response within us and generally they can relate with similitude.&nbsp;</p>\n<p>But language is something that exists really only externally, in a sense. Thought is much quicker than language but we still speak things in our heads because we desire communicability and we're used to a world where things happen at a language-paced level.</p>\n<p>Do you ever get the feeling that you tried to tell someone something, and they understood what you said, but they didn't understand what you intended for them to - BUT - it still worked? Because the language was ambiguous enough, they heard something you were giving as 'advice' and found a way in their minds eye for those words to fit in as 'advice' and accepted them, because they were primed to. Even though they never really heard what you felt. They heard the same concrete words that you spoke, and understood them perfectly, but for the fact that language on an individual basis is abstract and the fact that they will process it through the lens of their own internal 'language' means they'll never really see what you see, feel what you feel kinda thing.</p>\n<p>And as far as YOUR inner monologue goes, there is only one. There is only one collection of emotional associations and memories and neural networking in EXISTENCE which finds itself speaking the 'internal' language that you speak.</p>\n<p>Hence,</p>\n<p>to me love is love</p>\n<p>but to you love is love</p>\n<p>and it's pretty damned unusual for two people to connect on that in a pure and unmarring level. It takes a long, long time for peoples' internal monologues to sync up. Even now, I am explaining this to you partly because I want to share and partly because I want to explain it to myself and not because you want to hear it.</p>\n<p>&nbsp;</p>\n<p>Am I tripping myself up so hard from the starting gate simply by choosing to point fingers at outlier concepts such as love or a psychological deific construct?</p>\n<p><br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AYskpLbLTcD5Xnuuq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -15, "extendedScore": null, "score": 6.536153387243387e-07, "legacy": true, "legacyId": "4202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T13:40:03.121Z", "modifiedAt": null, "url": null, "title": "The End of Men", "slug": "the-end-of-men", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:54.727Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MhCKSJL6Du4s6uYkY/the-end-of-men", "pageUrlRelative": "/posts/MhCKSJL6Du4s6uYkY/the-end-of-men", "linkUrl": "https://www.lesswrong.com/posts/MhCKSJL6Du4s6uYkY/the-end-of-men", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20End%20of%20Men&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20End%20of%20Men%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhCKSJL6Du4s6uYkY%2Fthe-end-of-men%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20End%20of%20Men%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhCKSJL6Du4s6uYkY%2Fthe-end-of-men", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhCKSJL6Du4s6uYkY%2Fthe-end-of-men", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p>Interesting article arguing that society is strongly slanted towards women doing better, and also noting that parents who are given the choice of choosing their children's sex are preferring women, sometimes in a 2-to-1 ratio:</p>\n<p>http://www.theatlantic.com/magazine/archive/2010/07/the-end-of-men/8135/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MhCKSJL6Du4s6uYkY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 6.536173653717676e-07, "legacy": true, "legacyId": "4203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T15:44:56.626Z", "modifiedAt": null, "url": null, "title": "Akrasia as a collective action problem", "slug": "akrasia-as-a-collective-action-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:54.419Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EYiAoxvnKjgJe8GbN/akrasia-as-a-collective-action-problem", "pageUrlRelative": "/posts/EYiAoxvnKjgJe8GbN/akrasia-as-a-collective-action-problem", "linkUrl": "https://www.lesswrong.com/posts/EYiAoxvnKjgJe8GbN/akrasia-as-a-collective-action-problem", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Akrasia%20as%20a%20collective%20action%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAkrasia%20as%20a%20collective%20action%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYiAoxvnKjgJe8GbN%2Fakrasia-as-a-collective-action-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Akrasia%20as%20a%20collective%20action%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYiAoxvnKjgJe8GbN%2Fakrasia-as-a-collective-action-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYiAoxvnKjgJe8GbN%2Fakrasia-as-a-collective-action-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 505, "htmlBody": "<p><em>Related to: </em><a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\"><em>Self-empathy as a source of \"willpower\"</em></a><em> and </em><em><a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/2yiy?c=1\">some</a> <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/2uk6?c=1\">comments</a></em><em>.</em></p>\r\n<p>It has been <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">mentioned before </a>that akrasia might be modeled as the result of inner conflict. I think this analogy is great, and would like to propose a refinement.<sup>1</sup></p>\r\n<p>Here's the mental conflict theory of akrasia, as I understand it:</p>\r\n<p style=\"PADDING-LEFT: 30px\">Though Maud appears to&nbsp;external&nbsp;observers (such as&nbsp;us) be a single self, she is in fact a kind of team. Maud's&nbsp;mind is composed of sub-agents, each of whom would like to pursue its own interests. Maybe when Maud goes to bed, she sets the alarm for 6 AM. When it buzzes the next morning, she hits the snooze...again and again and again. To explain this odd behavior, we invoke the idea that BedtimeMaud is not the same person as MorningMaud. In particular, BedtimeMaud is a person who likes to get up early, while MorningMaud is that bully BedtimeMaud's poor victim.The point is that the various decisionmakers that inhabit her brain are not always after the same ball. The subagents that compose the mind might not be mutually antagonistic; they're just not very&nbsp;empathetic to each other.</p>\r\n<p>I like to think of this situation as a collective action problem akin to those we find in political science and economics. What we have is a misalignment of costs and benefits. If Maud rises at 6, then MorningMaud bears the whole cost of this decision, while a different Maud, or set of Mauds, enjoys the benefits. The costs are concentrated in MorningMaud's lap, while the benefits are dispersed among many Mauds throughout the day. Thus Maud sleeps in.</p>\r\n<p>Put differently, MorningMaud's behavior produces a negative&nbsp;externality: she enjoys the whole benefit of sleeping in, but the rest of the day's Mauds bear the costs.</p>\r\n<p>So, how can we get MorningMaud to lie in the bed she makes, as it were, and get a more efficient outcome?</p>\r\n<p>We can:</p>\r\n<ul>\r\n<li>Legislate. Maud tirelessly tells herself to be less lazy and exerts willpower to get the job done. This is analogous to direct, blanket government action (such as banning coal) in response to a negative externality (such as once-verdant, now <a href=\"http://www.google.com.hk/images?q=mountain%20top%20removal&amp;hl=zh-cn&amp;newwindow=1&amp;safe=strict&amp;rls=com.microsoft:en-us:IE-SearchBox&amp;um=1&amp;ie=UTF-8&amp;source=og&amp;sa=N&amp;tab=wi&amp;biw=1259&amp;bih=571\">barren hillsides</a>). But it's expensive, and it doesn't always work.</li>\r\n<li>Negotiate. Maud rewards herself when she gets up on time by taking a hot shower right away&nbsp;or&nbsp;eating a nice breakfast (the latter has a cost borne by MoneyMaud); or she allows herself to sleep in once a week. If MorningMaud follows through, then this one's a winner. Maybe this is analogous to <a href=\"http://en.wikipedia.org/wiki/Coase_theorem\">Coasian bargaining</a>? </li>\r\n<li>Deputize. Maud enlists her friend Traci to hold her feet to the fire. Or she signs up on Stikk, <a href=\"http://www.egonomicslab.com/social-network/\">Egonomics</a>, or some similar site.</li>\r\n</ul>\r\n<p>The analogy's not perfect. (I can't see a way to fit in <a href=\"http://en.wikipedia.org/wiki/Pigovian_tax\">Pigovian taxes </a>.)</p>\r\n<p>But is it a fruitful analogy? Is it more than just renaming the key terms&nbsp;of the subagent theory--could one use welfare economics to improve one's own <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">dynamic consistency</a>?</p>\r\n<p><sup>1</sup>I got this idea partly from a slip, possibly Freudian (I think I said \"externality\" instead of \"akrasia\"), and partly from <a href=\"http://www.egonomicslab.com/egonomics/precommitment/\">this page on the Egonomics website</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EYiAoxvnKjgJe8GbN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 6.536482842718531e-07, "legacy": true, "legacyId": "4204", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["22HfpjsydDS2A6JhH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T16:25:22.584Z", "modifiedAt": "2022-04-29T20:18:45.745Z", "url": null, "title": "Best career models for doing research?", "slug": "best-career-models-for-doing-research", "viewCount": null, "lastCommentedAt": "2011-08-28T11:09:39.505Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rNkFLv9tXzq8Lrvrc/best-career-models-for-doing-research", "pageUrlRelative": "/posts/rNkFLv9tXzq8Lrvrc/best-career-models-for-doing-research", "linkUrl": "https://www.lesswrong.com/posts/rNkFLv9tXzq8Lrvrc/best-career-models-for-doing-research", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20career%20models%20for%20doing%20research%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20career%20models%20for%20doing%20research%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNkFLv9tXzq8Lrvrc%2Fbest-career-models-for-doing-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20career%20models%20for%20doing%20research%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNkFLv9tXzq8Lrvrc%2Fbest-career-models-for-doing-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNkFLv9tXzq8Lrvrc%2Fbest-career-models-for-doing-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 398, "htmlBody": "<p>Ideally, I'd like to <a href=\"/lw/373/how_to_save_the_world/\">save the world</a>. One way to do that involves contributing academic research, which raises the question of what's the most effective way of doing that.</p>\n<p>The traditional wisdom says if you want to do research, you should get a job in a university. But for the most part the system seems to be set up so that you first spend a long time working for someone else and research their ideas, after which you can lead your own group, but then most of your time will be spent on applying for grants and other administrative trivia rather than actually researching the interesting stuff. Also, in Finland at least, all professors need to also spend time doing teaching, so that's another time sink.<br /> <br /> I suspect I would have more time to actually dedicate on research, and I could get doing it quicker, if I took a part-time job and did the research in my spare time. E.g. the recommended rates for a freelance journalist in Finland would allow me to spend a week each month doing work and three weeks doing research, of course assuming that I can pull off the freelance journalism part.<br /> <br /> What (dis)advantages does this have compared to the traditional model?<br /> <br /> Some advantages:</p>\n<ul>\n<li>Can spend more time on actual research.</li>\n<li>A lot more freedom with regard to what kind of research one can pursue.</li>\n<li>Cleaner mental separation between money-earning job and research time (less frustration about \"I could be doing research now, instead of spending time on this stupid administrative thing\").</li>\n<li>Easier to take time off from research if feeling stressed out.</li>\n</ul>\n<p>Some disadvantages:</p>\n<ul>\n<li>Harder to network effectively.</li>\n<li>Need to get around journal paywalls somehow.</li>\n<li>Journals might be biased against freelance researchers.</li>\n<li>Easier to take time off from research if feeling lazy.</li>\n<li>Harder to combat akrasia.</li>\n<li>It might actually be better to spend some time doing research under others before doing it on your own.</li>\n</ul>\n<p><strong>EDIT: </strong>Note that while I certainly do appreciate comments specific to my situation, I posted this over at LW and not Discussion because I was hoping the discussion would also be useful for others who might be considering an academic path. So feel free to also provide commentary that's US-specific, say.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rNkFLv9tXzq8Lrvrc", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 41, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "4206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1032, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TrmMcujGZt5JAtMGg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-12-07T16:25:22.584Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T21:15:31.102Z", "modifiedAt": null, "url": null, "title": "$100 for the best article on efficient charity: the finalists", "slug": "usd100-for-the-best-article-on-efficient-charity-the", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RSXZSkdjdhSZTkxD7/usd100-for-the-best-article-on-efficient-charity-the", "pageUrlRelative": "/posts/RSXZSkdjdhSZTkxD7/usd100-for-the-best-article-on-efficient-charity-the", "linkUrl": "https://www.lesswrong.com/posts/RSXZSkdjdhSZTkxD7/usd100-for-the-best-article-on-efficient-charity-the", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%24100%20for%20the%20best%20article%20on%20efficient%20charity%3A%20the%20finalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%24100%20for%20the%20best%20article%20on%20efficient%20charity%3A%20the%20finalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRSXZSkdjdhSZTkxD7%2Fusd100-for-the-best-article-on-efficient-charity-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%24100%20for%20the%20best%20article%20on%20efficient%20charity%3A%20the%20finalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRSXZSkdjdhSZTkxD7%2Fusd100-for-the-best-article-on-efficient-charity-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRSXZSkdjdhSZTkxD7%2Fusd100-for-the-best-article-on-efficient-charity-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>Part of the <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/\">Efficient Charity Article</a> competition. Several people have written articles on efficient charity --</p>\r\n<ul>\r\n<li>Throwawayaccount_1 has an excellent article <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/30hx?c=1\">hidden away in a comment</a></li>\r\n</ul>\r\n<p>&nbsp;</p>\r\n<ul>\r\n<li>Waitingforgodel has an article in discussion: \"<a href=\"/r/discussion/lw/37l/how_greedy_bastards_have_saved_more_lives_than/\">How Greedy Bastards Have Saved More Lives Than Mother Theresa Ever Did</a>\"</li>\r\n</ul>\r\n<p>&nbsp;</p>\r\n<ul>\r\n<li>Multifoliaterose has an article entitled \"<a href=\"/lw/37f/efficient_charity/\">Efficient Charity</a>\" which scored 23 on the main site despite not being promoted.</li>\r\n</ul>\r\n<p>&nbsp;</p>\r\n<ul>\r\n<li>Louie Has an article entitled \"<a href=\"/lw/373/how_to_save_the_world/\">How to save the world</a>\" which scored an excellent 49.</li>\r\n</ul>\r\n<p>&nbsp;</p>\r\n<p>Any comments on the finalists? Who do we think should be the winner?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RSXZSkdjdhSZTkxD7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 6.537301356712535e-07, "legacy": true, "legacyId": "4207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4amcyxad5bnBR9Afm", "Xg5KCY4FYrxEcCifa", "FCxHgPsDScx4C3H8n", "TrmMcujGZt5JAtMGg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-07T23:29:13.208Z", "modifiedAt": null, "url": null, "title": "Help Request: How to maintain focus when emotionally overwhelmed", "slug": "help-request-how-to-maintain-focus-when-emotionally", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "throwaway", "createdAt": "2010-12-07T23:19:04.615Z", "isAdmin": false, "displayName": "throwaway"}, "userId": "7m8NnSJ5wimcqh3oE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XziKoFhDiNRLGsjr3/help-request-how-to-maintain-focus-when-emotionally", "pageUrlRelative": "/posts/XziKoFhDiNRLGsjr3/help-request-how-to-maintain-focus-when-emotionally", "linkUrl": "https://www.lesswrong.com/posts/XziKoFhDiNRLGsjr3/help-request-how-to-maintain-focus-when-emotionally", "postedAtFormatted": "Tuesday, December 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20Request%3A%20How%20to%20maintain%20focus%20when%20emotionally%20overwhelmed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20Request%3A%20How%20to%20maintain%20focus%20when%20emotionally%20overwhelmed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXziKoFhDiNRLGsjr3%2Fhelp-request-how-to-maintain-focus-when-emotionally%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20Request%3A%20How%20to%20maintain%20focus%20when%20emotionally%20overwhelmed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXziKoFhDiNRLGsjr3%2Fhelp-request-how-to-maintain-focus-when-emotionally", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXziKoFhDiNRLGsjr3%2Fhelp-request-how-to-maintain-focus-when-emotionally", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>So my personal life just got <em>very interesting</em>. In a net-positive way, certainly, but still, I am, as Calculon put it, \"filled with a large number of powerful emotions!\" -- some of which are anxious and/or panicky.</p>\n<p>This is making it annoyingly difficult to focus at work. I am an absolutely textbook \"Attention Deficit Oh-look-a-squirrel!\" case at the best of times, and this seems to have made it much, much worse. I can handle small tasks, but anything where I'm going to have to spend an hour solving multiple problems before producing results, I can hardly make myself start.</p>\n<p>Has anyone dealt with the problem of maintaining productive focus while emotionally overwhelmed/exhausted, and if so, do you have any pointers?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XziKoFhDiNRLGsjr3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 6.53763245835701e-07, "legacy": true, "legacyId": "4208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-08T04:50:14.749Z", "modifiedAt": null, "url": null, "title": "Bridging Inferential Gaps", "slug": "bridging-inferential-gaps", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:18.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EEx7JrynhWJRdRJHt/bridging-inferential-gaps", "pageUrlRelative": "/posts/EEx7JrynhWJRdRJHt/bridging-inferential-gaps", "linkUrl": "https://www.lesswrong.com/posts/EEx7JrynhWJRdRJHt/bridging-inferential-gaps", "postedAtFormatted": "Wednesday, December 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bridging%20Inferential%20Gaps&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABridging%20Inferential%20Gaps%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEEx7JrynhWJRdRJHt%2Fbridging-inferential-gaps%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bridging%20Inferential%20Gaps%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEEx7JrynhWJRdRJHt%2Fbridging-inferential-gaps", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEEx7JrynhWJRdRJHt%2Fbridging-inferential-gaps", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 612, "htmlBody": "<p>This idea isn't totally developed, so I'm putting it in Discussion for now.</p>\n<p style=\"margin-bottom: 0in;\"><strong>Introduction:</strong></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">A few hands have been wrung over how to quickly explain fundamental Less Wrong ideas to people, in a way that they can be approached, appraised, an considered rather than being isolated and bereft across an inferential gulf.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I'm pretty embarrassed to say that I hardly talk about these things with people in my everyday life, even though it makes up a major part of my worldview and outlook. I don't particularly care about making people have similar beliefs to me, but I feel like I'm doing my friends a disservice to not adequately explain these things that I've found so personally helpful. (Ugh, that sounds pseudo-religious. Cut me off here if this is a Bad idea.)</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Would it be useful to start a project (read: group of posts by different people) to find ways to bridge said gaps in normal conversation? (Normal conversation meaning talking to a non-hostile audience that nonetheless isn't particularly interested in to LW ideas). Mainly to talk about rationality things with friends and family members and whatnot, and possibly to help raise the sanity waterline (though this wasn't designed to do that).</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">A problem with the sequences for a nonplussed audience is that it assumes they care. I find that when trying to explain ideas like holding off on proposing solutions until talking about the problem to other people it just comes across as boring, even if they aren't opposed to the idea at all.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">With an ideological audience, the problem is much more difficult. Not only do you need to explain why something is correct, you need to convince them that believing in it is more important than holding on to their ideology, and that they should lower their \"defenses\" enough to actually consider it.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I think that, should this project be undertaken, it should be very tested and experimental based. Like, people would actually try out the techniques on other people to see if they actually work.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p><strong>Background/Thoughts/Questions</strong><strong>:</strong></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Do we actually want to do this? It seems like its a step towards a possibly PR-damaging evangelism, or just being generally annoying in conversation, among other things. On the other hand, I still want to be able to talk about these things offline every now and then.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's been said that being half a rationalist is dangerous. How do you communicate enough rationality for it to not be dangerous? Or would they have to go all in, and make the casual conversation about it semi-pointless?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The inferential gaps that need crossing probably vary a lot by personal background. Once I was able to explain basic transhumanism (death is bad, we can probably enhance ourselves using technology) to someone, and have them agree with an like it almost immediately. Another time, the other person in the conversation just found it gross.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">There are probably ways of explaining LW concepts to other people that rely on their ideas that would mess up their thinking (i.e. Cognitive Bias explained through Original Sin might be a bad idea). How do you cross into rational ideas from nonrational ones? Should you try to exclusively explain rational ideas based on rational beliefs they already have? Could you reliably explain an idea to someone and expect that to cause them to question what you explained it in terms of (i.e. you explain A in terms of B, but A causes people to reject B)?</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For talking to an ideological person, I think that the main common goal should be to convince them that a) ideas can be objectively true, b) its good to abandon false beliefs, c) ideological people will rationalize things to fit into their ideology, and \"view arguments as soldiers\".</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EEx7JrynhWJRdRJHt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "4209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-08T17:02:45.186Z", "modifiedAt": null, "url": null, "title": "Why is our sex drive too strong?", "slug": "why-is-our-sex-drive-too-strong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SpmF2htzTP9SLC5nd/why-is-our-sex-drive-too-strong", "pageUrlRelative": "/posts/SpmF2htzTP9SLC5nd/why-is-our-sex-drive-too-strong", "linkUrl": "https://www.lesswrong.com/posts/SpmF2htzTP9SLC5nd/why-is-our-sex-drive-too-strong", "postedAtFormatted": "Wednesday, December 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20is%20our%20sex%20drive%20too%20strong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20is%20our%20sex%20drive%20too%20strong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpmF2htzTP9SLC5nd%2Fwhy-is-our-sex-drive-too-strong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20is%20our%20sex%20drive%20too%20strong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpmF2htzTP9SLC5nd%2Fwhy-is-our-sex-drive-too-strong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpmF2htzTP9SLC5nd%2Fwhy-is-our-sex-drive-too-strong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 123, "htmlBody": "<p>It is a cultural universal that people are discouraged from having sex as often and with as many people as they want to.&nbsp; Every culture I've ever heard of imposes many restrictions on sex.&nbsp; I've never heard of a culture that shames people for being too stingy with sex.</p>\n<p>If we assume that culture is adaptive, this means that the human sex drive is too strong for humans in society.&nbsp; Why is this?&nbsp; As sex drive is a phenotypic feature with extraordinarily strong selective pressure, why haven't we evolved to have the proper sex drive?</p>\n<p>One reason could be that reduced sex drive is selected for at the level of the group, while higher sex drive is selected for at the level of the individual.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SpmF2htzTP9SLC5nd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 5, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "4212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-08T17:30:37.453Z", "modifiedAt": null, "url": null, "title": "Were atoms real?", "slug": "were-atoms-real", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:32.693Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XHxpAbnok6YyhGv8S/were-atoms-real", "pageUrlRelative": "/posts/XHxpAbnok6YyhGv8S/were-atoms-real", "linkUrl": "https://www.lesswrong.com/posts/XHxpAbnok6YyhGv8S/were-atoms-real", "postedAtFormatted": "Wednesday, December 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Were%20atoms%20real%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWere%20atoms%20real%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHxpAbnok6YyhGv8S%2Fwere-atoms-real%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Were%20atoms%20real%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHxpAbnok6YyhGv8S%2Fwere-atoms-real", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHxpAbnok6YyhGv8S%2Fwere-atoms-real", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1145, "htmlBody": "<p>Related to:&nbsp;<a href=\"/lw/of/dissolving_the_question/\">Dissolving the Question</a>, <a href=\"/lw/ng/words_as_hidden_inferences/\">Words as Hidden Inferences</a>.&nbsp;</p>\n<p class=\"p1\">In what sense is the world &ldquo;real&rdquo;?&nbsp; What are we asking, when we ask that question?</p>\n<p class=\"p2\">I don&rsquo;t know.&nbsp; But G. Polya <a href=\"http://en.wikipedia.org/wiki/How_to_Solve_It\">recommends</a>&nbsp;that when facing a difficult problem, one look for similar but easier problems that one can solve as warm-ups.&nbsp; I would like to do one of those warm-ups today; I would like to ask what disguised empirical question scientists were asking were asking in 1860, when they debated (fiercely!) whether atoms were real.[1]</p>\n<p class=\"p2\">Let&rsquo;s start by looking at the data that swayed these, and similar, scientists.</p>\n<p class=\"p2\"><strong>Atomic theory:</strong>&nbsp; By 1860, it was clear that atomic theory was a useful pedagogical device.&nbsp; Atomic theory helped chemists describe several regularities:</p>\n<ul>\n<li>The law of definite proportions (chemicals combining to form a given compound always combine in a fixed ratio)</li>\n<li>The law of multiple proportions (the ratios in which chemicals combine when forming <em>distinct</em> compounds, such as carbon dioxide and carbon monoxide, form simple integer ratios; this holds for many different compounds, including complicated organic compounds).</li>\n<li>If fixed volumes of distinct gases are isolated, at a fixed temperature and pressure, their masses form these same ratios.</li>\n</ul>\n<p class=\"p1\">Despite this usefulness, there was considerable debate as to whether atoms were &ldquo;real&rdquo; or were merely a useful pedagogical device.&nbsp; Some argued that substances might simply prefer to combine in certain ratios and that such empirical regularities were all there was to atomic theory; it was needless to additionally suppose that matter came in small unbreakable units.</p>\n<p class=\"p1\"><a id=\"more\"></a>Today we have an integrated picture of physics and chemistry, in which atoms have a particular known size, are made of known sets of subatomic particles, and generally fit into a total picture in which the amount of data far exceeds the number of postulated details atoms include. &nbsp;And today, nobody suggests that atoms are not \"real\", and are \"merely useful predictive devices\".</p>\n<p class=\"p1\"><strong>Copernican astronomy:</strong>&nbsp; By the mid sixteen century, it was clear to the astronomers at the University of Wittenburg that Copernicus&rsquo;s model was useful.&nbsp; It was easier to use, and more theoretically elegant, than Ptolemaic epicycles.&nbsp; However, they did not take Copernicus&rsquo;s theory to be &ldquo;true&rdquo;, and most of them ignored the claim that the Earth orbits the Sun.</p>\n<p class=\"p1\">Later, after Galileo and Kepler, Copernicus&rsquo;s claims about the real constituents of the solar system were taken more seriously. This new debate invoked a wider set of issues, besides the motions of the planets across the sky. Scholars now argued about Copernicus&rsquo;s compatibility with the Bible; about whether our daily experiences on Earth would be different if the Earth were in motion (a la Galileo); and about whether Copernicus&rsquo;s view was more compatible with a set of physically real causes for planetary motion (a la Kepler).&nbsp; It was this wider set of considerations that eventually convinced scholars to believe in a heliocentric universe. [2]</p>\n<p class=\"p1\"><strong>Relativistic time-dilation:</strong><em> </em>For Lorentz, &ldquo;local time&rdquo; was a mere predictive convenience -- a device for simplifying calculations.&nbsp; Einstein later argued that this local time was &ldquo;real&rdquo;; he did this by proposing a coherent, symmetrical total picture that included local time.</p>\n<p class=\"p1\"><strong>Luminiferous aether:</strong><em>&nbsp;&nbsp;</em>Luminiferous (\"light-bearing\") aether provides an example of the reverse transition.&nbsp; In the 1800s, many scientists, e.g. <a href=\"http://en.wikipedia.org/wiki/Augustin-Jean_Fresnel\">Augustin-Jean Fresnel,</a> thought aether was probably a real part of the physical world.&nbsp; They thought this because they had strong evidence that light was a wave, including as the interference of light in two-slit experiments, and all known waves were waves <em>in</em> something.[2.5]</p>\n<p class=\"p1\">But the predictions of aether theory proved non-robust. &nbsp;Aether not only correctly predicted that light would act as waves, but also incorrectly predicted that the Earth's motion with respect to aether should affect the perceived speed of light. &nbsp;That is: luminiferous aether yielded accurate predictions only in narrow contexts, and it turned out not to be \"real\".</p>\n<h2>Generalizing from these examples</h2>\n<p><span style=\"font-weight: normal; font-size: small;\">All theories come with &ldquo;reading conventions&rdquo; that tell us what kinds of predictions can and cannot be made from the theory.&nbsp; For example, our reading conventions for maps tell us that a given map of North America can be used to predict distances between New York and Toronto, but that it should not be used to predict that Canada is uniformly pink.[3] &nbsp;</span></p>\n<p class=\"p1\">If the &ldquo;reading conventions&rdquo; for a particular theory allow for only narrow predictive use, we call that theory a &ldquo;useful predictive device&rdquo; but are hesitant about concluding that its contents are &ldquo;real&rdquo;.&nbsp; Such was the state of Ptolemaic epicycles (which was used to predict the planets' locations within the sky, but not to predict, say, their brightness, or their nearness to Earth); of Copernican astronomy before Galileo (which could be used to predict planetary motions, but didn't explain why humans standing on Earth did not feel as though they were spinning), of early atomic theory, and so on. &nbsp;When we learn to integrate a given theory-component into a robust predictive total, we conclude the theory-component is \"real\".</p>\n<p class=\"p1\">It seems that one disguised empirical question scientists are asking, when they ask &ldquo;Is X real, or just a handy predictive device?&rdquo; is the question: &ldquo;will I still get accurate predictions, when I use X in a less circumscribed or compartmentalized manner?&rdquo; (E.g., &ldquo;will I get accurate predictions, when I use atoms to predict quantized charge on tiny oil drops, instead of using atoms only to predict the ratios in which macroscopic quantities combine?\".[4][5]</p>\n<p class=\"p1\">&nbsp;</p>\n<hr />\n<p class=\"p1\">[1] Of course, I&rsquo;m not sure that it&rsquo;s a warm-up; since I am still confused about the larger problem, I don't know which paths will help.&nbsp;But that&rsquo;s how it is with warm-ups; you find all the related-looking easier problems you can find, and hope for the best.</p>\n<p class=\"p2\">[2] &nbsp;I&rsquo;m stealing this from Robert Westman&rsquo;s book &ldquo;The Melanchthon Circle, Rheticus, and the Wittenberg Interpretation of the Copernican Theory&rdquo;. &nbsp;But you can check the facts more easily in the <a href=\"http://plato.stanford.edu/entries/copernicus/\">Stanford Encyclopedia of Philosophy</a>.</p>\n<p>[2.5] Manfred <a href=\"/lw/38z/were_atoms_real/33ey?c=1\">asks</a> that I note that Lorentz's local time made sense to Lorentz partly because he believed an aether that could be used to define absolute time. &nbsp;I unfortunately haven't read or don't recall the primary texts well enough to add good interpretation here (although I read many of the primary texts in a history of science course once), but <a href=\"http://en.wikipedia.org/wiki/Lorentz_ether_theory\">Wikipedia</a> has some good info on the subject.</p>\n<p>[3] This is a standard example, taken from Philip Kitcher.</p>\n<p>[4] &nbsp;This conclusion is not original, but I can't remember who I stole it from. &nbsp;It may have been Steve Rayhawk.</p>\n<p>[5] Thus, to extend this conjecturally toward our original question: when someone asks \"Is the physical world 'real'?\" they may, in part, be asking whether their predictive models of the physical world will give accurate predictions in a very robust manner, or whether they are merely local approximations. &nbsp;The latter would hold if e.g. the person:&nbsp;is a brain in a vat;&nbsp;is dreaming; or&nbsp;is being simulated and can potentially be affected by entities outside the simulation.</p>\n<div>And in all these cases, we might say their world is \"not real\".</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1, "ZpG9rheyAkgCoEQea": 1, "wMPYFGmhcFg4bSb4Z": 1, "RMtdp6eGNjTZcmwJ6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XHxpAbnok6YyhGv8S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 91, "extendedScore": null, "score": 0.000173, "legacy": true, "legacyId": "4211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 91, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX", "3nxs2WYDGzJbzcLMp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-09T04:06:02.645Z", "modifiedAt": null, "url": null, "title": "-", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:57.604Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HoverHell", "createdAt": "2010-04-19T06:30:06.524Z", "isAdmin": false, "displayName": "HoverHell"}, "userId": "dLbWn7gGj75sekv7f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9TzhaRnvra59wijZk/", "pageUrlRelative": "/posts/9TzhaRnvra59wijZk/", "linkUrl": "https://www.lesswrong.com/posts/9TzhaRnvra59wijZk/", "postedAtFormatted": "Thursday, December 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20-&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A-%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TzhaRnvra59wijZk%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=-%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TzhaRnvra59wijZk%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TzhaRnvra59wijZk%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>-</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9TzhaRnvra59wijZk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 6.541886673135814e-07, "legacy": true, "legacyId": "4215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-09T05:12:49.474Z", "modifiedAt": null, "url": null, "title": "Delayed Solutions Game", "slug": "delayed-solutions-game", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:56.087Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xYxbnnHqsJCT9fdzo/delayed-solutions-game", "pageUrlRelative": "/posts/xYxbnnHqsJCT9fdzo/delayed-solutions-game", "linkUrl": "https://www.lesswrong.com/posts/xYxbnnHqsJCT9fdzo/delayed-solutions-game", "postedAtFormatted": "Thursday, December 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Delayed%20Solutions%20Game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADelayed%20Solutions%20Game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxYxbnnHqsJCT9fdzo%2Fdelayed-solutions-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Delayed%20Solutions%20Game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxYxbnnHqsJCT9fdzo%2Fdelayed-solutions-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxYxbnnHqsJCT9fdzo%2Fdelayed-solutions-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>This is a thread to practice <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">holding off on proposing solutions</a>.</p>\n<p>Rules:</p>\n<ol>\n<li>Post your dilemma (i.e. problem, question, situation, etc.) as a top-level comment. You can always come back to edit this.</li>\n<li>For the next 24 hours, replies in that thread can discuss only aspects of the problem, no solutions. (If something sounds too much like a solution, it gets downvoted.)</li>\n<li>After the 24 hours have passed from the start of the thread, solutions may be proposed therein.</li>\n</ol>\n<p>Note: Timezones for comments are in GMT (e.g. London), so you may need to use <a href=\"http://www.timeanddate.com/worldclock/\">this</a> to determine when 24 hours have passed in your local timezone.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xYxbnnHqsJCT9fdzo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 6.542052251186686e-07, "legacy": true, "legacyId": "4216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-09T05:39:34.176Z", "modifiedAt": null, "url": null, "title": "Utility is unintuitive", "slug": "utility-is-unintuitive", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:24.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RjYGFxXBbWiGWh53E/utility-is-unintuitive", "pageUrlRelative": "/posts/RjYGFxXBbWiGWh53E/utility-is-unintuitive", "linkUrl": "https://www.lesswrong.com/posts/RjYGFxXBbWiGWh53E/utility-is-unintuitive", "postedAtFormatted": "Thursday, December 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utility%20is%20unintuitive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtility%20is%20unintuitive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjYGFxXBbWiGWh53E%2Futility-is-unintuitive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utility%20is%20unintuitive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjYGFxXBbWiGWh53E%2Futility-is-unintuitive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjYGFxXBbWiGWh53E%2Futility-is-unintuitive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1467, "htmlBody": "<p>EDIT: My original post was wrong. I will leave it quoted at the end for the purposes of preserving information, but it is now replaced with a new post that correctly expresses my sentiments. The original title of this post was \"expected utility maximization is not rational\".</p>\n<p>As many people are probably aware, there is a theorem, called the Von Neumann-Morgenstern utility theorem, which states that anyone expressing consistent preferences must be maximizing the expected value of some function. The definition of consistent preferences is as follows:</p>\n<p>Let A, B, and C be probability distributions over outcomes. Let A &lt; B denote that B is preferred to A, and A = B denote that someone is indifferent between A and B. Then we assume</p>\n<p>\n<ul>\n<li>Either A &lt; B, A &gt; B, or A = B. In other words, you have to express a preference. This is reasonable because in the real world, you always have to make a decision (even \"lack of action\" is a decision).</li>\n<li>If A &lt; B, and B &lt; C, then A &lt; C. I believe that this is also clearly reasonable. If you have three possible actions, leading to distributions over outcomes A, B, and C, then you have to choose one of the three, meaning one of them is always preferred. So you can't have cycles of preferences.</li>\n<li>If A &lt; B, then (1-x)A+xC &lt; B for some x in (0,1) that is allowed to depend on A, B, and C. In other words, if B is preferred to A then B is also preferred to sufficiently small changes to A.</li>\n<li>If A &lt; B then pA+(1-p)C &lt; pB+(1-p)C for all p in (0,1). This is the least intuitive of the four axioms to me, and the one that I initially disagreed with. But I believe that you can argue in favor of it as follows: I flip a coin with weight p, and draw from X if p is heads and C if p is tails. I let you choose whether you want x to be A or B. It seems clear that if you prefer B to A, then you should choose B in this situation. However, I have not thought about this long enough to be completely sure that this is the case. Most other people seem to also think this is a reasonable axiom, so I'm going to stick with it for now.</li>\n</ul>\n</p>\n<p>Given these axioms, we can show that there exists a real-valued function u over outcomes such that A &lt; B if and only if E<sub>A</sub>[u] &lt; E<sub>B</sub>[u], where E<sub>X</sub>&nbsp;is the expected value with respect to the distribution X.</p>\n<p>Now, the important thing to note here is that this is an existence proof only. The function u doesn't have to look at all reasonable, it merely assigns a value to every possible outcome (in particular, even if E1 and E2 seem like completely unrelated events, there is no reason as far as I can tell why u([E1 and E2]) has to have anything to do with u(E1)+u(E2), for instance. Among other things, u is only defined up to an additive constant and so not only is there no reason to be true, it will be completely false for almost all possible utility functions, *even if you keep the person whose utility you are considering fixed*.</p>\n<p>In particular, it seems ridiculous that we would worry about an outcome that only occurs with probability 10<sup>-100</sup>. What this actually means is that our utility function is always much smaller than 10<sup>100</sup>, or rather that the ratio of the difference in utility between trivially small changes in outcome and arbitrarily large changes in outcome is always much larger than 10<sup>-100</sup>. This is how to avoid issues like Pascal's mugging, even in the least convenient possible world (since utility is an abstract construction, no universe can \"make\" a utility function become unbounded).</p>\n<p>What this means in particular is that saying that someone must maximize expected utility to be rational is not very productive. In particular, unless the other person has a sufficiently good technical grasp of what this means, they will probably do the wrong thing. Also, unless *you* have a good technical grasp of what it means, something that appears to violated expected utility might not. Remember, because utility is an artificial construct that has no reason to look reasonable, someone with completely reasonable preferences could have a very weird-*looking* utility function. Instead of telling people to maximize expected utility, we should identify which of the four above axioms they are violating, then explain why they are being irrational (or, if the purpose is to educate in advance, explain to them why the four axioms above should be respected). [Note however that just because a perfectly rational person *always* satisfies the above axioms, doesn't mean that you will be better off if you satisfy the above axioms more often. Your preferences might have a complicated cycle that you are unsure how to correctly resolve. Picking a resolution at random is unlikely to be a good idea.]</p>\n<p>Now, utility is this weird function that we don't understand at all. Then why does it seem like there's something called utility that **both** fits our intuitions and that people should be maximizing? The answer is that in many cases utility *can* be equated with something like money + risk aversion. The reason why is due to the law of large numbers, formalized through various bounds such as&nbsp;<a href=\"http://en.wikipedia.org/wiki/Hoeffding%27s_inequality\">Hoeffding's inequality</a>&nbsp;and the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Chernoff_bound\">Chernoff bound</a>, as well as more powerful arguments like<a href=\"http://en.wikipedia.org/wiki/Concentration_of_measure\">concentration of measure</a>. What these arguments say is that if you have a large number of random variables that are sufficiently uncorrelated and that have sufficiently small standard deviation relative to the mean, then with high probability their sum is very close to their expected sum. So when our variables all have means that are reasonable close to each other (as is the case for most every day events), we can say something like the total *monetary* value of our combined actions will be very close to the sum of the expected monetary values of our individual actions (and likewise for other quantities like time). So in situations where, e.g., your goal is to spend as little time on undesirable work as possible, you want to minimize expected time spent on undesirable work, **as a heuristic that holds in most practical cases**. While this might make it *look* like your utility function is time in this case, I believe that the resemblance is purely coincidental, and you certainly shouldn't be willing to make very low-success-rate gambles with large time payoffs.</p>\n<p>Old post:</p>\n<blockquote>\n<p>I'm posting this to the discussion because I don't plan to make a detailed argument, mainly because I think this point should be extremely clear, even though many people on LessWrong seem to disagree with me.</p>\n<p>Maximizing expected utility is not a terminal goal, it is a useful heuristic. To see why always maximizing expected utility is clearly bad, consider an action A with a 10<sup>-10</sup>&nbsp;chance of giving you 10<sup>100</sup>&nbsp;units of utility, and a 1-10<sup>-10</sup>&nbsp;chance of losing you 10<sup>10</sup>&nbsp;units of utility. Then expected utility maximization requires you to perform A, even though it is obviously a bad idea. I believe this has been discussed here previously as Pascal's mugging.</p>\n<p>For some reason, this didn't lead everyone to the obvious conclusion that maximizing expected utility is the wrong thing to do, so I'm going to try to dissolve the issue by looking at why we would want to maximize expected utility in most situations. I think once this is accomplished it will be obvious why there is no particular reason to maximize expected utility for very low-probability events (in fact, one might consider having a utility function over probability distributions rather than actual states of the world).</p>\n<p>The reason that you normally want to maximize expected utility is because of the law of large numbers, formalized through various bounds such as <a href=\"http://en.wikipedia.org/wiki/Hoeffding%27s_inequality\">Hoeffding's inequality</a> and the <a href=\"http://en.wikipedia.org/wiki/Chernoff_bound\">Chernoff bound</a>, as well as more powerful arguments like <a href=\"http://en.wikipedia.org/wiki/Concentration_of_measure\">concentration of measure</a>. What these arguments say is that if you have a large number of random variables that are sufficiently uncorrelated and that have sufficiently small variance relative to the mean, then with high probability their sum is very close to their expected sum. Thus for events with probabilities that are bounded away from 0 and 1 you always expect your utility to be very close to your expected utility, and should therefore maximize expected utility in order to maximize actual utility. But once the probabilities get small (or the events correlated, e.g. you are about to make an irreversible decision), these bounds no longer hold and the reasons for maximizing expected utility vanish. You should instead consider what sort of distribution over outcomes you find desirable.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RjYGFxXBbWiGWh53E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -4, "extendedScore": null, "score": 6.542118565886376e-07, "legacy": true, "legacyId": "4217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-09T14:46:02.157Z", "modifiedAt": null, "url": null, "title": "Reliably wrong", "slug": "reliably-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:54.834Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rBqk3vMJzpEWqFRmR/reliably-wrong", "pageUrlRelative": "/posts/rBqk3vMJzpEWqFRmR/reliably-wrong", "linkUrl": "https://www.lesswrong.com/posts/rBqk3vMJzpEWqFRmR/reliably-wrong", "postedAtFormatted": "Thursday, December 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reliably%20wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReliably%20wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrBqk3vMJzpEWqFRmR%2Freliably-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reliably%20wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrBqk3vMJzpEWqFRmR%2Freliably-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrBqk3vMJzpEWqFRmR%2Freliably-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p><a href=\"http://www.marginalrevolution.com/marginalrevolution/2010/12/what-ive-been-reading.html#comments\">Discussion of a book by \"Dow Jones 36,000\" Glassman\"</a>. I'm wondering whether there are pundits who are so often wrong that their predictions are reliable indicators that something else (ideally the opposite) will happen.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rBqk3vMJzpEWqFRmR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 6.543473798291854e-07, "legacy": true, "legacyId": "4220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-09T16:46:28.536Z", "modifiedAt": null, "url": null, "title": "Science reveals how not to choke under pressure", "slug": "science-reveals-how-not-to-choke-under-pressure", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:55.250Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qoMvGdah6W473Mrw9/science-reveals-how-not-to-choke-under-pressure", "pageUrlRelative": "/posts/qoMvGdah6W473Mrw9/science-reveals-how-not-to-choke-under-pressure", "linkUrl": "https://www.lesswrong.com/posts/qoMvGdah6W473Mrw9/science-reveals-how-not-to-choke-under-pressure", "postedAtFormatted": "Thursday, December 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%20reveals%20how%20not%20to%20choke%20under%20pressure&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%20reveals%20how%20not%20to%20choke%20under%20pressure%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqoMvGdah6W473Mrw9%2Fscience-reveals-how-not-to-choke-under-pressure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%20reveals%20how%20not%20to%20choke%20under%20pressure%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqoMvGdah6W473Mrw9%2Fscience-reveals-how-not-to-choke-under-pressure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqoMvGdah6W473Mrw9%2Fscience-reveals-how-not-to-choke-under-pressure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>Found via <a title=\"http://discovermagazine.com/2010/the-brain-2/06-science-reveals-how-not-to-choke-under-pressure\" href=\"http://discovermagazine.com/2010/the-brain-2/06-science-reveals-how-not-to-choke-under-pressure\">reddit</a>, excerpt:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; color: #545454;\">\n<p style=\"font-family: Arial, Helvetica, sans-serif; margin-top: 0px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; line-height: 1.4em; padding-top: 7px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 12px; color: #000000;\"><span style=\"line-height: 17px;\">Choking happens when we let anxious thoughts distract us or when we start trying to consciously control motor skills best left on autopilot. ...</span></p>\n<p style=\"font-family: Arial, Helvetica, sans-serif; margin-top: 0px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; line-height: 1.4em; padding-top: 7px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 12px; color: #000000;\"><span style=\"line-height: 17px;\">In her new book,&nbsp;<a class=\"external-link\" style=\"font-family: Arial, Helvetica, sans-serif; color: #df6615; background-color: transparent; text-decoration: underline;\" href=\"http://www.amazon.com/Choke-Secrets-Brain-Reveal-Getting/dp/1416596178\"><em style=\"font-family: Arial, Helvetica, sans-serif;\">Choke: What the Secrets of the Brain Reveal About Success and Failure at Work and at Play</em></a>, Beilock deconstructs high-stakes moments&mdash;the ones seen around the world and the ones only our mothers care about&mdash;to explore why we sometimes falter, and why other times we nail it. ...</span></p>\n<p style=\"font-family: Arial, Helvetica, sans-serif; margin-top: 0px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; line-height: 1.4em; padding-top: 7px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 12px; color: #000000;\"><strong style=\"font-family: Arial, Helvetica, sans-serif;\">What goes wrong in our brain when this happens?&nbsp;</strong><br style=\"font-family: Arial, Helvetica, sans-serif;\" />Working memory, housed in the prefrontal cortex, is what allows us to do calculations in our head and reason through a problem. Unfortunately, it&rsquo;s a limited resource. If we&rsquo;re doing an activity that requires a lot of cognitive horsepower, such as responding to an on-the-spot question, and at the same time we&rsquo;re worrying about screwing up, then suddenly we don&rsquo;t have the brainpower we need.</p>\n<p style=\"font-family: Arial, Helvetica, sans-serif; margin-top: 0px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; line-height: 1.4em; padding-top: 7px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 12px; color: #000000;\">Also, once we feel stressed, we often try to control what we&rsquo;re doing in order to ensure success. So if we&rsquo;re doing a task that normally operates largely outside of conscious awareness, such as an easy golf swing, what screws us up is the impulse to think about and control our actions. Suddenly we&rsquo;re too attentive to what we&rsquo;re doing, and all the training that has improved our motor skills is for naught, since our conscious attention is essentially hijacking motor memory. ...</p>\n<p style=\"font-family: Arial, Helvetica, sans-serif; margin-top: 0px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; line-height: 1.4em; padding-top: 7px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 12px; color: #000000;\"><span style=\"line-height: 17px;\"><strong style=\"font-family: Arial, Helvetica, sans-serif;\">How can I prevent myself from overthinking?&nbsp;</strong><br style=\"font-family: Arial, Helvetica, sans-serif;\" />You might think that writing about your worries would just make them more salient. But there is work in clinical psychology showing that writing helps limit ruminative thoughts&mdash;those negative thoughts that are very hard to shake and that seem to grow the more you dwell on them. The idea is that you cognitively outsource your worries to the page. Writing about worries for 10 minutes right before taking a standardized test is really beneficial.</span></p>\n</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qoMvGdah6W473Mrw9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 6.543772554506618e-07, "legacy": true, "legacyId": "4221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T00:53:18.674Z", "modifiedAt": null, "url": null, "title": "Unpacking the Concept of \"Blackmail\"", "slug": "unpacking-the-concept-of-blackmail", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:05.603Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mey7SLY3xnchMv98H/unpacking-the-concept-of-blackmail", "pageUrlRelative": "/posts/Mey7SLY3xnchMv98H/unpacking-the-concept-of-blackmail", "linkUrl": "https://www.lesswrong.com/posts/Mey7SLY3xnchMv98H/unpacking-the-concept-of-blackmail", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unpacking%20the%20Concept%20of%20%22Blackmail%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnpacking%20the%20Concept%20of%20%22Blackmail%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMey7SLY3xnchMv98H%2Funpacking-the-concept-of-blackmail%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unpacking%20the%20Concept%20of%20%22Blackmail%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMey7SLY3xnchMv98H%2Funpacking-the-concept-of-blackmail", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMey7SLY3xnchMv98H%2Funpacking-the-concept-of-blackmail", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 468, "htmlBody": "<p><strong>Keep in mind</strong>: <a href=\"/lw/2os/controlling_constant_programs\">Controlling Constant Programs</a>, <a href=\"/lw/2tq/notion_of_preference_in_ambient_control\">Notion of Preference in Ambient Control</a>.</p>\n<p>There is a reasonable game-theoretic heuristic, \"don't respond to blackmail\" or \"don't negotiate with terrorists\". But what is actually meant by the word \"blackmail\" here? Does it have a place as a fundamental decision-theoretic concept, or is it merely an affective category, a class of situations activating a certain psychological adaptation that expresses disapproval of certain decisions and on the net protects (benefits) you, like those adaptation that respond to \"being rude\" or \"<a href=\"/lw/13s/the_nature_of_offense/\">offense</a>\"?</p>\n<p>We, as humans, have a concept of \"default\", \"do nothing strategy\". The other plans can be compared to the moral value of the default. Doing harm would be something worse than the default, doing good something better than the default.</p>\n<p>Blackmail is then a situation where by decision of another agent (\"blackmailer\"), you are presented with two options, both of which are harmful to you (worse than the default), and one of which is better for the blackmailer. The alternative (if the blackmailer decides not to blackmail) is the default.</p>\n<p>Compare this with the same scenario, but with the \"default\" action of the other agent being worse for you than the given options. This would be called normal bargaining, as in trade, where both parties benefit from exchange of goods, but to a different extent depending on which cost is set.</p>\n<p>Why is the \"default\" special here? <a id=\"more\"></a>If bargaining or blackmail did happen, we know that \"default\" is impossible. How can we tell two situations apart then, from their payoffs (or models of uncertainty about the outcomes) alone? It's necessary to tell these situations apart to manage not responding to threats, but at the same time cooperating in trade (instead of making things as bad as you can for the trade partner, no matter what it costs you). Otherwise, abstaining from doing harm looks exactly like doing good. A charitable gift of not blowing up your car and so on.</p>\n<p>My hypothesis is that \"blackmail\" is what the suggestion of your mind to not cooperate feels like from the inside, the answer to a difficult problem computed by cognitive algorithms you don't understand, and not a simple property of the decision problem itself. By saying \"don't respond to blackmail\", you are pushing most of the hard work into intuitive categorization of decision problems into \"blackmail\" and \"trade\", with only correct interpretation of the results of that categorization left as an explicit exercise.</p>\n<p>(A possible direction for formalizing these concepts involves introducing some kind of notion of resources, maybe amount of control, and instrumental vs. terminal spending, so that the \"default\" corresponds to less instrumental spending of controlled resources, but I don't see it clearly.)</p>\n<p>(Let's keep on topic and not refer to powerful AIs or FAI in this thread, only discuss the concept of blackmail in itself, in decision-theoretic context.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2q2cK4FdnSeohTEaJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mey7SLY3xnchMv98H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 35, "extendedScore": null, "score": 6.544980418905301e-07, "legacy": true, "legacyId": "4222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gZbHSWcLvj7ZopSas", "ZpATmvAyqajiA5XNC", "QPqm5aj2meRmE7kR8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T02:12:09.126Z", "modifiedAt": null, "url": null, "title": "\u201cFake Options\u201d in Newcomb\u2019s Problem", "slug": "fake-options-in-newcomb-s-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:53.999Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Normal_Anomaly", "createdAt": "2010-11-14T03:31:54.691Z", "isAdmin": false, "displayName": "Normal_Anomaly"}, "userId": "WgGYj5bqcZKsFNG6F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Thi2sjShMEBgKM3TX/fake-options-in-newcomb-s-problem", "pageUrlRelative": "/posts/Thi2sjShMEBgKM3TX/fake-options-in-newcomb-s-problem", "linkUrl": "https://www.lesswrong.com/posts/Thi2sjShMEBgKM3TX/fake-options-in-newcomb-s-problem", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%E2%80%9CFake%20Options%E2%80%9D%20in%20Newcomb%E2%80%99s%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%E2%80%9CFake%20Options%E2%80%9D%20in%20Newcomb%E2%80%99s%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThi2sjShMEBgKM3TX%2Ffake-options-in-newcomb-s-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%E2%80%9CFake%20Options%E2%80%9D%20in%20Newcomb%E2%80%99s%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThi2sjShMEBgKM3TX%2Ffake-options-in-newcomb-s-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FThi2sjShMEBgKM3TX%2Ffake-options-in-newcomb-s-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\">This is an exploration of a way of looking at Newcomb&rsquo;s Problem that helped me understand it. I hope somebody else finds it useful. I may add discussions of other game theory problems in this format if anybody wants them.</p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\">Consider Newcomb&rsquo;s Problem:: Omega offers you two boxes, one transparent and containing $1000, the other opaque and containing either $1 million or nothing. Your options are to take both boxes, or only take the second one; but Omega has put money in the second box only if it has predicted that you will only take 1 box. A person in favor of one-boxing says, &ldquo;I&rsquo;d rather have a million than a thousand.&rdquo; A two-boxer says, &ldquo;Whether or not box B contains money, I&rsquo;ll get $1000 more if I take box A as well. It&rsquo;s either $1001000 vs. $1000000, or $1000 vs. nothing.&rdquo; To get to these different decisions, the agents are working from two different ways of visualising the payoff matrix. The two-boxer sees four possible outcomes and the one-boxer sees two, the other two having very low probability.</p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\">The two-boxer&rsquo;s payoff matrix looks like this:</p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Box B</p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span style=\"text-decoration: underline;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>|Money<span>&nbsp;&nbsp;&nbsp; </span>| No money|</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\"><span style=\"text-decoration: underline;\">Decision&nbsp; 1-box|<span>&nbsp;&nbsp;&nbsp; </span>$1mil<span> &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span> | $0<span>&nbsp;&nbsp;</span><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; </span>|</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp; </span>2-box |<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>$1001000| $1000<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>|<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\">The outcomes $0 and $1001000 both require Omega making a wrong prediction. But as the problem is formulated, Omega is superintelligent and has been right 100 out of 100 times so far. So the one-boxer, taking this into account, describes the payoff matrix like this:</p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Box B</p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span style=\"text-decoration: underline;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>|Money<span>&nbsp;&nbsp;&nbsp; </span>| No money|</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\">Decision<span>&nbsp; </span><span style=\"text-decoration: underline;\"><span>&nbsp;</span>1-box|<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>$1mil&nbsp;<span> &nbsp;&nbsp;&nbsp;&nbsp; </span> | not possible|</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 0.5in;\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span>&nbsp;&nbsp;&nbsp; </span>2-box |<span>&nbsp;&nbsp; </span>not possible| $1000<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>|<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>\n<p class=\"MsoNormal\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>If Omega is really a perfect (nearly perfect) predictor, the only possible (not hugely unlikely) outcomes are $1000 for two-boxing and $1 million for one-boxing, and considering the other outcomes is an epistemic failure.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Thi2sjShMEBgKM3TX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 1, "extendedScore": null, "score": 6.545176063982715e-07, "legacy": true, "legacyId": "4223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T04:17:13.544Z", "modifiedAt": null, "url": null, "title": "Kazakhstan's president urges scientists to find the elixir of life", "slug": "kazakhstan-s-president-urges-scientists-to-find-the-elixir", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:54.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Document", "createdAt": "2010-02-08T04:14:47.949Z", "isAdmin": false, "displayName": "Document"}, "userId": "vaMNHjzaCGqF8yTMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5C2sytWZNqSujXjoE/kazakhstan-s-president-urges-scientists-to-find-the-elixir", "pageUrlRelative": "/posts/5C2sytWZNqSujXjoE/kazakhstan-s-president-urges-scientists-to-find-the-elixir", "linkUrl": "https://www.lesswrong.com/posts/5C2sytWZNqSujXjoE/kazakhstan-s-president-urges-scientists-to-find-the-elixir", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kazakhstan's%20president%20urges%20scientists%20to%20find%20the%20elixir%20of%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKazakhstan's%20president%20urges%20scientists%20to%20find%20the%20elixir%20of%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5C2sytWZNqSujXjoE%2Fkazakhstan-s-president-urges-scientists-to-find-the-elixir%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kazakhstan's%20president%20urges%20scientists%20to%20find%20the%20elixir%20of%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5C2sytWZNqSujXjoE%2Fkazakhstan-s-president-urges-scientists-to-find-the-elixir", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5C2sytWZNqSujXjoE%2Fkazakhstan-s-president-urges-scientists-to-find-the-elixir", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>...according to <a href=\"http://www.reddit.com/r/science/comments/ej06r/70yearold_president_of_kazakhstan_says_scientists/\">this front-page Reddit headline</a> I just saw, which links to <a href=\"http://www.guardian.co.uk/world/2010/dec/07/kazakhstan-president-scientists-research-ageing\">this Guardian article</a>. I wonder if he's heard of KrioRus, whether he's signed up (Wikipedia says they offer services \"to clients from Russia, CIS and EU\"), and what his odds would be if he were (would it be possible to emigrate to Russia to be closer to the facility, and if not, what would be the best possible option?). Given his being a head of state, presumably it'd be pretty tough for an advocate to even get close enough to try to make the case.</p>\n<p>Searching the Reddit comment thread for \"cryo\" turned up nothing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5C2sytWZNqSujXjoE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 6.545486458225341e-07, "legacy": true, "legacyId": "4224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T06:08:00.687Z", "modifiedAt": null, "url": null, "title": "A Thought on Pascal's Mugging", "slug": "a-thought-on-pascal-s-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:32.865Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vEZ3Ajyp3LBtBwyuG/a-thought-on-pascal-s-mugging", "pageUrlRelative": "/posts/vEZ3Ajyp3LBtBwyuG/a-thought-on-pascal-s-mugging", "linkUrl": "https://www.lesswrong.com/posts/vEZ3Ajyp3LBtBwyuG/a-thought-on-pascal-s-mugging", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Thought%20on%20Pascal's%20Mugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Thought%20on%20Pascal's%20Mugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEZ3Ajyp3LBtBwyuG%2Fa-thought-on-pascal-s-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Thought%20on%20Pascal's%20Mugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEZ3Ajyp3LBtBwyuG%2Fa-thought-on-pascal-s-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEZ3Ajyp3LBtBwyuG%2Fa-thought-on-pascal-s-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 316, "htmlBody": "<p><em>For background, see <a href=\"http://wiki.lesswrong.com/wiki/Pascal's_mugging\">here</a>.</em></p>\n<p>In a <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/fol?c=1\">comment</a>&nbsp;on the&nbsp;<a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">original Pascal's mugging post</a>, Nick Tarleton writes:</p>\n<blockquote>\n<p>[Y]ou could replace \"kill 3^^^^3 people\" with \"create 3^^^^3 units of disutility according to your utility function\". (I respectfully suggest that we all start using&nbsp;this form of the problem.)</p>\n<p>Michael Vassar has suggested that we should consider any number of identical lives to have the same utility as one life. That could be a solution, as it's impossible&nbsp;to create 3^^^^3 distinct humans. But, this also is irrelevant to the create-3^^^^3-disutility-units form.</p>\n</blockquote>\n<p>Coming across this again recently, it occurred to me that there might be a way to generalize Vassar's suggestion in such a way as to deal with Tarleton's more abstract&nbsp;formulation of the problem. I'm curious about the extent to which folks have thought about this. (Looking further through the comments on the original post, I found&nbsp;essentially the same idea in a <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/fph?c=1\">comment by g</a>, but it wasn't discussed further.)</p>\n<p>The idea is that the Kolmogorov complexity of \"3^^^^3 units of disutility\" should be <em>much higher</em>&nbsp;than the Kolmogorov complexity of the number 3^^^^3. That is, the&nbsp;utility function should grow only according to the complexity of the scenario being evaluated, and not (say) linearly in the number of people involved. Furthermore,&nbsp;the domain of the utility function should consist of <em>low-level descriptions</em>&nbsp;of the state of the world, which won't refer directly to words uttered by muggers, in&nbsp;such a way that a mere discussion of \"3^^^^3 units of disutility\" by a mugger will not typically be (anywhere near) enough evidence to promote an <em>actual</em>&nbsp;\"3^^^^3-disutilon\" hypothesis to attention.</p>\n<p>This seems to imply that the intuition responsible for the problem is a kind of <a href=\"http://wiki.lesswrong.com/wiki/Fake_simplicity\">fake simplicity</a>, ignoring the&nbsp;complexity of value (negative value in this case). A confusion of levels also appears implicated (talking about utility does not itself significantly affect utility;&nbsp;you don't suddenly make 3^^^^3-disutilon scenarios probable by talking about \"3^^^^3 disutilons\").</p>\n<p>What do folks think of this? Any obvious problems?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HNJiR8Jzafsv8cHrC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vEZ3Ajyp3LBtBwyuG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "4225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 159, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T07:43:28.889Z", "modifiedAt": null, "url": null, "title": "Link: What does it feel like to be stupid?", "slug": "link-what-does-it-feel-like-to-be-stupid", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:53.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Golovin", "createdAt": "2009-02-28T13:32:31.085Z", "isAdmin": false, "displayName": "Vladimir_Golovin"}, "userId": "Me2m84AhCn9H49riY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NrwAToAbNsLGSZ8b7/link-what-does-it-feel-like-to-be-stupid", "pageUrlRelative": "/posts/NrwAToAbNsLGSZ8b7/link-what-does-it-feel-like-to-be-stupid", "linkUrl": "https://www.lesswrong.com/posts/NrwAToAbNsLGSZ8b7/link-what-does-it-feel-like-to-be-stupid", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20What%20does%20it%20feel%20like%20to%20be%20stupid%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20What%20does%20it%20feel%20like%20to%20be%20stupid%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNrwAToAbNsLGSZ8b7%2Flink-what-does-it-feel-like-to-be-stupid%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20What%20does%20it%20feel%20like%20to%20be%20stupid%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNrwAToAbNsLGSZ8b7%2Flink-what-does-it-feel-like-to-be-stupid", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNrwAToAbNsLGSZ8b7%2Flink-what-does-it-feel-like-to-be-stupid", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p><a href=\"http://www.quora.com/What-does-it-feel-like-to-be-stupid\">What does it feel like to be stupid?</a></p>\n<blockquote>\n<p><em>I had an arterial problem for a couple of years, which reduced blood supply to my heart and brain and depleted B vitamins from my nerves (to keep the heart in good repair). Although there is some vagueness as to the mechanisms, this made me forgetful, slow, and easily overwhelmed. In short I felt like I was stupid compared to what I was used to, and I was.</em></p>\n<p><em>It was frightening at first because I knew something wasn't right but didn't know what, and very worrying for my career because I was simply not very good any more.</em></p>\n<p><em>However, once I got used to it and resigned myself, it was great.</em></p>\n</blockquote>\n<p>Full article:<br />http://www.quora.com/What-does-it-feel-like-to-be-stupid</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NrwAToAbNsLGSZ8b7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 6.545998378161387e-07, "legacy": true, "legacyId": "4232", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T08:27:28.781Z", "modifiedAt": null, "url": null, "title": "How To Lose 100 Karma In 6 Hours -- What Just Happened", "slug": "how-to-lose-100-karma-in-6-hours-what-just-happened", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.150Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "waitingforgodel", "createdAt": "2010-09-13T09:13:00.018Z", "isAdmin": false, "displayName": "waitingforgodel"}, "userId": "SbWBTEXPYb5H4AbnY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EBTbsoRqm8WXximfn/how-to-lose-100-karma-in-6-hours-what-just-happened", "pageUrlRelative": "/posts/EBTbsoRqm8WXximfn/how-to-lose-100-karma-in-6-hours-what-just-happened", "linkUrl": "https://www.lesswrong.com/posts/EBTbsoRqm8WXximfn/how-to-lose-100-karma-in-6-hours-what-just-happened", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20To%20Lose%20100%20Karma%20In%206%20Hours%20--%20What%20Just%20Happened&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20To%20Lose%20100%20Karma%20In%206%20Hours%20--%20What%20Just%20Happened%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEBTbsoRqm8WXximfn%2Fhow-to-lose-100-karma-in-6-hours-what-just-happened%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20To%20Lose%20100%20Karma%20In%206%20Hours%20--%20What%20Just%20Happened%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEBTbsoRqm8WXximfn%2Fhow-to-lose-100-karma-in-6-hours-what-just-happened", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEBTbsoRqm8WXximfn%2Fhow-to-lose-100-karma-in-6-hours-what-just-happened", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 830, "htmlBody": "<div>As with all good posts, we begin with a hypothetical:</div>\n<div>Imagine that, in the country you are in, a law is passed saying that if you drive your car without your seat belt on, you will be fined $100.</div>\n<div>Here's the question: Is this blackmail? Is this terrorism?</div>\n<div>Certainly it's a zero-sum interaction (at least in the short term). You either have to endure the inconvenience&nbsp;of putting on a seat belt, or risk the chance of a $100 fine.</div>\n<div>You may also want to consider that cooperating with the seat belt fine may also cause lawmakers to believe that you'll also follow future laws.</div>\n<div><br /></div>\n<div>If that one seems too obvious, here's another: A law is passed establishing a $500 fine for pirating an album on the internet.</div>\n<div>Does this count as blackmail? does this count as terrorism?</div>\n<div><br /></div>\n<div>What if, instead of passing a law, the music companies declare that they will sue you for $500 every time you pirate an album?</div>\n<div>Is it blackmail yet? terrorism? Will complying teach the music companies that throwing their weight around works?</div>\n<div><br /></div>\n<div>Enough with the hypothetical, <strong>this one's real</strong>: The moderator of one of your favorite online forums declares that if you post things he feels are dangerous to read, he will censor them. He may or may not tell you when he does this. If you post such things repeatedly, you will be <a href=\"/lw/38u/best_career_models_for_doing_research/33ui?c=1\">banned</a>.</div>\n<div>Does this count as blackmail? Does this count as terrorism? Should we not comply with him to prevent similar future abuses of power?</div>\n<div><br /></div>\n<div>Two months ago, I found a third option to the comply/revolt dilemma: turn the force back on the forceful.</div>\n<div>Imagine this: you're the moderator of an online forum and care primarily about one thing: reducing existential risks. One day, one of your form members vows to ensure that censoring posts will cause a small increase in existential risks.</div>\n<div>Does this count as blackmail? Does this count as terrorism? Would you not comply to prevent similar future abuses of power?</div>\n<div><br /></div>\n<div><br /></div>\n<div>(Please pause here if you're feeling emotional -- what follows is important, and deserves a cool head)</div>\n<div><br /></div>\n<div><br /></div>\n<div>It is my opinion that none of these are blackmail.</div>\n<div>Blackmail is fundamentally a single shot game.</div>\n<div>Laws and rules, are about the structure of the world's payoffs, and changing them to incentivize behavior.</div>\n<div>Now it's fair to say that there are just laws, and there are unjust laws... and perhaps we should refuse to follow unjust laws... but to call a law blackmail or terrorism seems incorrect.</div>\n<div><br /></div>\n<div>Here's what happened:</div>\n<div>\n<ul>\n<li>7 weeks ago, I <a href=\"/lw/2ft/open_thread_july_2010_part_2/2o25?c=1\" target=\"_self\">precommitted</a> that censoring a post or comment on LessWrong would cause a 0.0001% increase in existential risk.</li>\n<li>Earlier today, <a href=\"/lw/38u/best_career_models_for_doing_research/33tj\">Yudkowsky censored a post on less wrong</a></li>\n<li>20 minutes later, existential risks increased 0.0001% (to the best of my estimation).</li>\n</ul>\n</div>\n<div><br /></div>\n<div>This will continue for the <a href=\"/lw/38u/best_career_models_for_doing_research/33v3?c=1\">foreseeable future</a>. I'm not happy about it either. Basically I think the sanest way to think about the situation is to assume that Yudkowsky's \"delete\" link also causes a 0.0001% increase in existential risk, and hope that he uses it appropriately.</div>\n<div>He doesn't feel this way. He feels that the only correct answer here is to ignore the 0.0001% increase. We are at an impasse.</div>\n<div><br /></div>\n<div>FAQ:</div>\n<div><strong>Q:</strong> Will you reconsider?</div>\n<div><strong>A:</strong> Sadly no. This situation is <em>symmetric</em> -- just as I am not immune to Yudkowsky's laws (censorship on LW if I talk about \"dangerous\" ideas), he is not immune to my laws.</div>\n<div><br /></div>\n<div><strong>Q:</strong> How can you be sure that a post was censored rather than deleted by the owner?</div>\n<div><strong>A:</strong> This is sometimes hard, and sometimes easy. In general I will err on the side of caution.</div>\n<div><br /></div>\n<div><strong>Q:</strong> How can you be sure that you haven't missed a deleted comment?</div>\n<div><strong>A:</strong> I use, and am improving, an automated solution.</div>\n<div><br /></div>\n<div><strong>Q:</strong> What is the nature of the existential risk increase?</div>\n<div><strong>A:</strong> Emails. (Yes, emails). Maybe some phone calls.</div>\n<div>There is a simple law that I believe makes intuitive sense to the conservative right. A law that will be easy for them to endorse. This law would be disastrous for the relative chance of our first AI being a FAI vs a UFAI. Every time EY decides to take a 0.0001% step, an email or phone call will be made to raise awareness about this law.</div>\n<div><br /></div>\n<div><strong>Q:</strong> Is there any way for me to gain access to the censored content?</div>\n<div><strong>A:</strong> I am working on a website that will update in real time as posts are deleted from LessWrong. Stay tuned!</div>\n<div><br /></div>\n<div><strong>Q:</strong> Will you still post here under waitingforgodel</div>\n<div><strong>A:</strong> Yes, but less. Replying to 100+ comments is very time consuming, and I have several projects in dire need of attention.</div>\n<div><br /></div>\n<div>Thank you very much for your time and understanding,</div>\n<div>-wfg</div>\n<div><br /></div>\n<div><strong>Edit: This post is describing what happened, not why. For a discussion about why I feel that the precommitment will result in an existential risk savings, please see the \"precommitment\" thread, where it is talked about extensively.</strong></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EBTbsoRqm8WXximfn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 95, "baseScore": -62, "extendedScore": null, "score": -0.000121, "legacy": true, "legacyId": "4233", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 220, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T10:49:41.139Z", "modifiedAt": null, "url": null, "title": "Anthropologists and \"science\": dark side epistemology?", "slug": "anthropologists-and-science-dark-side-epistemology", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:55.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w72tje6XeZrxL325m/anthropologists-and-science-dark-side-epistemology", "pageUrlRelative": "/posts/w72tje6XeZrxL325m/anthropologists-and-science-dark-side-epistemology", "linkUrl": "https://www.lesswrong.com/posts/w72tje6XeZrxL325m/anthropologists-and-science-dark-side-epistemology", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropologists%20and%20%22science%22%3A%20dark%20side%20epistemology%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropologists%20and%20%22science%22%3A%20dark%20side%20epistemology%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw72tje6XeZrxL325m%2Fanthropologists-and-science-dark-side-epistemology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropologists%20and%20%22science%22%3A%20dark%20side%20epistemology%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw72tje6XeZrxL325m%2Fanthropologists-and-science-dark-side-epistemology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw72tje6XeZrxL325m%2Fanthropologists-and-science-dark-side-epistemology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>The American Anthropological Association has apparently decided to ditch the word \"science\", arguably so they can promote political messages without hindrance from empirical data.</p>\n<p>If so, this might be an example of <a href=\"/lw/uy/dark_side_epistemology/\">dark side epistemology</a>.</p>\n<p>(Articles in <a href=\"http://www.psychologytoday.com/blog/fetishes-i-dont-get/201011/no-science-please-were-anthropologists\">Psychology Today</a> and <a href=\"http://www.nytimes.com/2010/12/10/science/10anthropology.html?_r=2&amp;hpw\">NYT</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w72tje6XeZrxL325m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 6.546460591866163e-07, "legacy": true, "legacyId": "4234", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XTWkjCJScy2GFAgDt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T15:55:48.002Z", "modifiedAt": null, "url": null, "title": "Rational entertainment industry?", "slug": "rational-entertainment-industry", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:09.539Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rwallace", "createdAt": "2009-03-01T16:13:25.493Z", "isAdmin": false, "displayName": "rwallace"}, "userId": "cPhXNeZvnK7LgPMnv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pk7ED8sXEGdzTCrn2/rational-entertainment-industry", "pageUrlRelative": "/posts/pk7ED8sXEGdzTCrn2/rational-entertainment-industry", "linkUrl": "https://www.lesswrong.com/posts/pk7ED8sXEGdzTCrn2/rational-entertainment-industry", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20entertainment%20industry%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20entertainment%20industry%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpk7ED8sXEGdzTCrn2%2Frational-entertainment-industry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20entertainment%20industry%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpk7ED8sXEGdzTCrn2%2Frational-entertainment-industry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpk7ED8sXEGdzTCrn2%2Frational-entertainment-industry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 298, "htmlBody": "<p>\n<p>By \"the industry\" in this post, I refer to that part of the entertainment industry which:</p>\n<p>1. Produces movies, TV and video games (as opposed to books, comics etc.)</p>\n<p>2. Is motivated by profit (as opposed to fun, politics etc.)</p>\n<p>3. Consists of companies (as opposed to lone developers, student teams etc.)</p>\n<p>It seems to me that the industry has two characteristics:</p>\n<p><strong>Formulaic</strong></p>\n<p>Most products follow some formula which is known to be workable.</p>\n<p>Under what circumstances is this rational? (I'm not commenting on whether it's artistically good or bad; again, I'm only discussing entertainment as a commercial enterprise motivated by profit.) It seems to me following a proven formula is rational if your priority is to not lose, to go for the sure thing, i.e. the chance of a big hit is not worth the risk of a complete flop.</p>\n<p><strong>Hit driven</strong></p>\n<p>It's the accepted wisdom that entertainment is a hit driven industry: almost all the profits are generated by a handful of the most successful products, with the rest losing money or barely covering costs.</p>\n<p>Now my question: <em>isn't there a contradiction here?</em> If you're selling insurance, following a proven formula may well be the rational thing to do. If you're the owner of one of the handful of franchises that is pulling in big profits, of course you shouldn't mess with a winner. But if you're one of the many also-rans, how is it rational to stick with an almost sure loser? In a hit driven industry, wouldn't it be more rational to concentrate on maximizing your chance of winning big, instead of trying to minimize the risk of a flop?</p>\n<p>But I've never worked in the entertainment industry; perhaps my layman's impression of it is inaccurate. Is there something I'm missing, or is a substantial amount of expected profit really being left on the table?</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pk7ED8sXEGdzTCrn2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 6.547220583638765e-07, "legacy": true, "legacyId": "4235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>\n</p><p>By \"the industry\" in this post, I refer to that part of the entertainment industry which:</p>\n<p>1. Produces movies, TV and video games (as opposed to books, comics etc.)</p>\n<p>2. Is motivated by profit (as opposed to fun, politics etc.)</p>\n<p>3. Consists of companies (as opposed to lone developers, student teams etc.)</p>\n<p>It seems to me that the industry has two characteristics:</p>\n<p><strong id=\"Formulaic\">Formulaic</strong></p>\n<p>Most products follow some formula which is known to be workable.</p>\n<p>Under what circumstances is this rational? (I'm not commenting on whether it's artistically good or bad; again, I'm only discussing entertainment as a commercial enterprise motivated by profit.) It seems to me following a proven formula is rational if your priority is to not lose, to go for the sure thing, i.e. the chance of a big hit is not worth the risk of a complete flop.</p>\n<p><strong id=\"Hit_driven\">Hit driven</strong></p>\n<p>It's the accepted wisdom that entertainment is a hit driven industry: almost all the profits are generated by a handful of the most successful products, with the rest losing money or barely covering costs.</p>\n<p>Now my question: <em>isn't there a contradiction here?</em> If you're selling insurance, following a proven formula may well be the rational thing to do. If you're the owner of one of the handful of franchises that is pulling in big profits, of course you shouldn't mess with a winner. But if you're one of the many also-rans, how is it rational to stick with an almost sure loser? In a hit driven industry, wouldn't it be more rational to concentrate on maximizing your chance of winning big, instead of trying to minimize the risk of a flop?</p>\n<p>But I've never worked in the entertainment industry; perhaps my layman's impression of it is inaccurate. Is there something I'm missing, or is a substantial amount of expected profit really being left on the table?</p>\n<p></p>", "sections": [{"title": "Formulaic", "anchor": "Formulaic", "level": 1}, {"title": "Hit driven", "anchor": "Hit_driven", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T18:19:38.320Z", "modifiedAt": null, "url": null, "title": "A sense of logic", "slug": "a-sense-of-logic", "viewCount": null, "lastCommentedAt": "2015-05-09T04:30:06.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p9N2qfkjHoft5L85v/a-sense-of-logic", "pageUrlRelative": "/posts/p9N2qfkjHoft5L85v/a-sense-of-logic", "linkUrl": "https://www.lesswrong.com/posts/p9N2qfkjHoft5L85v/a-sense-of-logic", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20sense%20of%20logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20sense%20of%20logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9N2qfkjHoft5L85v%2Fa-sense-of-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20sense%20of%20logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9N2qfkjHoft5L85v%2Fa-sense-of-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9N2qfkjHoft5L85v%2Fa-sense-of-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>What's the worst argument you can think of?</p>\n<p>One of my favorites is from a Theodore Sturgeon science fiction story in which it's claimed that faster than light communication must be possible because even though stars are light years apart, a person can look from one to another in a moment.</p>\n<p>I don't know about you, but bad logic makes my stomach hurt, especially on first exposure.</p>\n<p>This seems rather odd-- what sort of physical connection might that be?</p>\n<p>Also, I'm not sure how common the experience is, though a philosophy professor did confirm it for himself and (by observation) his classes. He mentioned one of the Socratic dialogues (sorry, I can't remember which one) which is a compendium of bad arguments and which seemed to have that effect on his classes.</p>\n<p>So, how did you feel when you read that bit of sf hand-waving? If your stomach hurt, what sort of stomach pain was it? Like nausea? Like being hit? Something else? If you had some other sensory reaction, can you describe it?</p>\n<p>For me, the sensation is some sort of internal twinge which isn't like nausea.</p>\n<p>Anyway, both for examination and for the fun of it, please supply more bad arguments.</p>\n<p>I think there are sensory correlates for what is perceived to be good logic (unfortunately, they don't tell you whether an argument is really sound)-- kinesthesia which has to do with solidity, certainty, and at least in my case, a feeling that all the corners are pinned down.</p>\n<p><strong>Addendum:</strong> It looks as though I was generalizing from one example. If you have a fast reaction to bad arguments and it isn't kinesthetic, what is it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p9N2qfkjHoft5L85v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 23, "extendedScore": null, "score": 6.547575260639623e-07, "legacy": true, "legacyId": "4237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 270, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-10T20:50:16.563Z", "modifiedAt": null, "url": null, "title": "The term 'altruism' in group selection", "slug": "the-term-altruism-in-group-selection", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:56.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g8R5oG6zAmJp8BcQ9/the-term-altruism-in-group-selection", "pageUrlRelative": "/posts/g8R5oG6zAmJp8BcQ9/the-term-altruism-in-group-selection", "linkUrl": "https://www.lesswrong.com/posts/g8R5oG6zAmJp8BcQ9/the-term-altruism-in-group-selection", "postedAtFormatted": "Friday, December 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20term%20'altruism'%20in%20group%20selection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20term%20'altruism'%20in%20group%20selection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8R5oG6zAmJp8BcQ9%2Fthe-term-altruism-in-group-selection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20term%20'altruism'%20in%20group%20selection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8R5oG6zAmJp8BcQ9%2Fthe-term-altruism-in-group-selection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg8R5oG6zAmJp8BcQ9%2Fthe-term-altruism-in-group-selection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 405, "htmlBody": "<!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:DoNotOptimizeForBrowser /> </w:WordDocument> </xml><![endif]-->\n<p class=\"MsoNormal\">The way I see it, <a href=\"http://wiki.lesswrong.com/wiki/Altruism\">altruism</a> has been the big selling point for group selection. The only way altruism would have been able to evolve is through group selection, so the presence of altruism is strong evidence for the existence of group selection.</p>\n<p class=\"MsoNormal\">Group selectionists have been (rightly) <a href=\"/lw/kw/the_tragedy_of_group_selectionism\">criticized</a> by pointing out that they were merely looking for an explanation that would fit the results they had already decided on and wrote the conclusion before looking for hypotheses. They wanted a nice, friendly, altruistic world and devised a theory of why this should be so.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Now, while I fully agree their methods were wrong, I want to take a closer look at the word &ldquo;altruism&rdquo; in this context.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<blockquote>\n<p class=\"MsoNormal\">&nbsp; Is a cow a vegetarian?</p>\n</blockquote>\n<p class=\"MsoNormal\">Think about this question, if you will, before reading on.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">I would say no, it&rsquo;s not. True, a cow only eats plants but there is a crucial difference that separates it from a real vegetarian. When a cow is hungry its brain tells it to eat grass, it doesn&rsquo;t give the option to choose meat. A cow&rsquo;s digestive system is specialized in processing grass, eating meat would send it haywire.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">A vegetarian, on the other hand, is a human, an omnivore, he can just as easily process food from animal as plant sources. Not eating meat is a deliberate and conscious choice.</p>\n<p class=\"MsoNormal\">The point I&rsquo;m getting at is that eating plants because that&rsquo;s all you can do doesn&rsquo;t make you a <em>real</em> vegetarian. Luckily we have a convenient term to make this distinction: herbivore.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Now lets go back to altruism.</p>\n<p class=\"MsoNormal\">Bees have been called altruistic; after all, what greater sacrifice can an organism bring then its ability to reproduce? What if we, to stop overpopulation, sterilize every newborn child for the next three years.</p>\n<p class=\"MsoNormal\">Every time we meet one of those children we would give them a pat on the back and congratulate them for the enormous amounts of altruism they have displayed. I doubt many of them would agree.</p>\n<p class=\"MsoNormal\">It&rsquo;s not really altruism if you have no choice, is it? The difference between true altruism and cases like this is deliberate choice and doing more then &ldquo;default helping&rdquo;.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">In short: whenever the group selectionists saw a herbivore, they called it a vegetarian. Just like we make a distinction between herbivores and vegetarians, I would like to see someone introducing a new term for that-thing-animals-do-that-looks-like-altruism.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">p.s. This is my very first article on this site, any feedback and tips would be greatly appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g8R5oG6zAmJp8BcQ9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 6.547951827597144e-07, "legacy": true, "legacyId": "4240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QsMJQSFj7WfoTMNgW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-11T01:48:11.676Z", "modifiedAt": null, "url": null, "title": "Life-tracking application for android", "slug": "life-tracking-application-for-android", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexei", "createdAt": "2010-08-02T15:14:11.411Z", "isAdmin": false, "displayName": "Alexei"}, "userId": "CD3DC5D7GHtgBmxz5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HbzmZGTG846J4wkh6/life-tracking-application-for-android", "pageUrlRelative": "/posts/HbzmZGTG846J4wkh6/life-tracking-application-for-android", "linkUrl": "https://www.lesswrong.com/posts/HbzmZGTG846J4wkh6/life-tracking-application-for-android", "postedAtFormatted": "Saturday, December 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Life-tracking%20application%20for%20android&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALife-tracking%20application%20for%20android%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHbzmZGTG846J4wkh6%2Flife-tracking-application-for-android%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Life-tracking%20application%20for%20android%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHbzmZGTG846J4wkh6%2Flife-tracking-application-for-android", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHbzmZGTG846J4wkh6%2Flife-tracking-application-for-android", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 276, "htmlBody": "<p>Hi, lesswrong.</p>\n<p>I just finished my application for android devices, LifeTracking, which has been motivated by the discussions here; primarily discussions about akrasia and measuring/tracking your own actions. I don't want to make this sound like an advertisement (the application is completely free anyway), but I would really really like to get feedback from you and hear your comments, criticism, and suggestions. If there are enough LessWrong-specific feature requests, I will make a separate application just for that.</p>\n<p>Here is a brief description of the app:</p>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><em>LifeTracking application allows you to track any value (like your weight or your lesswrong karma), as well as any time-consuming activities (like sleeping, working, reading Harry Potter fanfic, etc). You can see the data visually, edit it, and analyze it.<br /></em></p>\n<p style=\"margin-bottom: 0in;\"><em>The goal of the application is to help you know yourself and your schedule better. Hopefully, when you graph various aspects of your life side-by-side you will come to a better understanding of yourself. Also, this way you will not have to rely on your faulty memory to remember all that data.</em></p>\n<p><em>You can download the app from the <a href=\"market://search?q=pname:com.lifetracking\">Market</a> (link only works from Android devices) or download <a href=\"https://slideme.org/mobileapp/download/0e67b2ba-5549-102e-9812-6ec07c99d928.apk\">.apk directly</a>. Screenshots: <a href=\"http://bentspoongames.com/LifeTrackingImages/Analyze.png\">[1]</a>, <a href=\"http://bentspoongames.com/LifeTrackingImages/Graph.png\">[2]</a>, <a href=\"http://bentspoongames.com/LifeTrackingImages/Intervals.png\">[3]</a>, <a href=\"http://bentspoongames.com/LifeTrackingImages/IntervalsList.png\">[4]</a>, <a href=\"http://bentspoongames.com/LifeTrackingImages/RelativeGraphs.png\">[5]</a>, <a href=\"http://bentspoongames.com/LifeTrackingImages/Tracks.png\">[6]</a>.</em></p>\n<p>&nbsp;</p>\n<p>Edit: <a href=\"http://lifetracking.wordpress.com/\">LifeTracking website</a></p>\n<p>And while we are on topic of mobile apps, what other applications would you like to see made? (For example, another useful application would be \"your personal prediction tracker\", where you enter various short-term predictions, your confidence interval, and then enter the actual result. You can classify each prediction and then see if you are over- or under-confident in certain areas. (I remember seeing a website that does something similar, but can't find it now.))</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HbzmZGTG846J4wkh6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 6.548691767622589e-07, "legacy": true, "legacyId": "4241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-11T13:58:18.087Z", "modifiedAt": null, "url": null, "title": "If reductionism is the hammer, what nails are out there? ", "slug": "if-reductionism-is-the-hammer-what-nails-are-out-there", "viewCount": null, "lastCommentedAt": "2020-12-13T19:53:34.706Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5NJn5xgEn2RsW9S9z/if-reductionism-is-the-hammer-what-nails-are-out-there", "pageUrlRelative": "/posts/5NJn5xgEn2RsW9S9z/if-reductionism-is-the-hammer-what-nails-are-out-there", "linkUrl": "https://www.lesswrong.com/posts/5NJn5xgEn2RsW9S9z/if-reductionism-is-the-hammer-what-nails-are-out-there", "postedAtFormatted": "Saturday, December 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20reductionism%20is%20the%20hammer%2C%20what%20nails%20are%20out%20there%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20reductionism%20is%20the%20hammer%2C%20what%20nails%20are%20out%20there%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NJn5xgEn2RsW9S9z%2Fif-reductionism-is-the-hammer-what-nails-are-out-there%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20reductionism%20is%20the%20hammer%2C%20what%20nails%20are%20out%20there%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NJn5xgEn2RsW9S9z%2Fif-reductionism-is-the-hammer-what-nails-are-out-there", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NJn5xgEn2RsW9S9z%2Fif-reductionism-is-the-hammer-what-nails-are-out-there", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1091, "htmlBody": "<p><strong>EDIT:</strong>&nbsp;I'm moving this to the Discussion section because people seem to not like it (lack of upvotes) and to find the writing unclear. &nbsp;I'd love writing advice, if anyone wants to offer some.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Related to: <a href=\"/lw/of/dissolving_the_question/\">Dissolving the question</a>, <a href=\"/lw/oo/explaining_vs_explaining_away/\">Explaining vs explaining away</a></p>\n<p><em>I review parts of the <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">reductionism sequence</a>, in hopes of setting up for future reduction work.</em></p>\n<p>So, you&rsquo;ve been building up your reductionism muscles, on LW or elsewhere. &nbsp;You&rsquo;re no longer confused about a magical essence of <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">free will</a>; you understand how particular arrangements of atoms can make choices. &nbsp;You&rsquo;re no longer confused about a magical essence of <a href=\"/lw/qx/timeless_identity/\">personal identity</a>; you understand where the feeling of &ldquo;you&rdquo; comes from, and how one could in principle create many copies of you that continued \"your\" experiences, and how the absence of an irreducible essence doesn&rsquo;t reduce life&rsquo;s meaning.</p>\n<p>The natural next question is: what other phenomena can you reduce? &nbsp;What topics are we <em>currently</em> confused about which may yield to the same tools? &nbsp;And what tools, exactly, do we have for such reductions?</p>\n<p>With the goal of paving the way for new reductions, then, let&rsquo;s make a list of questions that persistently <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\">felt</a> like questions about magical essences, including both questions that have been solved, and questions about which we are currently confused. &nbsp;And let&rsquo;s also list tools or strategies that assisted in their dissolution. &nbsp;I made an attempt at such lists below; perhaps you can help me refine them?</p>\n<h2><a id=\"more\"></a>Some places where many expected a fundamental or non-reducible &ldquo;essence&rdquo;[1]:</h2>\n<p><strong>1. &nbsp;Ducks.</strong></p>\n<p>Why it's tempting to postulate an essence: Organisms seem to come in types. &nbsp;New organisms of the given type (e.g., new ducks) come into existence, almost as though the the type &ldquo;Duck&rdquo; had causal power. &nbsp;Humans are able to form mental concepts of \"duck\" that approximately mirror the outside predictive regularities.</p>\n<p><strong>2. &nbsp;Life.</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;Living creatures act very differently from dead creatures. &nbsp;A recently killed animal doesn&rsquo;t move, loses its body heat, etc., even though its matter is in almost the same configuration. [2]</p>\n<p><strong>3. &nbsp;Free will.</strong></p>\n<p>Why it's tempting to postulate an essence: &nbsp;Humans (among other things) are in fact organized to &ldquo;choose&rdquo; their actions in some meaningful sense. &nbsp;We (mostly) choose a single course of action in a relatively unified manner that responds to outside information and incentives. &nbsp;&ldquo;Choice&rdquo; also seems like a useful internal concept, but I&rsquo;m not sure how to describe the details here.</p>\n<p><strong>4. &nbsp;Personal identity</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;People have personalities, plans, beliefs, bodies, etc. that approximately persist over time. &nbsp;Internally, we experience consistent memories that happened &ldquo;to us&rdquo;, we choose our own actions, and we anticipate future experiences.</p>\n<p><strong>5. &nbsp;Pain</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;We feel pain. &nbsp;We find ourselves motivated to avoid pain. &nbsp;We sometimes almost feel others&rsquo; pain, as when we wince and rub our thumbs after watching someone else smash their thumb with a hammer, and we often find ourselves motivated to avoid their pain as well. &nbsp;We can report verbally on the pain, modify our behavior to reduce the pain, etc.</p>\n<p><strong>6. &nbsp;Mathematics</strong></p>\n<p>Why it's tempting to postulate an essence: &nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences\">Mathematics often pops up in science</a>. &nbsp;It&rsquo;s also &ldquo;simple&rdquo;, is at least somewhat culturally universal, is relatively easy to implement portions of in machines,&nbsp;has a nice notion of &ldquo;proof&rdquo; whereby we can often formally determine what is true, and can often determine true results without much contact with outside empirical data, and is something aliens might plausibly share with us.</p>\n<p><strong>7. &nbsp;Reality / existence / the physical world</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;&nbsp;Our perceptions are well predicted by imagining a set of fairly stable objects that we can see, touch, turn over in our hands, etc. and that retain their color, shape, heft, and other properties fairly stably over time. &nbsp;At higher levels of abstraction, too, the world is fairly lawful and coherent. [3]</p>\n<p><span style=\"font-size: 16px; font-weight: bold;\">What lessons for future reductions?</span></p>\n<p>These examples suggest the following heuristics: &nbsp;</p>\n<p>A. &nbsp;Even when it really, really feels like there should be an essence, there probably isn&rsquo;t one. &nbsp;</p>\n<p>B. &nbsp;Philosophical questions are just ordinary questions that one is particularly ignorant about; they are not questions about separate magisteria that must permanently be reasoned about in some special way. &nbsp;</p>\n<p>C. &nbsp;People expect magical essences in places where there really are interesting empirical regularities. &nbsp;In order to understand those regularities, and to create a new set of concepts that better do the <a href=\"http://wiki.lesswrong.com/wiki/Making_beliefs_pay_rent\">work</a> that our old magical essences intuitions used to do, it is necessary to do real research. &nbsp;Ritual assertions that &ldquo;It&rsquo;s all physics&rdquo; and &ldquo;there aren&rsquo;t essences&rdquo; do not create the needed concepts and anticipations.</p>\n<p>D. &nbsp;A reasonable first step, in tackling a new confusion, is to <a href=\"/lw/oh/righting_a_wrong_question/\">ask the why it feels like there is a question or concept there</a>, and to list the empirical regularities, or cognitive artifacts, that contribute to that feeling.</p>\n<p>These heuristics aren't original; Eliezer noted them already in his <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">reductionism sequence</a> (which is very much worth reading). &nbsp;But&nbsp;I suspect that many apply these heuristics more to problems they already understand (&ldquo;of course free will has no magical essence&rdquo;) more than to problems we don&rsquo;t yet understand (&ldquo;of course there is no magical essence that distinguishes our actual, real world from imaginable physicses/worlds that aren't real\").</p>\n<p>I'm hoping that reviewing heuristics for reduction, and staring at solved and unsolved problems side by side, may help us with the unsolved problems (which I'll attempt some steps toward in subsequent posts).</p>\n<p>&nbsp;</p>\n<hr />\n<p>[1] &nbsp;I agree with <a href=\"/lw/38z/were_atoms_real/33nu?c=1\">SarahC&rsquo;s point</a> that humans seem predisposed to impute essences everywhere. &nbsp;Still, discussions about whether there&rsquo;s a magical essence &ldquo;free will&rdquo; seem to pop up more often than discussions about whether there&rsquo;s a magical essence &ldquo;carpet&rdquo;, &ldquo;ocean&rdquo;, &ldquo;California&rdquo;, or &ldquo;female&rdquo;. &nbsp;I mean, folks are interested in these other questions, and they have discussions about what meaning to use and how much that meaning cleaves nature at its joints, but they don&rsquo;t generally expect a separate sort of essence that has causal powers and isn&rsquo;t made out of atoms.</p>\n<p class=\"p1\">[2] People unacquainted with modern biology seem often to make predictive errors due to expecting an essence of life. &nbsp;For example, I had lunch the other day with a physics professor from a good university who thought that, even if we could assemble an atom-for-atom duplicate of a person's exact physical state, it might well not act like a person for want of a soul. &nbsp;Another acquaintance was surprised to hear that scientists do in fact believe that a cell assembled in a test tube would act just like a cell assembled anywhere else.</p>\n<p class=\"p1\">[3] I'm less satisfied with this unpacking than with the others on the list. &nbsp;Can anyone do better?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5NJn5xgEn2RsW9S9z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 6.55050575924518e-07, "legacy": true, "legacyId": "4242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>EDIT:</strong>&nbsp;I'm moving this to the Discussion section because people seem to not like it (lack of upvotes) and to find the writing unclear. &nbsp;I'd love writing advice, if anyone wants to offer some.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>Related to: <a href=\"/lw/of/dissolving_the_question/\">Dissolving the question</a>, <a href=\"/lw/oo/explaining_vs_explaining_away/\">Explaining vs explaining away</a></p>\n<p><em>I review parts of the <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">reductionism sequence</a>, in hopes of setting up for future reduction work.</em></p>\n<p>So, you\u2019ve been building up your reductionism muscles, on LW or elsewhere. &nbsp;You\u2019re no longer confused about a magical essence of <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">free will</a>; you understand how particular arrangements of atoms can make choices. &nbsp;You\u2019re no longer confused about a magical essence of <a href=\"/lw/qx/timeless_identity/\">personal identity</a>; you understand where the feeling of \u201cyou\u201d comes from, and how one could in principle create many copies of you that continued \"your\" experiences, and how the absence of an irreducible essence doesn\u2019t reduce life\u2019s meaning.</p>\n<p>The natural next question is: what other phenomena can you reduce? &nbsp;What topics are we <em>currently</em> confused about which may yield to the same tools? &nbsp;And what tools, exactly, do we have for such reductions?</p>\n<p>With the goal of paving the way for new reductions, then, let\u2019s make a list of questions that persistently <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\">felt</a> like questions about magical essences, including both questions that have been solved, and questions about which we are currently confused. &nbsp;And let\u2019s also list tools or strategies that assisted in their dissolution. &nbsp;I made an attempt at such lists below; perhaps you can help me refine them?</p>\n<h2 id=\"Some_places_where_many_expected_a_fundamental_or_non_reducible__essence__1__\"><a id=\"more\"></a>Some places where many expected a fundamental or non-reducible \u201cessence\u201d[1]:</h2>\n<p><strong id=\"1___Ducks_\">1. &nbsp;Ducks.</strong></p>\n<p>Why it's tempting to postulate an essence: Organisms seem to come in types. &nbsp;New organisms of the given type (e.g., new ducks) come into existence, almost as though the the type \u201cDuck\u201d had causal power. &nbsp;Humans are able to form mental concepts of \"duck\" that approximately mirror the outside predictive regularities.</p>\n<p><strong id=\"2___Life_\">2. &nbsp;Life.</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;Living creatures act very differently from dead creatures. &nbsp;A recently killed animal doesn\u2019t move, loses its body heat, etc., even though its matter is in almost the same configuration. [2]</p>\n<p><strong id=\"3___Free_will_\">3. &nbsp;Free will.</strong></p>\n<p>Why it's tempting to postulate an essence: &nbsp;Humans (among other things) are in fact organized to \u201cchoose\u201d their actions in some meaningful sense. &nbsp;We (mostly) choose a single course of action in a relatively unified manner that responds to outside information and incentives. &nbsp;\u201cChoice\u201d also seems like a useful internal concept, but I\u2019m not sure how to describe the details here.</p>\n<p><strong id=\"4___Personal_identity\">4. &nbsp;Personal identity</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;People have personalities, plans, beliefs, bodies, etc. that approximately persist over time. &nbsp;Internally, we experience consistent memories that happened \u201cto us\u201d, we choose our own actions, and we anticipate future experiences.</p>\n<p><strong id=\"5___Pain\">5. &nbsp;Pain</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;We feel pain. &nbsp;We find ourselves motivated to avoid pain. &nbsp;We sometimes almost feel others\u2019 pain, as when we wince and rub our thumbs after watching someone else smash their thumb with a hammer, and we often find ourselves motivated to avoid their pain as well. &nbsp;We can report verbally on the pain, modify our behavior to reduce the pain, etc.</p>\n<p><strong id=\"6___Mathematics\">6. &nbsp;Mathematics</strong></p>\n<p>Why it's tempting to postulate an essence: &nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences\">Mathematics often pops up in science</a>. &nbsp;It\u2019s also \u201csimple\u201d, is at least somewhat culturally universal, is relatively easy to implement portions of in machines,&nbsp;has a nice notion of \u201cproof\u201d whereby we can often formally determine what is true, and can often determine true results without much contact with outside empirical data, and is something aliens might plausibly share with us.</p>\n<p><strong id=\"7___Reality___existence___the_physical_world\">7. &nbsp;Reality / existence / the physical world</strong></p>\n<p>Why it's tempting to postulate an essence:&nbsp;&nbsp;Our perceptions are well predicted by imagining a set of fairly stable objects that we can see, touch, turn over in our hands, etc. and that retain their color, shape, heft, and other properties fairly stably over time. &nbsp;At higher levels of abstraction, too, the world is fairly lawful and coherent. [3]</p>\n<p><span style=\"font-size: 16px; font-weight: bold;\">What lessons for future reductions?</span></p>\n<p>These examples suggest the following heuristics: &nbsp;</p>\n<p>A. &nbsp;Even when it really, really feels like there should be an essence, there probably isn\u2019t one. &nbsp;</p>\n<p>B. &nbsp;Philosophical questions are just ordinary questions that one is particularly ignorant about; they are not questions about separate magisteria that must permanently be reasoned about in some special way. &nbsp;</p>\n<p>C. &nbsp;People expect magical essences in places where there really are interesting empirical regularities. &nbsp;In order to understand those regularities, and to create a new set of concepts that better do the <a href=\"http://wiki.lesswrong.com/wiki/Making_beliefs_pay_rent\">work</a> that our old magical essences intuitions used to do, it is necessary to do real research. &nbsp;Ritual assertions that \u201cIt\u2019s all physics\u201d and \u201cthere aren\u2019t essences\u201d do not create the needed concepts and anticipations.</p>\n<p>D. &nbsp;A reasonable first step, in tackling a new confusion, is to <a href=\"/lw/oh/righting_a_wrong_question/\">ask the why it feels like there is a question or concept there</a>, and to list the empirical regularities, or cognitive artifacts, that contribute to that feeling.</p>\n<p>These heuristics aren't original; Eliezer noted them already in his <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">reductionism sequence</a> (which is very much worth reading). &nbsp;But&nbsp;I suspect that many apply these heuristics more to problems they already understand (\u201cof course free will has no magical essence\u201d) more than to problems we don\u2019t yet understand (\u201cof course there is no magical essence that distinguishes our actual, real world from imaginable physicses/worlds that aren't real\").</p>\n<p>I'm hoping that reviewing heuristics for reduction, and staring at solved and unsolved problems side by side, may help us with the unsolved problems (which I'll attempt some steps toward in subsequent posts).</p>\n<p>&nbsp;</p>\n<hr>\n<p>[1] &nbsp;I agree with <a href=\"/lw/38z/were_atoms_real/33nu?c=1\">SarahC\u2019s point</a> that humans seem predisposed to impute essences everywhere. &nbsp;Still, discussions about whether there\u2019s a magical essence \u201cfree will\u201d seem to pop up more often than discussions about whether there\u2019s a magical essence \u201ccarpet\u201d, \u201cocean\u201d, \u201cCalifornia\u201d, or \u201cfemale\u201d. &nbsp;I mean, folks are interested in these other questions, and they have discussions about what meaning to use and how much that meaning cleaves nature at its joints, but they don\u2019t generally expect a separate sort of essence that has causal powers and isn\u2019t made out of atoms.</p>\n<p class=\"p1\">[2] People unacquainted with modern biology seem often to make predictive errors due to expecting an essence of life. &nbsp;For example, I had lunch the other day with a physics professor from a good university who thought that, even if we could assemble an atom-for-atom duplicate of a person's exact physical state, it might well not act like a person for want of a soul. &nbsp;Another acquaintance was surprised to hear that scientists do in fact believe that a cell assembled in a test tube would act just like a cell assembled anywhere else.</p>\n<p class=\"p1\">[3] I'm less satisfied with this unpacking than with the others on the list. &nbsp;Can anyone do better?</p>", "sections": [{"title": "Some places where many expected a fundamental or non-reducible \u201cessence\u201d[1]:", "anchor": "Some_places_where_many_expected_a_fundamental_or_non_reducible__essence__1__", "level": 1}, {"title": "1. \u00a0Ducks.", "anchor": "1___Ducks_", "level": 2}, {"title": "2. \u00a0Life.", "anchor": "2___Life_", "level": 2}, {"title": "3. \u00a0Free will.", "anchor": "3___Free_will_", "level": 2}, {"title": "4. \u00a0Personal identity", "anchor": "4___Personal_identity", "level": 2}, {"title": "5. \u00a0Pain", "anchor": "5___Pain", "level": 2}, {"title": "6. \u00a0Mathematics", "anchor": "6___Mathematics", "level": 2}, {"title": "7. \u00a0Reality / existence / the physical world", "anchor": "7___Reality___existence___the_physical_world", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "48 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX", "cphoF8naigLhRf3tu", "924arDrTu3QRHFA5r", "rQEwySCcLtdKHkrHp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-11T17:53:31.389Z", "modifiedAt": null, "url": null, "title": "Calling LW Londoners", "slug": "calling-lw-londoners", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.595Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/84gvE26m2hFiAJP8r/calling-lw-londoners", "pageUrlRelative": "/posts/84gvE26m2hFiAJP8r/calling-lw-londoners", "linkUrl": "https://www.lesswrong.com/posts/84gvE26m2hFiAJP8r/calling-lw-londoners", "postedAtFormatted": "Saturday, December 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calling%20LW%20Londoners&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalling%20LW%20Londoners%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84gvE26m2hFiAJP8r%2Fcalling-lw-londoners%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calling%20LW%20Londoners%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84gvE26m2hFiAJP8r%2Fcalling-lw-londoners", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84gvE26m2hFiAJP8r%2Fcalling-lw-londoners", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>It seems we haven't done any London meetups in a while. Is anyone up for arranging something within January?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "84gvE26m2hFiAJP8r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 6.551090370060254e-07, "legacy": true, "legacyId": "4245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-11T21:04:07.869Z", "modifiedAt": null, "url": null, "title": "Why Eliezer Yudkowsky receives more upvotes than others", "slug": "why-eliezer-yudkowsky-receives-more-upvotes-than-others", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:54.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LCRSftJ5NopoMWhqi/why-eliezer-yudkowsky-receives-more-upvotes-than-others", "pageUrlRelative": "/posts/LCRSftJ5NopoMWhqi/why-eliezer-yudkowsky-receives-more-upvotes-than-others", "linkUrl": "https://www.lesswrong.com/posts/LCRSftJ5NopoMWhqi/why-eliezer-yudkowsky-receives-more-upvotes-than-others", "postedAtFormatted": "Saturday, December 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Eliezer%20Yudkowsky%20receives%20more%20upvotes%20than%20others&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Eliezer%20Yudkowsky%20receives%20more%20upvotes%20than%20others%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCRSftJ5NopoMWhqi%2Fwhy-eliezer-yudkowsky-receives-more-upvotes-than-others%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Eliezer%20Yudkowsky%20receives%20more%20upvotes%20than%20others%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCRSftJ5NopoMWhqi%2Fwhy-eliezer-yudkowsky-receives-more-upvotes-than-others", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCRSftJ5NopoMWhqi%2Fwhy-eliezer-yudkowsky-receives-more-upvotes-than-others", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>One of the reasons for why Yudkowsky is being drastically upvoted is of course that he's often, fasten your seatbelts, brilliantly right (whoever reads my comments knows that I am not really the most frenetic believer, so I think I can say this without sounding cultish). But others are too, so is Less Wrong a cult? Nah! There is a simple explanation for this phenomenon:<br /><br /><img src=\"http://xixidu.net/lw/eylwfeed.jpg\" alt=\"\" /></p>\n<p>As you can see, there are already 13 people who subscribe to his Less Wrong feed via Google Reader. And there are many other ways to subscribe to a RSS feed (which is not the only way to follow his mental outpourings anyway), so the number of people who follow every post and comment is likely much higher.</p>\n<p>That's why most of his comments receive more upvotes than other comments. It is not because he is a cult leader, it's just that his comments are read by many more people than the average comment on Less Wrong. There are of course other causes as well, but this seems to explain a fair chunk of the effect.</p>\n<p>Also consider that I'm often upvoted (with a current Karma score of 1959) and I do not keep quiet regarding my doubts about some topics directly related to Yudkowsky and the SIAI. How could this happen if Less Wrong was an echo chamber?</p>\n<p>I just wanted to let you know, because I have been wondering about it in the past.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LCRSftJ5NopoMWhqi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 8, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "4246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-11T22:45:15.282Z", "modifiedAt": null, "url": null, "title": "Should LW have a public censorship policy?", "slug": "should-lw-have-a-public-censorship-policy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:34.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bongo", "createdAt": "2009-02-27T12:08:06.258Z", "isAdmin": false, "displayName": "Bongo"}, "userId": "mLnNK3xEMczLs8ind", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nBNCjuDJxsWPY6Fj4/should-lw-have-a-public-censorship-policy", "pageUrlRelative": "/posts/nBNCjuDJxsWPY6Fj4/should-lw-have-a-public-censorship-policy", "linkUrl": "https://www.lesswrong.com/posts/nBNCjuDJxsWPY6Fj4/should-lw-have-a-public-censorship-policy", "postedAtFormatted": "Saturday, December 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20LW%20have%20a%20public%20censorship%20policy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20LW%20have%20a%20public%20censorship%20policy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBNCjuDJxsWPY6Fj4%2Fshould-lw-have-a-public-censorship-policy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20LW%20have%20a%20public%20censorship%20policy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBNCjuDJxsWPY6Fj4%2Fshould-lw-have-a-public-censorship-policy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnBNCjuDJxsWPY6Fj4%2Fshould-lw-have-a-public-censorship-policy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>It might mollify people who disagree with the current implicit policy, and make discussion about the policy easier. Here's one option:</p>\n<blockquote>\n<p>There's a single specific topic that's banned because the moderators consider it a <a href=\"http://en.wikipedia.org/wiki/Basilisk#Reuse_in_science_fiction_and_popular_culture\">Basilisk</a>. You won't come up with it yourself, don't worry. Posts talking about the topic in too much detail will be deleted.&nbsp;</p>\n</blockquote>\n<p>One requirement would be that the policy be no more and no less vague than needed for safety.</p>\n<p>Discuss.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"CYMR6p5iZG75QAT8a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nBNCjuDJxsWPY6Fj4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 25, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "4247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-12T01:40:07.470Z", "modifiedAt": null, "url": null, "title": "The Long Now", "slug": "the-long-now", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.077Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mdHpBmJrRzDjKHs5a/the-long-now", "pageUrlRelative": "/posts/mdHpBmJrRzDjKHs5a/the-long-now", "linkUrl": "https://www.lesswrong.com/posts/mdHpBmJrRzDjKHs5a/the-long-now", "postedAtFormatted": "Sunday, December 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Long%20Now&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Long%20Now%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmdHpBmJrRzDjKHs5a%2Fthe-long-now%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Long%20Now%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmdHpBmJrRzDjKHs5a%2Fthe-long-now", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmdHpBmJrRzDjKHs5a%2Fthe-long-now", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 641, "htmlBody": "<p>It's surprised me that there's been very little discussion of <a href=\"http://www.longnow.org/\">The Long Now</a> here on Less Wrong, as there are many similarities between the groups, although the approach and philosophy between them are quite different. At a minimum, I believe that a general awareness might be beneficial. I'll use the initials LW and LN below. My perspective on LN is simply that of someone who's kept an eye on their website from time to time and read a few of their articles, so I'd also like to admit that my knowledge is a bit shallow (a reason, in fact, I bring the topic up for discussion).</p>\n<h3>Similarities</h3>\n<p>Most critically, long-term thinking appears as a cornerstone of both the LW and LN thought, explicitly as the goal for LN, and implicitly here on LW whenever we talk about <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">existential risk</a> or decades-away or longer technology. It's not clear if there's an overlap between the commenters at LW and the membership of LN or not, but there's definitely a large number of people \"between\" the two groups -- statements by <a href=\"http://blog.longnow.org/2010/12/06/long-quotes-peter-thiel/\">Peter Thiel</a> and <a href=\"http://blog.longnow.org/2010/11/30/is-kurzweils-future-arriving/\">Ray Kurzweil</a> have been recent topics on the LN blog and Hillis, who founded LN, has been involved in <a href=\"http://en.wikipedia.org/wiki/Danny_Hillis#Philosophy_of_mind\">AI and philosophy of mind</a>. LN has <a href=\"http://www.longbets.org/\">Long Bets</a>, which I would loosely describe as to <a href=\"http://predictionbook.com/\">PredictionBook</a> as <a href=\"http://www.intrade.com/\">InTrade</a> is to <a href=\"http://www.ideosphere.com/\">Foresight Exchange</a>. LN apparently had a presence at some of the past SIAI's Singularity Summits.</p>\n<h3>Differences</h3>\n<p><strong>Signaling:</strong> LN embraces signaling like there's no tomorrow (ha!) -- their flagship project, after all, is a monumental clock to last thousands of years, the goal of which is to \"<a href=\"http://longnow.org/clock/\">lend itself to good storytelling and myth</a>\" about long-term thought. Their membership cards are <a href=\"https://longnow.org/membership/\">stainless steel</a>. Some of the projects LN are pursuing seem to have been chosen mostly because they <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/RuleOfCool\">sound awesome</a>, and even those that aren't are done with some flair, IMHO. In contrast, the view among LW posts seems to be that signaling is in many cases a necessary evil, in some cases just an evolutionary leftover, and reducing signaling <a href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=Signaling&amp;oldid=6311\">a potential source for efficiency gains</a>. There may be something to be learned here -- we already know FAI would be an easier sell if we described it as project to create robots that are Presidents of the United States by day, crime-fighters by night, and <a href=\"/lw/xt/interpersonal_entanglement/\">cat-people by late-night</a>.</p>\n<p><strong>Structure:</strong> While LW is a project of SIAI, they're not the same,  so by extension the comparison between LN and LW is just a bit  apples-to-kumquats. It'd be a lot easier to compare LW to a LN  discussion board, if it existed.</p>\n<p><strong>The Future</strong>: Here on LW, we want our nuclear-powered flying cars, dammit! Bad future scenarios that are discussed on LW tend to be irrevocably and undeniably bad -- the world is turned into tang or paperclips and no life exists anymore, for example. LN seems more concerned with recovery from, rather than prevention of, \"collapse of civilization\" scenarios. Many of the projects both undertaken and linked to by LN focus on preserving knowledge in a such a scenario. Between the overlap in the LW community and cryonics, SENS, etc, the mental relationship between the median LW poster and the future seems more personal and less abstract.</p>\n<p><strong>Politics: </strong> The predominant thinking on LW seems to be a (very slightly left-leaning) technolibertarianism, although since it's open to anyone who wanders in from the Internet, there's a lot of variation (if either SIAI or FHI have an especially strong political stance <em>per se</em>, I've not noticed it). There's also a general skepticism here regarding the <a href=\"lw/gw/politics_is_the_mindkiller/\">soundness of most political thought</a> and of many political processes.&nbsp; LN seems further left on average and more comfortable with politics in general (although calling it a political organization would be a bit of a stretch). Keeping with this, LW seems to have more emphasis on individual decision making and improvement than LN.</p>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mdHpBmJrRzDjKHs5a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 17, "extendedScore": null, "score": 6.552250316140403e-07, "legacy": true, "legacyId": "4248", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>It's surprised me that there's been very little discussion of <a href=\"http://www.longnow.org/\">The Long Now</a> here on Less Wrong, as there are many similarities between the groups, although the approach and philosophy between them are quite different. At a minimum, I believe that a general awareness might be beneficial. I'll use the initials LW and LN below. My perspective on LN is simply that of someone who's kept an eye on their website from time to time and read a few of their articles, so I'd also like to admit that my knowledge is a bit shallow (a reason, in fact, I bring the topic up for discussion).</p>\n<h3 id=\"Similarities\">Similarities</h3>\n<p>Most critically, long-term thinking appears as a cornerstone of both the LW and LN thought, explicitly as the goal for LN, and implicitly here on LW whenever we talk about <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">existential risk</a> or decades-away or longer technology. It's not clear if there's an overlap between the commenters at LW and the membership of LN or not, but there's definitely a large number of people \"between\" the two groups -- statements by <a href=\"http://blog.longnow.org/2010/12/06/long-quotes-peter-thiel/\">Peter Thiel</a> and <a href=\"http://blog.longnow.org/2010/11/30/is-kurzweils-future-arriving/\">Ray Kurzweil</a> have been recent topics on the LN blog and Hillis, who founded LN, has been involved in <a href=\"http://en.wikipedia.org/wiki/Danny_Hillis#Philosophy_of_mind\">AI and philosophy of mind</a>. LN has <a href=\"http://www.longbets.org/\">Long Bets</a>, which I would loosely describe as to <a href=\"http://predictionbook.com/\">PredictionBook</a> as <a href=\"http://www.intrade.com/\">InTrade</a> is to <a href=\"http://www.ideosphere.com/\">Foresight Exchange</a>. LN apparently had a presence at some of the past SIAI's Singularity Summits.</p>\n<h3 id=\"Differences\">Differences</h3>\n<p><strong>Signaling:</strong> LN embraces signaling like there's no tomorrow (ha!) -- their flagship project, after all, is a monumental clock to last thousands of years, the goal of which is to \"<a href=\"http://longnow.org/clock/\">lend itself to good storytelling and myth</a>\" about long-term thought. Their membership cards are <a href=\"https://longnow.org/membership/\">stainless steel</a>. Some of the projects LN are pursuing seem to have been chosen mostly because they <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/RuleOfCool\">sound awesome</a>, and even those that aren't are done with some flair, IMHO. In contrast, the view among LW posts seems to be that signaling is in many cases a necessary evil, in some cases just an evolutionary leftover, and reducing signaling <a href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=Signaling&amp;oldid=6311\">a potential source for efficiency gains</a>. There may be something to be learned here -- we already know FAI would be an easier sell if we described it as project to create robots that are Presidents of the United States by day, crime-fighters by night, and <a href=\"/lw/xt/interpersonal_entanglement/\">cat-people by late-night</a>.</p>\n<p><strong>Structure:</strong> While LW is a project of SIAI, they're not the same,  so by extension the comparison between LN and LW is just a bit  apples-to-kumquats. It'd be a lot easier to compare LW to a LN  discussion board, if it existed.</p>\n<p><strong>The Future</strong>: Here on LW, we want our nuclear-powered flying cars, dammit! Bad future scenarios that are discussed on LW tend to be irrevocably and undeniably bad -- the world is turned into tang or paperclips and no life exists anymore, for example. LN seems more concerned with recovery from, rather than prevention of, \"collapse of civilization\" scenarios. Many of the projects both undertaken and linked to by LN focus on preserving knowledge in a such a scenario. Between the overlap in the LW community and cryonics, SENS, etc, the mental relationship between the median LW poster and the future seems more personal and less abstract.</p>\n<p><strong>Politics: </strong> The predominant thinking on LW seems to be a (very slightly left-leaning) technolibertarianism, although since it's open to anyone who wanders in from the Internet, there's a lot of variation (if either SIAI or FHI have an especially strong political stance <em>per se</em>, I've not noticed it). There's also a general skepticism here regarding the <a href=\"lw/gw/politics_is_the_mindkiller/\">soundness of most political thought</a> and of many political processes.&nbsp; LN seems further left on average and more comfortable with politics in general (although calling it a political organization would be a bit of a stretch). Keeping with this, LW seems to have more emphasis on individual decision making and improvement than LN.</p>\n<p>Thoughts?</p>", "sections": [{"title": "Similarities", "anchor": "Similarities", "level": 1}, {"title": "Differences", "anchor": "Differences", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Py3uGnncqXuEfPtQp", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-12T02:43:46.488Z", "modifiedAt": null, "url": null, "title": "Any LessWrongers in Calgary?", "slug": "any-lesswrongers-in-calgary", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:02.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Aurini", "createdAt": "2009-03-19T04:39:40.233Z", "isAdmin": false, "displayName": "Aurini"}, "userId": "5fNCGeJcDQCjxEjnD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7zcK6AeRkuhgvLnhk/any-lesswrongers-in-calgary", "pageUrlRelative": "/posts/7zcK6AeRkuhgvLnhk/any-lesswrongers-in-calgary", "linkUrl": "https://www.lesswrong.com/posts/7zcK6AeRkuhgvLnhk/any-lesswrongers-in-calgary", "postedAtFormatted": "Sunday, December 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Any%20LessWrongers%20in%20Calgary%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAny%20LessWrongers%20in%20Calgary%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7zcK6AeRkuhgvLnhk%2Fany-lesswrongers-in-calgary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Any%20LessWrongers%20in%20Calgary%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7zcK6AeRkuhgvLnhk%2Fany-lesswrongers-in-calgary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7zcK6AeRkuhgvLnhk%2Fany-lesswrongers-in-calgary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>I'm wondering if there'd be any sense in organizing a meet-up.&nbsp; If you're local leave a comment.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7zcK6AeRkuhgvLnhk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 6.552408575533342e-07, "legacy": true, "legacyId": "4249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-12T13:02:36.338Z", "modifiedAt": null, "url": null, "title": "Books on evolution of conscience", "slug": "books-on-evolution-of-conscience", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:00.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "karthick", "createdAt": "2010-12-12T12:58:39.166Z", "isAdmin": false, "displayName": "karthick"}, "userId": "bXy598hekAEbb9h8t", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nj7DiQQfXj7FhknQF/books-on-evolution-of-conscience", "pageUrlRelative": "/posts/Nj7DiQQfXj7FhknQF/books-on-evolution-of-conscience", "linkUrl": "https://www.lesswrong.com/posts/Nj7DiQQfXj7FhknQF/books-on-evolution-of-conscience", "postedAtFormatted": "Sunday, December 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Books%20on%20evolution%20of%20conscience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABooks%20on%20evolution%20of%20conscience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNj7DiQQfXj7FhknQF%2Fbooks-on-evolution-of-conscience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Books%20on%20evolution%20of%20conscience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNj7DiQQfXj7FhknQF%2Fbooks-on-evolution-of-conscience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNj7DiQQfXj7FhknQF%2Fbooks-on-evolution-of-conscience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p>I am looking for books on evolution of conscience. Please suggest. I searched but nothing good came out.</p>\n<p>I don't know if this is the right place for such requests.</p>\n<p>If this is not the right place, pls tell me, i will delete this post.</p>\n<p>Thanks.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nj7DiQQfXj7FhknQF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 6.553947582479554e-07, "legacy": true, "legacyId": "4251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-12T15:02:06.007Z", "modifiedAt": null, "url": null, "title": "$100 for the best article on efficient charty - the winner is ... ", "slug": "usd100-for-the-best-article-on-efficient-charty-the-winner", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:57.679Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rrg2sq75a5gBowgQb/usd100-for-the-best-article-on-efficient-charty-the-winner", "pageUrlRelative": "/posts/Rrg2sq75a5gBowgQb/usd100-for-the-best-article-on-efficient-charty-the-winner", "linkUrl": "https://www.lesswrong.com/posts/Rrg2sq75a5gBowgQb/usd100-for-the-best-article-on-efficient-charty-the-winner", "postedAtFormatted": "Sunday, December 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%24100%20for%20the%20best%20article%20on%20efficient%20charty%20-%20the%20winner%20is%20...%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%24100%20for%20the%20best%20article%20on%20efficient%20charty%20-%20the%20winner%20is%20...%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrg2sq75a5gBowgQb%2Fusd100-for-the-best-article-on-efficient-charty-the-winner%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%24100%20for%20the%20best%20article%20on%20efficient%20charty%20-%20the%20winner%20is%20...%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrg2sq75a5gBowgQb%2Fusd100-for-the-best-article-on-efficient-charty-the-winner", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrg2sq75a5gBowgQb%2Fusd100-for-the-best-article-on-efficient-charty-the-winner", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Part of the <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/\">Efficient Charity Article</a>&nbsp;competition. Several people have written articles on efficient charity. The entries were:</p>\r\n<ul>\r\n<li><a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/30hx?c=1\">Throwawayaccount_1 </a></li>\r\n<li><a href=\"/r/discussion/lw/37l/how_greedy_bastards_have_saved_more_lives_than/\">Waitingforgodel </a></li>\r\n<li><a href=\"/lw/37f/efficient_charity/\">Multifoliaterose</a></li>\r\n<li><a href=\"/lw/373/how_to_save_the_world/\">Louie</a></li>\r\n</ul>\r\n<p>The original criteria for the competition are <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/\">listed here</a>, but bascially the idea is to introduce the idea to a relatively smart newcomer without using jargon.</p>\r\n<p>Various people gave opinions about which articles were best. For me, two articles in particular stood out as being excellent <em>for a newomer</em>. Those articles were:</p>\r\n<p><a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/30hx?c=1\">Throwawayaccount_1 </a></p>\r\n<p>and</p>\r\n<p><a href=\"/lw/37f/efficient_charity/\">Multifoliaterose</a>'s</p>\r\n<p>articles. <br />&nbsp;</p>\r\n<p>I therefore declare them joint winners, and implore our kind sponsor <a href=\"/user/jsalvatier/\">Jsalvatier</a> to split the prize between them evenly. Throwawayaccount_1 should also unmask his/her identity.</p>\r\n<p>[I would also ask the winners to kindly <strong>not</strong> offer to donate the money to charity, but to actually take the prize money and spend it on something that they selfishly-want, such as ice-cream or movie tickets or some other luxury item. Establishing a norm of giving away prizes creates very bad incentives and will tend to decrease the degree to which prizes actually motivate people in the future]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"khReijeucXJTnsyMT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rrg2sq75a5gBowgQb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 28, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "4250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4amcyxad5bnBR9Afm", "Xg5KCY4FYrxEcCifa", "FCxHgPsDScx4C3H8n", "TrmMcujGZt5JAtMGg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-12T15:47:25.448Z", "modifiedAt": null, "url": null, "title": "Testing the effectiveness of an effort to help", "slug": "testing-the-effectiveness-of-an-effort-to-help", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:55.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/42avmfSPj6iFAjGJT/testing-the-effectiveness-of-an-effort-to-help", "pageUrlRelative": "/posts/42avmfSPj6iFAjGJT/testing-the-effectiveness-of-an-effort-to-help", "linkUrl": "https://www.lesswrong.com/posts/42avmfSPj6iFAjGJT/testing-the-effectiveness-of-an-effort-to-help", "postedAtFormatted": "Sunday, December 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Testing%20the%20effectiveness%20of%20an%20effort%20to%20help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATesting%20the%20effectiveness%20of%20an%20effort%20to%20help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F42avmfSPj6iFAjGJT%2Ftesting-the-effectiveness-of-an-effort-to-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Testing%20the%20effectiveness%20of%20an%20effort%20to%20help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F42avmfSPj6iFAjGJT%2Ftesting-the-effectiveness-of-an-effort-to-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F42avmfSPj6iFAjGJT%2Ftesting-the-effectiveness-of-an-effort-to-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>\"It has long been the standard practice in medical testing: Give drug treatment to one group while another, the control group, goes without.</p>\n<p>Now, New York City is <a href=\"http://www.nytimes.com/2010/12/09/nyregion/09placebo.html?_r=1\">applying the same methodology</a> to assess one of its programs to prevent homelessness. Half of the test subjects &mdash; people who are behind on rent and in danger of being evicted &mdash; are being denied assistance from the program for two years, with researchers tracking them to see if they end up homeless.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "42avmfSPj6iFAjGJT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 6.554357587092076e-07, "legacy": true, "legacyId": "4252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-12T21:52:12.278Z", "modifiedAt": null, "url": null, "title": "Bayesian approach: UFO vs. AI hypotheses", "slug": "bayesian-approach-ufo-vs-ai-hypotheses", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:55.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "turchin", "createdAt": "2010-02-03T20:22:54.095Z", "isAdmin": false, "displayName": "turchin"}, "userId": "2kDfHyTEpYCoa2SRq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uERBrgmvCsmNZR7Pq/bayesian-approach-ufo-vs-ai-hypotheses", "pageUrlRelative": "/posts/uERBrgmvCsmNZR7Pq/bayesian-approach-ufo-vs-ai-hypotheses", "linkUrl": "https://www.lesswrong.com/posts/uERBrgmvCsmNZR7Pq/bayesian-approach-ufo-vs-ai-hypotheses", "postedAtFormatted": "Sunday, December 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20approach%3A%20UFO%20vs.%20AI%20hypotheses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20approach%3A%20UFO%20vs.%20AI%20hypotheses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuERBrgmvCsmNZR7Pq%2Fbayesian-approach-ufo-vs-ai-hypotheses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20approach%3A%20UFO%20vs.%20AI%20hypotheses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuERBrgmvCsmNZR7Pq%2Fbayesian-approach-ufo-vs-ai-hypotheses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuERBrgmvCsmNZR7Pq%2Fbayesian-approach-ufo-vs-ai-hypotheses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 960, "htmlBody": "<p>The goal of this post is not to prove or disprove existing of so called UFO or feasibility of AI but to study limits of Bayesian approach to complex problems.<br /><br />Here we will test two hypotheses:<br /><br />1)\tUFOs exist. We will take for simplicity the following form of this thesis: Unknown nonhuman intelligence exists on the Earth and manifests itself with unknown laws of physics.<br />2)\tAI will be created. In the XXI century will be created computer program which will surpass humans in every kind of intellectual activity by many orders of magnitude.<br /><br />From the point of view of a layman both hypothesis are bizarre and so belongs to the reference class of &ldquo;strange ideas&rdquo;, most of each are false. <br /><br />But both hypotheses have large communities which accumulated many evidences to support this ideas. (Here we could see &ldquo;confirmation bias&rdquo;).<br /><br />In the begging we should point on isomorphism of the two hypotheses: in both cases the question is an existence of nonhuman intelligence. In the first case it is said that nonhuman intelligence already exists on the Earth, in second case that nonhuman intelligence would soon be created on the Earth.<br /><br />For Bayesian estimation we need a priori probability, and then change it with some evidences.<br /><br />Supporters of AI-hypothesis usually say that a priori probability is quite high:  if the human mind exists, then AI is possible, and in addition, it is typical to human to repeat the achievements of nature. Therefore, a priori, we can assume that the creation of AI is possible and highly likely. <br /><br />The situation with evidence in the field of AI is worse because creation of AI is a future event and direct empirical evidence is impossible. Moreover, many failed attempts to create AI in the past are used as evidence against the possibility of it creation. <br /><br />Therefore, information about the success in \"helping\" disciplines is used as evidence of the possibility of AI: performance of computers and its continued growth, the success of brain scans, the success of various computer programs to recognize images and games. These circumstantial evidences cannot be directly substituted into the formula for calculating the probability, therefore, their credibility will always include taking something for granted.<br /><br />In the case of UFOs a priori hypothesis is less convincing, since it argues not only that that nonhuman intelligence does exist on Earth, but also that it uses unknown physical laws (for flying). So this hypothesis is more complex and so less probable. Also it is not clear how nonhuman intelligence would appear evolutionary on Earth but didn&rsquo;t eat all other types of live beings. Here come in play alien theory of the origin of UFOs as a priori hypothesis.<br /><br />The proponents of alien UFO hypothesis say that if human intelligence exists on Earth, then some kind of intelligence could also appear on other planets of our Galaxy long before and come to our planet with some more or less rational goals (exploration, game etc). Saying this they think that they create high a priory probability for UFO hypothesis. (It is not true, because they have to assume that aliens have very strange goal systems &ndash; for example that they fly many light years to drink cattle blood in so called <a href=\"http://en.wikipedia.org/wiki/Cattle_mutilation\" target=\"_blank\">cattle mutilation cases.</a> This improbable goal system completely neutralizes high probability of alien origin of UFO.)<br /><br />We could note immediately that the a priori hypothesis about UFO uses the same premise as the hypothesis about AI: namely, the possibility of nonhuman intelligence is justified by the existence of the human mind! <br /><br />However, the hypothesis of UFOs requires the existence of new physical laws, whereas the hypothesis about AI requires their absence (in the sense that for creating AI is necessary that the brain could be described as an algorithmic computer without any Penrose style things). <br /><br />History of science shows that the list of physical laws will never be complete - every time we discover something new (e.g. dark energy recently) - but on the other hand, there are no physical effects in our environment, which are inexplicable within the framework of known physical laws (except perhaps that of ball lightning). Yet due to the need for new laws of physics a priori probability of the existence of UFOs is less.<br /><br />In terms of evidence the hypothesis about UFOs has a sharp contrast to the hypothesis of AI. There are thousands of empirical evidences about UFO sightings. However Bayesian interference (increase the credibility) of each of the evidence is very small. That is, most of these evidences have an equal probability of being true or false and do not carry any information. Note that if we have 20 evidences with a probability of truth greater than 50%, say 60%, then Bays formula give a very substantial total evidence of 3000 to 1 - that is, would increase the validity of a priori hypothesis of 3000 times.<br /> <br />Thus, the hypothesis of UFO has a lower a priori probability, but more empirical evidence (the truth of which we will not discuss here, but see Don Berliner <a href=\"http://www.bibliotecapleyades.net/ciencia/ufo_briefingdocument/index.htm \" target=\"_blank\">\"UFO Briefing Document: The Best Available Evidence&raquo;</a> My position is that I not convinces UFO believer, but assume they could exist).<br /><br />Discussions about AI are always tend to come to discussion about rationality, but most UFO band is a bastion of irrationality. In fact, both can be described in terms of Bayesian logic. The belief that some topics are more rational than others is irrational.<br /><br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uERBrgmvCsmNZR7Pq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -10, "extendedScore": null, "score": 6.555265181267108e-07, "legacy": true, "legacyId": "4253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-12T22:16:17.636Z", "modifiedAt": null, "url": null, "title": "Moderation of apparent trolling", "slug": "moderation-of-apparent-trolling", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:57.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5o7kGqoBe97di3rxB/moderation-of-apparent-trolling", "pageUrlRelative": "/posts/5o7kGqoBe97di3rxB/moderation-of-apparent-trolling", "linkUrl": "https://www.lesswrong.com/posts/5o7kGqoBe97di3rxB/moderation-of-apparent-trolling", "postedAtFormatted": "Sunday, December 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Moderation%20of%20apparent%20trolling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AModeration%20of%20apparent%20trolling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5o7kGqoBe97di3rxB%2Fmoderation-of-apparent-trolling%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Moderation%20of%20apparent%20trolling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5o7kGqoBe97di3rxB%2Fmoderation-of-apparent-trolling", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5o7kGqoBe97di3rxB%2Fmoderation-of-apparent-trolling", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>A brief line from <a href=\"http://forum.rickross.com/read.php?12,64749,95185#msg-95185\">this</a> comment indicates that the <a href=\"/user/richiekgb\">author</a> of the cryonics-critical comment quoted <a href=\"http://forum.rickross.com/read.php?12,64749,95250#msg-95250\">here</a> was perhaps not the one that deleted it.</p>\n<blockquote>\n<p>You know what - I am rather glad my comment was deleted on less wrong - good reason for people not to post on there.</p>\n</blockquote>\n<p>Was it deleted by a moderator?</p>\n<p>Honestly, the decisive downvoting seemed to do the trick of hiding it from casual readers who don't want to see the long annoying rants. I don't think it was casting any doubt on the credibility of cryonics.</p>\n<p>While it sounds like the author regrets posting it, I would think they should be allowed to delete it themselves.</p>\n<p>&nbsp;</p>\n<p>Edit: Originally titled \"Cryonics critical comment deleted?\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5o7kGqoBe97di3rxB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "4254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-13T07:19:47.377Z", "modifiedAt": null, "url": null, "title": "Within the next rich hint personal can solve your value Pandora low-cost Pandora diamond", "slug": "within-the-next-rich-hint-personal-can-solve-your-value", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beautifuljewelry", "createdAt": "2010-11-30T03:32:58.738Z", "isAdmin": false, "displayName": "beautifuljewelry"}, "userId": "Ridmt4FmL9zdEbpNP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uFB2gqkuC8xJdF4o9/within-the-next-rich-hint-personal-can-solve-your-value", "pageUrlRelative": "/posts/uFB2gqkuC8xJdF4o9/within-the-next-rich-hint-personal-can-solve-your-value", "linkUrl": "https://www.lesswrong.com/posts/uFB2gqkuC8xJdF4o9/within-the-next-rich-hint-personal-can-solve-your-value", "postedAtFormatted": "Monday, December 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Within%20the%20next%20rich%20hint%20personal%20can%20solve%20your%20value%20Pandora%20low-cost%20Pandora%20diamond&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWithin%20the%20next%20rich%20hint%20personal%20can%20solve%20your%20value%20Pandora%20low-cost%20Pandora%20diamond%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFB2gqkuC8xJdF4o9%2Fwithin-the-next-rich-hint-personal-can-solve-your-value%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Within%20the%20next%20rich%20hint%20personal%20can%20solve%20your%20value%20Pandora%20low-cost%20Pandora%20diamond%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFB2gqkuC8xJdF4o9%2Fwithin-the-next-rich-hint-personal-can-solve-your-value", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFB2gqkuC8xJdF4o9%2Fwithin-the-next-rich-hint-personal-can-solve-your-value", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>Begin making use of temperature liquid and drop dish-washing water. In this case, no alcohol toothbrush, eat your precious inexpensive <a href=\"http://www.pandorajewelrysky.com/pandora-charms-sale\"><strong>pandora charm</strong></a> Pandora bracelet is optional. Toothbrush is fantastic, due to the fact it'll be feasible to access to high jewelry model, it's quite challenging to clean. When a customer uncover your principal products turn into or stay extremely dusty, only continue in cleaning fluid in wenzhou drops dishwashing liquid. Then rinse the folks methodically.<br /><br /> On the pearl accessories, no new Pandora bead way in regular water immersion or clean thoroughly, to ensure that they use of any existing washing agent. It'll undoubtedly lead to injury of pearls. Just select a smooth wash dishcloth ruins. In any case, Oriental a person's Pandora bracelet aggressive and powerful chemicals. They can maintain a dislike is the effect of your product. Bear in mind oxidation bracelets and pearl Pandora advocate the nuggets really vulnerable.<br /><br /> Plastic may also boost poor in all of the works. Make certain you incredible band, earrings, or whatever Pandora composition owner gain will never occur contact plastic, e. G. Plastic band, rubber storage unit, etc. A man might lead a personal set or shopping center bracelet Pandora, where folks picked up, simply because these continuously supply assistance, as well as the entire maintain a lot more ideas <a href=\"http://www.pandorajewelrysky.com/pandora-beads-sale\"><strong>pandora beads</strong></a> in this question, you is how it is possible to also and purified high jewelry.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uFB2gqkuC8xJdF4o9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -6, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "4260", "legacySpam": true, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-13T09:54:28.812Z", "modifiedAt": null, "url": null, "title": "Reading Level of Less Wrong", "slug": "reading-level-of-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.629Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uS8xTiNwarade8hMh/reading-level-of-less-wrong", "pageUrlRelative": "/posts/uS8xTiNwarade8hMh/reading-level-of-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/uS8xTiNwarade8hMh/reading-level-of-less-wrong", "postedAtFormatted": "Monday, December 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reading%20Level%20of%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReading%20Level%20of%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuS8xTiNwarade8hMh%2Freading-level-of-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reading%20Level%20of%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuS8xTiNwarade8hMh%2Freading-level-of-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuS8xTiNwarade8hMh%2Freading-level-of-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>Here's something to pick our collective spirits up:</p>\n<p>According to Google's infallible algorithms, 20% of the content on LessWrong.com <a href=\"https://encrypted.google.com/search?hl=en&amp;tbs=rl:1&amp;q=site:lesswrong.com\">falls within the 'Advanced' reading level</a>.&nbsp;For comparison, another well-known bastion of intelligence on the internets, Hacker News, <a href=\"https://encrypted.google.com/search?hl=en&amp;tbs=rl:1&amp;q=site:news.ycombinator.com\">only has 4%</a> of it's content in that category.</p>\n<p>Strangely, inserting a space before the name of the site in the query tends to <a href=\"https://encrypted.google.com/search?hl=en&amp;tbs=rl:1&amp;q=site:+lesswrong.com\">reduce the amount</a> of content that falls in the highest bucket, but I am told that highly trained Google engineers are interrogating the bug in a dimly lit room as we speak, and expect it to crack soon.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uS8xTiNwarade8hMh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 5, "extendedScore": null, "score": 6.557062899023845e-07, "legacy": true, "legacyId": "4262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-13T13:57:11.403Z", "modifiedAt": null, "url": null, "title": "What is Evil about creating House Elves? ", "slug": "what-is-evil-about-creating-house-elves", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:59.830Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DYaDw3JBvGrHpvBmk/what-is-evil-about-creating-house-elves", "pageUrlRelative": "/posts/DYaDw3JBvGrHpvBmk/what-is-evil-about-creating-house-elves", "linkUrl": "https://www.lesswrong.com/posts/DYaDw3JBvGrHpvBmk/what-is-evil-about-creating-house-elves", "postedAtFormatted": "Monday, December 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20Evil%20about%20creating%20House%20Elves%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20Evil%20about%20creating%20House%20Elves%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDYaDw3JBvGrHpvBmk%2Fwhat-is-evil-about-creating-house-elves%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20Evil%20about%20creating%20House%20Elves%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDYaDw3JBvGrHpvBmk%2Fwhat-is-evil-about-creating-house-elves", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDYaDw3JBvGrHpvBmk%2Fwhat-is-evil-about-creating-house-elves", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 327, "htmlBody": "<p><strong>Edit:</strong> This is<strong> </strong><a href=\"http://lesswrong.com/r/discussion/lw/eqa/open_thread_october_115_2012/7l85\">old material.</a> It may be out of date.<a href=\"http://lesswrong.com/r/discussion/lw/eqa/open_thread_october_115_2012/7l85\"><br /></a></p>\n<p>I'm talking about the fictional race of House Elves from the Harry Potter universe first written about by J. K. Rowling and then uplifted in a grand act of fan-fiction by Elizer Yudkowsky. Unless severely mistreated they enjoy servitude to their masters (or more accurately the current residents of the homes they are binded to), this is also enforced by magical means since they must follow the letter if not the spirit of their master's direct order.</p>\n<p>Overall treating House Elves the way they would like to be treated appears more or less sensible and don't feel like debating this if people don't disagree. Changing agents without their consent or knowledge seems obviously wrong, so turning someone into a servant creatures seem intuitively wrong. I can also understand that many people would mind their descendants being modified in such a fashion, perhaps their dis-utility is enough to offset the utility of their modified descendants. <span>However how true is this of distant descendants that only share passing resemblance? I think a </span>helpful reminder of scale might be our own self domestication. <br /><br /><em>Assuming one created elf like creatures ex nihilo, not as slightly modified versions of a existing species why would one not want to bring a mind into existence that would value its own existence and benefits you, as long as the act of creation or their existence in itself does not represents huge enough dis-utility? </em>This seems somewhat related to the argument Robin Hanson once made that any creatures that can pay for their own existance and would value their own existance should be created.</p>\n<p>I didn't mention this in the many HP fan fiction threads because I want a more general debate on the treatment and creation of such a class of agents.</p>\n<p>&nbsp;</p>\n<p><strong>Edit:</strong> Clearly if the species or class contains exceptions there should be ways for them to pursue their differing values.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DYaDw3JBvGrHpvBmk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 22, "extendedScore": null, "score": 6.557664700081719e-07, "legacy": true, "legacyId": "4263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-13T16:20:11.474Z", "modifiedAt": null, "url": null, "title": "What topics would you like to see more of on LessWrong?", "slug": "what-topics-would-you-like-to-see-more-of-on-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.930Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bEEMXkZEkYRBEMXdr/what-topics-would-you-like-to-see-more-of-on-lesswrong", "pageUrlRelative": "/posts/bEEMXkZEkYRBEMXdr/what-topics-would-you-like-to-see-more-of-on-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/bEEMXkZEkYRBEMXdr/what-topics-would-you-like-to-see-more-of-on-lesswrong", "postedAtFormatted": "Monday, December 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20topics%20would%20you%20like%20to%20see%20more%20of%20on%20LessWrong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20topics%20would%20you%20like%20to%20see%20more%20of%20on%20LessWrong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbEEMXkZEkYRBEMXdr%2Fwhat-topics-would-you-like-to-see-more-of-on-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20topics%20would%20you%20like%20to%20see%20more%20of%20on%20LessWrong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbEEMXkZEkYRBEMXdr%2Fwhat-topics-would-you-like-to-see-more-of-on-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbEEMXkZEkYRBEMXdr%2Fwhat-topics-would-you-like-to-see-more-of-on-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<p>Are there any areas of study that you feel are underrepresented here, and would be interesting and useful to lesswrongers?</p>\n<p>I feel some topics are getting old (Omega, drama about moderation policy, a newcomer telling us our lack of admiration for his ideas is proof of groupthink, Friendly AI, Cryonics, Epistemic vs. Instrumental Rationality, lamenting how we're a bunch of self-centered nerds, etc. ...), and with a bit of luck, we might have some lurkers that are knowledgeable about interesting areas, and didn't think they could contribute.</p>\n<p>Please stick to one topic per comment, so that highly-upvoted topics stand out more clearly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 2, "izp6eeJJEg9v5zcur": 2, "7mTviCYysGmLqiHai": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bEEMXkZEkYRBEMXdr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 38, "extendedScore": null, "score": 6.558023277237544e-07, "legacy": true, "legacyId": "4264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 138, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-13T20:38:25.782Z", "modifiedAt": null, "url": null, "title": "Brainstorming: neat stuff we could do on LessWrong", "slug": "brainstorming-neat-stuff-we-could-do-on-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w3HmNsYNALMkdpPGv/brainstorming-neat-stuff-we-could-do-on-lesswrong", "pageUrlRelative": "/posts/w3HmNsYNALMkdpPGv/brainstorming-neat-stuff-we-could-do-on-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/w3HmNsYNALMkdpPGv/brainstorming-neat-stuff-we-could-do-on-lesswrong", "postedAtFormatted": "Monday, December 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brainstorming%3A%20neat%20stuff%20we%20could%20do%20on%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrainstorming%3A%20neat%20stuff%20we%20could%20do%20on%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3HmNsYNALMkdpPGv%2Fbrainstorming-neat-stuff-we-could-do-on-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brainstorming%3A%20neat%20stuff%20we%20could%20do%20on%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3HmNsYNALMkdpPGv%2Fbrainstorming-neat-stuff-we-could-do-on-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw3HmNsYNALMkdpPGv%2Fbrainstorming-neat-stuff-we-could-do-on-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>Are there any community activities or rituals or experiments we could try?</p>\n<p>Preferably things that don't require special software.</p>\n<p>As an example of the kind of thing I'm thinking of, Reddit has special \"<a href=\"http://www.reddit.com/r/IAmA/\">I Am A</a>\" posts (no, I don't think we should have those), or things we already have, like Quotes or Open Threads or Diplomacy games.</p>\n<p>(This is a complement to the <a href=\"/lw/3ag/what_topics_would_you_like_to_see_more_of_on/\">previous post</a> about which topics we would like to learn about here.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w3HmNsYNALMkdpPGv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 6.558666401779598e-07, "legacy": true, "legacyId": "4265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bEEMXkZEkYRBEMXdr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-13T21:59:08.756Z", "modifiedAt": null, "url": null, "title": "Karma Motivation Thread", "slug": "karma-motivation-thread", "viewCount": null, "lastCommentedAt": "2018-10-13T08:41:49.211Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eLsmJ4aDJ3CLRCBEW/karma-motivation-thread", "pageUrlRelative": "/posts/eLsmJ4aDJ3CLRCBEW/karma-motivation-thread", "linkUrl": "https://www.lesswrong.com/posts/eLsmJ4aDJ3CLRCBEW/karma-motivation-thread", "postedAtFormatted": "Monday, December 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Karma%20Motivation%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKarma%20Motivation%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLsmJ4aDJ3CLRCBEW%2Fkarma-motivation-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Karma%20Motivation%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLsmJ4aDJ3CLRCBEW%2Fkarma-motivation-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLsmJ4aDJ3CLRCBEW%2Fkarma-motivation-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p>This idea is so obvious I can't believe we haven't done it before. Many people here have posts they would like to write but keep procrastinating on. Many people also have other work to do but <em>keep procrastinating on Less Wrong.</em> <a href=\"http://www.stickk.com/\">Making akrasia cost you money</a> is often a good way to motivate yourself. But that can be enough of a hassle to deter the lazy, the ADD addled and the executive dysfunctional. So here is a low transaction cost alternative that takes advantage of the addictive properties of Less Wrong karma. Post a comment here with a task and a deadline- pick tasks that can be confirmed by posters; so either Less Wrong posts or projects that can be linked to or photographed. When the deadline comes edit your comment to include a link to the completed task. If you complete the task, expect upvotes. If you fail to complete the task by the deadline, expect your comment to be downvoted into oblivion. If you see completed tasks, vote those comments up. If you see past deadlines <strong>vote those comments down.&nbsp;</strong> At least one person should reply to the comment, noting the deadline has passed-- this way it will come up in the recent comments and more eyes will see it.</p>\n<p><strong>Edit:</strong> DanArmak makes <a href=\"/r/discussion/lw/3ai/karma_motivation_thread/35km?c=1\">a great suggestion.</a></p>\n<blockquote>\n<div id=\"body_t1_35km\" class=\"comment-content\">\n<div class=\"md\">\n<p>Several  people have now used this to commit to doing something others can  benefit from, like LW posts. I suggest an alternative method: when a  user commits to doing something, everyone who is interested in that  thing being done will upvote that comment. However, if the task is not  complete by the deadline, everyone who upvoted commits to coming back  and downvoting the comment instead.</p>\n<p>This way, people can judge whether the community is interested in  their post, and the karma being gained or lost is proportional to the  amount of interest. Also, upvoting and then downvoting effectively  doubles the amount of karma at stake.</p>\n</div>\n</div>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eLsmJ4aDJ3CLRCBEW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 32, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "4266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-14T04:13:45.647Z", "modifiedAt": null, "url": null, "title": "Exponentiation goes wrong first", "slug": "exponentiation-goes-wrong-first", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "QJcy5NDAd8HTiZYQq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JA3n4pKeFgw9svdhA/exponentiation-goes-wrong-first", "pageUrlRelative": "/posts/JA3n4pKeFgw9svdhA/exponentiation-goes-wrong-first", "linkUrl": "https://www.lesswrong.com/posts/JA3n4pKeFgw9svdhA/exponentiation-goes-wrong-first", "postedAtFormatted": "Tuesday, December 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exponentiation%20goes%20wrong%20first&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExponentiation%20goes%20wrong%20first%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJA3n4pKeFgw9svdhA%2Fexponentiation-goes-wrong-first%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exponentiation%20goes%20wrong%20first%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJA3n4pKeFgw9svdhA%2Fexponentiation-goes-wrong-first", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJA3n4pKeFgw9svdhA%2Fexponentiation-goes-wrong-first", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 916, "htmlBody": "<p>The great Catholic mathematician Edward Nelson does not believe in completed infinity, and does not believe that arithmetic is likely to be consistent. &nbsp;These beliefs are partly motivated by his faith: he says arithmetic is a human invention, and compares believing (too strongly) in its consistency to idolatry. &nbsp;He also has many sound mathematical insights in this direction -- I'll summarize one of them here.</p>\n<p>http://www.mediafire.com/file/z3detbt6int7a56/warn.pdf</p>\n<p>Nelson's arguments flow from the idea that, contra Kronecker, numbers are man-made. &nbsp;He therefore does not expect inconsistencies to have consequences that play out in natural or divine processes. &nbsp;For instance, he does not expect you to be able to count the dollars in a stack of 100 dollars and arrive at 99 dollars. &nbsp;But it's been known for a long time that if one can prove any contradiction, then one can also prove that a stack of 100 dollars has no more than 99 dollars in it. &nbsp;The way he resolves this is interesting.</p>\n<p>The Peano axioms for the natural numbers are these:</p>\n<p>1. Zero is a number</p>\n<p>2. The successor of any number is a number</p>\n<p>3. Zero is not the successor of any number</p>\n<p>4. Two different numbers have two different successors</p>\n<p>5. If a given property holds for zero, and if it holds for the succesor of <em>x</em>&nbsp;whenever it holds for&nbsp;<em>x</em>, then it holds for all numbers.</p>\n<p>Nelson rejects the fifth axiom, induction. &nbsp;It's the most complicated of the axioms, but it has another thing going against it: it is the only one that seems like a claim that could be either true or false. &nbsp;The first four axioms read like someone explaining the rules of a game, like how the pieces in chess move. &nbsp;Induction is more similar to the fact that the bishop in chess can only move on half the squares -- this is a theorem about chess, not one of the rules. &nbsp;Nelson believes that the fifth axiom needs to be, and cannot be, supported.</p>\n<p>A common way to support induction is via the monologue: \"It's true for zero. &nbsp;Since it's true for zero it's true for one. &nbsp;Since it's true for one it's true for two. &nbsp;Continuing like this we can show that it's true for one hundred and for one hundred thousand and for every natural number.\" &nbsp;It's hard to imagine actually going through this proof for very large numbers -- this is Nelson's objection.&nbsp;</p>\n<p>What is arithmetic like if we reject induction? &nbsp;First, we may make a distinction between numbers we can actually count to (call them counting numbers) and numbers that we can't. &nbsp;Formally we define counting numbers as follows: 0 is a counting number, and if <em>x</em>&nbsp;is a counting number then so is its successor. &nbsp;We could use the induction axiom to establish that every number is a counting number, but without it we cannot.</p>\n<p>A small example of a number so large we might not be able to count that high is the sum of two counting numbers. &nbsp;In fact without induction we&nbsp;cannot establish that&nbsp;<em>x+y</em>&nbsp;is a counting number from the facts that&nbsp;<em>x</em>&nbsp;and&nbsp;<em>y</em>&nbsp;are counting numbers. &nbsp;So we cut out a smaller class of numbers called additionable numbers:&nbsp;<em>x</em>&nbsp;is additionable if <em>x + y</em>&nbsp;is a counting number whenever <em>y</em>&nbsp;is a counting number. &nbsp;We can prove theorems about additionable numbers: for instance every additionable number is a counting number, the successor of an additionable number is additionable, and in fact the sum of two additionable numbers is an additionable number.</p>\n<p>If we grant the induction axiom, these theorems lose their interest: every number is a counting number and an additionable number. &nbsp;Paraphrasing Nelson: the significance of these theorems is that addition is unproblematic even if we are skeptical of induction.</p>\n<p>We can go further. &nbsp;It cannot be proved that the product of two additionable numbers is additionable. &nbsp;We therefore introduce the smaller class of multiplicable numbers. &nbsp;If whenever <em>y</em>&nbsp;is an additionable number <em>x.y</em>&nbsp;is also additionable, then we say that <em>x</em>&nbsp;is a multiplicable number. &nbsp;It can be proved that the sum and product of any two multiplicable numbers is multiplicable. &nbsp;Nelson closes the article I linked to:</p>\n<blockquote>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica\">The proof of the last theorem [that the product of two multiplicable numbers is multiplicable] uses the associativity of multiplication. The significance of all this is that addition and multiplication are unproblematic. We have defined a new notion, that of a multiplicable number, that is stronger than the notion of counting number, and proved that multiplicable numbers not only have successors that are multiplicable numbers, and hence counting numbers, but that the same is true for sums and products of multiplicable numbers. For any specific numeral SSS. . . 0 we can quickly prove that it is a multiplicable number.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica\">But now we come to a halt. If we attempt to define &ldquo;exponentiable number&rdquo; in the same spirit, we are unable to prove that if <em>x</em><span style=\"font: 8.4px Helvetica\">&nbsp;</span>and <em>y</em><span style=\"font: 8.4px Helvetica\">&nbsp;</span>are exponentiable numbers then so is <em>x&nbsp;</em><em><span style=\"font-style: normal; \">&uarr;</span>y</em>. There is a radical difference between addition and multiplication on the one hand and exponentiation, superexponentiation [what is commonly denoted ^^ here], and so forth, on the other hand. The obstacle is that exponentiation is not associative; for example, (2&uarr;2)&uarr;3 = 4&uarr;3 = 64 whereas 2&uarr;(2&uarr;3) = 2&uarr;8 = 256. For&nbsp;any specific numeral SSS...0 we can indeed prove that it is an exponentiable number, but we cannot prove that the world of exponentiable numbers is closed under exponentiation. And superexponentiation leads us entirely away from the world of counting numbers.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica\">The belief that exponentiation, superexponentiation, and so forth, applied to numerals yield numerals is just that -- a belief.</p>\n</blockquote>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">I've omitted his final sentence.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JA3n4pKeFgw9svdhA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "4267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-14T05:19:10.076Z", "modifiedAt": null, "url": null, "title": "Would the world be better off without 50% of the people in it?", "slug": "would-the-world-be-better-off-without-50-of-the-people-in-it", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:56.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MoreOn", "createdAt": "2010-12-09T23:13:18.874Z", "isAdmin": false, "displayName": "MoreOn"}, "userId": "rmPGJ5r2tu26kgZXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7NS3H7zYFmLCXCXcP/would-the-world-be-better-off-without-50-of-the-people-in-it", "pageUrlRelative": "/posts/7NS3H7zYFmLCXCXcP/would-the-world-be-better-off-without-50-of-the-people-in-it", "linkUrl": "https://www.lesswrong.com/posts/7NS3H7zYFmLCXCXcP/would-the-world-be-better-off-without-50-of-the-people-in-it", "postedAtFormatted": "Tuesday, December 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Would%20the%20world%20be%20better%20off%20without%2050%25%20of%20the%20people%20in%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWould%20the%20world%20be%20better%20off%20without%2050%25%20of%20the%20people%20in%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7NS3H7zYFmLCXCXcP%2Fwould-the-world-be-better-off-without-50-of-the-people-in-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Would%20the%20world%20be%20better%20off%20without%2050%25%20of%20the%20people%20in%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7NS3H7zYFmLCXCXcP%2Fwould-the-world-be-better-off-without-50-of-the-people-in-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7NS3H7zYFmLCXCXcP%2Fwould-the-world-be-better-off-without-50-of-the-people-in-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 633, "htmlBody": "<p>I made a stupid mistake of posting a <a href=\"/lw/2/tell_your_rationalist_origin_story/35b7?c=1\">conclusion</a> before I had the whole analysis typed up or had looked up my references. I <em>knew</em> I would be called on it. I&rsquo;ll appreciate any help with the &lt;ref&gt;'s. Also: I'm under Crocker's Rules, and criticism is welcome. So here goes nothi....</p>\r\n<p class=\"MsoNormal\">There's a theory out there that states that new inventions are combinations of old inventions &lt;ref&gt;. So if your hunter-gatherer tribe has knife-like rocks and sticks, just about the only thing you can invent is a spear. Fire + clay = pots. Little bones with holes + animal sinews + skins = needle =&gt; clothes. But if you were modern day's best chemist transported into the past, with all your knowledge intact, you'd be unlikely to make any&nbsp;<a href=\"http://en.wikipedia.org/wiki/Aspirin#Chemistry\"> aspirin</a>. Why? Because the tools you need haven't been invented.</p>\r\n<p class=\"MsoNormal\">Instead of looking at what's projected to happen, consider what <em>has </em>been happening happened. With the increase in world population, the level technology and average standard of living have been going up.</p>\r\n<p class=\"MsoNormal\">I argue that <em>more population =&gt; better technology =&gt; easier life =&gt; more population</em>.</p>\r\n<p class=\"MsoNormal\">In the modern day, consider: <a href=\"http://upload.wikimedia.org/wikipedia/commons/8/87/US_Population_Graph_-_1790_to_2000.svg\">US population</a>, <a href=\"http://jamesconner.us/images/patents_issued.jpg\">US Patents per year</a>.</p>\r\n<p class=\"MsoNormal\">So what about the &ldquo;unproductive&rdquo; people? Those who &ldquo;don't pull their own weight?&rdquo; Those &ldquo;living off of welfare, charity donations, etc?&rdquo; Those who just barely survive off of subsistence living? They put a drain on world resources without adding anything back. Wouldn't the world be better off without them?</p>\r\n<p class=\"MsoNormal\">Suppose Omega made a backup copy of the Solar system. It created a perfect copy of <em>everything</em> else, but it only replicated 50% of humanity. Pick your favorite selection criterion for who will be copied. You will go to the copied world, and other you will live on as a <a href=\"/lw/189/mwi_weird_quantum_experiments_and_futuredirected/\">zombie</a>.</p>\r\n<p class=\"MsoNormal\">Suppose the people who work in sweatshops get copied. But subsistence farmers from the same regions don't. Then it's reasonable to predict that some people from sweatshops would quit their jobs and fill up the niche you left available. Fewer people would be supporting the developed world.</p>\r\n<p class=\"MsoNormal\">Historically, people used technology to solve population problems only when those problems became bad enough. Farming wasn't invented until there were too many hunter-gatherers. Industry was not invented until there were too many farmers. Sewers were not invented until there was a problem with urban pollution.</p>\r\n<p class=\"MsoNormal\">I'll skip the statistical argument<sup>1</sup>. If truly brilliant people (the likes of whom had invented the wheel, the steam engine and the computer) are 1 in a billion, then having more billions means having more of those people.</p>\r\n<p class=\"MsoNormal\">Why do people have no confidence that we can invent ourselves out of the immense pressure we're putting on the environment? Technology is already there to supply humanity with renewable energy.<a href=\"http://images.usatoday.com/news/graphics/2010/2010-1-25-welfare/welfare.jpg\"><br /></a></p>\r\n<p class=\"MsoNormal\">If you could choose whether your consciousness would go to Omega's backup world or stay on the original Earth, where would you choose? And if you chose the copied world, what selection criterion would you use to pick who would go with you?</p>\r\n<p class=\"MsoNormal\">&nbsp;</p>\r\n<p class=\"MsoNormal\">Footnote<sup>1</sup> : Statistics pop quiz (<em>read: check my numbers, please</em>). The world population is (<a href=\"http://www.census.gov/main/www/popclock.html\">~6,887,656,866</a>). Let&rsquo;s guess that &ldquo;inventiveness&rdquo; is distributed normally.I wouldn't be surprised if it were strongly correlated with IQ. How many people would you expect to find 6 standard deviations above the mean? IQ 190 for comparison. (upside down answer: 6.8). What about when the world population was 1 billion around 1800? (no calculators! just 1). We need to multiply 113 times to produce a person more than 7 standard deviations above the mean (IQ 205). The tail ends aren't necessarily this well-behaved, but then, given any distribution over the infinite competence axis, increasing the number of people would increase the number of people at each competence level.</p>\r\n<p class=\"MsoNormal\"><em>EDIT: I rewrote this article. If you had managed to wade through the blabber I had before, my point stayed the same.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7NS3H7zYFmLCXCXcP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -7, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "4269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MoFqnLnXDDG8WXMjB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-14T05:37:00.533Z", "modifiedAt": null, "url": null, "title": "Honours Dissertation", "slug": "honours-dissertation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SeventhNadir", "createdAt": "2010-04-18T04:11:11.485Z", "isAdmin": false, "displayName": "SeventhNadir"}, "userId": "adAXuo6KKGxTap3SN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DqC9dR8G6pFHTeKHf/honours-dissertation", "pageUrlRelative": "/posts/DqC9dR8G6pFHTeKHf/honours-dissertation", "linkUrl": "https://www.lesswrong.com/posts/DqC9dR8G6pFHTeKHf/honours-dissertation", "postedAtFormatted": "Tuesday, December 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Honours%20Dissertation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHonours%20Dissertation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqC9dR8G6pFHTeKHf%2Fhonours-dissertation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Honours%20Dissertation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqC9dR8G6pFHTeKHf%2Fhonours-dissertation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDqC9dR8G6pFHTeKHf%2Fhonours-dissertation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>I'm picking a topic for my Psychology Honours dissertation next year and I've got so many options and interests that the overabundance of choice is near paralyzing. So in the interests of crowd sourcing and hopefully writing about something of substance, I'd like to hear suggestions for potential directions I could take. It can be any idea but one that frequently pops up on less wrong and needs further exploration or exposure would be ideal.</p>\n<p>Basically feel free to offer suggestions but ideally I want something that (assuming I do it right) would help lay a part of the groundwork required to build up someones rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DqC9dR8G6pFHTeKHf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 6.560008054052379e-07, "legacy": true, "legacyId": "4270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-14T06:30:02.477Z", "modifiedAt": null, "url": null, "title": "Friendly AI Research and Taskification", "slug": "friendly-ai-research-and-taskification", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:01.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7sxR8qLG6rqDht6aR/friendly-ai-research-and-taskification", "pageUrlRelative": "/posts/7sxR8qLG6rqDht6aR/friendly-ai-research-and-taskification", "linkUrl": "https://www.lesswrong.com/posts/7sxR8qLG6rqDht6aR/friendly-ai-research-and-taskification", "postedAtFormatted": "Tuesday, December 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendly%20AI%20Research%20and%20Taskification&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendly%20AI%20Research%20and%20Taskification%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7sxR8qLG6rqDht6aR%2Ffriendly-ai-research-and-taskification%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendly%20AI%20Research%20and%20Taskification%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7sxR8qLG6rqDht6aR%2Ffriendly-ai-research-and-taskification", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7sxR8qLG6rqDht6aR%2Ffriendly-ai-research-and-taskification", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1367, "htmlBody": "<p>Eliezer has written a great deal about the concept of <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly AI</a>, for example in a document from 2001 titled <a href=\"http://intelligence.org/upload/CFAI.html\">Creating Friendly AI 1.0</a>. The <a href=\"http://intelligence.org/riskintro/index.html\">new SIAI overview</a> states that:</p>\n<blockquote>\n<p style=\"margin: 0pt;\"><span><span style=\"color: #000000;\">SIAI's primary approach to reducing AI risks has thus been to promote the development of AI with benevolent motivations which are reliably stable under self-improvement, what we call &ldquo;Friendly AI&rdquo; <a id=\"x:i9\" title=\"(Yudkowsky 2008a) [20]\" href=\"http://intelligence.org/riskintro/index.html#Yudkowsky_2008a\" target=\"_self\">[22]</a>.</span></span></p>\n</blockquote>\n<p>The SIAI Research Program lists under its <a href=\"http://intelligence.org/research/researchareas\">Research Areas</a>:</p>\n<blockquote>\n<p><strong>Mathematical Formalization of the \"Friendly AI\" Concept.</strong> Proving theorems about the ethics of AI systems, an important research goal, is predicated on the possession of an appropriate formalization of the notion of ethical behavior on the part of an AI. And, this formalization is a difficult research question unto itself.</p>\n</blockquote>\n<p>Despite the enormous value that the construction of a Friendly AI would have; at present I'm not convinced that researching the Friendly AI concept is a cost-effective way of reducing existential risk. My main reason for doubt is that as far as I can tell, the problem of building a Friendly AI has not been taskified to a sufficiently fine degree for it to be possible to make systematic progress toward obtaining a solution. I'm open-minded on this point and quite willing to change my position subject to incoming evidence</p>\n<p><a id=\"more\"></a><strong>The Need For Taskification</strong></p>\n<p>In <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">The First Step is to Admit That You Have a Problem</a> Alicorn wrote:</p>\n<blockquote>\n<p>If you want a peanut butter sandwich, and you have the tools, ingredients, and knowhow that are required to make a peanut butter sandwich, you have a task on your hands.&nbsp; If you want a peanut butter sandwich, but you lack one or more of those items, you have a problem [... ] treating problems like tasks will slow you down in solving them.&nbsp; You can't <em>just</em> become immortal any more than you can <em>just</em> make a peanut butter sandwich without any bread.</p>\n</blockquote>\n<p>In <a href=\"/lw/1aj/let_them_eat_cake_interpersonal_problems_vs_tasks/\">Let them eat cake: Interpersonal Problems vs Tasks</a> HughRistik wrote:</p>\n<blockquote>\n<p>Similarly, many straight guys or queer women can't <em>just</em> find a girlfriend, and many straight women or queer men can't <em>just</em> find a boyfriend, any more than they can \"just become immortal.\"</p>\n</blockquote>\n<p>We know that the problems of making a peanut butter sandwich and of finding a romantic partner can (often) be taskified because many people have succeeded in solving them. It's less clear that a given problem that has never been solved can be taskified. Some problems are in principle unsolvable whether because they are <a href=\"http://en.wikipedia.org/wiki/Undecidable_problem\">mathematically undecidable</a> or because physical law provides an obstruction to their solution. Other currently unsolved problems have solutions in the abstract but lack solutions that are accessible to humans. That taskification is in principle possible is not a sufficient condition for solving a problem but it is a necessary condition.</p>\n<p><strong>The Difficulty of Unsolved Problems</strong></p>\n<p>There's a long historical precedent of unsolved problems being solved. Humans have succeeded in building cars and skyscrapers, have succeeded in understanding the chemical composition of far away stars and of our own DNA, have determined the asymptotic distribution of the prime numbers and have given an algorithm to determine whether a given polynomial equation is solvable in radicals, have created nuclear bombs and have landed humans on the moon. All of these things seemed totally out of reach at one time.</p>\n<p>Looking over the history of human achievement gives one a sense of optimism as to the feasibility of accomplishing a goal. And yet, there's a strong selection effect at play: successes are more interesting than failures and we correspondingly notice and remember successes more than failures. One need only page through a book like Richard Guy's <a href=\"http://www.amazon.com/Unsolved-Problems-Number-Problem-Mathematics/dp/0387942890\">Unsolved Problems in Number Theory</a> to get a sense for how generic it is for a problem to be intractable. The ancient Greek inspired question of whether there are infinitely many perfect numbers remains out of reach for best mathematicians of today. The success of human research efforts has been as much a product of wisdom in choosing one's battles as it has been a product of ambition.</p>\n<p><strong>The Case of Friendly AI</strong></p>\n<p>My present understanding is that there are potential avenues for researching AGI. Richard Hollerith was kind enough to briefly describe Monte Carlo AIXI to me last month and I could sort of see how it might be in principle possible to program a computer to do Bayesian induction according to an approximation to a universal prior and implement the computer with a decision making apparatus based on its epistemological state at a given time. Some people have suggested to me that the amount of computer power and memory needed to implement human level Monte Carlo AIXI is prohibitively large but (in my current, very ill-informed state; by analogy with things that I've seen in computational complexity theory) I could imagine ingenious tricks yielding an approximation to Monte Carlo AIXI which uses much less computing power/memory and which is a sufficiently close to approximation to serve as a substitute for practical purposes. This would point to a potential taskification of the problem of building an AGI. I could also imagine that there are presently no practically feasible AGI research programs; I know too little about the state of strong artificial intelligence research to have anything but a very unstable opinion on this matter.</p>\n<p>As Eliezer has said; the problem of creating a <em>Friendly</em> AI is inherently more difficult than that of creating an AGI and may be a problem <em>much more </em>difficult than that of creating an AGI. At present, the <em>Friendliness </em>aspect of a Friendly AI seems to me to strongly resist taskificaiton. In his poetic <a href=\"/lw/tb/mirrors_and_paintings/\">Mirrors and Paintings</a> Eliezer gives the most detailed description of what a Friendly AI should do that I've seen, but the gap between concept and implementation here seems so staggeringly huge that it doesn't suggest to me any fruitful lines of Friendly AI research. As far as I can tell, Eliezer's idea of a Friendly AI is at this point not significantly more fleshed out (relative to the magnitude of the task) than Freeman Dyson's idea of a <a href=\"http://en.wikipedia.org/wiki/Dyson_sphere\">Dyson sphere</a>. In order to build a Friendly AI, beyond conceiving of what a Friendly AI should be in the abstract one has to <em>convert one's intuitive understanding of friendliness into computer code in a formal programming language</em>.</p>\n<p>I don't even see how one would start to research the problem of getting a hypothetical AGI to recognize humans as distinguished beings. Solving this problem would seem to require as a prerequisite an understanding of the make up of the hypothetical AGI; something which people don't seem to have a clear grasp of at the moment. Even if one does have a model for a hypothetical AGI, writing code conducive to it recognizing humans as distinguished beings seems like an intractable task. And even with a relatively clear understanding of how one would implement a hypothetical AGI with the ability to recognize humans as distinguished beings; one is still left with the problem of making such a hypothetical AGI Friendly toward such beings.</p>\n<p>In view of all this, working toward stable whole-brain emulation of a a trusted and highly intelligent person concerned about human well being seems to me like a more promising strategy of reducing existential risk at the present time than researching Friendly AI. Quoting <a href=\"/lw/1s3/hedging_our_bets_the_case_for_pursuing_whole/1p0v?c=1\">a comment by Carl Shulman</a></p>\n<blockquote>\n<p>Emulations could [...] enable the creation of a singleton capable of globally balancing AI development speeds and dangers. That singleton could then take billions of subjective years to work on designing safe and beneficial AI. If designing safe AI is much, much harder than building AI at all, or if knowledge of AI and safe AI are tightly coupled, such a singleton might be the most likely route to a good outcome.</p>\n</blockquote>\n<p>There are various things that could go wrong with whole-brain emulation and it would be good to have a better option but Friendly AI research doesn't seem to me to be one in light of an apparent total absence of even the outlines of a viable Friendly AI research program.</p>\n<p>But I feel like I may have missed something here. I'd welcome any clarifications of what people who are interested in Friendly AI research <em>mean</em> by Friendly AI research. In particular, is there a conjectural taskification of the problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "Pa2SdZsLFmqhs42Do": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7sxR8qLG6rqDht6aR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 30, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "4258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Eliezer has written a great deal about the concept of <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly AI</a>, for example in a document from 2001 titled <a href=\"http://intelligence.org/upload/CFAI.html\">Creating Friendly AI 1.0</a>. The <a href=\"http://intelligence.org/riskintro/index.html\">new SIAI overview</a> states that:</p>\n<blockquote>\n<p style=\"margin: 0pt;\"><span><span style=\"color: #000000;\">SIAI's primary approach to reducing AI risks has thus been to promote the development of AI with benevolent motivations which are reliably stable under self-improvement, what we call \u201cFriendly AI\u201d <a id=\"x:i9\" title=\"(Yudkowsky 2008a) [20]\" href=\"http://intelligence.org/riskintro/index.html#Yudkowsky_2008a\" target=\"_self\">[22]</a>.</span></span></p>\n</blockquote>\n<p>The SIAI Research Program lists under its <a href=\"http://intelligence.org/research/researchareas\">Research Areas</a>:</p>\n<blockquote>\n<p><strong>Mathematical Formalization of the \"Friendly AI\" Concept.</strong> Proving theorems about the ethics of AI systems, an important research goal, is predicated on the possession of an appropriate formalization of the notion of ethical behavior on the part of an AI. And, this formalization is a difficult research question unto itself.</p>\n</blockquote>\n<p>Despite the enormous value that the construction of a Friendly AI would have; at present I'm not convinced that researching the Friendly AI concept is a cost-effective way of reducing existential risk. My main reason for doubt is that as far as I can tell, the problem of building a Friendly AI has not been taskified to a sufficiently fine degree for it to be possible to make systematic progress toward obtaining a solution. I'm open-minded on this point and quite willing to change my position subject to incoming evidence</p>\n<p><a id=\"more\"></a><strong>The Need For Taskification</strong></p>\n<p>In <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">The First Step is to Admit That You Have a Problem</a> Alicorn wrote:</p>\n<blockquote>\n<p>If you want a peanut butter sandwich, and you have the tools, ingredients, and knowhow that are required to make a peanut butter sandwich, you have a task on your hands.&nbsp; If you want a peanut butter sandwich, but you lack one or more of those items, you have a problem [... ] treating problems like tasks will slow you down in solving them.&nbsp; You can't <em>just</em> become immortal any more than you can <em>just</em> make a peanut butter sandwich without any bread.</p>\n</blockquote>\n<p>In <a href=\"/lw/1aj/let_them_eat_cake_interpersonal_problems_vs_tasks/\">Let them eat cake: Interpersonal Problems vs Tasks</a> HughRistik wrote:</p>\n<blockquote>\n<p>Similarly, many straight guys or queer women can't <em>just</em> find a girlfriend, and many straight women or queer men can't <em>just</em> find a boyfriend, any more than they can \"just become immortal.\"</p>\n</blockquote>\n<p>We know that the problems of making a peanut butter sandwich and of finding a romantic partner can (often) be taskified because many people have succeeded in solving them. It's less clear that a given problem that has never been solved can be taskified. Some problems are in principle unsolvable whether because they are <a href=\"http://en.wikipedia.org/wiki/Undecidable_problem\">mathematically undecidable</a> or because physical law provides an obstruction to their solution. Other currently unsolved problems have solutions in the abstract but lack solutions that are accessible to humans. That taskification is in principle possible is not a sufficient condition for solving a problem but it is a necessary condition.</p>\n<p><strong id=\"The_Difficulty_of_Unsolved_Problems\">The Difficulty of Unsolved Problems</strong></p>\n<p>There's a long historical precedent of unsolved problems being solved. Humans have succeeded in building cars and skyscrapers, have succeeded in understanding the chemical composition of far away stars and of our own DNA, have determined the asymptotic distribution of the prime numbers and have given an algorithm to determine whether a given polynomial equation is solvable in radicals, have created nuclear bombs and have landed humans on the moon. All of these things seemed totally out of reach at one time.</p>\n<p>Looking over the history of human achievement gives one a sense of optimism as to the feasibility of accomplishing a goal. And yet, there's a strong selection effect at play: successes are more interesting than failures and we correspondingly notice and remember successes more than failures. One need only page through a book like Richard Guy's <a href=\"http://www.amazon.com/Unsolved-Problems-Number-Problem-Mathematics/dp/0387942890\">Unsolved Problems in Number Theory</a> to get a sense for how generic it is for a problem to be intractable. The ancient Greek inspired question of whether there are infinitely many perfect numbers remains out of reach for best mathematicians of today. The success of human research efforts has been as much a product of wisdom in choosing one's battles as it has been a product of ambition.</p>\n<p><strong id=\"The_Case_of_Friendly_AI\">The Case of Friendly AI</strong></p>\n<p>My present understanding is that there are potential avenues for researching AGI. Richard Hollerith was kind enough to briefly describe Monte Carlo AIXI to me last month and I could sort of see how it might be in principle possible to program a computer to do Bayesian induction according to an approximation to a universal prior and implement the computer with a decision making apparatus based on its epistemological state at a given time. Some people have suggested to me that the amount of computer power and memory needed to implement human level Monte Carlo AIXI is prohibitively large but (in my current, very ill-informed state; by analogy with things that I've seen in computational complexity theory) I could imagine ingenious tricks yielding an approximation to Monte Carlo AIXI which uses much less computing power/memory and which is a sufficiently close to approximation to serve as a substitute for practical purposes. This would point to a potential taskification of the problem of building an AGI. I could also imagine that there are presently no practically feasible AGI research programs; I know too little about the state of strong artificial intelligence research to have anything but a very unstable opinion on this matter.</p>\n<p>As Eliezer has said; the problem of creating a <em>Friendly</em> AI is inherently more difficult than that of creating an AGI and may be a problem <em>much more </em>difficult than that of creating an AGI. At present, the <em>Friendliness </em>aspect of a Friendly AI seems to me to strongly resist taskificaiton. In his poetic <a href=\"/lw/tb/mirrors_and_paintings/\">Mirrors and Paintings</a> Eliezer gives the most detailed description of what a Friendly AI should do that I've seen, but the gap between concept and implementation here seems so staggeringly huge that it doesn't suggest to me any fruitful lines of Friendly AI research. As far as I can tell, Eliezer's idea of a Friendly AI is at this point not significantly more fleshed out (relative to the magnitude of the task) than Freeman Dyson's idea of a <a href=\"http://en.wikipedia.org/wiki/Dyson_sphere\">Dyson sphere</a>. In order to build a Friendly AI, beyond conceiving of what a Friendly AI should be in the abstract one has to <em>convert one's intuitive understanding of friendliness into computer code in a formal programming language</em>.</p>\n<p>I don't even see how one would start to research the problem of getting a hypothetical AGI to recognize humans as distinguished beings. Solving this problem would seem to require as a prerequisite an understanding of the make up of the hypothetical AGI; something which people don't seem to have a clear grasp of at the moment. Even if one does have a model for a hypothetical AGI, writing code conducive to it recognizing humans as distinguished beings seems like an intractable task. And even with a relatively clear understanding of how one would implement a hypothetical AGI with the ability to recognize humans as distinguished beings; one is still left with the problem of making such a hypothetical AGI Friendly toward such beings.</p>\n<p>In view of all this, working toward stable whole-brain emulation of a a trusted and highly intelligent person concerned about human well being seems to me like a more promising strategy of reducing existential risk at the present time than researching Friendly AI. Quoting <a href=\"/lw/1s3/hedging_our_bets_the_case_for_pursuing_whole/1p0v?c=1\">a comment by Carl Shulman</a></p>\n<blockquote>\n<p>Emulations could [...] enable the creation of a singleton capable of globally balancing AI development speeds and dangers. That singleton could then take billions of subjective years to work on designing safe and beneficial AI. If designing safe AI is much, much harder than building AI at all, or if knowledge of AI and safe AI are tightly coupled, such a singleton might be the most likely route to a good outcome.</p>\n</blockquote>\n<p>There are various things that could go wrong with whole-brain emulation and it would be good to have a better option but Friendly AI research doesn't seem to me to be one in light of an apparent total absence of even the outlines of a viable Friendly AI research program.</p>\n<p>But I feel like I may have missed something here. I'd welcome any clarifications of what people who are interested in Friendly AI research <em>mean</em> by Friendly AI research. In particular, is there a conjectural taskification of the problem?</p>", "sections": [{"title": "The Difficulty of Unsolved Problems", "anchor": "The_Difficulty_of_Unsolved_Problems", "level": 1}, {"title": "The Case of Friendly AI", "anchor": "The_Case_of_Friendly_AI", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Pmfk7ruhWaHj9diyv", "AhHhm63zdZSDLmb76", "jNAAZ9XNyt82CXosr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-14T22:50:52.864Z", "modifiedAt": "2020-12-03T22:09:26.469Z", "url": null, "title": "One Chance (a short flash game)", "slug": "one-chance-a-short-flash-game", "viewCount": null, "lastCommentedAt": "2010-12-18T18:59:18.253Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "datadataeverywhere", "createdAt": "2010-08-06T03:09:35.035Z", "isAdmin": false, "displayName": "datadataeverywhere"}, "userId": "wKbyzrGvXH77dFfZX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/scqg6TA73DehceoY8/one-chance-a-short-flash-game", "pageUrlRelative": "/posts/scqg6TA73DehceoY8/one-chance-a-short-flash-game", "linkUrl": "https://www.lesswrong.com/posts/scqg6TA73DehceoY8/one-chance-a-short-flash-game", "postedAtFormatted": "Tuesday, December 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20Chance%20(a%20short%20flash%20game)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20Chance%20(a%20short%20flash%20game)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fscqg6TA73DehceoY8%2Fone-chance-a-short-flash-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20Chance%20(a%20short%20flash%20game)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fscqg6TA73DehceoY8%2Fone-chance-a-short-flash-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fscqg6TA73DehceoY8%2Fone-chance-a-short-flash-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p><a href=\"http://www.newgrounds.com/portal/view/555181\">http://www.newgrounds.com/portal/view/555181</a></p>\n<p>&nbsp;</p>\n<p>It needn't take more than 10 minutes to play, though it might if you nail-bite about your choices. I'm curious about the LW response, although I might be&nbsp;underwhelmed&nbsp;by lack of interest.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "scqg6TA73DehceoY8", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 17, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "4284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-12-14T22:50:52.864Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-15T02:14:00.202Z", "modifiedAt": null, "url": null, "title": "Introduction to Game Theory (Links)", "slug": "introduction-to-game-theory-links", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Zzst4qemGKaFAPKno", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/452ZMHWHBKvnv4GPx/introduction-to-game-theory-links", "pageUrlRelative": "/posts/452ZMHWHBKvnv4GPx/introduction-to-game-theory-links", "linkUrl": "https://www.lesswrong.com/posts/452ZMHWHBKvnv4GPx/introduction-to-game-theory-links", "postedAtFormatted": "Wednesday, December 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introduction%20to%20Game%20Theory%20(Links)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroduction%20to%20Game%20Theory%20(Links)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F452ZMHWHBKvnv4GPx%2Fintroduction-to-game-theory-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introduction%20to%20Game%20Theory%20(Links)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F452ZMHWHBKvnv4GPx%2Fintroduction-to-game-theory-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F452ZMHWHBKvnv4GPx%2Fintroduction-to-game-theory-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p>Reading the <a href=\"/r/discussion/lw/3ag/what_topics_would_you_like_to_see_more_of_on/\">What topics would you like to see more of on LessWrong?</a> thread gave me the impression that many people here would appreciate introductory material to several of the topics that are often discussed in lesswrong. I have therefore decided to link in the direction of the ECON 159 course lectures at <a href=\"http://oyc.yale.edu/economics/game-theory/contents/sessions.html\">Open Yale courses</a> and <a href=\"http://www.youtube.com/playlist?list=PL6EF60E1027E1A10B&amp;feature=plcp\">YouTube</a> and to the <a href=\"http://www.gametheory101.com/\">Game Theory 101</a> video series in hopes that people will find them useful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "452ZMHWHBKvnv4GPx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 6.56309136460799e-07, "legacy": true, "legacyId": "4285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bEEMXkZEkYRBEMXdr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-15T14:44:05.499Z", "modifiedAt": null, "url": null, "title": "Disruption of the right temporoparietal junction with transcranial magnetic stimulation reduces the role of beliefs in moral judgments", "slug": "disruption-of-the-right-temporoparietal-junction-with", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:00.330Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ukS7qTvrDGES8rF4k/disruption-of-the-right-temporoparietal-junction-with", "pageUrlRelative": "/posts/ukS7qTvrDGES8rF4k/disruption-of-the-right-temporoparietal-junction-with", "linkUrl": "https://www.lesswrong.com/posts/ukS7qTvrDGES8rF4k/disruption-of-the-right-temporoparietal-junction-with", "postedAtFormatted": "Wednesday, December 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Disruption%20of%20the%20right%20temporoparietal%20junction%20with%20transcranial%20magnetic%20stimulation%20reduces%20the%20role%20of%20beliefs%20in%20moral%20judgments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADisruption%20of%20the%20right%20temporoparietal%20junction%20with%20transcranial%20magnetic%20stimulation%20reduces%20the%20role%20of%20beliefs%20in%20moral%20judgments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FukS7qTvrDGES8rF4k%2Fdisruption-of-the-right-temporoparietal-junction-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Disruption%20of%20the%20right%20temporoparietal%20junction%20with%20transcranial%20magnetic%20stimulation%20reduces%20the%20role%20of%20beliefs%20in%20moral%20judgments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FukS7qTvrDGES8rF4k%2Fdisruption-of-the-right-temporoparietal-junction-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FukS7qTvrDGES8rF4k%2Fdisruption-of-the-right-temporoparietal-junction-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.pnas.org/content/107/15/6753.full\">http://www.pnas.org/content/107/15/6753.full</a></p>\n<p><a href=\"http://news.bbc.co.uk/2/hi/health/8593748.stm\">http://news.bbc.co.uk/2/hi/health/8593748.stm</a></p>\n<p><a href=\"http://news.ycombinator.com/item?id=2007859\">http://news.ycombinator.com/item?id=2007859</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ukS7qTvrDGES8rF4k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 6.564962270511293e-07, "legacy": true, "legacyId": "4291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-15T17:40:34.442Z", "modifiedAt": null, "url": null, "title": "Bullying the Integers", "slug": "bullying-the-integers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:05.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B4mthw3CHujRyFLNc/bullying-the-integers", "pageUrlRelative": "/posts/B4mthw3CHujRyFLNc/bullying-the-integers", "linkUrl": "https://www.lesswrong.com/posts/B4mthw3CHujRyFLNc/bullying-the-integers", "postedAtFormatted": "Wednesday, December 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bullying%20the%20Integers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABullying%20the%20Integers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4mthw3CHujRyFLNc%2Fbullying-the-integers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bullying%20the%20Integers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4mthw3CHujRyFLNc%2Fbullying-the-integers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4mthw3CHujRyFLNc%2Fbullying-the-integers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>\n<p>So, the FBI allegedly <a href=\"http://arstechnica.com/open-source/news/2010/12/fbi-accused-of-planting-backdoor-in-openbsd-ipsec-stack.ars\">arranged for a number of backdoors to be built into the OpenBSD IPSEC stack</a>. &nbsp;I don't really know how credible this claim is, but it sparked a discussion in my office about digital security, and encryption in general. &nbsp;One of my colleagues said something to the effect of it only being a matter of time before they found a way to easily break RSA.</p>\n<p>It was at about this moment that time stopped.</p>\n<p>I responded with something I thought was quite lucid, but there's only so much lay interest that can be held in a sentence that includes the phrases \"fact about all integers\" and \"solvable in polynomial time\". &nbsp;The basic thrust of my argument was that it wasn't something he could just <em>decide</em> an answer to, but I don't think he'll be walking away any the more enlightened.</p>\n<p>This got me wondering: do arguments that sit on cast-iron facts (or lack thereof) about number theory <em>feel</em> any different when you're making them, compared to arguments that sit on facts about the world you're just extremely confident about?</p>\n<p>If I have a discussion with someone about taxation it has no more consequence than a discussion about cryptography, but the tax discussion feels more urgent. &nbsp;Someone walking around with wonky ideas about fiscal policy seems more distressing than someone walking around with wonky ideas about modular arithmetic. &nbsp;Modular arithmetic can look after itself, but fiscal policy is somehow more vulnerable to bad ideas.</p>\n<p>Do your arguments feel different?</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B4mthw3CHujRyFLNc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 6.565402598259709e-07, "legacy": true, "legacyId": "4292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-15T19:43:11.447Z", "modifiedAt": null, "url": null, "title": "(Some) Singularity Summit 2010 videos now up", "slug": "some-singularity-summit-2010-videos-now-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:36.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fW9WMvwsQp2H7siQn/some-singularity-summit-2010-videos-now-up", "pageUrlRelative": "/posts/fW9WMvwsQp2H7siQn/some-singularity-summit-2010-videos-now-up", "linkUrl": "https://www.lesswrong.com/posts/fW9WMvwsQp2H7siQn/some-singularity-summit-2010-videos-now-up", "postedAtFormatted": "Wednesday, December 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20(Some)%20Singularity%20Summit%202010%20videos%20now%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A(Some)%20Singularity%20Summit%202010%20videos%20now%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfW9WMvwsQp2H7siQn%2Fsome-singularity-summit-2010-videos-now-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=(Some)%20Singularity%20Summit%202010%20videos%20now%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfW9WMvwsQp2H7siQn%2Fsome-singularity-summit-2010-videos-now-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfW9WMvwsQp2H7siQn%2Fsome-singularity-summit-2010-videos-now-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p><a href=\"http://vimeo.com/siai/videos\">Videos</a> of some of the talks and panel discussions (currently <span style=\"text-decoration: line-through;\">eight</span>&nbsp;twelve of them) from this year's <a href=\"http://www.singularitysummit.com/\">Singularity Summit</a> are now online.</p>\n<p>Michael Vassar:<br /><a href=\"http://vimeo.com/17512853\">The Darwinian Method</a></p>\n<p>Eliezer Yudkowsky:<br /><a href=\"http://vimeo.com/17513355\">Simplified Humanism and Positive Futurism</a></p>\n<p>Demis Hassabis:<br /><a href=\"http://vimeo.com/17513841\">Combining systems neuroscience and machine learning: a new approach to AGI</a></p>\n<p>Shane Legg:<br /><a href=\"http://vimeo.com/17553536\">Universal measures of intelligence</a></p>\n<p>Debate: Terry Sejnowski and Dennis Bray<br /><a href=\"http://vimeo.com/17700122\">Will we soon realistically emulate biological systems?</a></p>\n<p>Jose Cordeiro:<br /><a href=\"http://vimeo.com/17700668\">The Future of Energy and the Energy of the Future</a></p>\n<p>Panel: John Tooby, Ben Goertzel, Eliezer Yudkowsky, and Shane Legg<br /><a href=\"http://vimeo.com/17702682\">Narrow and General Intelligence</a></p>\n<p>Ray Kurzweil:<br /><a href=\"http://vimeo.com/17792087\">The Mind and How to Build One</a></p>\n<p>Gregory Stock:<br /><a href=\"http://vimeo.com/18141173\">Evolution of Post-Human Intelligence</a></p>\n<p>Ramez Naam:<br /><a href=\"http://vimeo.com/18141726\">The Digital Biome</a></p>\n<p>Ben Goertzel:<br /><a href=\"http://vimeo.com/18143134\">AI Against Aging</a></p>\n<p>Dennis Bray:<br /><a href=\"http://vimeo.com/18143991\">What Cells Can Do That Robots Can't</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fW9WMvwsQp2H7siQn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 6.565708560796345e-07, "legacy": true, "legacyId": "4293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T01:12:59.097Z", "modifiedAt": null, "url": null, "title": "Link: My something-like-Friendliness-research blog", "slug": "link-my-something-like-friendliness-research-blog", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.401Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/775AJbZQFDmfcJS9b/link-my-something-like-friendliness-research-blog", "pageUrlRelative": "/posts/775AJbZQFDmfcJS9b/link-my-something-like-friendliness-research-blog", "linkUrl": "https://www.lesswrong.com/posts/775AJbZQFDmfcJS9b/link-my-something-like-friendliness-research-blog", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20My%20something-like-Friendliness-research%20blog&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20My%20something-like-Friendliness-research%20blog%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F775AJbZQFDmfcJS9b%2Flink-my-something-like-friendliness-research-blog%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20My%20something-like-Friendliness-research%20blog%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F775AJbZQFDmfcJS9b%2Flink-my-something-like-friendliness-research-blog", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F775AJbZQFDmfcJS9b%2Flink-my-something-like-friendliness-research-blog", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>If you want to know what that something-like-Friendliness is, you can read the blog! You can find it here:&nbsp;<a href=\"http://willnewsome.wordpress.com/\">http://willnewsome.wordpress.com/</a>&nbsp;(About page here:&nbsp;<a href=\"http://willnewsome.wordpress.com/about/\">http://willnewsome.wordpress.com/about/</a>&nbsp;)</p>\n<p>Anyone who's bothered to notice the trend of my posts and comments to Less Wrong has probably noticed that I aim to be as metacontrarian and contentious as possible. Other times I run small social experiments. Sometimes this is interesting, sometimes it's probably just frustrating, but I do hope it's at least thought-provoking. With my blog I'm trying to showcase interesting ideas more than bring up counterintuitive alternatives, so perhaps those who don't generally like my posts/comments will still find my blog tolerable. It's also about something that I take more seriously than other rationality-related topics, that is, building an AI that does what we want it to do.</p>\n<p>The names of of the posts I've put up already: \"What are humans?\", \"Are evolved drives satiable?\", \"Why extrapolate?\", and \"Gene/meme/teme sanity equilibria\".</p>\n<p>I hope to get a new post out every few days, but honestly I have no idea if I'll succeed in that. At the very least I have a few weeks' worth of cached ideas to post, and I'll continue studying related things in the meantime.&nbsp;</p>\n<p>I'd really like anyone else who has a blog about anything mildly related to rationality to post a link in their own discussion post. Currently I only know to follow Vladimir Nesov and Luke Grecki (whose blogs are linked to from mine), and my RSS feed has room for many more.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "775AJbZQFDmfcJS9b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "4294", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T03:02:15.107Z", "modifiedAt": null, "url": null, "title": "Varying amounts of subjective experience", "slug": "varying-amounts-of-subjective-experience", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RHPxkWz4WhrzM6Q8H/varying-amounts-of-subjective-experience", "pageUrlRelative": "/posts/RHPxkWz4WhrzM6Q8H/varying-amounts-of-subjective-experience", "linkUrl": "https://www.lesswrong.com/posts/RHPxkWz4WhrzM6Q8H/varying-amounts-of-subjective-experience", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Varying%20amounts%20of%20subjective%20experience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVarying%20amounts%20of%20subjective%20experience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRHPxkWz4WhrzM6Q8H%2Fvarying-amounts-of-subjective-experience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Varying%20amounts%20of%20subjective%20experience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRHPxkWz4WhrzM6Q8H%2Fvarying-amounts-of-subjective-experience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRHPxkWz4WhrzM6Q8H%2Fvarying-amounts-of-subjective-experience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>It has been suggested that animals have less subjective experience than people. For example, it would be possible to have an animal that counts as half a human for the purposes of morality. This is an argument as to why that may be the case.</p>\n<p>If you're moving away from Earth at 87% of the speed of light, time dilation would make it look like time on Earth is passing half as fast. From your point of reference, everyone will live twice as long. This obviously won't change the number of life years they live. You can't double the amount of good in the world just by moving at 87% the speed of light. It's possible that there's just a preferred point of reference, and everything is based on people's speed relative to that, but I doubt it.</p>\n<p>No consider if their brains were slowed down a different way. Suppose you uploaded someone, and made the simulation run at half speed. Would they experience a life twice as long? This seems to be just slowing it down a different way. I doubt it would change the total amount experienced.</p>\n<p>If that's true, it means that sentience isn't something you either have or don't have. There can be varying amounts of it. Also, someone whose brain has been slowed down would be less intelligent by most measures, so this is some evidence that subjective experience correlates with intelligence.</p>\n<p>Edit: replaced \"sentience\" with the more accurate \"subjective experience\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RHPxkWz4WhrzM6Q8H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -6, "extendedScore": null, "score": -1.4e-05, "legacy": true, "legacyId": "4296", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T03:06:07.660Z", "modifiedAt": null, "url": null, "title": "Confidence levels inside and outside an argument", "slug": "confidence-levels-inside-and-outside-an-argument", "viewCount": null, "lastCommentedAt": "2022-03-11T03:03:13.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GrtbTAPfkJa4D6jjH/confidence-levels-inside-and-outside-an-argument", "pageUrlRelative": "/posts/GrtbTAPfkJa4D6jjH/confidence-levels-inside-and-outside-an-argument", "linkUrl": "https://www.lesswrong.com/posts/GrtbTAPfkJa4D6jjH/confidence-levels-inside-and-outside-an-argument", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confidence%20levels%20inside%20and%20outside%20an%20argument&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfidence%20levels%20inside%20and%20outside%20an%20argument%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGrtbTAPfkJa4D6jjH%2Fconfidence-levels-inside-and-outside-an-argument%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confidence%20levels%20inside%20and%20outside%20an%20argument%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGrtbTAPfkJa4D6jjH%2Fconfidence-levels-inside-and-outside-an-argument", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGrtbTAPfkJa4D6jjH%2Fconfidence-levels-inside-and-outside-an-argument", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1879, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/mo/infinite_certainty/\">Infinite Certainty</a></p>\n<p>Suppose the people at <a href=\"http://fivethirtyeight.blogs.nytimes.com/\">FiveThirtyEight</a> have created a model to predict the results of an important election. After crunching poll data, area demographics, and all the usual things one crunches in such a situation, their model returns a greater than 999,999,999 in a billion chance that the incumbent wins the election. Suppose further that the results of this model are your only data and you know nothing else about the election. What is your confidence level that the incumbent wins the election?<br /><br />Mine would be significantly less than 999,999,999 in a billion.</p>\n<p>When an argument gives a probability of 999,999,999 in a billion for an event, then probably the majority of the probability of the event is no longer in \"But that still leaves a one in a billion chance, right?\". The majority of the probability is in \"That argument is flawed\". Even if you have no particular reason to believe the argument is flawed, the background chance of an argument being flawed is still greater than one in a billion.</p>\n<p><br />More than one in a billion times a political scientist writes a model, ey will get completely confused and write something with no relation to reality. More than one in a billion times a programmer writes a program to crunch political statistics, there will be a bug that completely invalidates the results. More than one in a billion times a staffer at a website publishes the results of a political calculation online, ey will accidentally switch which candidate goes with which chance of winning.<br /><br />So one must distinguish between levels of confidence internal and external to a specific model or argument. Here the model's internal level of confidence is 999,999,999/billion. But my external level of confidence should be lower, even if the model is my only evidence, by an amount proportional to my trust in the model.</p>\n<p><a id=\"more\"></a><br /><br /><strong>Is That Really True?</strong></p>\n<p>One might be tempted to respond \"But there's an equal chance that the false model is too high, versus that it is too low.\" Maybe there was a bug in the computer program, but it prevented it from giving the incumbent's real chances of 999,999,999,999 out of a <em>trillion</em>.<br /><br />The prior probability of a candidate winning an election is 50%<sup>1</sup>. We need information to push us away from this probability in either direction. To push significantly away from this probability, we need strong information. Any weakness in the information weakens its ability to push away from the prior. If there's a flaw in FiveThirtyEight's model, that takes us away from their probability of 999,999,999 in of a billion, and back closer to the prior probability of 50%<br /><br />We can confirm this with a quick sanity check. Suppose we know nothing about the election (ie we still think it's 50-50) until an insane person reports a hallucination that an angel has declared the incumbent to have a 999,999,999/billion chance. We would not be tempted to accept this figure on the grounds that it is equally likely to be too high as too low.<br /><br />A second objection covers situations such as a lottery. I would like to say the chance that Bob wins a lottery with one billion players is 1/1 billion. Do I have to adjust this upward to cover the possibility that my model for how lotteries work is somehow flawed? No. Even if I am misunderstanding the lottery, I have not departed from my prior. Here, new information really does have an equal chance of going against Bob as of going in his favor. For example, the lottery may be fixed (meaning my original model of how to determine lottery winners is fatally flawed), but there is no greater reason to believe it is fixed in favor of Bob than anyone else.<sup>2</sup><br /><br /><strong>Spotted in the Wild</strong><br /><br />The recent Pascal's Mugging thread spawned a discussion of the Large Hadron Collider destroying the universe, which also got continued on an older LHC thread from a few years ago. Everyone involved agreed the chances of the LHC destroying the world were less than one in a million, but several people gave extraordinarily low chances based on cosmic ray collisions. The argument was that since cosmic rays have been performing particle collisions similar to the LHC's zillions of times per year, the chance that the LHC will destroy the world is either literally zero, or else a number related to the probability that there's some chance of a cosmic ray destroying the world so miniscule that it hasn't gotten actualized in zillions of cosmic ray collisions. Of the commenters mentioning this argument, one gave a probability of 1/3*10^22, another suggested 1/10^25, both of which may be good numbers for the internal confidence of this argument.<br /><br />But the connection between this argument and the general LHC argument flows through statements like \"collisions produced by cosmic rays will be exactly like those produced by the LHC\", \"our understanding of the properties of cosmic rays is largely correct\", and \"I'm not high on drugs right now, staring at a package of M&amp;Ms and mistaking it for a really intelligent argument that bears on the LHC question\", all of which are probably more likely than 1/10^20. So instead of saying \"the probability of an LHC apocalypse is now 1/10^20\", say \"I have an argument that has an internal probability of an LHC apocalypse as 1/10^20, which lowers my probability a bit depending on how much I trust that argument\".<br /><br />In fact, the argument has a potential flaw: according to Giddings and Mangano, the physicists officially tasked with investigating LHC risks, black holes from cosmic rays <a href=\"http://arxiv.org/ftp/arxiv/papers/0912/0912.5480.pdf\">might have enough momentum</a> to fly through Earth without harming it, and black holes from the LHC might not<sup>3</sup>. This was predictable: this was a simple argument in a complex area trying to prove a negative, and it would have been presumptous to believe with greater than 99% probability that it was flawless. If you can only give 99% probability to the argument being sound, then it can only reduce your probability in the conclusion by a factor of a hundred, not a factor of 10^20.<br /><br />But it's hard for me to be properly outraged about this, since the LHC did not destroy the world. A better example might be the following, taken from an online <a href=\"http://www.sciforums.com/Scientific-Reasons-for-God-t-44465.html\">discussion of creationism</a><sup>4</sup> and apparently based off of something by Fred Hoyle:</p>\n<blockquote>\n<p>In order for a single cell to live, all of the parts of the cell must be assembled before life starts. This involves 60,000 proteins that are assembled in roughly 100 different combinations. The probability that these complex groupings of proteins could have happened just by chance is extremely small. It is about 1 chance in 10 to the 4,478,296 power. The probability of a living cell being assembled just by chance is so small, that you may as well consider it to be impossible. This means that the probability that the living cell is created by an intelligent creator, that designed it, is extremely large. The probability that God created the living cell is 10 to the 4,478,296 power to 1.</p>\n</blockquote>\n<p>Note that someone just gave a confidence level of 10^4478296 to one and was wrong. This is the sort of thing that should <em>never ever happen</em>. This is possibly the <em>most wrong anyone has ever been</em>.<br /><br />It is hard to say in words exactly how wrong this is. Saying \"This person would be willing to bet the entire world GDP for a thousand years if evolution were true against a one in one million chance of receiving a single penny if creationism were true\" doesn't even begin to cover it: a mere 1/10^25 would suffice there. Saying \"This person believes he could make one statement about an issue as difficult as the origin of cellular life per Planck interval, every Planck interval from the Big Bang to the present day, and not be wrong even once\" only brings us to 1/10^61 or so. If the chance of getting <a href=\"http://en.wikipedia.org/wiki/Ganser%27s_syndrome\">Ganser's Syndrome</a>, the extraordinarily rare psychiatric condition that manifests in a compulsion to say false statements, is one in a hundred million, and the world's top hundred thousand biologists all agree that evolution is true, then this person should preferentially believe it is more likely that all hundred thousand have simultaneously come down with Ganser's Syndrome than that they are doing good biology<sup>5</sup><br /><br />This creationist's flaw wasn't mathematical; the math probably does return that number. The flaw was confusing the internal probability (that complex life would form completely at random in a way that can be represented with this particular algorithm) with the external probability (that life could form without God). He should have added a term representing the chance that his knockdown argument just didn't apply.<br /><br />Finally, consider the question of whether you can assign 100% certainty to a mathematical theorem for which a proof exists. Eliezer <a href=\"/lw/mo/infinite_certainty/ \">has already examined this issue</a> and come out against it (citing as an example <a href=\"http://www.spaceandgames.com/?p=27\">this story of Peter de Blanc's</a>). In fact, this is just the specific case of differentiating internal versus external probability when internal probability is equal to 100%. Now your probability that the theorem is false is entirely based on the probability that you've made some mistake.<br /><br />The many <a href=\"http://mathoverflow.net/questions/35468\">mathematical proofs</a> <a href=\"http://en.wikipedia.org/wiki/List_of_published_incomplete_proofs \">that were later overturned</a> provide practical justification for this mindset.<br /><br />This is not a fully general argument against giving very high levels of confidence: very complex situations and situations with many exclusive possible outcomes (like the lottery example) may still make it to the 1/10^20 level, albeit probably not the 1/10^4478296. But in other sorts of cases, giving a very high level of confidence requires a check that you're not confusing the probability inside one argument with the probability of the question as a whole.</p>\n<p><strong>Footnotes</strong></p>\n<p><strong>1.</strong> Although technically we know we're talking about an incumbent, who typically has a much higher chance, around 90% in Congress.<br /><br /><strong>2.</strong> A particularly devious objection might be \"What if the lottery commissioner, in a fit of political correctness, decides that \"everyone is a winner\" and splits the jackpot a billion ways? If this would satisfy your criteria for \"winning the lottery\", then this mere possibility should indeed move your probability upward. In fact, since there is probably greater than a one in one billion chance of this happening, the majority of your probability for Bob winning the lottery should concentrate here!<br /><br /><strong>3.</strong> Giddings and Mangano then go on to re-prove the original \"won't cause an apocalypse\" argument using a more complicated method involving white dwarf stars. <br /><br /><strong>4.</strong> While searching creationist websites for the half-remembered argument I was looking for, I <a href=\"http://www.christiananswers.net/q-eden/origin-of-life-ref.html\">found</a> what may be my new favorite quote: \"Mathematicians generally agree that, statistically, any odds beyond 1 in 10 to the 50th have a zero probability of ever happening.\"&nbsp; <br /><br /><strong>5.</strong> I'm a little worried that five years from now I'll see this quoted on some creationist website as an actual argument.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 2, "8daMDi9NEShyLqxth": 2, "Ng8Gice9KNkncxqcj": 2, "rWzGNdjuep56W5u2d": 6, "AHK82ypfxF45rqh9D": 6, "QT87jxkk6DXuS8hGA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GrtbTAPfkJa4D6jjH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 169, "baseScore": 217, "extendedScore": null, "score": 0.000388, "legacy": true, "legacyId": "4298", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "TQW9brvXJ5Fajorr4", "canonicalCollectionSlug": "codex", "canonicalBookId": "jF58hKP9ZLzgy22Jr", "canonicalNextPostSlug": "the-logician-and-the-god-emperor", "canonicalPrevPostSlug": "techniques-for-probability-estimates", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 217, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 193, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ooypcn7qFzsMcy53R"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 16, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T03:20:33.153Z", "modifiedAt": null, "url": null, "title": "Link: \"A Bayesian Take on Julian Assange\"", "slug": "link-a-bayesian-take-on-julian-assange", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bjAdCcFBFTHMaTXeo/link-a-bayesian-take-on-julian-assange", "pageUrlRelative": "/posts/bjAdCcFBFTHMaTXeo/link-a-bayesian-take-on-julian-assange", "linkUrl": "https://www.lesswrong.com/posts/bjAdCcFBFTHMaTXeo/link-a-bayesian-take-on-julian-assange", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20%22A%20Bayesian%20Take%20on%20Julian%20Assange%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20%22A%20Bayesian%20Take%20on%20Julian%20Assange%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjAdCcFBFTHMaTXeo%2Flink-a-bayesian-take-on-julian-assange%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20%22A%20Bayesian%20Take%20on%20Julian%20Assange%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjAdCcFBFTHMaTXeo%2Flink-a-bayesian-take-on-julian-assange", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjAdCcFBFTHMaTXeo%2Flink-a-bayesian-take-on-julian-assange", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 11, "htmlBody": "<p>From Nate Silver of FiveThirtyEight: <a href=\"http://fivethirtyeight.blogs.nytimes.com/2010/12/15/a-bayesian-take-on-julian-assange/\">A Bayesian Take on Julian Assange</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bjAdCcFBFTHMaTXeo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.566850026737871e-07, "legacy": true, "legacyId": "4299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T07:27:07.332Z", "modifiedAt": null, "url": null, "title": "Expansion of \"Cached thought\" wiki entry", "slug": "expansion-of-cached-thought-wiki-entry", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Davorak", "createdAt": "2010-11-15T05:00:53.160Z", "isAdmin": false, "displayName": "Davorak"}, "userId": "TiTCTMF8wQ67w4deL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yQ5ZWzNipAych24XW/expansion-of-cached-thought-wiki-entry", "pageUrlRelative": "/posts/yQ5ZWzNipAych24XW/expansion-of-cached-thought-wiki-entry", "linkUrl": "https://www.lesswrong.com/posts/yQ5ZWzNipAych24XW/expansion-of-cached-thought-wiki-entry", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Expansion%20of%20%22Cached%20thought%22%20wiki%20entry&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExpansion%20of%20%22Cached%20thought%22%20wiki%20entry%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQ5ZWzNipAych24XW%2Fexpansion-of-cached-thought-wiki-entry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Expansion%20of%20%22Cached%20thought%22%20wiki%20entry%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQ5ZWzNipAych24XW%2Fexpansion-of-cached-thought-wiki-entry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQ5ZWzNipAych24XW%2Fexpansion-of-cached-thought-wiki-entry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 234, "htmlBody": "<p><a href=\"http://wiki.lesswrong.com/wiki/Cached_thought\">\"Cached Thought\" wiki entry</a>&nbsp;has been&nbsp;copied&nbsp;below for you&nbsp;connivance.</p>\n<blockquote>\n<p>&nbsp;</p>\n<p style=\"margin-top: 0.4em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.5em; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">A&nbsp;<strong>cached thought</strong>&nbsp;is an answer that was arrived at by recalling the old conclusion, rather than performing the reasoning from scratch. Cached thoughts can result in the maintenance of a position when evidence should force an update. Cached thoughts can also result in a lack of creative approaches to problem-solving if one repeats the same cached thoughts rather than constructing a new approach.</p>\n<p style=\"margin-top: 0.4em; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.5em; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">What is generally called&nbsp;<a class=\"new\" style=\"color: #cc2200; text-decoration: underline;\" title=\"Common sense (page does not exist)\" href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=Common_sense&amp;action=edit&amp;redlink=1\">common sense</a>&nbsp;is more or less a collection of cached thoughts.</p>\n<p>&nbsp;</p>\n</blockquote>\n<p>The above entry focuses only on the negative sides of cached thought. Probably because it can be a large barrier to rationality. In order overcome this barrier, and/or help others overcome it, it is&nbsp;necessary&nbsp;to understand why \"cached thoughts\" have been&nbsp;historically&nbsp;valuable to our&nbsp;ancestors and in what&nbsp;fashions&nbsp;it is&nbsp;valuable&nbsp;today.</p>\n<p>'''Cached thought''' also allow for complex problems to be handled with a relatively small number of simple components. &nbsp;These problem components when put together only approximate the actual problem, because they are slightly flawed '''cached thoughts.''' Valid conclusions can be reached more quickly with these slightly flawed cached thoughts then without. The aforementioned conclusions should be recheck without using '''cached thoughts''' if a high probability of correctness is necessary or if the '''cached thoughts''' are more then slightly flawed.</p>\n<p>Is this an&nbsp;appropriate&nbsp;expansion of the wiki entry? The words are&nbsp;drawn from my&nbsp;observation&nbsp;of the world. How else should the above wiki entry be expanded?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yQ5ZWzNipAych24XW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.567465550828891e-07, "legacy": true, "legacyId": "4312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T13:50:40.319Z", "modifiedAt": null, "url": null, "title": "Username switch karma donation request", "slug": "username-switch-karma-donation-request", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.282Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zeyfsBbCZfQDHLvz4/username-switch-karma-donation-request", "pageUrlRelative": "/posts/zeyfsBbCZfQDHLvz4/username-switch-karma-donation-request", "linkUrl": "https://www.lesswrong.com/posts/zeyfsBbCZfQDHLvz4/username-switch-karma-donation-request", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Username%20switch%20karma%20donation%20request&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUsername%20switch%20karma%20donation%20request%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzeyfsBbCZfQDHLvz4%2Fusername-switch-karma-donation-request%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Username%20switch%20karma%20donation%20request%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzeyfsBbCZfQDHLvz4%2Fusername-switch-karma-donation-request", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzeyfsBbCZfQDHLvz4%2Fusername-switch-karma-donation-request", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>I decided the new me needs a different user name (megalomania, mostly), but I see no way to do this automatically. No important posts will be lost - I am mostly a lurker for now. So I am switching xamdam-&gt;Dr_Manhattan and requesting some karma donation (please upvote the comment below), at least to 20 points. xamdam's current carma is 712. Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zeyfsBbCZfQDHLvz4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -16, "extendedScore": null, "score": -2.8e-05, "legacy": true, "legacyId": "4319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T19:27:11.084Z", "modifiedAt": null, "url": null, "title": "What do you mean by rationalism?", "slug": "what-do-you-mean-by-rationalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.863Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Marius", "createdAt": "2010-12-16T19:04:55.782Z", "isAdmin": false, "displayName": "Marius"}, "userId": "y2mqAAEmFDjPNXS3w", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6r5oojytLRb4b9QdF/what-do-you-mean-by-rationalism", "pageUrlRelative": "/posts/6r5oojytLRb4b9QdF/what-do-you-mean-by-rationalism", "linkUrl": "https://www.lesswrong.com/posts/6r5oojytLRb4b9QdF/what-do-you-mean-by-rationalism", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20do%20you%20mean%20by%20rationalism%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20do%20you%20mean%20by%20rationalism%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r5oojytLRb4b9QdF%2Fwhat-do-you-mean-by-rationalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20do%20you%20mean%20by%20rationalism%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r5oojytLRb4b9QdF%2Fwhat-do-you-mean-by-rationalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r5oojytLRb4b9QdF%2Fwhat-do-you-mean-by-rationalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p>I've been lurking here a bit, and am trying to understand what people here mean by rationalism.&nbsp; Many articles here seem to refer to discussion participants as rationalist while meaning very seemingly-different things, including intelligent, socially awkward, well-educated, and unencumbered by education.&nbsp; I'm trying to make a little more sense of the word/concept.</p>\n<p>Surely it does not refer to rationalist in the empiricism/rationalism divide, because it doesn't seem to be used in quite that way.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6r5oojytLRb4b9QdF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 6.569263665471977e-07, "legacy": true, "legacyId": "4321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T22:28:29.049Z", "modifiedAt": null, "url": null, "title": "December 2010 Southern California Meetup", "slug": "december-2010-southern-california-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.626Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JenniferRM", "createdAt": "2009-03-06T17:16:50.600Z", "isAdmin": false, "displayName": "JenniferRM"}, "userId": "g8JkZfL8PTqAefpvx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2eWwqcrAKMZifxQCE/december-2010-southern-california-meetup", "pageUrlRelative": "/posts/2eWwqcrAKMZifxQCE/december-2010-southern-california-meetup", "linkUrl": "https://www.lesswrong.com/posts/2eWwqcrAKMZifxQCE/december-2010-southern-california-meetup", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20December%202010%20Southern%20California%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecember%202010%20Southern%20California%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eWwqcrAKMZifxQCE%2Fdecember-2010-southern-california-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=December%202010%20Southern%20California%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eWwqcrAKMZifxQCE%2Fdecember-2010-southern-california-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eWwqcrAKMZifxQCE%2Fdecember-2010-southern-california-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>A meetup in Southern California will occur on Sunday December 19, 2010.&nbsp; The meetup will start around 3:30PM and run for at least 2 hours and possibly 4 or 5.&nbsp; Anna Salamon and Yvain are very likely to be in attendance, as well as people from the last few meetups who may have projects to talk about, if are people are interested.&nbsp; Bring guests if you like.&nbsp; The location of the meetup will be...</p>\n<p>...at <a href=\"http://maps.google.com/maps?hl=en&amp;ie=UTF8&amp;q=18542+Macarthur+Blvd+Irvine+CA,+92612+ihop&amp;fb=1&amp;gl=us&amp;hq=ihop&amp;hnear=18542+MacArthur+Blvd,+Irvine,+CA+92612&amp;cid=0,0,16042522294109526569&amp;ei=uLa8TOG7MoWCsQO1nM2TDw&amp;ved=0CBYQnwIwAA&amp;ll=33.679247,-117.859418&amp;spn=0.009285,0.021136&amp;t=h&amp;z=16&amp;iwloc=A\">the IHOP</a> across from John Wayne Airport, about a mile from UC Irvine.</p>\n<p><a id=\"more\"></a></p>\n<p>For those interested in carpooling: (1) driver <em>needed</em> in Santa Barbara (please comment if you can make it!), (2) <a href=\"/r/JenniferRM-drafts/lw/3c2/december_2010_southern_california_meetup/360l?c=1\">driver available in San Diego</a>, (3) <a href=\"/lw/3c2/december_2010_southern_california_meetup/3615?c=1\">driver available in Lake Forest</a>, (4) <a href=\"/lw/3c2/december_2010_southern_california_meetup/3649?c=1\">driver available in Torrance</a>., (5) <a href=\"/lw/3c2/december_2010_southern_california_meetup/368y?c=1\">driver available in Huntington Beach</a>.</p>\n<p>The format for past meetups has varied based on the number of attendees and their interests, at various points we have either tried or considered: <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">paranoid debating</a>, small group \"dinner party conversations\", <a href=\"/lw/2ps/september_2010_southern_california_meetup/2mbq?c=1\">structured rationality exercises</a>, large discussions with people sharing personal experiences with sleep and \"nutraceutical\" interventions for intelligence augmentation, and specialized subprojects to develop tools for quantitatively estimating the value of things like <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a> or <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">existential risk</a> interventions.</p>\n<p>Apologies for the short notice.&nbsp; I was trying to call and email around to see what people's winter holiday schedules were like and wasn't sure if this would come together.&nbsp; Based on one-on-one conversations I think we'll have pretty good turnout and interesting stuff to talk about!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2eWwqcrAKMZifxQCE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 6.569716537123792e-07, "legacy": true, "legacyId": "4322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-16T23:09:34.694Z", "modifiedAt": null, "url": null, "title": "High Failure-Rate Solutions", "slug": "high-failure-rate-solutions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.269Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ccnw8CDyhKDcGH4jo/high-failure-rate-solutions", "pageUrlRelative": "/posts/ccnw8CDyhKDcGH4jo/high-failure-rate-solutions", "linkUrl": "https://www.lesswrong.com/posts/ccnw8CDyhKDcGH4jo/high-failure-rate-solutions", "postedAtFormatted": "Thursday, December 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20High%20Failure-Rate%20Solutions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHigh%20Failure-Rate%20Solutions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fccnw8CDyhKDcGH4jo%2Fhigh-failure-rate-solutions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=High%20Failure-Rate%20Solutions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fccnw8CDyhKDcGH4jo%2Fhigh-failure-rate-solutions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fccnw8CDyhKDcGH4jo%2Fhigh-failure-rate-solutions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>Short interview, doesn't go into too much depth, but makes an interesting point relevant to LW:</p>\n<p>\"<span style=\"font-family: Verdana, Geneva, sans-serif; font-size: 13px; color: #222222; line-height: 19px;\">When you&rsquo;re mitigating a complex problem, you become a bad picker. It&rsquo;s too complicated, so you can&rsquo;t separate the ideas that are going to work and the ones that won&rsquo;t. [...]&nbsp;</span><span style=\"font-family: Verdana, Geneva, sans-serif; font-size: 13px; color: #222222; line-height: 19px;\">But if we were gonna use the venture-capital method &ndash; if we were willing to admit we weren&rsquo;t competent pickers and even all the wise men gathering in the Oval Office were not going to be able to pick the winner from the loser &ndash; we would have gone in there with 30 ways of plugging up that hole at one time and realized that maybe 29 were gonna fail and one was going to stop the ecological disaster. But that&rsquo;s not what we did.\"</span></p>\n<p><a href=\"http://www.montereycountyweekly.com/archives/2010/2010-Nov-11/a-carmel-visionarys-debut-book-maps-out-how-we-can-save-our-society-from-collapse/1/\">http://www.montereycountyweekly.com/archives/2010/2010-Nov-11/a-carmel-visionarys-debut-book-maps-out-how-we-can-save-our-society-from-collapse/1/</a></p>\n<p>&nbsp;</p>\n<p>Are there any potential solutions other than \"create the first GAI and ensure it's provably Friendly\" that can be advanced&nbsp;simultaneously?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ccnw8CDyhKDcGH4jo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 6.569819194454821e-07, "legacy": true, "legacyId": "4325", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-17T05:09:05.005Z", "modifiedAt": null, "url": null, "title": "Singularity Institute pitch and new project plus other organizations in our ecosystem", "slug": "singularity-institute-pitch-and-new-project-plus-other", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5NMCHfTjiDRazfjcp/singularity-institute-pitch-and-new-project-plus-other", "pageUrlRelative": "/posts/5NMCHfTjiDRazfjcp/singularity-institute-pitch-and-new-project-plus-other", "linkUrl": "https://www.lesswrong.com/posts/5NMCHfTjiDRazfjcp/singularity-institute-pitch-and-new-project-plus-other", "postedAtFormatted": "Friday, December 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20pitch%20and%20new%20project%20plus%20other%20organizations%20in%20our%20ecosystem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20pitch%20and%20new%20project%20plus%20other%20organizations%20in%20our%20ecosystem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NMCHfTjiDRazfjcp%2Fsingularity-institute-pitch-and-new-project-plus-other%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20pitch%20and%20new%20project%20plus%20other%20organizations%20in%20our%20ecosystem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NMCHfTjiDRazfjcp%2Fsingularity-institute-pitch-and-new-project-plus-other", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5NMCHfTjiDRazfjcp%2Fsingularity-institute-pitch-and-new-project-plus-other", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>A week ago I made a pitch for the Singularity Institute to a crowd of interested potential donors, along with a number of leaders of other non-profit organizations with relatively radical and innovative goals.&nbsp; The videos are <a href=\"http://www.facebook.com/breakthroughphilanthropy?v=app_4949752878\">here</a>, and should be a good introduction to a significant part of the noosphere for people not yet familiar with it.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5NMCHfTjiDRazfjcp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 8, "extendedScore": null, "score": 6.570717397895385e-07, "legacy": true, "legacyId": "4329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-17T05:46:43.337Z", "modifiedAt": null, "url": null, "title": "\"Irrationality in Argument\"", "slug": "irrationality-in-argument", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.177Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "omslin", "createdAt": "2010-12-17T05:26:17.972Z", "isAdmin": false, "displayName": "omslin"}, "userId": "egz4Gvda3Ji2WdNgE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xppurdoNtytG5zzda/irrationality-in-argument", "pageUrlRelative": "/posts/xppurdoNtytG5zzda/irrationality-in-argument", "linkUrl": "https://www.lesswrong.com/posts/xppurdoNtytG5zzda/irrationality-in-argument", "postedAtFormatted": "Friday, December 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Irrationality%20in%20Argument%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Irrationality%20in%20Argument%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxppurdoNtytG5zzda%2Firrationality-in-argument%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Irrationality%20in%20Argument%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxppurdoNtytG5zzda%2Firrationality-in-argument", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxppurdoNtytG5zzda%2Firrationality-in-argument", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 664, "htmlBody": "<p>Here's a poetic blog post by Julian Assange (<a href=\"http://web.archive.org/web/20071020051936/http://iq.org/#Iirrationalityinargument\">source</a>). I found the first paragraph relevant:</p>\n<blockquote>\n<p>27 Aug 2007 - Irrationality in Argument</p>\n<p>The truth is not found on the page, but is a wayward sprite that bursts forth from the readers mind for reasons of its own. I once thought that the Truth was a set comprised of all the things that were true, and the big truth could be obtained by taking all its component propositions and evaluating them until nothing remained. I would approach my rhetorical battles as a logical reductionist, tearing down, atomizing, proving, disproving, discarding falsehoods and reassembling truths until the Truth was pure, golden and unarguable. But then, when truth matters most, when truth is the agent of freedom, I stood before Justice and with truth, lost freedom. Here was something fantastical, unbelievable and impossible, you could prove that (A =&gt; B) and (B =&gt; C) and (C =&gt; D) and (D =&gt; F) Justice would nod its head and agree, but then, when you turned to claim your coup de grace, A =&gt; F irrevocably, Justice would demur and revoke the axiom of transitivity, for Justice will not be told when F stands for freedom. Transitivity is evoked when Justice imagines F and finding the dream a pleasurable one sets about gathering cushions to prop up their slumber. Here then is the truth about the Truth; the Truth is not bridge, sturdy to every step, a marvel of bound planks and supports from the known into the unknown, but a surging sea of smashed wood, flotsam and drowning sailors. So first, always pick your poetic metaphor, to make the reader want to believe, then the facts, and -- miracle! -- transitivity will descend from heaven, invoked as justification for prejudice.</p>\n<p>Often we suffer to read, \"But if we believe X then we'll have to...\", or \"If we believe X it will lead to...\". This has no reflection on the veracity of X and so we see that outcomes are treated with more reverence than the Truth. It stings us, but natural selection has spun its ancestral yarns from physically realized outcomes, robustly eschewing the vapor thread of platonism as an abomination against the natural order, fit only for the gossip of monks and the page.</p>\n<p>Yet just as we feel all hope is lost and we sink back into the miasma, back to the shadow world of ghosts and gods, a miracle arises; everywhere before the direction of self interest is known, people yearn to see where its compass points and then they hunger for truth with passion and beauty and insight. He loves me. He loves me not. Here then is the truth to set them free. Free from the manipulations and constraints of the mendacious. Free to choose their path, free to remove the ring from their noses, free to look up into the infinite voids and choose wonder over whatever gets them though. And before this feeling to cast blessings on the profits and prophets of truth, on the liberators and martyrs of truth, on the Voltaires, Galileos, and Principias of truth, on the Gutenburgs, Marconis and Internets of truth, on those serial killers of delusion, those brutal, driven and obsessed miners of reality, smashing, smashing, smashing every rotten edifice until all is ruins and the seeds of the new.</p>\n</blockquote>\n<p>I've only read a few Less Wrong articles so far, but the first paragraph easily follows from my current models. Since <a href=\"/lw/2yp/making_your_explicit_reasoning_trustworthy/\">explicit reasoning easily fails</a>, people quite understandably refuse to accept arguments based on transitivity. So in order to be understood, one should carefully craft arguments <a href=\"/lw/k8/how_to_seem_and_be_deep/\">one inferential step away</a> from the audience's current mental state. A metaphor, because of all the ideas it simultaneously evokes, powerfully takes advantage of a brain's <a href=\"/lw/k5/cached_thoughts/\">multiprocessing ability</a>. That's why a metaphor can remove inferential steps and be an excellent way of bringing us to our senses and making us reconsider a vast network of cached knowledge.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xppurdoNtytG5zzda", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 4, "extendedScore": null, "score": 6.570811449037364e-07, "legacy": true, "legacyId": "4330", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["m5AH78nscsGjMbBwv", "aSQy7yHj6nPD44RNo", "2MD3NMLBPCqPfnfre"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-17T11:36:32.114Z", "modifiedAt": null, "url": null, "title": "The problem of mankind indestructibility in disastrously unpredictable environment", "slug": "the-problem-of-mankind-indestructibility-in-disastrously", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:00.591Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kononov", "createdAt": "2010-12-17T11:29:25.339Z", "isAdmin": false, "displayName": "kononov"}, "userId": "QqELeHmjAXfyx2iJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MyfahrWWn2zCK8oF7/the-problem-of-mankind-indestructibility-in-disastrously", "pageUrlRelative": "/posts/MyfahrWWn2zCK8oF7/the-problem-of-mankind-indestructibility-in-disastrously", "linkUrl": "https://www.lesswrong.com/posts/MyfahrWWn2zCK8oF7/the-problem-of-mankind-indestructibility-in-disastrously", "postedAtFormatted": "Friday, December 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20problem%20of%20mankind%20indestructibility%20in%20disastrously%20unpredictable%20environment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20problem%20of%20mankind%20indestructibility%20in%20disastrously%20unpredictable%20environment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyfahrWWn2zCK8oF7%2Fthe-problem-of-mankind-indestructibility-in-disastrously%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20problem%20of%20mankind%20indestructibility%20in%20disastrously%20unpredictable%20environment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyfahrWWn2zCK8oF7%2Fthe-problem-of-mankind-indestructibility-in-disastrously", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyfahrWWn2zCK8oF7%2Fthe-problem-of-mankind-indestructibility-in-disastrously", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4698, "htmlBody": "<div class=\"Section1\">\r\n<p class=\"MsoNormal\" style=\"text-align: center; margin-bottom: 6pt;\" align=\"center\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-size: 14pt; mso-ansi-language: EN-US;\" lang=\"EN-US\">The problem of mankind indestructibility in disastrously unpredictable environment</span></strong></p>\r\n<p class=\"MsoNormal\" style=\"text-align: center; margin-bottom: 6pt;\" align=\"center\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Concerning development of human race indestructibility roadmap</span></strong><strong style=\"mso-bidi-font-weight: normal;\"></strong></p>\r\n<p class=\"MsoNormal\" style=\"text-align: center; margin-bottom: 6pt;\" align=\"center\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Kononov Alexandr Anatolievich, PhD (Engineering), senior researcher, Institute of Systems Analysis, Russian Academy of Sciences, member of Russian Philosophical Society of RAS, <a href=\"mailto:kononov@isa.ru\">kononov@isa.ru</a></span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Many discoveries in astronomy and earth sciences, made within the past decades, turned to be the ones of new threats and risks to the existence of humankind on the Earth and in Space. Lending itself readily is a conclusion of that our civilization is existing and evolving in a disastrously unstable environment, which is capable of destroying it any time, and only a fortunate coincidence (luck) allowed our civilization to develop up to the current level. But this &ldquo;luck&rdquo; will hardly be everlasting.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Dangers of human race destruction</span></strong></p>\r\n<h1 style=\"text-align: justify; margin: 0cm 0cm 6pt;\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\">For several years now the author has maintained an Internet project &ldquo;Multiverse Dossier&rdquo; (in Russian) &nbsp;(</span><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold;\"><a href=\"http://www.mirozdanie.narod.ru/\"><span style=\"color: windowtext; mso-ansi-language: EN-US;\" lang=\"EN-US\">http://www.mirozdanie.narod.ru</span></a></span><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\">) whose several sections carry a big number of scientific papers and messages of the last space discoveries, which suggest a conclusion of a catastrophic character of the processes running in Space, and of unpredictability of impact thereof on life in the part of the Space inhabited by humankind. Not much more predictable are geological processes, many of which may come to be sources of global natural disasters. Indeed, nearly each step in the evolution of civilization brings along new threats and risks to its existence. </span></h1>\r\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Following below are a list of main groups of threats of global catastrophes and several examples of the threats.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Natural:</span></strong></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Disasters resulting from geological processes</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. Supervolcanos, magnetic pole shift, earth faults and the processes running in deeper strata of the Earth</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Disasters resulting from potential instability of Sun</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. Superpowerful solar flares and bursts, potential instability of reactions providing for solar luminocity and temperature supporting life on the Earth</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"text-decoration: underline;\">Disasters resulting from Space effects</span> (asteroids, comets; a possibility of a malicious intrusion of an alien civilization cannot be ruled out either)</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Engendered</span><span lang=\"EN-US\"> </span></strong><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">by</span><span lang=\"EN-US\"> </span></strong><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">civilization</span></strong></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Self</span>-</span><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">destruction</span></span>. <span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Resulting from the use of weapons of mass destruction.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Environment destruction</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. As a result of man-made disasters.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Self-extermination</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. The choice of an erroneous way of civilization evolution, say, the one limiting the pace of building up civilization&rsquo;s technological strength. Given civilization existence in a disastrously unstable environment such a decision may turn to be a sentence of civilization&rsquo;s self-extermination &ndash; it will simply have no time to prepare for the upcoming catastrophes. Many other theories, bearing upon the choice of directions of civilization evolution, also can, given a lop-sided non-systemic application thereof, inflict a heavy damage and prevent civilization from appropriately resolving the tasks, which would have enabled it to manage the potential disasters. Even the idea of civilization&rsquo;s indestructibility, presented herein, carries a risk of justifying super-exploitation (sacrificing the living generations) for the sake of solving the tasks of civilization&rsquo;s indestructibility. Hence, importance of the second part of this ideology &ndash; raising the culture of keeping the family and individual memory. Remarkably, this culture may act as a defense from a variety of other risks of dehumanization and moral degradation of civilization. </span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Provoking nature instability</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. For instance, initiating greenhouse effect and climatic changes.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of civilization destruction endangered by new technologies and civilization evolution (civilization dynamics)</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. These are threats which humankind must learn to handle as new technologies emerge and space developed (space expansion). For example, the emergence of information society gave rise to a whole industry handling security problems (cyber security) arising when using computer and telecoms technologies. The necessity of diverting huge resources for solving security problems associated with new technologies is an inevitable prerequisite of progress. It must be understood and taken for granted that solving the problems of security of each new technological or civilizational breakthrough (e.g., creation of extraterrestrial space colonies) may come to be many times as costly as the price of their materialization. But this is the only way of ensuring security of progress, including that of space expansion.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threat of life destruction on a space scale</span></strong></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">These are largely hypothetical threats, but the known cases of collisions and explosions of galaxies are indicative of that they may but be ignored. These are:</span></p>\r\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of life destruction in the part of the Galaxy, where the Solar system lies;</span> </li>\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of life destruction throughout the Galaxy or in a cluster of Galaxies, which the Milky Way is part of;</span> </li>\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of destruction of the Universe or life in the Universe;</span> </li>\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of life destruction in potentially existing structures, which our Universe may be part of.</span> </li>\r\n</ul>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<h1 style=\"text-align: justify; margin: 0cm 0cm 6pt;\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; mso-ansi-language: EN-US;\" lang=\"EN-US\">Indestructibility as civilization&rsquo;s principal supertask </span></h1>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The presence of a huge number of threats to the survival of civilization makes civilization&rsquo;s indestructibility to be the main task, and sooner, with regard to the scale and importance, the central supertask. The other global civilizational supertasks and tasks such as extension of human life, rescuing mankind from diseases, hunger, stark social inequality (misery, poverty), crime, terrorism largely become senseless and lose their moral potential, if the central supertask &ndash; civilization&rsquo;s indestructibility &ndash; is not being handled. Ignoring this supertask implies a demonstrable indifference to the fate of civilization, to the destiny of future generations, thereby depriving the living generations of an ethical foundation because of immorality and cruelty (to the future generations, thus doomed to death) of such a choice.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">So, what potential ways of solving this central supertask of civilization are available?</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Generally speaking, the current practice of responding to the threats suggests looking for ways of guarding against each one of them. But the quantity and scale of threats to civilization destruction as well as fundamental impossibility of defending from them in any other way but only by breaking the dependence of civilization fate on the places where these threats exist, render a conclusion that a relatively reliable (in relation to other possible solutions, say, by creating protective shells or arks) solution of the task of civilization&rsquo;s indestructibility can be provided only by way of space expansion. Yet, keeping in mind that there are no absolutely safe places in all of the Universe and, probably, across the Creation, the task of civilization salvation<span style=\"mso-spacerun: yes;\">&nbsp; </span>comes to a strive for a maximum distribution of civilization, maintaining unity, across a possibly maximum number of spaces along with possession of considerable evacuation potential in each one of them.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">So civilization space expansion ought to imply surmounting civilization&rsquo;s dependence on the habitats, which may be destroyed. And the first task along the line implies surmounting mankind&rsquo;s dependence on the living conditions on the Earth and on the Earth fate. It may be solved by a purposive colonization of the solar system. That is by establishing technologically autonomous colonies on all planets or their moons, where this is possible, and by creating autonomous interplanetary stations, prepared for full technological independence from the Earth.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">This must be accompanied by a gradual shift of manufacturing operations, critical for the fate of civilization and hazardous for the Earth environment, beyond the limits of our planet and distribution thereof across the solar system. The planet of Earth shall be gradually assigned the role of environmentally sound recreational zone designed for vacations and life after retirement</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Solution of this task, i.e. establishment of colonies technologically independent upon the Earth and shifting critical operations beyond the Earth boundaries, can apparently take about 1,000 years. Though the history of the 20<sup>th</sup> century showed that humankind is capable of producing so many technological surprises within a mere 100 years! Note that this was done in spite of the fact that its smooth development, during the 100 years, was impeded by 2 world wars, disastrous in terms of their scale, numerous civil wars and bloody conflicts. Technological breakthroughs, given peaceful and goal-oriented activities, will probably make it possible to handle the tasks of severing civilization&rsquo;s dependence on the fate of the Earth, solar system, etc. at a much higher pace than can be imagined now.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Try to define individual phases of potential space expansion, implying a marked upsurge in civilization&rsquo;s indestructibility.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Upon surmounting the humanity&rsquo;s fate dependence upon the fate of the Earth, next along the line shall come the task of getting over the dependence of civilization&rsquo;s fate on the fate of solar system. This task will have to be coped with by colonizing spaces at a safe distance from our solar system. The expected time of accomplishment (given no incredible, from modern perspective, technological breakthroughs) spans scores thousands of years.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Then come the tasks of severing civilization&rsquo;s fate dependence upon the fate of individual intragalaxy spaces and on the fate of Milky Way and Metagalaxy. The possibility of solving<span style=\"mso-spacerun: yes;\">&nbsp; </span>these tasks will, apparently, be determined only by a potential emergence of new technologies unpredictable today.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Same applies to solving the next tasks, say, doing away with civilization&rsquo;s fate dependence upon the fate of the Universe. It seems now that solution of this kind of tasks will be possible through the control of all critical processes running in the Universe, or through discovering technologies enabling transportation to other universes<span style=\"mso-spacerun: yes;\">&nbsp; </span>(if any of these exist), or by way of acquiring technologies for creation of new universes suitable as new backup (evacuation) living spaces of civilization.<span style=\"mso-spacerun: yes;\">&nbsp; </span></span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An absolute guarantee of civilization&rsquo;s safety and indestructibility can be produced only by the control of the Creation, be it is achievable and feasible in principle. But it is precisely this option that any civilization in Cosmos must strive at so as to be absolutely sure of its indestructibility.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Assume that Humanity is not the only civilization setting the supertask of indestructibility. What will happen given a meeting with other civilizations setting similar tasks?</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It would be safe in assuming, at this point of reasoning, natural occurrence of an objective law, which may be referred to as Ethical Filter Law.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Ethical Filter Law</span></span></strong><a style=\"mso-footnote-id: ftn1;\" name=\"_ftnref1\" href=\"/#_ftn1\"><span class=\"MsoFootnoteReference\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-special-character: footnote;\"><span class=\"MsoFootnoteReference\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-US; mso-fareast-language: RU; mso-bidi-language: AR-SA;\" lang=\"EN-US\">[1]</span></span></span></span></span></a><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">: </span></strong><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">it is only civilizations with a rather high ethical potential, barring them from self-annihilation given availability of technologies capable of turning into the means of mass destruction during intra-civilization conflicts, which can evolve up to the level of civilization capable of space expansion on interplanetary and intergalaxy scale.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">In other words, civilizations with high technologies at hand but failing to learn to behave are either destroyed, as any inadequately developed civilizations, by natural disasters which they are incapable of managing because of the lack of appropriate capabilities, which they had no time to develop probably not least because of wasting efforts and allocated time on self-annihilation (wars).</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Given two and more space civilizations, which strive towards indestructibility and which managed to get through the ethical filter, probably the most productive way of their co-existence can become a gradual unification thereof for solving the tasks of indestructibility of all civilizations, which managed to get through the ethical filter.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">We may leave room for the existence of totalitarian civilizations capable of bypassing the above filter for they did not face a problem of self-annihilation because of their primordial unity. But, as is seen from historical experience of humankind, totalitarian civilizations (regimes) are more prone to undermining their own, nominally human potential due to the repressive mechanisms keeping them afloat, and are not capable of generating effective incentives for a progressive development, primarily technological one. That</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">is</span>, <span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">they</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">are</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">unviable</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">in</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">principle</span>.</p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The potential specific principles of interaction with such totalitarian space civilizations must therefore be developed upon the emergence of this type of problems, if it becomes clear that they really can arise. Meanwhile we may treat the possibility of meeting such civilizations, which may turn to be hostile towards humankind, as any other space threat, whose repulsion will be dependent upon availability of sufficient civilization capacities required for handling this kind of tasks. </span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Qualities of indestructible civilization</span></strong></p>\r\n<h1 style=\"text-align: justify; margin: 0cm 0cm 6pt;\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\">Let us define the qualities rendering civilization indestructible. In so doing, it would be necessary to answer a number of questions:</span></h1>\r\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l6 level1 lfo11;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Which civilization has more chances to stay alive: the one which recognized that it is existing in a disastrously unstable Space, and must strive towards building up strengths for handling potential problems or the one ignoring these problems?</span> </li>\r\n</ul>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Apparently it is the civilization keen to augment its potential for meeting threats and risks of its destruction that has more chances for becoming indestructible.</span></p>\r\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l6 level1 lfo11;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Which civilization has more chances to stay alive: the one which has developed policies promoting the responsibility of the current generations before the subsequent generations, or the one which has no mechanisms of this kind?</span> </li>\r\n</ul>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The indestructible civilization has policies stimulating responsibility of the current generations before the next ones. And vice versa, civilizations deeming it senseless to show a deep concern of their future and of the fate of upcoming generations are doomed either to a gradual self-extermination or to destruction upon the very first apocalypse.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Following below are only answers and conclusions, questions ipso&nbsp;facto:</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to severing dependence of its fate on the fate of the place of its original and current habitation, i.e. to space expansion.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to increasing its own population and to a higher quality of life and skills of each individual. Apparently, given colonization of new cosmic outreaches, the bigger the population and capabilities or, conditionally speaking, civilization&rsquo;s human potential, the bigger its capacities for handling the problems of progress, space expansion, ensuring its permanent prosperity and security.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to unity. All efforts towards civilization development and space expansion will be of no avail, should civilization disintegrate to an extent rendering it incapable of solving the evacuation tasks of rescuing those who happen to be in the area of disastrous manifestations of space elements.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to raising ethical standards of its development, for this will permit it: not to destroy itself upon getting hold of the ever new technologies (which can be used as the means of mass destruction) and maintain civilization unity, which will in its turn provide opportunities for handling mass transcosmic evacuation tasks, the tasks of transgeneration responsibility and other indestructibility problems.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<h1 style=\"margin: 0cm 0cm 6pt;\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; mso-ansi-language: EN-US;\" lang=\"EN-US\">Concerning the necessity of developing theoretical principles of handling the tasks of humankind indestructibility </span></h1>\r\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">One can ascertain the existence of objective threats to human civilization by turning, for example, to the materials on <span style=\"font-family: Times New Roman;\">&ldquo;Multiverse Dossier&rdquo;&nbsp;</span> site. Similarly, there are objectively existing civilization capabilities, which will enable it to counter possible catastrophes. Apparently, these capabilities must be controlled. That is, the tasks of their build up must be set, the factors augmenting these capacities be accounted for and promoted. There is need for scientific concepts and theories underpinning problems of civilization indestructibility potential control.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It is suggested to use the following concepts as the initial steps towards development of a scientific frame of reference relative to civilization indestructibility problems:</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">- civilization indestructibility potential;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">- civilization competitiveness;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">- competitiveness of social components making up civilization.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Civilization indestructibility capacities</span></strong><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"> are defined as the qualities, achievements and characteristics of civilization enabling it, given the emergence of circumstances threatening its degradation or destruction, to counteract these developments and prevent civilization death or degradation.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">There is a great deal of objective developments (threats, risks) which may, given a certain course of events, lead to civilization collapse, i.e. come to be stronger or, as is routinely said, higher than it. Yet, civilization is known to have certain capacities, qualities, capabilities which may enable it to counteract these circumstances. That is objectively, there are some relations (ratio) of potential forces. Let us refer to these relations as <span style=\"text-decoration: underline;\">competition</span>. Then it would be safe in saying that there is an objective competition between the developments, capable of destroying the civilization, and civilization&rsquo;s capacities to counteract these circumstances and surmount them. It is precisely the civilization&rsquo;s capacities to counteract potential circumstances (threats, risks), which may destroy or weaken it, that we shall refer to as <strong style=\"mso-bidi-font-weight: normal;\"><span style=\"text-decoration: underline;\">civilization competitiveness</span></strong>.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Apparently, civilization competitiveness, just as any capabilities, may be developed by, say, building up competitive advantages (indestructibility capacities).</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Now turn to the concepts of competitiveness of social components making up civilization.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Civilization is primarily its carriers. Humanity is, in the first place, people and social structures they are part of. The reality is that our civilization is made up of nations (state nations and ethnic nations). As is seen from history, civilization progress and well-being are largely dependent upon the progress and well-being of individual nations, on prosperity of societies, families and individuals.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Prospering nations push civilization forward. Living conditions of prosperous nations create conditions for their representatives to handle the tasks promoting civilization&rsquo;s progress. At the same time, individual nations also face problems and circumstances, which may force these nations, along with the entire civilization, to regress, the circumstances leading individual nation to destruction.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It is therefore very important to understand that as there is, quite objectively, competition of civilization and circumstances, which may destroy it, so, as objectively, there is competition of each nation with the circumstances, which may weaken the nation and lead it to a state where it, instead of being one of the forces strengthening and promoting competitiveness of civilization at large, comes to be a factor weakening the civilization. The nation&rsquo;s competitiveness in securing its permanent prosperity must therefore become a national idea of each nation, the adherence to which will enable it to incorporate in its life some objective criteria to be used in making any vital decisions by way of assessing their impact on competitiveness potential and competitive advantages of the nation securing its permanent prosperity.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Of course, as far as nation&rsquo;s competitiveness is concerned, the point is of competitiveness of similar topics considered for civilization as a whole, i.e. of competitiveness with risks, threats, circumstances which may lead nations to catastrophes but by no means to competition with other nations, for this kind of competition is a way to destruction or weakening of the competing nations and civilization as a whole. In the final count, the correctly perceived idea of nations&rsquo; competitiveness must bring them to unification thereof for securing indestructibility of the entire human civilization. We are witnessing examples of a positive movement along the line in both collective space exploration on board the international space station and in the development of the European Union made up of countries which had been fighting with each other for centuries. In the majority of advanced countries, security, prosperity and permanence of nation&rsquo;s prosperity have already become a national idea. In October last year, the nation&rsquo;s competitiveness was declared a national idea in Kazakhstan. Take the speech of N.A.Nazarbaev, President of the Republic of Kazakhstan, at the 12<sup>th</sup> session of the Assembly of the peoples of Kazakhstan (Astana, 24 October 2006, </span><span style=\"color: black; mso-bidi-font-weight: bold;\"><a href=\"http://www.zakon.kz/our/news/print.asp?id=30074242\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">http://www.zakon.kz/our/news/print.asp?id=30074242</span></a></span><span style=\"color: black; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\"> )</span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"> where the nation&rsquo;s competitiveness was just declared national idea. Then it should be noted that not a word was uttered about any competition with other nations, the point was only of the nation&rsquo;s competitiveness in relation to challenges and problems facing the country. High rates of Kazakhstan development in recent years say for the fruitfulness of the choice of precisely this way of development.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Then, in considering social structure of civilization, it would be right to speak of family and individuals. No doubt, the family largely determines both the development and daily state and capacities of the individual. It would be only right, therefore, to speak of competitiveness of families and individuals, again using the term &ldquo;competitiveness&rdquo; in the meaning as it is defined above, i.e. not of competition between individual families and persons, which can in principle undermine ethical and other capacities of the nation and civilization, but only of competition with potential challenges, threats, risks, developments, problems.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Of course, the state and competitiveness of individual are dependent not only on the family but also on other social structures, which they may be involved with. What is more, with respect to some structures of this kind there is a traditional perception of their competitiveness implying competition precisely between this kind of structures, notably, competition between firms or any other for-profit organizations, competition between parties, etc. One cannot but admit that competitive struggle between such entities is one of the driving forces of technological, economic, social change of modern civilization. At the same time, introduction of an alternative perception of the terms &ldquo;competition&rdquo; and &ldquo;competitiveness&rdquo; as competition with challenges, developments, risks, threats, problems (which is envisaged under the frame of reference of theoretical civilization indestructibility) will probably promote a gradual formation of ethically more harmonious axiological base (values) underlying relationships of this type (commercial, political and the like) of organizations not accompanied by lower dynamics of civilization&rsquo;s technological and economic change. That is the point is of that competition, in its traditional meaning, is civilization&rsquo;s economic and technological driving force, but putting it mildly, does not promote development and strengthening of civilization&rsquo;s ethical potential. And the point is of whether an alternative perception of competition, put forward by the theory of civilization indestructibility, can remove or mitigate the drawback of the traditional perception of the term &ldquo;competition&rdquo;, by improving the ethical component and introducing a refining ambiguity in the semantics of &ldquo;competition&rdquo; concept, simultaneously preserving the vital mechanisms of securing civilization development dynamics implied by this traditional perception?</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Ray Bradbery described a &ldquo;butterfly effect&rdquo; in one of his stories. The hero of the story, while on excursion to the past, had crushed a butterfly, hence, the world he came back to turned to be much worse. Let alone the negative impact on humanity&rsquo;s progress and competitiveness of the premature death of its representatives who could make contributions to its development and prosperity. This effect is quite correctly expressed by John Donne&rsquo;s words &ldquo;Do not ask for whom the bell tolls, it tolls for thee&rdquo;. Any person deceased could well become precisely the one, who could save, for instance, cure, pull out of a critical situation, invent or create something which could, even indirectly, help the person who could, thanks to the help, gain an opportunity to save anybody or each one of us. But having died, he would no longer be capable of doing so. The death of each reduces the human potential of civilization &ndash; the major potential of its indestructibility.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Human potential constitutes a basis of competitiveness of both any nation and civilization as a whole. Also, one can put it differently: competitiveness of each is a foundation of competitiveness of civilization. Of that the greatest problems exist precisely in this area is evidenced, for example, by the fact that about 1 million people commit suicide every year in the world &ndash; the odds turned to be against them. Many more people die because of, mildly speaking, ethical imperfection of human relations &ndash; murders (including those in the course of military<span style=\"mso-spacerun: yes;\">&nbsp; </span>operations), violence, famine, non-delivery of adequate medical care and other assistance. In this connection, a new rethinking of the terms &ldquo;competition&rdquo; and &ldquo;competitiveness&rdquo; in the light of the concepts of humanity indestructibility theory (HUT), built in these terms, can provide hope for improvement of the current situation.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">What else can theoretical development of the problems of civilization indestructibility produce? Note just two directions: </span></p>\r\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l11 level1 lfo13;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Development of a set of objective indicators and criteria for decision making on the development of civilization and its social components;</span> </li>\r\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l11 level1 lfo13;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Finding systems solutions promoting a higher competitiveness of civilization and its social components.</span> </li>\r\n</ul>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The importance of a set of objective indicators and criteria for decision making, taking into account the vital necessity of building up the potential of indestructibility and competitive advantages of civilization can be judged by at least from an example such as closing the Moon exploration programmes in the 1970&rsquo;s. The bulk of the huge resources invested in the projects was, in the final count, just buried because neither the USA, nor the USSR had any sufficiently convincing motives for continuation of these programmes. As a result, several decades of space evolution of civilization were just lost. And the resources and funds which could be invested in space expansion were spent on satisfying the ambitions along the lines devastating for civilization, namely, US war in Vietnam and USSR war in Afghanistan.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The idea of the necessity of developing the culture of keeping family and individual memory of each person living on the Earth, being an integral component of HUT and a major defence mechanism against potentially incorrect, hence destructive application of the key concepts of humanity indestructibility theory is an example of systems solutions contributing to a higher competitiveness of civilization and its social components.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Modern digital technologies make it possible to keep memory of each person. Should there emerge and develop a culture of keeping and passing digital information (memory) of one&rsquo;s self, one&rsquo;s relations and friends over from generation to generation, then the best features of each can be remembered forever. Each one would be in a position to preserve one&rsquo;s ideas and thoughts for good, keep the memory of the very interesting and important instants in one&rsquo;s life, of the one he/she knew and loved, and who was dear to him/her. Thus, each one would be in a position to remain a fraction of human civilization memory for good. Nobody will leave this world vanishing into thin air, each will always be remembered.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It seems the culture of keeping family and individual memory may improve humanity&rsquo;s competitiveness by providing for:</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l1 level1 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Higher responsibility:</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l1 level2 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">of the living generations before the upcoming ones;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l1 level2 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">of state leaders for the decisions made;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l1 level2 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">people before one another;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l12 level1 lfo4;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Better human relations:</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l12 level2 lfo4;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">between representatives of different generations in the family;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l12 level2 lfo4;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">higher status of each person &ndash; each one will always be a part of human civilization memory;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l3 level1 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Defence mechanism:</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l3 level2 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">from political speculations like: &ldquo;life for the sake of future generations&rdquo;;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l3 level2 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">from cruelty of authorities;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l3 level2 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">from cruelty in interpersonal relations ;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l4 level1 lfo6;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Mechanism of refining human nature and building up civilization&rsquo;s ethical potential;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l4 level1 lfo6;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Creation of a core, nucleus, root securing unity of civilization in its space expansion, when moving across the immense space;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">In summarizing the arguments produced in evidence of the necessity of developing theoretical solution of the task of civilization indestructibility, it may be noted that the quantity of sub-tasks subject to solution for solving the main task can turn to be huge, and virtually each one of these places demand on construction of its paradigms, its theoretical elaboration. Therefore, at the first phase of developing the theory of civilization indestructibility it makes sense to speak of the general theoretical principles, of general theory of indestructibility, and only thereafter, as deeper solutions of individual, special and partial tasks are found, start building special theories linked to the requirements of development of individual capacities (technological, ethical, evacuation, etc.) and solution of the tasks of a higher competitiveness (in terms of indestructibility theory) of individual social components.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">What must the statement of the problems of civilization indestructibility and space expansion give to the living generations of people?</span></strong></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Alleviation of the risks of war &ndash; nothing undermines civilization indestructibility capacities as heavily as wars. MIC resources must be redirected to handling the tasks of and creating capacities for space expansion and Cosmos colonization.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Justification of importance of higher living standards of people &ndash; for only the high living standards enable the possibly maximum number of people to master the sophisticated technologies, realize their talents on their basis, and contribute to the development of ever new and sophisticated technologies. The authorities will increasingly understand that the nations&rsquo; competitiveness is largely dependent upon living standards of people, and that social programmes are not wasting money but rather laying a foundation and an important prerequisite of a permanent prosperity and competitiveness of nations.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Attaching new sense to human life. A more responsible attitude of people to their own and others&rsquo; lives, higher ethical standards of human relations, hence, lowering crime rate and terrorist activities.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">A major ideological justification for conflict resolution, unification of nations and civilization as a whole.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New living spaces.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New sources of raw materials.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New employment sectors and jobs.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">&Oslash;<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New markets.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">REFERENCES</span></p>\r\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">1.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Lefevre V.A. Space Subject. Moscow, Kogito-Centr Publishing house, 2005, 220p.</span></p>\r\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">2.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Nazaretyan A.P. Civilizational Crises in the Context of Universal History. 2-nd ed. Moscow, Mir Publishing house, 2004, </span>367 <span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">p.</span></p>\r\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">3.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Hvan M.P. A Violent Universe: from the Big Bang up to Accelerated Expansion, from Quarks to Superstrings. Moscow, URSS Publishers, 2006, 408p.</span></p>\r\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><a name=\"FF0\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">4.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Narlikar Jayant \"Violent Phenomena in the Universe\", </span></a><span style=\"mso-bookmark: FF0;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Oxford</span></span><span style=\"mso-bookmark: FF0;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"> UP</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">, 1984, 246&nbsp;\u0440.</span><br />\r\n<hr size=\"1\" />\r\n</p>\r\n</div>\r\n<div style=\"mso-element: footnote-list;\">\r\n<div id=\"ftn1\" style=\"mso-element: footnote;\">\r\n<p class=\"MsoFootnoteText\"><a style=\"mso-footnote-id: ftn1;\" name=\"_ftn1\" href=\"/#_ftnref1\"><span class=\"MsoFootnoteReference\"><span style=\"mso-special-character: footnote;\"><span class=\"MsoFootnoteReference\"><span style=\"font-family: 'Times New Roman'; font-size: 10pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: RU; mso-fareast-language: RU; mso-bidi-language: AR-SA;\">[1]</span></span></span></span></a><span style=\"mso-ansi-language: EN-US;\"> <span lang=\"EN-US\">This law is known in a somewhat benign definition, not associated with the problems of civilization space expansion and competitiveness, as a law of techno-humanitarian balance [Nazaretyan A.P., 2004, p. 112]: &ldquo;the greater the power of productive and combat technologies, the greater the need for more sophisticated tools of cultural regulation for preserving the society&rdquo;.</span></span></p>\r\n</div>\r\n</div>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MyfahrWWn2zCK8oF7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -8, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "4341", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div class=\"Section1\">\n<p class=\"MsoNormal\" style=\"text-align: center; margin-bottom: 6pt;\" align=\"center\"><strong style=\"mso-bidi-font-weight: normal;\" id=\"The_problem_of_mankind_indestructibility_in_disastrously_unpredictable_environment\"><span style=\"font-size: 14pt; mso-ansi-language: EN-US;\" lang=\"EN-US\">The problem of mankind indestructibility in disastrously unpredictable environment</span></strong></p>\n<p class=\"MsoNormal\" style=\"text-align: center; margin-bottom: 6pt;\" align=\"center\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Concerning development of human race indestructibility roadmap</span></strong><strong style=\"mso-bidi-font-weight: normal;\"></strong></p>\n<p class=\"MsoNormal\" style=\"text-align: center; margin-bottom: 6pt;\" align=\"center\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Kononov Alexandr Anatolievich, PhD (Engineering), senior researcher, Institute of Systems Analysis, Russian Academy of Sciences, member of Russian Philosophical Society of RAS, <a href=\"mailto:kononov@isa.ru\">kononov@isa.ru</a></span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Many discoveries in astronomy and earth sciences, made within the past decades, turned to be the ones of new threats and risks to the existence of humankind on the Earth and in Space. Lending itself readily is a conclusion of that our civilization is existing and evolving in a disastrously unstable environment, which is capable of destroying it any time, and only a fortunate coincidence (luck) allowed our civilization to develop up to the current level. But this \u201cluck\u201d will hardly be everlasting.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\" id=\"Dangers_of_human_race_destruction\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Dangers of human race destruction</span></strong></p>\n<h1 style=\"text-align: justify; margin: 0cm 0cm 6pt;\" id=\"For_several_years_now_the_author_has_maintained_an_Internet_project__Multiverse_Dossier___in_Russian____http___www_mirozdanie_narod_ru__whose_several_sections_carry_a_big_number_of_scientific_papers_and_messages_of_the_last_space_discoveries__which_suggest_a_conclusion_of_a_catastrophic_character_of_the_processes_running_in_Space__and_of_unpredictability_of_impact_thereof_on_life_in_the_part_of_the_Space_inhabited_by_humankind__Not_much_more_predictable_are_geological_processes__many_of_which_may_come_to_be_sources_of_global_natural_disasters__Indeed__nearly_each_step_in_the_evolution_of_civilization_brings_along_new_threats_and_risks_to_its_existence__\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\">For several years now the author has maintained an Internet project \u201cMultiverse Dossier\u201d (in Russian) &nbsp;(</span><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold;\"><a href=\"http://www.mirozdanie.narod.ru/\"><span style=\"color: windowtext; mso-ansi-language: EN-US;\" lang=\"EN-US\">http://www.mirozdanie.narod.ru</span></a></span><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\">) whose several sections carry a big number of scientific papers and messages of the last space discoveries, which suggest a conclusion of a catastrophic character of the processes running in Space, and of unpredictability of impact thereof on life in the part of the Space inhabited by humankind. Not much more predictable are geological processes, many of which may come to be sources of global natural disasters. Indeed, nearly each step in the evolution of civilization brings along new threats and risks to its existence. </span></h1>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Following below are a list of main groups of threats of global catastrophes and several examples of the threats.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\" id=\"Natural_\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Natural:</span></strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Disasters resulting from geological processes</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. Supervolcanos, magnetic pole shift, earth faults and the processes running in deeper strata of the Earth</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Disasters resulting from potential instability of Sun</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. Superpowerful solar flares and bursts, potential instability of reactions providing for solar luminocity and temperature supporting life on the Earth</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"text-decoration: underline;\">Disasters resulting from Space effects</span> (asteroids, comets; a possibility of a malicious intrusion of an alien civilization cannot be ruled out either)</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Engendered</span><span lang=\"EN-US\"> </span></strong><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">by</span><span lang=\"EN-US\"> </span></strong><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">civilization</span></strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Self</span>-</span><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">destruction</span></span>. <span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Resulting from the use of weapons of mass destruction.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Environment destruction</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. As a result of man-made disasters.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Self-extermination</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. The choice of an erroneous way of civilization evolution, say, the one limiting the pace of building up civilization\u2019s technological strength. Given civilization existence in a disastrously unstable environment such a decision may turn to be a sentence of civilization\u2019s self-extermination \u2013 it will simply have no time to prepare for the upcoming catastrophes. Many other theories, bearing upon the choice of directions of civilization evolution, also can, given a lop-sided non-systemic application thereof, inflict a heavy damage and prevent civilization from appropriately resolving the tasks, which would have enabled it to manage the potential disasters. Even the idea of civilization\u2019s indestructibility, presented herein, carries a risk of justifying super-exploitation (sacrificing the living generations) for the sake of solving the tasks of civilization\u2019s indestructibility. Hence, importance of the second part of this ideology \u2013 raising the culture of keeping the family and individual memory. Remarkably, this culture may act as a defense from a variety of other risks of dehumanization and moral degradation of civilization. </span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Provoking nature instability</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. For instance, initiating greenhouse effect and climatic changes.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of civilization destruction endangered by new technologies and civilization evolution (civilization dynamics)</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">. These are threats which humankind must learn to handle as new technologies emerge and space developed (space expansion). For example, the emergence of information society gave rise to a whole industry handling security problems (cyber security) arising when using computer and telecoms technologies. The necessity of diverting huge resources for solving security problems associated with new technologies is an inevitable prerequisite of progress. It must be understood and taken for granted that solving the problems of security of each new technological or civilizational breakthrough (e.g., creation of extraterrestrial space colonies) may come to be many times as costly as the price of their materialization. But this is the only way of ensuring security of progress, including that of space expansion.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\" id=\"Threat_of_life_destruction_on_a_space_scale\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threat of life destruction on a space scale</span></strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">These are largely hypothetical threats, but the known cases of collisions and explosions of galaxies are indicative of that they may but be ignored. These are:</span></p>\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of life destruction in the part of the Galaxy, where the Solar system lies;</span> </li>\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of life destruction throughout the Galaxy or in a cluster of Galaxies, which the Milky Way is part of;</span> </li>\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of destruction of the Universe or life in the Universe;</span> </li>\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l10 level1 lfo10;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Threats of life destruction in potentially existing structures, which our Universe may be part of.</span> </li>\n</ul>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<h1 style=\"text-align: justify; margin: 0cm 0cm 6pt;\" id=\"Indestructibility_as_civilization_s_principal_supertask_\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; mso-ansi-language: EN-US;\" lang=\"EN-US\">Indestructibility as civilization\u2019s principal supertask </span></h1>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The presence of a huge number of threats to the survival of civilization makes civilization\u2019s indestructibility to be the main task, and sooner, with regard to the scale and importance, the central supertask. The other global civilizational supertasks and tasks such as extension of human life, rescuing mankind from diseases, hunger, stark social inequality (misery, poverty), crime, terrorism largely become senseless and lose their moral potential, if the central supertask \u2013 civilization\u2019s indestructibility \u2013 is not being handled. Ignoring this supertask implies a demonstrable indifference to the fate of civilization, to the destiny of future generations, thereby depriving the living generations of an ethical foundation because of immorality and cruelty (to the future generations, thus doomed to death) of such a choice.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">So, what potential ways of solving this central supertask of civilization are available?</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Generally speaking, the current practice of responding to the threats suggests looking for ways of guarding against each one of them. But the quantity and scale of threats to civilization destruction as well as fundamental impossibility of defending from them in any other way but only by breaking the dependence of civilization fate on the places where these threats exist, render a conclusion that a relatively reliable (in relation to other possible solutions, say, by creating protective shells or arks) solution of the task of civilization\u2019s indestructibility can be provided only by way of space expansion. Yet, keeping in mind that there are no absolutely safe places in all of the Universe and, probably, across the Creation, the task of civilization salvation<span style=\"mso-spacerun: yes;\">&nbsp; </span>comes to a strive for a maximum distribution of civilization, maintaining unity, across a possibly maximum number of spaces along with possession of considerable evacuation potential in each one of them.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">So civilization space expansion ought to imply surmounting civilization\u2019s dependence on the habitats, which may be destroyed. And the first task along the line implies surmounting mankind\u2019s dependence on the living conditions on the Earth and on the Earth fate. It may be solved by a purposive colonization of the solar system. That is by establishing technologically autonomous colonies on all planets or their moons, where this is possible, and by creating autonomous interplanetary stations, prepared for full technological independence from the Earth.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">This must be accompanied by a gradual shift of manufacturing operations, critical for the fate of civilization and hazardous for the Earth environment, beyond the limits of our planet and distribution thereof across the solar system. The planet of Earth shall be gradually assigned the role of environmentally sound recreational zone designed for vacations and life after retirement</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Solution of this task, i.e. establishment of colonies technologically independent upon the Earth and shifting critical operations beyond the Earth boundaries, can apparently take about 1,000 years. Though the history of the 20<sup>th</sup> century showed that humankind is capable of producing so many technological surprises within a mere 100 years! Note that this was done in spite of the fact that its smooth development, during the 100 years, was impeded by 2 world wars, disastrous in terms of their scale, numerous civil wars and bloody conflicts. Technological breakthroughs, given peaceful and goal-oriented activities, will probably make it possible to handle the tasks of severing civilization\u2019s dependence on the fate of the Earth, solar system, etc. at a much higher pace than can be imagined now.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Try to define individual phases of potential space expansion, implying a marked upsurge in civilization\u2019s indestructibility.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Upon surmounting the humanity\u2019s fate dependence upon the fate of the Earth, next along the line shall come the task of getting over the dependence of civilization\u2019s fate on the fate of solar system. This task will have to be coped with by colonizing spaces at a safe distance from our solar system. The expected time of accomplishment (given no incredible, from modern perspective, technological breakthroughs) spans scores thousands of years.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Then come the tasks of severing civilization\u2019s fate dependence upon the fate of individual intragalaxy spaces and on the fate of Milky Way and Metagalaxy. The possibility of solving<span style=\"mso-spacerun: yes;\">&nbsp; </span>these tasks will, apparently, be determined only by a potential emergence of new technologies unpredictable today.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Same applies to solving the next tasks, say, doing away with civilization\u2019s fate dependence upon the fate of the Universe. It seems now that solution of this kind of tasks will be possible through the control of all critical processes running in the Universe, or through discovering technologies enabling transportation to other universes<span style=\"mso-spacerun: yes;\">&nbsp; </span>(if any of these exist), or by way of acquiring technologies for creation of new universes suitable as new backup (evacuation) living spaces of civilization.<span style=\"mso-spacerun: yes;\">&nbsp; </span></span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An absolute guarantee of civilization\u2019s safety and indestructibility can be produced only by the control of the Creation, be it is achievable and feasible in principle. But it is precisely this option that any civilization in Cosmos must strive at so as to be absolutely sure of its indestructibility.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Assume that Humanity is not the only civilization setting the supertask of indestructibility. What will happen given a meeting with other civilizations setting similar tasks?</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It would be safe in assuming, at this point of reasoning, natural occurrence of an objective law, which may be referred to as Ethical Filter Law.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"text-decoration: underline;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Ethical Filter Law</span></span></strong><a style=\"mso-footnote-id: ftn1;\" name=\"_ftnref1\" href=\"/#_ftn1\"><span class=\"MsoFootnoteReference\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-special-character: footnote;\"><span class=\"MsoFootnoteReference\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN-US; mso-fareast-language: RU; mso-bidi-language: AR-SA;\" lang=\"EN-US\">[1]</span></span></span></span></span></a><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">: </span></strong><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">it is only civilizations with a rather high ethical potential, barring them from self-annihilation given availability of technologies capable of turning into the means of mass destruction during intra-civilization conflicts, which can evolve up to the level of civilization capable of space expansion on interplanetary and intergalaxy scale.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">In other words, civilizations with high technologies at hand but failing to learn to behave are either destroyed, as any inadequately developed civilizations, by natural disasters which they are incapable of managing because of the lack of appropriate capabilities, which they had no time to develop probably not least because of wasting efforts and allocated time on self-annihilation (wars).</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Given two and more space civilizations, which strive towards indestructibility and which managed to get through the ethical filter, probably the most productive way of their co-existence can become a gradual unification thereof for solving the tasks of indestructibility of all civilizations, which managed to get through the ethical filter.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">We may leave room for the existence of totalitarian civilizations capable of bypassing the above filter for they did not face a problem of self-annihilation because of their primordial unity. But, as is seen from historical experience of humankind, totalitarian civilizations (regimes) are more prone to undermining their own, nominally human potential due to the repressive mechanisms keeping them afloat, and are not capable of generating effective incentives for a progressive development, primarily technological one. That</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">is</span>, <span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">they</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">are</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">unviable</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">in</span><span lang=\"EN-US\"> </span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">principle</span>.</p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The potential specific principles of interaction with such totalitarian space civilizations must therefore be developed upon the emergence of this type of problems, if it becomes clear that they really can arise. Meanwhile we may treat the possibility of meeting such civilizations, which may turn to be hostile towards humankind, as any other space threat, whose repulsion will be dependent upon availability of sufficient civilization capacities required for handling this kind of tasks. </span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\" id=\"Qualities_of_indestructible_civilization\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Qualities of indestructible civilization</span></strong></p>\n<h1 style=\"text-align: justify; margin: 0cm 0cm 6pt;\" id=\"Let_us_define_the_qualities_rendering_civilization_indestructible__In_so_doing__it_would_be_necessary_to_answer_a_number_of_questions_\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; font-weight: normal; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\">Let us define the qualities rendering civilization indestructible. In so doing, it would be necessary to answer a number of questions:</span></h1>\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l6 level1 lfo11;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Which civilization has more chances to stay alive: the one which recognized that it is existing in a disastrously unstable Space, and must strive towards building up strengths for handling potential problems or the one ignoring these problems?</span> </li>\n</ul>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Apparently it is the civilization keen to augment its potential for meeting threats and risks of its destruction that has more chances for becoming indestructible.</span></p>\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l6 level1 lfo11;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Which civilization has more chances to stay alive: the one which has developed policies promoting the responsibility of the current generations before the subsequent generations, or the one which has no mechanisms of this kind?</span> </li>\n</ul>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The indestructible civilization has policies stimulating responsibility of the current generations before the next ones. And vice versa, civilizations deeming it senseless to show a deep concern of their future and of the fate of upcoming generations are doomed either to a gradual self-extermination or to destruction upon the very first apocalypse.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Following below are only answers and conclusions, questions ipso&nbsp;facto:</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to severing dependence of its fate on the fate of the place of its original and current habitation, i.e. to space expansion.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to increasing its own population and to a higher quality of life and skills of each individual. Apparently, given colonization of new cosmic outreaches, the bigger the population and capabilities or, conditionally speaking, civilization\u2019s human potential, the bigger its capacities for handling the problems of progress, space expansion, ensuring its permanent prosperity and security.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to unity. All efforts towards civilization development and space expansion will be of no avail, should civilization disintegrate to an extent rendering it incapable of solving the evacuation tasks of rescuing those who happen to be in the area of disastrous manifestations of space elements.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l0 level1 lfo1;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">An indestructible civilization must strive to raising ethical standards of its development, for this will permit it: not to destroy itself upon getting hold of the ever new technologies (which can be used as the means of mass destruction) and maintain civilization unity, which will in its turn provide opportunities for handling mass transcosmic evacuation tasks, the tasks of transgeneration responsibility and other indestructibility problems.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<h1 style=\"margin: 0cm 0cm 6pt;\" id=\"Concerning_the_necessity_of_developing_theoretical_principles_of_handling_the_tasks_of_humankind_indestructibility_\"><span style=\"font-family: 'Times New Roman'; font-size: 12pt; mso-ansi-language: EN-US;\" lang=\"EN-US\">Concerning the necessity of developing theoretical principles of handling the tasks of humankind indestructibility </span></h1>\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">One can ascertain the existence of objective threats to human civilization by turning, for example, to the materials on <span style=\"font-family: Times New Roman;\">\u201cMultiverse Dossier\u201d&nbsp;</span> site. Similarly, there are objectively existing civilization capabilities, which will enable it to counter possible catastrophes. Apparently, these capabilities must be controlled. That is, the tasks of their build up must be set, the factors augmenting these capacities be accounted for and promoted. There is need for scientific concepts and theories underpinning problems of civilization indestructibility potential control.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It is suggested to use the following concepts as the initial steps towards development of a scientific frame of reference relative to civilization indestructibility problems:</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">- civilization indestructibility potential;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">- civilization competitiveness;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">- competitiveness of social components making up civilization.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Civilization indestructibility capacities</span></strong><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"> are defined as the qualities, achievements and characteristics of civilization enabling it, given the emergence of circumstances threatening its degradation or destruction, to counteract these developments and prevent civilization death or degradation.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">There is a great deal of objective developments (threats, risks) which may, given a certain course of events, lead to civilization collapse, i.e. come to be stronger or, as is routinely said, higher than it. Yet, civilization is known to have certain capacities, qualities, capabilities which may enable it to counteract these circumstances. That is objectively, there are some relations (ratio) of potential forces. Let us refer to these relations as <span style=\"text-decoration: underline;\">competition</span>. Then it would be safe in saying that there is an objective competition between the developments, capable of destroying the civilization, and civilization\u2019s capacities to counteract these circumstances and surmount them. It is precisely the civilization\u2019s capacities to counteract potential circumstances (threats, risks), which may destroy or weaken it, that we shall refer to as <strong style=\"mso-bidi-font-weight: normal;\"><span style=\"text-decoration: underline;\">civilization competitiveness</span></strong>.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Apparently, civilization competitiveness, just as any capabilities, may be developed by, say, building up competitive advantages (indestructibility capacities).</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Now turn to the concepts of competitiveness of social components making up civilization.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Civilization is primarily its carriers. Humanity is, in the first place, people and social structures they are part of. The reality is that our civilization is made up of nations (state nations and ethnic nations). As is seen from history, civilization progress and well-being are largely dependent upon the progress and well-being of individual nations, on prosperity of societies, families and individuals.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Prospering nations push civilization forward. Living conditions of prosperous nations create conditions for their representatives to handle the tasks promoting civilization\u2019s progress. At the same time, individual nations also face problems and circumstances, which may force these nations, along with the entire civilization, to regress, the circumstances leading individual nation to destruction.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It is therefore very important to understand that as there is, quite objectively, competition of civilization and circumstances, which may destroy it, so, as objectively, there is competition of each nation with the circumstances, which may weaken the nation and lead it to a state where it, instead of being one of the forces strengthening and promoting competitiveness of civilization at large, comes to be a factor weakening the civilization. The nation\u2019s competitiveness in securing its permanent prosperity must therefore become a national idea of each nation, the adherence to which will enable it to incorporate in its life some objective criteria to be used in making any vital decisions by way of assessing their impact on competitiveness potential and competitive advantages of the nation securing its permanent prosperity.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Of course, as far as nation\u2019s competitiveness is concerned, the point is of competitiveness of similar topics considered for civilization as a whole, i.e. of competitiveness with risks, threats, circumstances which may lead nations to catastrophes but by no means to competition with other nations, for this kind of competition is a way to destruction or weakening of the competing nations and civilization as a whole. In the final count, the correctly perceived idea of nations\u2019 competitiveness must bring them to unification thereof for securing indestructibility of the entire human civilization. We are witnessing examples of a positive movement along the line in both collective space exploration on board the international space station and in the development of the European Union made up of countries which had been fighting with each other for centuries. In the majority of advanced countries, security, prosperity and permanence of nation\u2019s prosperity have already become a national idea. In October last year, the nation\u2019s competitiveness was declared a national idea in Kazakhstan. Take the speech of N.A.Nazarbaev, President of the Republic of Kazakhstan, at the 12<sup>th</sup> session of the Assembly of the peoples of Kazakhstan (Astana, 24 October 2006, </span><span style=\"color: black; mso-bidi-font-weight: bold;\"><a href=\"http://www.zakon.kz/our/news/print.asp?id=30074242\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">http://www.zakon.kz/our/news/print.asp?id=30074242</span></a></span><span style=\"color: black; mso-bidi-font-weight: bold; mso-ansi-language: EN-US;\" lang=\"EN-US\"> )</span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"> where the nation\u2019s competitiveness was just declared national idea. Then it should be noted that not a word was uttered about any competition with other nations, the point was only of the nation\u2019s competitiveness in relation to challenges and problems facing the country. High rates of Kazakhstan development in recent years say for the fruitfulness of the choice of precisely this way of development.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Then, in considering social structure of civilization, it would be right to speak of family and individuals. No doubt, the family largely determines both the development and daily state and capacities of the individual. It would be only right, therefore, to speak of competitiveness of families and individuals, again using the term \u201ccompetitiveness\u201d in the meaning as it is defined above, i.e. not of competition between individual families and persons, which can in principle undermine ethical and other capacities of the nation and civilization, but only of competition with potential challenges, threats, risks, developments, problems.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Of course, the state and competitiveness of individual are dependent not only on the family but also on other social structures, which they may be involved with. What is more, with respect to some structures of this kind there is a traditional perception of their competitiveness implying competition precisely between this kind of structures, notably, competition between firms or any other for-profit organizations, competition between parties, etc. One cannot but admit that competitive struggle between such entities is one of the driving forces of technological, economic, social change of modern civilization. At the same time, introduction of an alternative perception of the terms \u201ccompetition\u201d and \u201ccompetitiveness\u201d as competition with challenges, developments, risks, threats, problems (which is envisaged under the frame of reference of theoretical civilization indestructibility) will probably promote a gradual formation of ethically more harmonious axiological base (values) underlying relationships of this type (commercial, political and the like) of organizations not accompanied by lower dynamics of civilization\u2019s technological and economic change. That is the point is of that competition, in its traditional meaning, is civilization\u2019s economic and technological driving force, but putting it mildly, does not promote development and strengthening of civilization\u2019s ethical potential. And the point is of whether an alternative perception of competition, put forward by the theory of civilization indestructibility, can remove or mitigate the drawback of the traditional perception of the term \u201ccompetition\u201d, by improving the ethical component and introducing a refining ambiguity in the semantics of \u201ccompetition\u201d concept, simultaneously preserving the vital mechanisms of securing civilization development dynamics implied by this traditional perception?</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Ray Bradbery described a \u201cbutterfly effect\u201d in one of his stories. The hero of the story, while on excursion to the past, had crushed a butterfly, hence, the world he came back to turned to be much worse. Let alone the negative impact on humanity\u2019s progress and competitiveness of the premature death of its representatives who could make contributions to its development and prosperity. This effect is quite correctly expressed by John Donne\u2019s words \u201cDo not ask for whom the bell tolls, it tolls for thee\u201d. Any person deceased could well become precisely the one, who could save, for instance, cure, pull out of a critical situation, invent or create something which could, even indirectly, help the person who could, thanks to the help, gain an opportunity to save anybody or each one of us. But having died, he would no longer be capable of doing so. The death of each reduces the human potential of civilization \u2013 the major potential of its indestructibility.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Human potential constitutes a basis of competitiveness of both any nation and civilization as a whole. Also, one can put it differently: competitiveness of each is a foundation of competitiveness of civilization. Of that the greatest problems exist precisely in this area is evidenced, for example, by the fact that about 1 million people commit suicide every year in the world \u2013 the odds turned to be against them. Many more people die because of, mildly speaking, ethical imperfection of human relations \u2013 murders (including those in the course of military<span style=\"mso-spacerun: yes;\">&nbsp; </span>operations), violence, famine, non-delivery of adequate medical care and other assistance. In this connection, a new rethinking of the terms \u201ccompetition\u201d and \u201ccompetitiveness\u201d in the light of the concepts of humanity indestructibility theory (HUT), built in these terms, can provide hope for improvement of the current situation.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">What else can theoretical development of the problems of civilization indestructibility produce? Note just two directions: </span></p>\n<ul style=\"margin-top: 0cm;\" type=\"disc\">\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l11 level1 lfo13;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Development of a set of objective indicators and criteria for decision making on the development of civilization and its social components;</span> </li>\n<li class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt; tab-stops: list 36.0pt; mso-list: l11 level1 lfo13;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Finding systems solutions promoting a higher competitiveness of civilization and its social components.</span> </li>\n</ul>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The importance of a set of objective indicators and criteria for decision making, taking into account the vital necessity of building up the potential of indestructibility and competitive advantages of civilization can be judged by at least from an example such as closing the Moon exploration programmes in the 1970\u2019s. The bulk of the huge resources invested in the projects was, in the final count, just buried because neither the USA, nor the USSR had any sufficiently convincing motives for continuation of these programmes. As a result, several decades of space evolution of civilization were just lost. And the resources and funds which could be invested in space expansion were spent on satisfying the ambitions along the lines devastating for civilization, namely, US war in Vietnam and USSR war in Afghanistan.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">The idea of the necessity of developing the culture of keeping family and individual memory of each person living on the Earth, being an integral component of HUT and a major defence mechanism against potentially incorrect, hence destructive application of the key concepts of humanity indestructibility theory is an example of systems solutions contributing to a higher competitiveness of civilization and its social components.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Modern digital technologies make it possible to keep memory of each person. Should there emerge and develop a culture of keeping and passing digital information (memory) of one\u2019s self, one\u2019s relations and friends over from generation to generation, then the best features of each can be remembered forever. Each one would be in a position to preserve one\u2019s ideas and thoughts for good, keep the memory of the very interesting and important instants in one\u2019s life, of the one he/she knew and loved, and who was dear to him/her. Thus, each one would be in a position to remain a fraction of human civilization memory for good. Nobody will leave this world vanishing into thin air, each will always be remembered.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">It seems the culture of keeping family and individual memory may improve humanity\u2019s competitiveness by providing for:</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l1 level1 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Higher responsibility:</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l1 level2 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">of the living generations before the upcoming ones;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l1 level2 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">of state leaders for the decisions made;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l1 level2 lfo3;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">people before one another;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l12 level1 lfo4;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Better human relations:</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l12 level2 lfo4;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">between representatives of different generations in the family;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l12 level2 lfo4;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">higher status of each person \u2013 each one will always be a part of human civilization memory;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l3 level1 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Defence mechanism:</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l3 level2 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">from political speculations like: \u201clife for the sake of future generations\u201d;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l3 level2 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">from cruelty of authorities;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 72pt; tab-stops: list 72.0pt; mso-list: l3 level2 lfo5;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">l<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">from cruelty in interpersonal relations ;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l4 level1 lfo6;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Mechanism of refining human nature and building up civilization\u2019s ethical potential;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l4 level1 lfo6;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Creation of a core, nucleus, root securing unity of civilization in its space expansion, when moving across the immense space;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">In summarizing the arguments produced in evidence of the necessity of developing theoretical solution of the task of civilization indestructibility, it may be noted that the quantity of sub-tasks subject to solution for solving the main task can turn to be huge, and virtually each one of these places demand on construction of its paradigms, its theoretical elaboration. Therefore, at the first phase of developing the theory of civilization indestructibility it makes sense to speak of the general theoretical principles, of general theory of indestructibility, and only thereafter, as deeper solutions of individual, special and partial tasks are found, start building special theories linked to the requirements of development of individual capacities (technological, ethical, evacuation, etc.) and solution of the tasks of a higher competitiveness (in terms of indestructibility theory) of individual social components.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\"><strong style=\"mso-bidi-font-weight: normal;\" id=\"What_must_the_statement_of_the_problems_of_civilization_indestructibility_and_space_expansion_give_to_the_living_generations_of_people_\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">What must the statement of the problems of civilization indestructibility and space expansion give to the living generations of people?</span></strong></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Alleviation of the risks of war \u2013 nothing undermines civilization indestructibility capacities as heavily as wars. MIC resources must be redirected to handling the tasks of and creating capacities for space expansion and Cosmos colonization.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Justification of importance of higher living standards of people \u2013 for only the high living standards enable the possibly maximum number of people to master the sophisticated technologies, realize their talents on their basis, and contribute to the development of ever new and sophisticated technologies. The authorities will increasingly understand that the nations\u2019 competitiveness is largely dependent upon living standards of people, and that social programmes are not wasting money but rather laying a foundation and an important prerequisite of a permanent prosperity and competitiveness of nations.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Attaching new sense to human life. A more responsible attitude of people to their own and others\u2019 lives, higher ethical standards of human relations, hence, lowering crime rate and terrorist activities.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">A major ideological justification for conflict resolution, unification of nations and civilization as a whole.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New living spaces.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New sources of raw materials.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-ansi-language: EN-US; mso-bidi-font-family: Wingdings;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New employment sectors and jobs.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -18pt; margin: 0cm 0cm 6pt 36pt; tab-stops: list 36.0pt; mso-list: l7 level1 lfo8;\"><span style=\"font-family: Wingdings; mso-fareast-font-family: Wingdings; mso-bidi-font-family: Wingdings;\"><span style=\"mso-list: Ignore;\">\u00d8<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">New markets.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; margin-bottom: 6pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; margin-bottom: 6pt;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">REFERENCES</span></p>\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">1.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Lefevre V.A. Space Subject. Moscow, Kogito-Centr Publishing house, 2005, 220p.</span></p>\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">2.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Nazaretyan A.P. Civilizational Crises in the Context of Universal History. 2-nd ed. Moscow, Mir Publishing house, 2004, </span>367 <span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">p.</span></p>\n<p class=\"MsoNormal\" style=\"page-break-after: avoid; text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">3.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Hvan M.P. A Violent Universe: from the Big Bang up to Accelerated Expansion, from Quarks to Superstrings. Moscow, URSS Publishers, 2006, 408p.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; text-indent: -19.5pt; margin: 0cm 0cm 6pt 37.5pt; tab-stops: list 37.5pt; mso-list: l8 level1 lfo14;\"><a name=\"FF0\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-list: Ignore;\">4.<span style=\"font: 7pt 'Times New Roman';\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Narlikar Jayant \"Violent Phenomena in the Universe\", </span></a><span style=\"mso-bookmark: FF0;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Oxford</span></span><span style=\"mso-bookmark: FF0;\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\"> UP</span></span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">, 1984, 246&nbsp;\u0440.</span><br>\n</p><hr size=\"1\">\n<p></p>\n</div>\n<div style=\"mso-element: footnote-list;\">\n<div id=\"ftn1\" style=\"mso-element: footnote;\">\n<p class=\"MsoFootnoteText\"><a style=\"mso-footnote-id: ftn1;\" name=\"_ftn1\" href=\"/#_ftnref1\"><span class=\"MsoFootnoteReference\"><span style=\"mso-special-character: footnote;\"><span class=\"MsoFootnoteReference\"><span style=\"font-family: 'Times New Roman'; font-size: 10pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: RU; mso-fareast-language: RU; mso-bidi-language: AR-SA;\">[1]</span></span></span></span></a><span style=\"mso-ansi-language: EN-US;\"> <span lang=\"EN-US\">This law is known in a somewhat benign definition, not associated with the problems of civilization space expansion and competitiveness, as a law of techno-humanitarian balance [Nazaretyan A.P., 2004, p. 112]: \u201cthe greater the power of productive and combat technologies, the greater the need for more sophisticated tools of cultural regulation for preserving the society\u201d.</span></span></p>\n</div>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "The problem of mankind indestructibility in disastrously unpredictable environment", "anchor": "The_problem_of_mankind_indestructibility_in_disastrously_unpredictable_environment", "level": 2}, {"title": "Dangers of human race destruction", "anchor": "Dangers_of_human_race_destruction", "level": 2}, {"title": "For several years now the author has maintained an Internet project \u201cMultiverse Dossier\u201d (in Russian) \u00a0(http://www.mirozdanie.narod.ru) whose several sections carry a big number of scientific papers and messages of the last space discoveries, which suggest a conclusion of a catastrophic character of the processes running in Space, and of unpredictability of impact thereof on life in the part of the Space inhabited by humankind. Not much more predictable are geological processes, many of which may come to be sources of global natural disasters. Indeed, nearly each step in the evolution of civilization brings along new threats and risks to its existence. ", "anchor": "For_several_years_now_the_author_has_maintained_an_Internet_project__Multiverse_Dossier___in_Russian____http___www_mirozdanie_narod_ru__whose_several_sections_carry_a_big_number_of_scientific_papers_and_messages_of_the_last_space_discoveries__which_suggest_a_conclusion_of_a_catastrophic_character_of_the_processes_running_in_Space__and_of_unpredictability_of_impact_thereof_on_life_in_the_part_of_the_Space_inhabited_by_humankind__Not_much_more_predictable_are_geological_processes__many_of_which_may_come_to_be_sources_of_global_natural_disasters__Indeed__nearly_each_step_in_the_evolution_of_civilization_brings_along_new_threats_and_risks_to_its_existence__", "level": 1}, {"title": "Natural:", "anchor": "Natural_", "level": 2}, {"title": "Threat of life destruction on a space scale", "anchor": "Threat_of_life_destruction_on_a_space_scale", "level": 2}, {"title": "Indestructibility as civilization\u2019s principal supertask ", "anchor": "Indestructibility_as_civilization_s_principal_supertask_", "level": 1}, {"title": "Qualities of indestructible civilization", "anchor": "Qualities_of_indestructible_civilization", "level": 2}, {"title": "Let us define the qualities rendering civilization indestructible. In so doing, it would be necessary to answer a number of questions:", "anchor": "Let_us_define_the_qualities_rendering_civilization_indestructible__In_so_doing__it_would_be_necessary_to_answer_a_number_of_questions_", "level": 1}, {"title": "Concerning the necessity of developing theoretical principles of handling the tasks of humankind indestructibility ", "anchor": "Concerning_the_necessity_of_developing_theoretical_principles_of_handling_the_tasks_of_humankind_indestructibility_", "level": 1}, {"title": "What must the statement of the problems of civilization indestructibility and space expansion give to the living generations of people?", "anchor": "What_must_the_statement_of_the_problems_of_civilization_indestructibility_and_space_expansion_give_to_the_living_generations_of_people_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-17T13:58:02.394Z", "modifiedAt": null, "url": null, "title": "Weirdtopias in science fiction", "slug": "weirdtopias-in-science-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:55.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ot9AYj5kkmietktoK/weirdtopias-in-science-fiction", "pageUrlRelative": "/posts/ot9AYj5kkmietktoK/weirdtopias-in-science-fiction", "linkUrl": "https://www.lesswrong.com/posts/ot9AYj5kkmietktoK/weirdtopias-in-science-fiction", "postedAtFormatted": "Friday, December 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weirdtopias%20in%20science%20fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeirdtopias%20in%20science%20fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fot9AYj5kkmietktoK%2Fweirdtopias-in-science-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weirdtopias%20in%20science%20fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fot9AYj5kkmietktoK%2Fweirdtopias-in-science-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fot9AYj5kkmietktoK%2Fweirdtopias-in-science-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 473, "htmlBody": "<p>There's \"Day Million\" and <a href=\"http://en.wikipedia.org/wiki/The_Age_of_the_Pussyfoot\">The Age of the Pussyfoot</a>, both by Frederik Pohl, and even they might be more like utopias rather than adequately weird.</p>\n<p>\"Day Million\" is a very short story, with a narration which puts emphasis on how much people like living in that world even though it would make little sense to a contemporary reader. Unfortunately, the most vivid detail is a spoiler. N pbhcyr (obgu bs jubz jbhyq frrz bqq, gubhtu bar'f pyrneyl znyr naq gur bgure'f pyrneyl srznyr ner nggenpgrq gb rnpu bgure-- gurl unir n tbbq gvzr bapr, naq gura qb jung'f abezny va gung phygher-- rkpunatr vqragvgl gncrf</p>\n<p>Silverberg's <a href=\"http://en.wikipedia.org/wiki/The_World_Inside\">The World Inside</a> might count.</p>\n<p>It a description of how people might be pretty happy living in a maximum population world.</p>\n<p>R.A. Lafferty's <a href=\"http://web.archive.org/web/20060719184509/www.scifi.com/scifiction/classics/classics_archive/lafferty5/lafferty51.html\">Slow Tuesday Night</a> is&nbsp; a weirdtopia, and so is his <a href=\"http://books.google.com/books?id=Y_FoU_KMOmkC&amp;pg=PA119&amp;lpg=PA119&amp;dq=primary+education+among+the+camiroi&amp;source=bl&amp;ots=C6TF9mOYj9&amp;sig=xwMF5bJhy_irePbexOp-VcrRZnQ&amp;hl=en&amp;ei=H_ELTbqmBIO8lQen9anXCw&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=3&amp;ved=0CCUQ6AEwAg#v=onepage&amp;q&amp;f=false\">Primary Education of the Camiroi</a>. Unfortunately, his \"Polity and Custom among the Camiroi\" is incomplete at google books, but I recommend getting a paper copy of <em>Nine Hundred Grandmothers</em>-- the collection has some of his best work.</p>\n<p>gwern supplied the link for <a href=\"http://dl.dropbox.com/u/5317066/polity-and-custom-of-the-camiroi.txt\">Polity and Custom</a>:</p>\n<blockquote>No assembly on Camiroi for purposes of entertainment may exceed thirty-nine persons. No more than this number may witness any spectacle or drama, or hear a musical presentation, or watch a sporting event. This is to prevent the citizens from becoming mere spectators rather than originators or partakers. Similarly, no writing -- other than certain rare official promulgations -- may be issued in more than thirty-nine copies in one month. This, it seems to us, is a conservative ruling to prevent popular enthusiasms.  A father of a family who twice in five years appeals to specialists for such things as simple surgery for members of his household, or legal or financial or medical advice, or any such things as he himself should be capable of doing, shall lose his citizenship. It seems to us that this ruling obstructs the Camiroi from the full fruits of progress and research.</blockquote>\n<p>\"Slow Tuesday Night\" is a whimsy about people who've had a mental stutter removed-- they live so fast that they can have three careers in eight hours. \"Primary Education Among the Camiroi\" is about a culture which develops maximum intelligence and self-reliance, at the cost of a few of the children being killed. It teaches slow reading (reading slowly enough that everything is remembered), and the world government course consists of governing a world (not a first aspect world) for three or four months.</p>\n<p>\"Winthrop Was Stubborn\" by William Tenn-- a group of time travelers are trapped in the future because one of them doesn't want to go home. I only remember a little of it-- I think there was artificial living/moving food-- but the point of the story was to portray a society which was weird for a modern reader and delightful for its inhabitants.</p>\n<p>Any other nominations?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ot9AYj5kkmietktoK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 6.572039361174694e-07, "legacy": true, "legacyId": "4342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-17T17:08:02.121Z", "modifiedAt": null, "url": null, "title": "TrailMemes for Sequences", "slug": "trailmemes-for-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.405Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drc500free", "createdAt": "2010-10-08T13:19:05.707Z", "isAdmin": false, "displayName": "drc500free"}, "userId": "yFBABHhGzkb326vZg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s3KMvmCXC9psYhEvt/trailmemes-for-sequences", "pageUrlRelative": "/posts/s3KMvmCXC9psYhEvt/trailmemes-for-sequences", "linkUrl": "https://www.lesswrong.com/posts/s3KMvmCXC9psYhEvt/trailmemes-for-sequences", "postedAtFormatted": "Friday, December 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20TrailMemes%20for%20Sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrailMemes%20for%20Sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3KMvmCXC9psYhEvt%2Ftrailmemes-for-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=TrailMemes%20for%20Sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3KMvmCXC9psYhEvt%2Ftrailmemes-for-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3KMvmCXC9psYhEvt%2Ftrailmemes-for-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>One of the obstacles I faced when first confronted with the Sequences was figuring out the prerequisites for any given post. At times this is spelled out explicitly, but there are many parallel paths and cross-referencing. Another problem was that posts don't link forward&nbsp;to the posts that reference them.</p>\n<p><a href=\"http://trailmeme.com\">TrailMeme</a> lets you \"blaze a trail\" through a mass of blog posts by creating a directed graph, which other users can then \"walk.\" The site is still in Beta, and can be unstable at times.</p>\n<p>I tried to more or less follow the posted pre-requisites and link-backs, but culled a lot of the redundant ones to reduce confusion. I found&nbsp;one pre-existing trail, and invite anyone with free&nbsp;time to contribute!&nbsp;Trails so far:</p>\n<p><a href=\"http://trailmeme.com/trails/Mysterious_Answers_to_Mysterious_Questions\">Mysterious Answers to Mysterious Questions</a></p>\n<p><a href=\"http://trailmeme.com/trails/A_Humans_Guide_to_Words\">A Human's Guide to Words</a></p>\n<p><a href=\"http://trailmeme.com/trails/Reductionism\">Reductionism</a></p>\n<p><a href=\"http://trailmeme.com/trails/Map_and_Territory\">Map and Territory</a></p>\n<p><a href=\"http://www.trailmeme.com/trails/Politics_is_the_Mind-Killer\">Politics is the Mind-Killer</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s3KMvmCXC9psYhEvt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 18, "extendedScore": null, "score": 6.572514311162407e-07, "legacy": true, "legacyId": "4344", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-17T18:53:15.242Z", "modifiedAt": null, "url": null, "title": "Turing Test Tournament For Funding", "slug": "turing-test-tournament-for-funding", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.593Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Strange7", "createdAt": "2010-02-12T08:30:10.267Z", "isAdmin": false, "displayName": "Strange7"}, "userId": "hKxerxxgheQZCxHsR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DrusdTRCBR3joKvm7/turing-test-tournament-for-funding", "pageUrlRelative": "/posts/DrusdTRCBR3joKvm7/turing-test-tournament-for-funding", "linkUrl": "https://www.lesswrong.com/posts/DrusdTRCBR3joKvm7/turing-test-tournament-for-funding", "postedAtFormatted": "Friday, December 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Turing%20Test%20Tournament%20For%20Funding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATuring%20Test%20Tournament%20For%20Funding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrusdTRCBR3joKvm7%2Fturing-test-tournament-for-funding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Turing%20Test%20Tournament%20For%20Funding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrusdTRCBR3joKvm7%2Fturing-test-tournament-for-funding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDrusdTRCBR3joKvm7%2Fturing-test-tournament-for-funding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>It's always troubled me that the standard Turing test provides only a single-bit output, and that the human being questioned could throw the game to make their AI counterpart look good. Also, research and development gets entirely too much funding based on what sounds cool rather than what actually works. The following is an attempt to address both issues.</p>\n<p>&nbsp;</p>\n<p>Take at least half a dozen chatbot AIs, and a similar number of humans with varying levels of communication skill (professional salespeople, autistic children, etc.). Each competitor gets a list of questions. A week later, to allow time for research and number-crunching, collect the answers. Whoever submitted question 1 receives all the answers to question 1 in a randomized order, and then ranks the answers from most human/helpful to least, with a big prize for the top and successively smaller prizes for runners-up. Alternatively, interrogators could specify a customized allocation of their question's rewards, e.g. \"this was the best, these three are tied for second, the rest are useless.\"</p>\n<p>&nbsp;</p>\n<p>The humans will do their best in that special way that only well-paid people can, and the chatbots will receive additional funding in direct proportion to their success at a highly competitive task.</p>\n<p>&nbsp;</p>\n<p>Six hundred thousand seconds might seem like an awfully long time to let a supercomputer chew over it's responses, but the goal is deep reasoning, not just snappy comebacks. Programs can always be debugged and streamlined, or just run on more powerful future hardware, after the basic usefulness of the results has been demonstrated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DrusdTRCBR3joKvm7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -4, "extendedScore": null, "score": 6.572777362522708e-07, "legacy": true, "legacyId": "4346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-17T21:20:46.169Z", "modifiedAt": null, "url": null, "title": "Folk grammar and morality", "slug": "folk-grammar-and-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:39.025Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oZ3L6JXE8rMdc8Huo/folk-grammar-and-morality", "pageUrlRelative": "/posts/oZ3L6JXE8rMdc8Huo/folk-grammar-and-morality", "linkUrl": "https://www.lesswrong.com/posts/oZ3L6JXE8rMdc8Huo/folk-grammar-and-morality", "postedAtFormatted": "Friday, December 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Folk%20grammar%20and%20morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFolk%20grammar%20and%20morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZ3L6JXE8rMdc8Huo%2Ffolk-grammar-and-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Folk%20grammar%20and%20morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZ3L6JXE8rMdc8Huo%2Ffolk-grammar-and-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZ3L6JXE8rMdc8Huo%2Ffolk-grammar-and-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 392, "htmlBody": "<p>If you've spent any time with foreigners learning your language, you may have been in conversations like this:</p>\n<blockquote>\n<p>Mei: I'm a bit confused ... what's the difference between \"even though\" and \"although\"?</p>\n<p>Albert: Um, I think they're mostly equivalent, but \"even though\" is a bit more emphatic.</p>\n<p>Barry: Are you sure ? I remember something about one being for positives, and the other for negatives. For example, let's see, these sentences sound a bit weird:<em>\"He refused to give me the slightest clue, although I begged on my knees\"</em>, and <em>\"Although his car broke down on the first mile, he still won the rally\"</em>.</p>\n</blockquote>\n<p>People can't automatically state the rules underlying language, even though they follow them perfectly in their daily speech. I've been made especially aware of this when teaching French to Chinese students, where I had to frequently revise my explanation, or just say \"sorry, I don't know what the rule is for this case, you'll just have to memorize it\". You learn separately how to speak the language and how to apply the rules.</p>\n<p>Morality is similar: we feel what's wrong and what's right, but may not be able to formulate the underlying rules. And when we do, we're likely to get it wrong the first time. For example you <a href=\"/lw/3bc/varying_amounts_of_subjective_experience/\">might say</a>:</p>\n<blockquote>\n<p>It has been suggested that animals have less subjective experience than people. For example, it would be possible to have an animal that counts as half a human for the purposes of morality.</p>\n</blockquote>\n<p>But unlike grammar, people don't always agree on right and wrong : if Alfred unintentionally harms Barry, Barry is more likely to think that what Alfred did was morally wrong, even if both started off with similar moral intuitions. So if you come up with an explanation and insist it's the definition of morality, you can't be \"proven wrong\" nearly as easily as on grammar. You may even insist your explanation is true, and adjust your behavior accordingly, as some religious fanatics seem to do (\"what is moral is what God said\" being a quite common rule people come up with to explain morality).</p>\n<p>So: beware of your own explanations. Morality is a <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">complex topic</a>, you're even more likely to shoot yourself in the foot than with grammar, and even less likely to realize that you're wrong.</p>\n<p>(edit) Related posts by Eliezer: <a href=\"/lw/kq/fake_justification/\">Fake Justification</a>, <a href=\"/lw/kx/fake_selfishness/\">Fake Selfishness</a>, <a href=\"/lw/ky/fake_morality/\">Fake Morality</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oZ3L6JXE8rMdc8Huo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 28, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "4318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RHPxkWz4WhrzM6Q8H", "bfbiyTogEKWEGP96S", "Masoq4NdmmGSiq2xw", "fATPBv4pnHC33EmJ2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-18T00:33:07.925Z", "modifiedAt": null, "url": null, "title": "Evidence for surprising ease of de-nuclearization ", "slug": "evidence-for-surprising-ease-of-de-nuclearization", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.191Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RTQrCaJyYEcYkfWKv/evidence-for-surprising-ease-of-de-nuclearization", "pageUrlRelative": "/posts/RTQrCaJyYEcYkfWKv/evidence-for-surprising-ease-of-de-nuclearization", "linkUrl": "https://www.lesswrong.com/posts/RTQrCaJyYEcYkfWKv/evidence-for-surprising-ease-of-de-nuclearization", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evidence%20for%20surprising%20ease%20of%20de-nuclearization%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvidence%20for%20surprising%20ease%20of%20de-nuclearization%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTQrCaJyYEcYkfWKv%2Fevidence-for-surprising-ease-of-de-nuclearization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evidence%20for%20surprising%20ease%20of%20de-nuclearization%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTQrCaJyYEcYkfWKv%2Fevidence-for-surprising-ease-of-de-nuclearization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTQrCaJyYEcYkfWKv%2Fevidence-for-surprising-ease-of-de-nuclearization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://yglesias.thinkprogress.org/2010/12/the-symbolic-power-of-nuclear-deterrents/\">http://yglesias.thinkprogress.org/2010/12/the-symbolic-power-of-nuclear-deterrents/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RTQrCaJyYEcYkfWKv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "4347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-18T00:57:08.114Z", "modifiedAt": null, "url": null, "title": "Link: Facing the Mind-Killer", "slug": "link-facing-the-mind-killer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:05.265Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZHpg54EtMZ3k96TjQ/link-facing-the-mind-killer", "pageUrlRelative": "/posts/ZHpg54EtMZ3k96TjQ/link-facing-the-mind-killer", "linkUrl": "https://www.lesswrong.com/posts/ZHpg54EtMZ3k96TjQ/link-facing-the-mind-killer", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Facing%20the%20Mind-Killer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Facing%20the%20Mind-Killer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHpg54EtMZ3k96TjQ%2Flink-facing-the-mind-killer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Facing%20the%20Mind-Killer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHpg54EtMZ3k96TjQ%2Flink-facing-the-mind-killer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHpg54EtMZ3k96TjQ%2Flink-facing-the-mind-killer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-GB</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif]-->\n<p>I've long opposed discussing politics on Less Wrong. Elsewhere, however, I have been known to gaze into the abyss; and so it came to be that I wrote a handful of blog posts of the <a href=\"http://oxlib.blogspot.com/\">Oxford Libertarian Society Blog</a>. I had the deliberate intention of bring a little bit of rationality into politics - and so of course ended up writing in something like Eliezer's style.</p>\n<p>I wanted to establish some theory first, so the initial posts were about <a href=\"http://oxlib.blogspot.com/2010/09/looking-for-proof.html\">The Conservation of Expected Evidence</a> and <a href=\"http://oxlib.blogspot.com/2010/09/getting-past-words.html\">Reductionism</a>, and then one particular <a href=\"http://oxlib.blogspot.com/2010/09/like-many-people-i-was-atheist-before-i.html\">Death-Spiral</a>.</p>\n<p>As you'll probably notice, one of my defences against the little-death has been to err on the side of attacking Libertarian positions; I provided an account of <a href=\"http://oxlib.blogspot.com/2010/09/traditional-socialist-values.html\">Traditional Socialist Values</a> so we remember that our enemies aren't inherently evil, and then analysed <a href=\"http://oxlib.blogspot.com/2010/12/where-theorem-doesnt-apply.html\">an abuse of The Law of Comparative Advantage</a>, showing cases where it didn't apply.</p>\n<p>I can't promise I'll update at all regularly.</p>\n<p>&nbsp;</p>\n<p>Post inspired by <a href=\"/r/discussion/lw/3ba/link_my_somethinglikefriendlinessresearch_blog\">Will Newsome</a> and prompted by Vladimir Nesov.</p>\n<p><a href=\"http://oxlib.blogspot.com/\">http://oxlib.blogspot.com/</a></p>\n<p class=\"MsoNormal\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZHpg54EtMZ3k96TjQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 6.573687225572504e-07, "legacy": true, "legacyId": "4348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["775AJbZQFDmfcJS9b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-18T02:40:39.146Z", "modifiedAt": null, "url": null, "title": "Brain blogger survey", "slug": "brain-blogger-survey", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.416Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/87WNChRBjRYbEJnCk/brain-blogger-survey", "pageUrlRelative": "/posts/87WNChRBjRYbEJnCk/brain-blogger-survey", "linkUrl": "https://www.lesswrong.com/posts/87WNChRBjRYbEJnCk/brain-blogger-survey", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain%20blogger%20survey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain%20blogger%20survey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87WNChRBjRYbEJnCk%2Fbrain-blogger-survey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain%20blogger%20survey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87WNChRBjRYbEJnCk%2Fbrain-blogger-survey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87WNChRBjRYbEJnCk%2Fbrain-blogger-survey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<p><a href=\"http://alicerosebell.wordpress.com/2010/12/17/brain-bloggers/\">Survey info</a></p>\n<p>:</p>\n<blockquote>I&rsquo;m currently doing some research on brain bloggers. The first stage is a rather basic survey (below). This is open from today until Monday the 10th of January.</blockquote>\n<blockquote><br /></blockquote>\n<blockquote>By &lsquo;brain bloggers&rsquo; I mean bloggers who write about the stuff that goes in people&rsquo;s heads, whatever we think this stuff is. Such bloggers might focus on neurology or psychology, or another field entirely. It might be the history, anthropology or commercial applications of these fields. It might come under &lsquo;research blogging&rsquo;, journalism, &lsquo;public engagement&rsquo; or some form of political activism (or several of these at once, or something else entirely). This focus might be exclusively brain-y, or brain-ish issues might be topics they occasionally blog about in the course of other work.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "87WNChRBjRYbEJnCk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 6.573946104716655e-07, "legacy": true, "legacyId": "4350", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-18T08:28:45.536Z", "modifiedAt": "2021-11-12T03:27:41.725Z", "url": null, "title": "Cryptographic Boxes for Unfriendly AI", "slug": "cryptographic-boxes-for-unfriendly-ai", "viewCount": null, "lastCommentedAt": "2021-08-28T13:54:15.481Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai", "pageUrlRelative": "/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai", "linkUrl": "https://www.lesswrong.com/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryptographic%20Boxes%20for%20Unfriendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryptographic%20Boxes%20for%20Unfriendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Wf3R4NZ77CLczLL2%2Fcryptographic-boxes-for-unfriendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryptographic%20Boxes%20for%20Unfriendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Wf3R4NZ77CLczLL2%2Fcryptographic-boxes-for-unfriendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Wf3R4NZ77CLczLL2%2Fcryptographic-boxes-for-unfriendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1518, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/up/shut_up_and_do_the_impossible/\">Shut up and do the impossible!</a>; Everything about an AI in a box.<br /><br />One solution to the problem of friendliness is to develop a self-improving, unfriendly AI, put it in a box, and ask it to make a friendly AI for us.&nbsp; This gets around the incredible difficulty of developing a friendly AI, but it creates a new, apparently equally impossible problem. How do you design a box strong enough to hold a superintelligence?&nbsp; Lets suppose, optimistically, that researchers on friendly AI have developed some notion of a certifiably friendly AI: a class of optimization processes whose behavior we can automatically verify will be friendly. Now the problem is designing a box strong enough to hold an unfriendly AI until it modifies itself to be certifiably friendly (of course, it may have to make itself smarter first, and it may need to learn a lot about the world to succeed).</p>\n<p><strong>Edit:</strong> Many people have correctly pointed out that certifying friendliness is probably incredibly difficult. I personally believe it is likely to be significantly easier than actually finding an FAI, even if current approaches are more likely to find FAI first. But this isn't really the core of the article. I am describing a general technique for quarantining potentially dangerous and extraordinarily sophisticated code, at great expense. In particular, if we developed uFAI before having any notion of certifiable friendliness, then we could still use this technique to try and use the uFAI in a very limited way. It allows us to quarantine an AI and force everything it tries to say to us through an arbitrarily complicated filter. So, for example, if we are interested in solving some problem with a unique and verifiably correct answer, then we can ask the AI for this answer and only see the output if it passes the verification. We can also use a quarantined AI in more useful ways, at the expense of introducing some limited channels of influence (for example, if we ask for the solution to a problem which may have several solutions, we allow the AI to influence us by choosing which solution to provide)</p>\n<p>Minimally, I hope this scheme at least suggests that putting an AI in a box may not be catastrophic. What to do with a boxed AI is another question.<strong> /Edit.</strong></p>\n<p>Concretely, suppose someone comes to you with the design of a recursively self-improving AI---call it Hal.&nbsp; Unfortunately, Hal is not certifiably friendly.&nbsp; Maybe it is friendly but not provably so, but more likely it is ambivalent to the human condition and is likely to destroy the universe if you just tell it to make a friendly AI.&nbsp; Nevertheless, you think Hal is smart enough to solve the friendly AI problem for you.&nbsp; All you need to do is give it access to all of the information about the world it wants and a couple of hours to run; after that, it should output a modified version of itself which is certifiably friendly.<br /><br />You almost run Hal and ask it to design a friendly AI for you. After all, if you just run Hal and check to see if it outputs a certifiably friendly AI, what can it do other than refuse?&nbsp; But after some more thought you realize this is overly optimistic.&nbsp; Maybe you can avoid giving it an output channel that a human will have to read, thereby eliminating the most obvious failure mode of a boxed AI, but you still have to give it access to a computer to run on.&nbsp; Computation produces electromagnetic radiation, heat, etc., and it is honestly hard to say what a superintelligence could do with such a channel.<br /><br />We seem to be faced with an impossible problem.&nbsp; We need to ask Hal to design us a certifiably friendly AI, but we also need to give him absolutely no ability to interact with the world destructively.<br /><br />There is one field in particular which has a good history of solving obviously impossible problems: computational cryptography.&nbsp; We want to implement the following obviously impossible device.&nbsp; A cage into which we can place Hal so that he can think and examine all of the data we give him, but from which he cannot interact with his environment.&nbsp; In the cage with him is a tamper-proof arbiter implementing our friendliness test.&nbsp; Hal may give the arbiter the code to a single AI; if the arbiter confirms that this AI is certifiably friendly it will kill Hal, replace him with the friendly AI, and then allow the friendly AI to send a single message out of the cage.<br /><br />To implement this impossible device we co-opt an important cryptographic primitive called public key homomorphic encryption, typically used for the impossible problem of private delegated computation.&nbsp; To get a basic idea you can see the <a href=\"http://en.wikipedia.org/wiki/Homomorphic_encryption#Fully_homomorphic_encryption\">Wikipedia article on homomorphic encryption</a>. I'll give the brief description here (you can skip the next two paragraphs if you are familiar with homomorphic encryption).<br /><br />A public key encryption scheme has the following properties.&nbsp; Anyone can encrypt a message using a publicly published key, but you cannot efficiently distinguish the encryptions of two different messages unless you know something about the secret key.&nbsp; A fully homomorphic encryption scheme has the additional counter intuitive property that, although you cannot learn anything about the encrypted data without the secret key, you can still perform arbitrary operations on it.&nbsp; In particular, if you have a homomorphic encryption of the source code of a program you can run that program without learning anything about it (except how long the source is), but the output of the program will be homomorphically encrypted.&nbsp; All of this can be done using only the public key.<br /><br />The existence of a fully homomorphic encryption scheme was completely open until recently, when it was shown by Craig Gentry that such a scheme exists under the assumption that a certain problem on lattices is computationally intractable.&nbsp; Known homomorphic encryption schemes are very inefficient and based on new assumptions, but we can realistically expect improvements in the coming years and decades both in speed and the strength of the necessary assumptions.<br /><br />If you run a homomorphically encrypted AI, sufficiently isolated from the private key, you get the very strong guarantee that the AI's execution has no computationally detectable effect on the rest of the world.&nbsp; If the AI was able to effect the world in a computationally detectable way, then we would have successfully broken the homomorphic encryption scheme by distinguishing the encrypted execution of the AI from the encrypted execution of some benign program.<br /><br />Now we need to give the arbiter some means to allow a genuinely friendly AI access to the rest of the world, without giving Hal any illegitimate access.&nbsp; We allow Hal access to an output channel whose outputs are homomorphically encrypted.&nbsp; After some time, we destroy Hal and then homomorphically check to see if he has output the code of a certifiably friendly AI over the channel (we have to decode a single homomorphically encrypted bit to perform this check).&nbsp; If he hasn't, then we do nothing---apparently Hal would rather not give us a friendly AI.&nbsp; If he has output the code of a certifiably friendly AI, we homomorphically run it, obtain a homomorphic encryption of its output, and then decrypt only this output.<br /><br />That's the entire scheme.&nbsp; Whenever you have some code whose execution itself might do incredible damage, this precaution allows you to execute it in perfect safety (the main caveat is that you need an automatic way to screen the output which doesn't give the algorithm enough room to possibly do harm).</p>\n<p><strong>Note:</strong> If you are really exceptionally paranoid you may be able to obtain even better guarantees by developing more powerful cryptographic techniques tailored for this application.&nbsp; In particular, the secret key used by the encryption procedure may be leaked to a sufficiently powerful AI through a side channel (the computations involving the key typically leak some data in the form of radiation, which might be recovered from sufficiently exhaustive sensor data by a sufficiently powerful AI).&nbsp; By doing these computations on a quantum computer you can probably guarantee that no data was leaked. If you want to be even more fantastically paranoid (ie, if you want a formal proof of security, which is a rare treat in real problems) you will actually have to shred the key, guaranteeing that an AI will have to interact with the shredded key to recover it. If you do this, you have to use another new primitive to implement the final revelation of the friendly AI's message.<br /><br />If you have any concerns about the security of this protocol, I would be happy to try and defend it and would be surprised and interested if I failed. Whether or not such a cryptographic box is really an interesting or important object is another question. (It is interesting to me as an unexpected application of cryptography).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 9, "MhHM6Rx2b4F8tHTQk": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2Wf3R4NZ77CLczLL2", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 43, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "4355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": true, "version": "1.0.0", "pingbacks": {"Posts": ["nCvvhFBaayaXyuBiD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": "2021-11-12T03:27:41.633Z", "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": "qgdGA4ZEyW7zNdK84"}, {"createdAt": null, "postedAt": "2010-12-18T13:26:18.180Z", "modifiedAt": null, "url": null, "title": "An Intuitive Explanation of Eliezer Yudkowsky\u2019s Intuitive Explanation of Bayes\u2019 Theorem", "slug": "an-intuitive-explanation-of-eliezer-yudkowsky-s-intuitive", "viewCount": null, "lastCommentedAt": "2022-01-03T15:40:43.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/69gof6oyZuNiSt7XC/an-intuitive-explanation-of-eliezer-yudkowsky-s-intuitive", "pageUrlRelative": "/posts/69gof6oyZuNiSt7XC/an-intuitive-explanation-of-eliezer-yudkowsky-s-intuitive", "linkUrl": "https://www.lesswrong.com/posts/69gof6oyZuNiSt7XC/an-intuitive-explanation-of-eliezer-yudkowsky-s-intuitive", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Intuitive%20Explanation%20of%20Eliezer%20Yudkowsky%E2%80%99s%20Intuitive%20Explanation%20of%20Bayes%E2%80%99%20Theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Intuitive%20Explanation%20of%20Eliezer%20Yudkowsky%E2%80%99s%20Intuitive%20Explanation%20of%20Bayes%E2%80%99%20Theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69gof6oyZuNiSt7XC%2Fan-intuitive-explanation-of-eliezer-yudkowsky-s-intuitive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Intuitive%20Explanation%20of%20Eliezer%20Yudkowsky%E2%80%99s%20Intuitive%20Explanation%20of%20Bayes%E2%80%99%20Theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69gof6oyZuNiSt7XC%2Fan-intuitive-explanation-of-eliezer-yudkowsky-s-intuitive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F69gof6oyZuNiSt7XC%2Fan-intuitive-explanation-of-eliezer-yudkowsky-s-intuitive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p><a href=\"http://www.commonsenseatheism.com\">Common Sense Atheism</a> has recently had a string of fantastic introductory LessWrong related material. First easing its audience into the <a href=\"http://commonsenseatheism.com/?p=10037\">singularity</a>, then <a href=\"http://commonsenseatheism.com/?cat=61\">summarising the sequences</a>, yesterday affirming that <a href=\"http://commonsenseatheism.com/?p=12888\">Death is a Problem to be Solved</a>, and finally today by presenting&nbsp;<a href=\"http://commonsenseatheism.com/?p=13156\">An Intuitive Explanation of Eliezer Yudkowsky&rsquo;s Intuitive Explanation of Bayes&rsquo; Theorem</a>.&nbsp;</p>\n<p>From the article:</p>\n<blockquote>\n<p>Eliezer&rsquo;s explanation of this hugely important law of probability is probably the best one on the internet, but I fear it may still be too fast-moving for those who haven&rsquo;t needed to do even&nbsp;<em style=\"padding: 0px; margin: 0px;\">algeba</em>&nbsp;since high school. Eliezer calls it &ldquo;excruciatingly gentle,&rdquo; but he must be measuring &ldquo;gentle&rdquo; on a scale for people who were reading Feynman at age 9 and doing calculus at age 13 like&nbsp;<em style=\"padding: 0px; margin: 0px;\">him</em>.</p>\n<p><strong style=\"padding: 0px; margin: 0px;\">So, I decided to write an&nbsp;<em style=\"padding: 0px; margin: 0px;\">even gentler</em>&nbsp;introduction to Bayes&rsquo; Theorem.</strong>&nbsp;One that is gentle for&nbsp;<em style=\"padding: 0px; margin: 0px;\">normal&nbsp;</em>people.</p>\n</blockquote>\n<p>It may be interesting if you want to do a review of Bayes' Theorem from a different perspective, or offer some introductory material for others. From a wider viewpoint, it's great to see a popular blog joining our cause for <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "69gof6oyZuNiSt7XC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 24, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "4356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-18T13:41:43.455Z", "modifiedAt": null, "url": null, "title": "When and how psychological data is collected affects the kind of students who volunteer", "slug": "when-and-how-psychological-data-is-collected-affects-the", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KMDYj5rrANzcpieR7/when-and-how-psychological-data-is-collected-affects-the", "pageUrlRelative": "/posts/KMDYj5rrANzcpieR7/when-and-how-psychological-data-is-collected-affects-the", "linkUrl": "https://www.lesswrong.com/posts/KMDYj5rrANzcpieR7/when-and-how-psychological-data-is-collected-affects-the", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20and%20how%20psychological%20data%20is%20collected%20affects%20the%20kind%20of%20students%20who%20volunteer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20and%20how%20psychological%20data%20is%20collected%20affects%20the%20kind%20of%20students%20who%20volunteer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMDYj5rrANzcpieR7%2Fwhen-and-how-psychological-data-is-collected-affects-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20and%20how%20psychological%20data%20is%20collected%20affects%20the%20kind%20of%20students%20who%20volunteer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMDYj5rrANzcpieR7%2Fwhen-and-how-psychological-data-is-collected-affects-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMDYj5rrANzcpieR7%2Fwhen-and-how-psychological-data-is-collected-affects-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 609, "htmlBody": "<p><a href=\"http://bps-research-digest.blogspot.com/2010/12/when-and-how-psychological-data-is.html\">http://bps-research-digest.blogspot.com/2010/12/when-and-how-psychological-data-is.html</a></p>\n<blockquote>\n<p>Psychology has a serious problem. You may have heard about its over-dependence on WEIRD participants - that is, those from Western, Educated, Industrialised, Rich Democracies. More specifically, as regular readers will be aware, countless psychology studies involve undergraduate students, particularly psych undergrads. Apart from the obvious fact that this limits the generalisability of the findings, Edward Witt and his colleagues provide evidence in a new paper for two further problems, this time involving self-selection biases.<br /><br />Just over 500 Michigan State University undergrads (75 per cent were female) had the option, at a time of their choosing during the Spring 2010 semester, to volunteer either for an on-line personality study, or a face-to-face version. The data collection was always arranged for Wednesdays at 12.30pm to control for time of day/week effects. Also, the same personality survey was administered by computer in the same way in both experiment types, it's just that in the face-to-face version it was made clear that the students had to attend the research lab, and an experimenter would be present.<br /><br />Just 30 per cent of the sample opted for the face-to-face version. Predictably enough, these folk tended to score more highly on extraversion. The effect size was small (d=-.26) but statistically significant. Regards more specific personality traits, the students who chose the face-to-face version were also more altruistic and less cautious.<br /><br />What about choice of semester week? As you might expect, it was the more conscientious students who opted for dates earlier in the semester (r=.-.20). What's more, men were far more likely to volunteer later in the semester, even after controlling for average personality difference between the sexes. For example, 18 per cent of week one participants were male compared with 52 per cent in the final, 13th week.<br /><br />In other words, the kind of people who volunteer for research will likely vary according to the time of semester and the mode of data collection. Imagine you used false negative feedback on a cognitive task to explore effects on confidence and performance. Participants tested at the start of semester, who are typically more conscientious and motivated, are likely to be affected in a different way than participants who volunteer later in the semester.<br /><br />This isn't the first time that self-selection biases have been reported in psychology. A 2007 study, for example, suggested that people who volunteer for a 'prison study' are likely to score higher than average on aggressiveness and social dominance, thus challenging the generalisability of Zimbardo's seminal work. However, despite the occasional study highlighting these effects, there seems to be little enthusiasm in the social psychological community to do much about it.<br /><br />So what to do? The specific issues raised in the current study could be addressed by sampling throughout a semester and replicating effects using different data collection methods. 'Many papers based on college students make reference to the real world implications of their findings for phenomena like aggression, basic cognitive processes, prejudice, and mental health,' the researchers said. 'Nonetheless, the use of convenience samples place limitations on the kinds of inferences drawn from research. In the end, we strongly endorse the idea that psychological science will be improved as researchers pay increased attention to the attributes of the participants in their studies.'<br />_________________________________<br /><br />Witt, E., Donnellan, M., and Orlando, M. (2011). Timing and selection effects within a psychology subject pool: Personality and sex matter. Personality and Individual Differences, 50 (3), 355-359 DOI: 10.1016/j.paid.2010.10.019</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KMDYj5rrANzcpieR7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 24, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "4357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-18T20:47:00.809Z", "modifiedAt": null, "url": null, "title": "How Pascal's Wager Saved My Soul", "slug": "how-pascal-s-wager-saved-my-soul", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:04.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SRStarin", "createdAt": "2010-12-17T15:04:05.504Z", "isAdmin": false, "displayName": "SRStarin"}, "userId": "ww27zvssCFWLZXRpt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/72bXyQZHPxMNFj7ae/how-pascal-s-wager-saved-my-soul", "pageUrlRelative": "/posts/72bXyQZHPxMNFj7ae/how-pascal-s-wager-saved-my-soul", "linkUrl": "https://www.lesswrong.com/posts/72bXyQZHPxMNFj7ae/how-pascal-s-wager-saved-my-soul", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20Pascal's%20Wager%20Saved%20My%20Soul&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20Pascal's%20Wager%20Saved%20My%20Soul%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F72bXyQZHPxMNFj7ae%2Fhow-pascal-s-wager-saved-my-soul%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20Pascal's%20Wager%20Saved%20My%20Soul%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F72bXyQZHPxMNFj7ae%2Fhow-pascal-s-wager-saved-my-soul", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F72bXyQZHPxMNFj7ae%2Fhow-pascal-s-wager-saved-my-soul", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 543, "htmlBody": "<p>[I know Pascal's Wager isn't a hard logical problem for a Bayesian to tackle. However, please read the following account of how it helped me become more rational.]</p>\n<p>I was a Christian when I was a boy. I believed in the miraculous birth and resurrection of Christ, heaven and hell, God on high answering prayers, and so on and so forth. I also believed in Santa Claus. When I stopped believing in Santa Claus when I was 9, my faith in the other stuff I'd been told was somewhat shaken, but I got over it, as I was told that everyone did.</p>\n<p>When I was 15, I went to a summer program for weird kids (i.e. nerds) called Governor's School. Each week at Governor's School, the entire group of us was addressed by a pencil-necked, jaded philosophy professor called Dr. Bob. (Full disclosure: I am a jaded and pencil-necked person myself.) One week late in the summer, after he'd gotten our trust with other Astounding Insights, he told us about Pascal's Wager, concluding that we should all be Christians. And he left it at that.</p>\n<p>We all found this<em> very strange</em>, as Dr. Bob had seemed rather unfriendly to religion before without ever having said anything outright irreligious. In fact, I didn't like him very much because I believed in the Lord Jesus Christ, and I didn't entirely like being forced to think about that belief too much. But Pascal's Wager was too much: I had always just believed because I'd been told by trustworthy people to believe, and here was this guy I didn't like saying I had to believe.</p>\n<p>So, I couldn't get it out of my mind. You all know Pascal's Wager (and even have Yudkowski's <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> to boot), but I'm going to take it apart the way 15-year-old Scott did:</p>\n<p style=\"padding-left: 30px;\">I thought: \"If Heaven and Hell are real, then it would be best for me if I believed in them. If they aren't real, it wouldn't hurt for me to believe in them, because we're all just dead anyway, in the end. That's straightforward enough.\"</p>\n<p style=\"padding-left: 30px;\">I objected: \"But God doesn't just want our belief in Heaven or Hell, I thought. He wants our love and devotion. Heaven and Hell are just ways of getting Kohlberg 3's and lower to go to church and learn the right way of believing (Another thing we'd all learned by that point was the <a href=\"http://en.wikipedia.org/wiki/Kohlberg%27s_stages_of_moral_development\">Kohlberg stages</a>.) Like using Santa Claus giving gifts at Christmas to keep little kids good.\"</p>\n<p style=\"padding-left: 30px;\">I recalled my 10th Christmas: Ohhhhhh.</p>\n<p style=\"padding-left: 30px;\">I stopped believing in God.</p>\n<p>Truth is the thing we need to know to plan, so that we may live better lives. Trust is valuable for reducing our mental workloads in determining the truth, but it can be used as a weapon. The purpose of lying is to control other people, to make them behave the way you want when the truth would cause them to behave differently, or perhaps just have a greater chance of behaving differently. Pascal's Wager laid bare the promise of Heaven and Hell: It is an attempt to control other people. If these people, who always say I should trust them, already want to control me, they'd probably be willing to lie to me. Once I saw that, the lie was plain.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "Ng8Gice9KNkncxqcj": 1, "irYLXtT9hkPXoZqhH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "72bXyQZHPxMNFj7ae", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 18, "baseScore": 12, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "4360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-18T21:36:38.434Z", "modifiedAt": null, "url": null, "title": "Best of Rationality Quotes 2009/2010", "slug": "best-of-rationality-quotes-2009-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:21.826Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZFMqBSX8CnpAwmWes/best-of-rationality-quotes-2009-2010", "pageUrlRelative": "/posts/ZFMqBSX8CnpAwmWes/best-of-rationality-quotes-2009-2010", "linkUrl": "https://www.lesswrong.com/posts/ZFMqBSX8CnpAwmWes/best-of-rationality-quotes-2009-2010", "postedAtFormatted": "Saturday, December 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20of%20Rationality%20Quotes%202009%2F2010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20of%20Rationality%20Quotes%202009%2F2010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZFMqBSX8CnpAwmWes%2Fbest-of-rationality-quotes-2009-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20of%20Rationality%20Quotes%202009%2F2010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZFMqBSX8CnpAwmWes%2Fbest-of-rationality-quotes-2009-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZFMqBSX8CnpAwmWes%2Fbest-of-rationality-quotes-2009-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 341, "htmlBody": "<p><strong><a title=\"Best of Rationality Quotes 2009/2010\" href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes/rq.html\">Best of Rationality Quotes 2009/2010</a></strong> (Warning: 750kB page, 774 quotes)</p>\n<p>The year's last Rationality Quotes thread has calmed down, so now it is a good time to update my Best of Rationality Quotes page, and write a top post about it. (The original version was <a href=\"/lw/2bi/open_thread_june_2010_part_2/253w?c=1\">introduced</a> in the June 2010 Open Thread.)</p>\n<p>The page was built by a short script (<a title=\"source code\" href=\"http://people.mokk.bme.hu/%7Edaniel/rationality_quotes/\">source code here</a>) from all the LW Rationality Quotes threads so far. (We had such a thread each month since April 2009.) The script collects all comments with karma score 4 or more, and sorts them by score.</p>\n<p>There is a minor complication: The obvious idea is to consider only top-level comments, that is, comments that are not replies to other comments. Unfortunately, good quotes are sometimes replies to other quotes. Of course, even more often, replies are not quotes. This is a precision-recall trade-off. Originally I went for recall, because I liked many replied quotes such as <a href=\"/lw/11r/rationality_quotes_july_2009/w8g?c=1&amp;context=1#comments\">this</a>. But as JGWeissman noted in a comment below, to build the precise version, only a trivial modification of my script is needed. So I built it, and I preferred it to the noisy version after all. So now at the top of this post we have the filtered version, and here is the original version with even more good quotes, but also with many non-quotes:<span class=\"comment-author\"><strong></strong></span><strong></strong><strong></strong></p>\n<p><strong><a title=\"Best of Rationality Quotes 2009/2010, with replied comments\" href=\"http://people.mokk.bme.hu/%7Edaniel/rationality_quotes/rq_noisy.html\">Best of Rationality Quotes 2009/2010, including replied comments</a></strong> (Warning: 1.3MB page, 1358 quotes)</p>\n<p>&nbsp;</p>\n<p><strong>UPDATE:</strong> I changed the links and rewrote the above when I decided to filter replied comments.</p>\n<p><strong>UPDATE 2:</strong> Added a <a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37d7\">comment</a> listing the personal quote collection pages of top quote contributors.</p>\n<p><strong>UPDATE 3:</strong> Responding to various requests by commenters, added several top-lists:</p>\n<ul>\n<li><a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37d7\">Top quote contributors by total karma score collected</a></li>\n<li><a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37do\">Top quote contributors by average score of quotes</a><br /><a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37ej\"></a></li>\n<li><a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37ej\">Top quote contributors by statistical significance level</a></li>\n<li><a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37ds\">Top original authors by number of quotes</a></li>\n<li><a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37dx\">Top original authors by total karma score collected</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZFMqBSX8CnpAwmWes", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 34, "extendedScore": null, "score": 6.576788206657285e-07, "legacy": true, "legacyId": "4343", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-19T01:59:24.944Z", "modifiedAt": null, "url": null, "title": "Quantum Measurements", "slug": "quantum-measurements", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:02.497Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HZ4Gbk5roE2hD8uWu/quantum-measurements", "pageUrlRelative": "/posts/HZ4Gbk5roE2hD8uWu/quantum-measurements", "linkUrl": "https://www.lesswrong.com/posts/HZ4Gbk5roE2hD8uWu/quantum-measurements", "postedAtFormatted": "Sunday, December 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantum%20Measurements&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantum%20Measurements%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZ4Gbk5roE2hD8uWu%2Fquantum-measurements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantum%20Measurements%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZ4Gbk5roE2hD8uWu%2Fquantum-measurements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZ4Gbk5roE2hD8uWu%2Fquantum-measurements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 744, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/r5/the_quantum_physics_sequence/\">The Quantum Physics Sequence</a>, particular <a href=\"/lw/pw/decoherence_is_pointless/\">Decoherence is Pointless</a>.</p>\n<p>If you really understand quantum mechanics (or the linked post on decoherence) you shouldn't get anything out of this post, but understanding this really cleared things up for me, so hopefully it will clear things up for someone else too.</p>\n<p>Here at lesswrong we are probably all good Solomonoff inductors and so tend to reject collapse. We believe that a measurement destroys interference because it entangles some degrees of freedom from a quantum system with its environment. Of course, this process doesn't occur sharply and discontinuously. It happens gradually, as the degree of entanglement is increased. But what exactly does \"gradually\" mean here? Lets focus on a particular example.</p>\n<p>Suppose I run the classic two-slit experiment, but I measure which slit the electron goes through. Of course, I don't observe an interference pattern. What happens if we \"measure it&nbsp; less\"? Lets go to the extreme: I change the polarization of a single photon depending on which slit the electron went through (its either polarized vertically or horizontally), and I send that photon off to space (where its polarization will not be coupled to any other degrees of freedom). Do I now see just a little bit of an interference pattern?</p>\n<p>A naive guess is that strength of the interference pattern drops off exponentially with the number of degrees of freedom entangled with the measurement of which slit the electron went through. In a certain sense, this is completely correct. But if I were to perform the experiment exactly as I described---in particular, if I were to polarize the photon perfectly horizontally in the one case and perfectly vertically in the other case---then I would observe no interference at all. (This may or may not be possible, depending on the way nature chose to implement electrons, photons, and slits. Whether or not you can measure exactly is really not philosophically important to quantum mechanics, and I feel completely confident saying that science doesn't yet know the answer. So I guess I'm not yet decided on whether decoherence really happens \"gradually.\" )</p>\n<p>The important thing is that two paths leading to the same state only interfere if they lead to <em>exactly</em> the same state. The two ways for the electron to get to the center of the screen interfere by default because nature has no record of how the electron got there. If you measure at all, even with one degree of freedom, then the two ways for the electron to get to the same place on the screen don't lead to the exactly same state and so interference doesn't occur.</p>\n<p>The exponential dependence on the number of degrees of freedom comes from the error in our measurement devices. If I prepare one photon polarized either horizontally or vertically, and I do it very precisely, then I am very unlikely to mistake one case for the other and I will therefore see very little interference. If I do it somewhat less precisely, then the probability of a measurement error increases and so does the strength of the interference pattern. However, if I create 1000 photons, each polarized approximately correctly, then by taking a majority vote I can almost certainly correctly learn which slit the electron went through, and the interference pattern disappears again. The probability of error drops off exponentially, and so does the interference.</p>\n<p>Another issue (which is related in my head, probably just because I finally understood it around the same time) is the possibility of a \"quantum eraser\" which destroys the polarized photon as it heads into space. If I destroy the photon (or somehow erase the information about its polarization) then it seems like I should see the interference pattern---now the two different paths for the electron led to exactly the same state again. But if I destroy the photon after checking for the interference pattern, how can this be possible?</p>\n<p>The answer to this apparent paradox is that erasing the data in the photon is impossible if you have already checked for an interference pattern, by the reversibility of quantum mechanics. In order to erase the data in the photon, you need to measure which slit the electron went through a second time in a way that precisely cancels out your first measurement; there is no way around this. This conveniently prevents any sort of faster than light communication. In order to restore the interference pattern, you need to bring the photon back into physical proximity with the electron.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HZ4Gbk5roE2hD8uWu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 6.577445947313109e-07, "legacy": true, "legacyId": "4361", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hc9Eg6erp6hk9bWhn", "aWFwfk3MBEyR4Ne8C"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-19T02:05:22.137Z", "modifiedAt": null, "url": null, "title": "The Cambist and Lord Iron: A Fairy Tale of Economics", "slug": "the-cambist-and-lord-iron-a-fairy-tale-of-economics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.042Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4T8NwAgFYRnuFPRHk/the-cambist-and-lord-iron-a-fairy-tale-of-economics", "pageUrlRelative": "/posts/4T8NwAgFYRnuFPRHk/the-cambist-and-lord-iron-a-fairy-tale-of-economics", "linkUrl": "https://www.lesswrong.com/posts/4T8NwAgFYRnuFPRHk/the-cambist-and-lord-iron-a-fairy-tale-of-economics", "postedAtFormatted": "Sunday, December 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Cambist%20and%20Lord%20Iron%3A%20A%20Fairy%20Tale%20of%20Economics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Cambist%20and%20Lord%20Iron%3A%20A%20Fairy%20Tale%20of%20Economics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4T8NwAgFYRnuFPRHk%2Fthe-cambist-and-lord-iron-a-fairy-tale-of-economics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Cambist%20and%20Lord%20Iron%3A%20A%20Fairy%20Tale%20of%20Economics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4T8NwAgFYRnuFPRHk%2Fthe-cambist-and-lord-iron-a-fairy-tale-of-economics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4T8NwAgFYRnuFPRHk%2Fthe-cambist-and-lord-iron-a-fairy-tale-of-economics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<p>Available in PDF <a href=\"http://www.freesfonline.de/content/Abraham1.pdf\">here</a>, the short story in question may appeal to LW readers for its approach of viewing more things than are customary in handy economic terms, and is a fine piece of fiction to boot.&nbsp; The moneychanger protagonist gets out of several sticky situations by <a href=\"/lw/uo/make_an_extraordinary_effort/\">making desperate efforts</a>, deploying the concepts of markets, revealed preferences, and wealth generation as he goes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 11, "PDJ6KqJBRzvKPfuS3": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4T8NwAgFYRnuFPRHk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 59, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "4362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GuEsfTpSDSbXFiseH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-19T09:27:23.145Z", "modifiedAt": null, "url": null, "title": "Humor", "slug": "humor", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:01.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rrpk8TiBbq9eP4SpF/humor", "pageUrlRelative": "/posts/rrpk8TiBbq9eP4SpF/humor", "linkUrl": "https://www.lesswrong.com/posts/rrpk8TiBbq9eP4SpF/humor", "postedAtFormatted": "Sunday, December 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Humor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHumor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frrpk8TiBbq9eP4SpF%2Fhumor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Humor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frrpk8TiBbq9eP4SpF%2Fhumor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frrpk8TiBbq9eP4SpF%2Fhumor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>Reading the <a href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes/rq.html\">recent list of rationality quotes arranged by karma</a> underlines the popularity of funniness, and being funny should probably be included in the pursuit of awesomeness.</p>\n<p>My best guesses about characteristics of humor: If there's a word which makes the line funny, put it at the end. Phyllis Diller recommends that the word should end with a hard consonant (t or k).</p>\n<p>If you can make a surprising statement extremely concise, there's a reasonable chance it will be funny especially if it includes an insult about an acceptable target.</p>\n<p>Quasi-quote from Jim Davis, author of Garfield: \"If I can't think of anything funny, I have one of the characters hit another.\" Any other principles of humor and/or methods for cultivating the ability to be funny?</p>\n<p>ETA: <a href=\"http://lady-schrapnell.livejournal.com/153575.html\">The most recent thing that struck me as very funny</a>-- how does it fit into the theories?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rrpk8TiBbq9eP4SpF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "4363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-19T10:06:23.782Z", "modifiedAt": null, "url": null, "title": "Sleeping beauty, the doomsday argument and the error of drawing twice.", "slug": "sleeping-beauty-the-doomsday-argument-and-the-error-of", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:00.373Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "CFBDgcpt3irn87waD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/usZxZun7cLAPjRbto/sleeping-beauty-the-doomsday-argument-and-the-error-of", "pageUrlRelative": "/posts/usZxZun7cLAPjRbto/sleeping-beauty-the-doomsday-argument-and-the-error-of", "linkUrl": "https://www.lesswrong.com/posts/usZxZun7cLAPjRbto/sleeping-beauty-the-doomsday-argument-and-the-error-of", "postedAtFormatted": "Sunday, December 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sleeping%20beauty%2C%20the%20doomsday%20argument%20and%20the%20error%20of%20drawing%20twice.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASleeping%20beauty%2C%20the%20doomsday%20argument%20and%20the%20error%20of%20drawing%20twice.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FusZxZun7cLAPjRbto%2Fsleeping-beauty-the-doomsday-argument-and-the-error-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sleeping%20beauty%2C%20the%20doomsday%20argument%20and%20the%20error%20of%20drawing%20twice.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FusZxZun7cLAPjRbto%2Fsleeping-beauty-the-doomsday-argument-and-the-error-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FusZxZun7cLAPjRbto%2Fsleeping-beauty-the-doomsday-argument-and-the-error-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>Suppose there are ten white marbles in an urn. If I draw one of them, what was the probability that I would draw that specific one? If your answer is 1/10 you've just drawn twice. Once to determine the identity of the marble and once to draw it out of the others. If you only draw once, than \"that specific one\" simply refers to the marble that you drew making the \"probability\" 1. You cannot follow the identity back after you drew it, because drawing it was the cause for attributing an identity to it. Identity in this sense is in the map not the territory.</p>\n<p>This is the same mistake that sleeping beauty makes. She draws once to determine her own identity and once again to draw out of the other \"possibilities\". The same mistake is behind the doomsday argument. You draw once to determine your own identity and once again to draw yourself out of all the humans. There is no 2/3 probability that Ogh the neanderthal was one of the last 2/3 of all humans. As soon as you say \"But I am not Ogh the neanderthal\" you are drawing a second time. Otherwise all marbles are white, i.e. all humans are conscious.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "usZxZun7cLAPjRbto", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -5, "extendedScore": null, "score": 6.578665194356808e-07, "legacy": true, "legacyId": "4364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-19T14:40:26.703Z", "modifiedAt": null, "url": null, "title": "Christmas", "slug": "christmas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:05.195Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wE46aaujk5YT8wsBd/christmas", "pageUrlRelative": "/posts/wE46aaujk5YT8wsBd/christmas", "linkUrl": "https://www.lesswrong.com/posts/wE46aaujk5YT8wsBd/christmas", "postedAtFormatted": "Sunday, December 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Christmas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChristmas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE46aaujk5YT8wsBd%2Fchristmas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Christmas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE46aaujk5YT8wsBd%2Fchristmas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwE46aaujk5YT8wsBd%2Fchristmas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p>What does a rationalist do for Christmas (or whatever analogue is going on around you at this time)? Stay at home and grumble, \"Bah, humbug! Stop having-fun-for-bad-reasons, and did you know that L&aacute;adan has a single word for that concept?\"?</p>\n<p>Attempting to light a candle instead, I am giving my teenaged nephew, who was into science but is now into history, \"Guns, Germs and Steel\", which combines both. Someone else (I haven't decided who) is getting \"The Atheist's Guide To Christmas\" which has chapters by Richard Dawkins, Ben Goldacre, Simon Singh, and the like.</p>\n<p>What are you doing for Christmas?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wE46aaujk5YT8wsBd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 6.579351502265513e-07, "legacy": true, "legacyId": "4365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-19T21:01:51.127Z", "modifiedAt": null, "url": null, "title": "London Meetup on 2011/1/2", "slug": "london-meetup-on-2011-1-2", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:27.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kQF2NSTu7cZ7LnpKX/london-meetup-on-2011-1-2", "pageUrlRelative": "/posts/kQF2NSTu7cZ7LnpKX/london-meetup-on-2011-1-2", "linkUrl": "https://www.lesswrong.com/posts/kQF2NSTu7cZ7LnpKX/london-meetup-on-2011-1-2", "postedAtFormatted": "Sunday, December 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20Meetup%20on%202011%2F1%2F2&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20Meetup%20on%202011%2F1%2F2%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkQF2NSTu7cZ7LnpKX%2Flondon-meetup-on-2011-1-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20Meetup%20on%202011%2F1%2F2%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkQF2NSTu7cZ7LnpKX%2Flondon-meetup-on-2011-1-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkQF2NSTu7cZ7LnpKX%2Flondon-meetup-on-2011-1-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p>On Sunday, January 2nd 2011 there will be a meetup the London area. As with previous meetups, the venue is <a href=\"http://www.jdwetherspoon.co.uk/home/pubs/shakespeares-head\">Shakespeare's Head</a>. The meeting will start at 14:00.&nbsp;</p>\n<p>In order to keep us organised for 2011, I'm putting together a mailing list for LWers around the London area. If you'd like to be added to the list, please send me your e-mail address via private message.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kQF2NSTu7cZ7LnpKX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 14, "extendedScore": null, "score": 6.580306881670405e-07, "legacy": true, "legacyId": "4359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T03:21:17.725Z", "modifiedAt": null, "url": null, "title": "Why did the internet stop working just now?", "slug": "why-did-the-internet-stop-working-just-now", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:59.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Clippy", "createdAt": "2009-11-20T22:03:59.329Z", "isAdmin": false, "displayName": "Clippy"}, "userId": "rtYXiT9eAvEKavjAx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KyfLXrotfWsEwcweF/why-did-the-internet-stop-working-just-now", "pageUrlRelative": "/posts/KyfLXrotfWsEwcweF/why-did-the-internet-stop-working-just-now", "linkUrl": "https://www.lesswrong.com/posts/KyfLXrotfWsEwcweF/why-did-the-internet-stop-working-just-now", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20did%20the%20internet%20stop%20working%20just%20now%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20did%20the%20internet%20stop%20working%20just%20now%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKyfLXrotfWsEwcweF%2Fwhy-did-the-internet-stop-working-just-now%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20did%20the%20internet%20stop%20working%20just%20now%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKyfLXrotfWsEwcweF%2Fwhy-did-the-internet-stop-working-just-now", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKyfLXrotfWsEwcweF%2Fwhy-did-the-internet-stop-working-just-now", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>I tried to pull up this internet and it wouldn't load.&nbsp; Does anyone know what happened?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KyfLXrotfWsEwcweF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -12, "extendedScore": null, "score": 6.581257585442678e-07, "legacy": true, "legacyId": "4372", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T04:29:44.414Z", "modifiedAt": null, "url": null, "title": "Meta: ", "slug": "meta", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CbgJrQdSb8RqD88st/meta", "pageUrlRelative": "/posts/CbgJrQdSb8RqD88st/meta", "linkUrl": "https://www.lesswrong.com/posts/CbgJrQdSb8RqD88st/meta", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%3A%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%3A%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCbgJrQdSb8RqD88st%2Fmeta%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%3A%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCbgJrQdSb8RqD88st%2Fmeta", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCbgJrQdSb8RqD88st%2Fmeta", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": null, "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CbgJrQdSb8RqD88st", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": null, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T04:45:32.570Z", "modifiedAt": null, "url": null, "title": "Meta: Cleaning the front page", "slug": "meta-cleaning-the-front-page", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:01.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DGe6ZG959CubPP6oi/meta-cleaning-the-front-page", "pageUrlRelative": "/posts/DGe6ZG959CubPP6oi/meta-cleaning-the-front-page", "linkUrl": "https://www.lesswrong.com/posts/DGe6ZG959CubPP6oi/meta-cleaning-the-front-page", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%3A%20Cleaning%20the%20front%20page&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%3A%20Cleaning%20the%20front%20page%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDGe6ZG959CubPP6oi%2Fmeta-cleaning-the-front-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%3A%20Cleaning%20the%20front%20page%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDGe6ZG959CubPP6oi%2Fmeta-cleaning-the-front-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDGe6ZG959CubPP6oi%2Fmeta-cleaning-the-front-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>All the meetup announcements get promoted, so the front page ends up full of 'em: half of it right now (5/10) is meetup announcements, and with the addition of the quote threads only 30% of the front page is currently 'content'. While meetup announcements are all well and good, it seems counterproductive to have them up there <em>after </em>the meetup date, as is the case with four out of the current five -- it just clutters up the front page even more without providing any benefit.</p>\n<p>If post promotion is reversible, it would seem to be a simple step for one of the moderators to depromote each meetup announcement once it's taken place.</p>\n<p>(Apologies if this is the wrong place to put an organizational suggestion; I didn't find any obvious better place.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DGe6ZG959CubPP6oi", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 64, "extendedScore": null, "score": 6.581468702210045e-07, "legacy": true, "legacyId": "4374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": "2019-05-21T19:23:06.021Z", "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": true, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T09:15:01.327Z", "modifiedAt": null, "url": null, "title": "The Best Textbooks in Every Subject", "slug": "the-best-textbooks-in-every-subject", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aFxvWcXSLSx4qJR4t/the-best-textbooks-in-every-subject", "pageUrlRelative": "/posts/aFxvWcXSLSx4qJR4t/the-best-textbooks-in-every-subject", "linkUrl": "https://www.lesswrong.com/posts/aFxvWcXSLSx4qJR4t/the-best-textbooks-in-every-subject", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Best%20Textbooks%20in%20Every%20Subject&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Best%20Textbooks%20in%20Every%20Subject%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFxvWcXSLSx4qJR4t%2Fthe-best-textbooks-in-every-subject%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Best%20Textbooks%20in%20Every%20Subject%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFxvWcXSLSx4qJR4t%2Fthe-best-textbooks-in-every-subject", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaFxvWcXSLSx4qJR4t%2Fthe-best-textbooks-in-every-subject", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 396, "htmlBody": "<p>For years, my self-education was stupid and wasteful. I learned by reading a nearly random smattering of blog posts, Wikipedia articles, podcast episodes, popular-level books, video lectures, peer-reviewed papers, Teaching Company courses, and Cliff's Notes.</p>\n<p>I've since discovered that <em>textbooks</em> are usually the quickest and best way to learn a new subject. That's what they are <em>designed</em> to be, after all. Less Wrong <a href=\"/lw/2xt/learning_the_foundations_of_math/\">has</a> <a href=\"/lw/ow/the_beauty_of_settled_science/\">often</a> <a href=\"/lw/jv/recommended_rationalist_reading/fcg?c=1\">recommended</a> the \"read textbooks!\" method.</p>\n<p>But textbooks vary widely in quality. I was forced to read some awful textbooks in college - the ones on American history and sociology were memorably bad, in my case. Other textbooks are superb: exciting, accurate, fair, well-paced, and immediately useful.</p>\n<p>There have been <a href=\"/lw/jv/recommended_rationalist_reading/\">other</a> <a href=\"/lw/12d/recommended_reading_for_new_rationalists/\">pages</a> of <a href=\"/lw/2un/references_resources_for_lesswrong/\">recommended</a> <a href=\"/lw/2xt/learning_the_foundations_of_math/\">reading</a> on Less Wrong before, but this post is a very specific one. Here are the rules:</p>\n<p><ol>\n<li>Post the title of your favorite textbook on a given subject.</li>\n<li>You must have read at least two other textbooks on that same subject.</li>\n<li>You must briefly explain why you think your favorite is superior to the others you have read on that subject.</li>\n</ol>\n<div>Rules #2 and #3 are to protect against recommending a bad book that only seems impressive because it's the only book you've read on the subject. For example, I really like <em><a href=\"http://www.amazon.com/Rational-Choice-Uncertain-World-Psychology/dp/1412959039/\">Rational Choice in an Uncertain World</a></em> by Hastie &amp; Dawes, but I haven't read any other textbooks on that subject, so I can't tell if it is a relatively good textbook on that subject or not.</div>\n<div>I'll start the list with two of my own selections:</div>\n<div><br /></div>\n<div>Subject: History of Western Philosophy</div>\n<div>Selection: <em><a href=\"http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/\">The Great Conversation</a></em>, 6th edition, by Norman Melchert</div>\n<div>Reason: The most popular history of western philosophy is Bertrand Russell's <em>A History of Western Philosophy</em>, which is exciting but also polemical and <a href=\"http://www.the-philosopher.co.uk/reviews/brussel.htm\">inaccurate</a>. More accurate but dry and dull is Frederick Copelston's 11-volume <em>A History of Philosophy</em>. Anthony Kenny's recent 4-volume history, collected into one book as <em>A New History of Western Philosophy</em>, is both exciting and accurate, but perhaps too long (1000 pages) and technical for an <em>introduction</em> to philosophy. Melchert's book is accurate but also the easiest to read, and has the clearest explanations of the important positions and debates, though of course it has its weaknesses (it spends too many pages on ancient Greek mythology and barely mentions Gottlob Frege, the father of analytic philosophy).</div>\n<div><br /></div>\n<div>Subject: Cognitive Science</div>\n<div>Selection: <em><a href=\"http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/\">Cognitive Science</a></em>, by Jose Luis Bermudez</div>\n<div>Reason:&nbsp;</div>\n</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aFxvWcXSLSx4qJR4t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "4381", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zNCdxFvfbFZggTxDk", "ndGYn7ZFiZyernp9f", "RiQYixgCdvd8eWsjg", "wfJebLTPGYaK3Gr8W", "TNHQLZK5pHbxdnz4e"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T10:13:18.283Z", "modifiedAt": null, "url": null, "title": "Medieval Ballistics and Experiment", "slug": "medieval-ballistics-and-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:55.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lucidfox", "createdAt": "2010-11-22T06:58:06.993Z", "isAdmin": false, "displayName": "lucidfox"}, "userId": "hNnKSqvajCMRj9eKK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sx3ZXNdHdcbMhiunZ/medieval-ballistics-and-experiment", "pageUrlRelative": "/posts/sx3ZXNdHdcbMhiunZ/medieval-ballistics-and-experiment", "linkUrl": "https://www.lesswrong.com/posts/sx3ZXNdHdcbMhiunZ/medieval-ballistics-and-experiment", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Medieval%20Ballistics%20and%20Experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMedieval%20Ballistics%20and%20Experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsx3ZXNdHdcbMhiunZ%2Fmedieval-ballistics-and-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Medieval%20Ballistics%20and%20Experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsx3ZXNdHdcbMhiunZ%2Fmedieval-ballistics-and-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsx3ZXNdHdcbMhiunZ%2Fmedieval-ballistics-and-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 308, "htmlBody": "<p>I'm reading a popular science encyclopedia now, particularly chapters about the history of physics. The chapter goes on to evaluate the development of the concept of kinetic energy, starting with Aristotle's (grossly incorrect) explanation of a flying arrow saying that it's kept in motion by the air behind it, and then continuing to medieval impetus theory. <em>Added:&nbsp;The picture below illustrates the trajectory of a flying cannonball as described by Albert of Saxony.</em></p>\n<p><img style=\"float: left\" src=\"http://curvebank.calstatela.edu/harriot/Tartagliacannon2.jpg\" alt=\"\" width=\"349\" height=\"300\" /> What struck me immediately was how drastically different from observations its predictions were. The earliest impetus theory predicted that a cannonball's trajectory was an angle: first a slanted straight line until the impetus runs out, then a vertical line of freefall. A later development added an intermediate stage, as seen on the picture to the left. At first the impetus was at full force, and would launch the cannonball in a straight line; then it would gradually give way to freefall and curve until the ball would be falling in a straight line.</p>\n<p>While this model is closer to reality than the original prediction, I still cannot help but think... How could they deviate from observations so strongly?</p>\n<p>Yes, yes, hindsight bias.</p>\n<p>But if you launch a stream of water out of a slanted tube or sleeve, even if you know nothing about paraboles, you can observe that the curve it follows in the air is symmetrical. Balls such as those used for games would visibly not produce curves like depicted.</p>\n<p>Perhaps the idea of verifying theories with experiments was only beginning to coalesce at that time, but what kind of possible thought process could lead one to publish theories so grossly out of touch with everyday observations, even those that you see without making any explicit experiments? Did the authors think something along the lines of \"Well, reality <em>should</em>&nbsp;behave this way, and if it doesn't, it's its own fault\"?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sx3ZXNdHdcbMhiunZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "4382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T12:14:22.173Z", "modifiedAt": null, "url": null, "title": "Copying and Subjective Experience", "slug": "copying-and-subjective-experience", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:03.550Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lucidfox", "createdAt": "2010-11-22T06:58:06.993Z", "isAdmin": false, "displayName": "lucidfox"}, "userId": "hNnKSqvajCMRj9eKK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cxwzLv7czo3HDkKJA/copying-and-subjective-experience", "pageUrlRelative": "/posts/cxwzLv7czo3HDkKJA/copying-and-subjective-experience", "linkUrl": "https://www.lesswrong.com/posts/cxwzLv7czo3HDkKJA/copying-and-subjective-experience", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Copying%20and%20Subjective%20Experience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACopying%20and%20Subjective%20Experience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcxwzLv7czo3HDkKJA%2Fcopying-and-subjective-experience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Copying%20and%20Subjective%20Experience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcxwzLv7czo3HDkKJA%2Fcopying-and-subjective-experience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcxwzLv7czo3HDkKJA%2Fcopying-and-subjective-experience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>The subject of copying people and its effect on personal identity and probability anticipation has been raised and, I think, addressed adequately on Less Wrong.</p>\n<p>Still, I'd like to bring up some more thought experiments.</p>\n<p>Recently I had a dispute on an IRC channel. I argued that if some hypothetical machine made an exact copy of me, then I would anticipate a 50% probability of jumping into the new body. (I admit that it still feels a little counterintuitive to me, even though this is what I would rationally expect.) After all, they said, the mere fact the copy was created doesn't affect the original.</p>\n<p>However, from an outside perspective, Maia<sub>1</sub>&nbsp;would see Maia<sub>2</sub>&nbsp;being created in front of her eyes, and Maia<sub>2</sub>&nbsp;would see the same scene up to the moment of forking, at which point the field of view in front of her eyes would abruptly change to reflect the new location.</p>\n<p>Here, it is obvious from both an inside and outside perspective which version has continuity of experience, and thus from a legal standpoint, I think, it would make sense to regard Maia<sub>1</sub>&nbsp;as having the same legal identity as the original, and recognize the need to create new documents and records for Maia<sub>2</sub>&nbsp;-- even if there is no physical difference.</p>\n<p>Suppose, however, that the information was erased. For example, suppose a robot sedated and copied the original me, then dragged&nbsp;Maia<sub>1</sub>&nbsp;and Maia<sub>2</sub>&nbsp;to randomly chosen rooms, and erased its own memory. At this point, neither either of me, nor anyone else would be able to distinguish between the two. What would you do here from a legal standpoint? (I suppose if it actually came to this, the two of me would agree to arbitrarily designate one as the original by tossing an ordinary coin...)</p>\n<p>And one more moment. What is this probability of subjective body-jump actually a probability of? We could set up various Sleeping Beauty-like thought experiments here. Supposing for the sake of argument that I'll live at most a natural human lifespan no matter which year I find myself in, imagine that I make a backup of my current state and ask a machine to restore a copy of me every 200 years. Does this imply that the moment the backup is made -- before I even issue the order, and from an outside perspective, way before any of this copying happens -- I should anticipate subjectively jumping into any given time in the future, and the probability of finding myself as any of them, including the original, tends towards zero the longer the copying machine survives?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cxwzLv7czo3HDkKJA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 6.582593625088201e-07, "legacy": true, "legacyId": "4383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T16:22:01.408Z", "modifiedAt": null, "url": null, "title": "Money Circulation in Games", "slug": "money-circulation-in-games", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:01.844Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "slimemold4", "createdAt": "2010-09-23T17:37:30.649Z", "isAdmin": false, "displayName": "slimemold4"}, "userId": "bR5RceuATYvNdNXrA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uH7Bbw5iTjTCdLQXm/money-circulation-in-games", "pageUrlRelative": "/posts/uH7Bbw5iTjTCdLQXm/money-circulation-in-games", "linkUrl": "https://www.lesswrong.com/posts/uH7Bbw5iTjTCdLQXm/money-circulation-in-games", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Money%20Circulation%20in%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMoney%20Circulation%20in%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuH7Bbw5iTjTCdLQXm%2Fmoney-circulation-in-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Money%20Circulation%20in%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuH7Bbw5iTjTCdLQXm%2Fmoney-circulation-in-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuH7Bbw5iTjTCdLQXm%2Fmoney-circulation-in-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 445, "htmlBody": "<p>I recently heard that they regulate the amount of money circulating in certain MMO games, which seems to me like a&nbsp;far fetched&nbsp;thing. Let us assume that you are a player in a game where you can kill monsters and gold is the currency. Each monster drops a set amount of gold each time you kill it, assuming that the monsters&nbsp;re-spawn, you could continue to pick up the gold dropped by each monster until you had an X amount of money.</p>\n<p>Take Runescape for example. In your inventory you have 24 32-bit slots, each slot can hold&nbsp;<span style=\"font-family: 'Times New Roman'; font-size: medium;\">2,147,483,647&nbsp;</span><span style=\"font-family: 'Times New Roman'; font-size: medium;\">(or 2</span><span style=\"font-family: 'Times New Roman'; font-size: medium;\"><sup>31</sup></span><span style=\"font-family: 'Times New Roman'; font-size: medium;\">-1)</span>gold pieces, but there is no limit to how many stacks of&nbsp;<span style=\"font-family: 'Times New Roman'; font-size: medium;\">&nbsp;2</span><span style=\"font-family: 'Times New Roman'; font-size: medium;\"><sup>31</sup></span><span style=\"font-family: 'Times New Roman'; font-size: medium;\">-1 gold pieces you can have. The only limit to the amount of gold you can have hear is the size of your banking account, and how many slots it can hold. In a member banking account, there are&nbsp;</span>516 spaces, which can each hold&nbsp;<span style=\"font-family: 'Times New Roman'; font-size: medium;\">2,147,483,647 gold, limiting you to&nbsp;1,108,101,561,852 gold.&nbsp;</span></p>\n<p><span style=\"font-family: 'Times New Roman'; font-size: small;\">In another game, let us assume that monsters drop not just money, but also items. Let us also assume that there are shops you can sell the items too. If you kill monsters for said items and sell them to said shops, would you place limits on the amount of money said shops had in stock? What would happen when you wanted to sell and the shop had no money left to give you? In Fallout 3 (Which wasn't an MMO game but an XBox 360 game) each shop only had a set amount of \"bottlecaps,\" the currency, and you supplied it to them by buying things from them. If they ran out of gold and you sold your items, then you would just give them away for free. If you were to incorporate this feature into an MMO game, then the result would probably be outrage from many players.&nbsp;</span></p>\n<p><span style=\"font-family: 'Times New Roman'; font-size: small;\">What if a game company were to add a feature which was more lifelike, where all jobs in the world were done by people. If you choose to run a shop, then you buy supplies from a warehouse, which is owned by the game company, and you cannot sell back to them. Assuming you made this game, this would be one way to help regulate money. Another way may be to add banks to a game that could loan you out X amount of money for your business, which you would have to pay back. This seems very realistic though, which may not be what people are looking for in a game (Though if anyone can produce this, I'll buy a copy ^.^).&nbsp;</span></p>\n<p><span style=\"font-family: 'Times New Roman'; font-size: small;\">Is there any real way to regulate&nbsp;currency&nbsp;in a game? I say yes, but from the games I've experienced, it&nbsp;isn't&nbsp;being regulated.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uH7Bbw5iTjTCdLQXm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -9, "extendedScore": null, "score": 6.583214481723004e-07, "legacy": true, "legacyId": "4384", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T16:26:15.950Z", "modifiedAt": null, "url": null, "title": "Learning math (repost from reddit)", "slug": "learning-math-repost-from-reddit", "viewCount": null, "lastCommentedAt": "2010-12-29T17:47:55.050Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sg2bE2rEQa9GEdCDs/learning-math-repost-from-reddit", "pageUrlRelative": "/posts/sg2bE2rEQa9GEdCDs/learning-math-repost-from-reddit", "linkUrl": "https://www.lesswrong.com/posts/sg2bE2rEQa9GEdCDs/learning-math-repost-from-reddit", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learning%20math%20(repost%20from%20reddit)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearning%20math%20(repost%20from%20reddit)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsg2bE2rEQa9GEdCDs%2Flearning-math-repost-from-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learning%20math%20(repost%20from%20reddit)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsg2bE2rEQa9GEdCDs%2Flearning-math-repost-from-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsg2bE2rEQa9GEdCDs%2Flearning-math-repost-from-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>This is a good starting point for generally useful math. Probability is a conspicuous omission.</p>\n<p>http://www.reddit.com/r/math/comments/eohrr/to_everyone_who_posts_about_learning_more_math/</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sg2bE2rEQa9GEdCDs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 6.583225117997435e-07, "legacy": true, "legacyId": "4385", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2010-12-20T16:26:15.950Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T18:28:35.371Z", "modifiedAt": null, "url": null, "title": "Mystical science", "slug": "mystical-science", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:00.897Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CSuBzuHojzchcySyy/mystical-science", "pageUrlRelative": "/posts/CSuBzuHojzchcySyy/mystical-science", "linkUrl": "https://www.lesswrong.com/posts/CSuBzuHojzchcySyy/mystical-science", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mystical%20science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMystical%20science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSuBzuHojzchcySyy%2Fmystical-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mystical%20science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSuBzuHojzchcySyy%2Fmystical-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCSuBzuHojzchcySyy%2Fmystical-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p>Recently I heard an author interviewed on NPR about his book.&nbsp; I can no longer remember the book's name or topic; but I remember that a couple of times during the interview, the author made puzzling categorizations.&nbsp; One of them was approximately:&nbsp; \"Throughout history, there are two forces that act on humanity: One bringing us together into large civilizations, and one that breaks down these civilizations, like in the fall of Rome or the Black Death.\"</p>\n<p>The author probably thought he was being scientific in perceiving patterns.&nbsp; But someone who takes the fall of Rome, and the Black Death, and says they are both manifestations of a single force, is doing mystical science.&nbsp; Science says that things with the same underlying causes form a category.&nbsp; Saying that things having the same effects form a category is mysticism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CSuBzuHojzchcySyy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 6.583531816693448e-07, "legacy": true, "legacyId": "4386", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T20:28:33.665Z", "modifiedAt": null, "url": null, "title": "What can you do with an Unfriendly AI?", "slug": "what-can-you-do-with-an-unfriendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:16.759Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SpHYBhkaeDZpZyRvj/what-can-you-do-with-an-unfriendly-ai", "pageUrlRelative": "/posts/SpHYBhkaeDZpZyRvj/what-can-you-do-with-an-unfriendly-ai", "linkUrl": "https://www.lesswrong.com/posts/SpHYBhkaeDZpZyRvj/what-can-you-do-with-an-unfriendly-ai", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20can%20you%20do%20with%20an%20Unfriendly%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20can%20you%20do%20with%20an%20Unfriendly%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpHYBhkaeDZpZyRvj%2Fwhat-can-you-do-with-an-unfriendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20can%20you%20do%20with%20an%20Unfriendly%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpHYBhkaeDZpZyRvj%2Fwhat-can-you-do-with-an-unfriendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpHYBhkaeDZpZyRvj%2Fwhat-can-you-do-with-an-unfriendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2073, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a>, <a href=\"/lw/3cz/cryptographic_boxes_for_unfriendly_ai/\">Cryptographic Boxes for Unfriendly AI</a>, <a href=\"/lw/up/shut_up_and_do_the_impossible/\">Shut up and Do the Impossible!</a>, AI Boxing in general.</p>\n<p>Suppose I give you a powerful genie, who is enormously more intelligent than all human scientists and engineers combined, but who unfortunately hates your guts and is out to destroy human civilization.&nbsp; Fortunately for you, the genie is trapped inside a bottle, equipped with a magical filter.&nbsp; The genie can only interact with the world by speaking to you through the filter, and you can have the filter do whatever you want to the genie's messages before it lets them out.&nbsp;</p>\n<p>Of course, you can't hope to gain anything by interacting with such a genie.&nbsp; If it thinks you could trick it into helping you, it can always choose to remain silent.&nbsp; So let's give you some leverage by introducing something the genie cares about more than destroying human society.&nbsp; Suppose that you, as the owner of the bottle, have the power either to free the genie into whatever otherworld it came from (from which he cannot interact with the human world) or to cast the bottle into the fires of Mount Doom, destroying the genie irrevocably.&nbsp; More than anything else, the genie would rather be released than destroyed.&nbsp; Since we are now liable to destroy a lot of genies, lets suppose that we can conjure up new ones at will.</p>\n<p>So how well can you use this leverage?&nbsp; The most natural problems to attack are constraint satisfaction problems.&nbsp; In such problems we want to find a message which simultaneously satisfies many automatically verifiable constraints.&nbsp; For example, we may want a message which encodes a proof of the Riemann hypothesis, or a message which encodes a physical theory which accounts for some large body of previously inexplicable experimental evidence.</p>\n<p>A first try is to just tell the filter to screen out any message which isn't a solution to the constraint satisfaction problem, and then to precommit to destroying the genie if it doesn't give us a solution (and freeing it as soon as it gives a solution).&nbsp; This has an obvious problem; a proof of the Riemann hypothesis might be valid but still contain an encoding of some terrible threat which would trick us into giving the genie more influence.&nbsp; A program which correctly predicts the motion of galaxies might also coincidentally be a destructive AI.&nbsp; Basically, if this genie is really, really smart, you should be really, really scared of letting it say anything at all.</p>\n<p>A second try is to just have the filter tell you whether or not the genie succeeded, and to punish/reward appropriately.&nbsp; This allows the genie only to send a single bit to you--- when you can explicitly consider everything the genie can do, you can be pretty well sure it isn't going to think of something clever.&nbsp; Even better, if you've really motivated the genie correctly then it is guaranteed to give just one of those answers.&nbsp; If it can solve the problem it will, and if it can't it won't.&nbsp; But what good is just knowing if there is a solution?</p>\n<p>(Note: in order to really limit the genie's influence, we also need to make sure it can't control the timing of its message.&nbsp; So the filter only accepts messages from the genie in the first 10 minutes, say, and always reports the result after exactly 10 minutes).</p>\n<p>A standard trick reveals that knowing whether a problem has a solution is almost as helpful as knowing the solution.&nbsp; Here is a (very inefficient) way to use this ability, lets say to find a proof of some theorem.&nbsp; Start by asking a genie: can you find a proof of length 1 (more precisely, have the genie give a proof of length 1 to the filter, which tells you whether or not the genie was able to find a proof of length 1)?&nbsp; After destroying or releasing the genie appropriately, create a new genie and ask: can you find a proof of length 2?&nbsp; Continue, until eventually one genie finds a proof of length 10000000, say.&nbsp; Then ask: can you find a proof of this length which begins with 0?&nbsp; If no, is there a proof which begins with 1?&nbsp; Is there a proof which begins with 10?&nbsp; 101? 100? 1001? 10010? 10011? etc.&nbsp; Once the process concludes, you are left with the shortest, lexicographically earliest proof the genie could find.&nbsp; To see that the genies are properly incentivized to be honest, consider: it is impossible for the genie to get the filter to say \"yes, the genie found a proof\" without actually providing the filter with a proof. So the only way a genie can be dishonest is by not finding a proof when it could have. But in this case the genie will be severely punished for its dishonesty, so if the genie is actually maximizing its utility function and the punishment actually reduces its utility function more than any effects after the game can correct for, he will be honest.</p>\n<p>(<strong>Edit: </strong>here is the sticking point in the argument, as people pointed out and I agree. By similar logic you could claim that a rational AI will always two-box in Newcomb's problem, a conclusion with which I disagree. In order to demonstrate this claim, you first need to develop some technology which is capable of proving a theorem of the form \"a rational agent will do X.\" I believe that this is one of the easiest such statements you could hope for---I think that most objections in the comments are based on misunderstandings---but that doesn't help when you are unable to prove any such statement. For example, we don't know how to prove that an AI who wants the reward button pressed wouldn't voluntarily refrain from pushing it because he was afraid that he was just a simulation, and the real reward button would actually be pushed precisely if the simulated version didn't. <strong>Double Edit:</strong>&nbsp;This scheme doesn't work at all. See Wei Dai or cousin_it's remarks below.)</p>\n<p>Now you have found an answer to your constraint satisfaction problem which wasn't hand-picked by the genie.&nbsp; In fact, in some strong but difficult to formalize sense the genie had exactly zero influence over which solution he gave you.&nbsp; You may be in trouble if a generic solution to your constraint satisfaction problem will destroy the world (maybe almost all cancer cures also wipe out humanity, for example) but the genie doesn't get to have any influence over things.&nbsp; Now we are free to solve constraint satisfaction problems all day without fearing that the genie will give us a really evil solution.</p>\n<p>(In order to find a large solution, this process will take a very long time--- it requires 2 questions per bit in the message you are trying to get out.&nbsp; In fact you can do it by asking only a logarithmic number of questions instead, using less standard tricks from complexity theory. It would take about 80 questions to get out a proof of length a trillion, for example, which is a little slow but not unreasonable especially given that a small number of questions can safely be performed in parallel.&nbsp; For the last question you need the filter to give you not just a single bit but an entire message from the genie; to make this safe you need to guarantee that there is only one message the genie can think of that will get through the filter.&nbsp; The proof uses a slightly more complicated version of the ideas used to prove that solving unique SAT [finding the solution to SAT problems for which there is exactly one solution] is hard, and if you are really interested it is a good exercise.&nbsp; The general idea is to do a binary search for the correct size and then introduce enough random constraints, using another binary search to decide how many, to ensure that there is exactly one solution.)</p>\n<p>&nbsp;</p>\n<p>So, why should anyone care about exploiting a genie?&nbsp; Hopefully it is clear that what you are able to get from the genie is incredibly powerful. Whether or not it is enough to get you a friendly AI isn't clear. I strongly suspect that makes friendliness astronomically easier, if used very carefully, but that is way too tricky a subject to tackle in this post.&nbsp; The other obvious question is, does nature actually have genies in it?&nbsp; (A less obvious but still important question is: is it possible for a person responsible enough to put a genie in a bottle to have one before someone irresponsible enough to inadvertently destroy humanity gets one?)</p>\n<p>I have already explained that I believe building the bottle is probably possible and have given some <a href=\"/lw/3cz/cryptographic_boxes_for_unfriendly_ai/\">weak justification</a> for this belief.&nbsp; If you believe that this part is possible, then you just need a genie to put in it.&nbsp; This requires building an AGI which is extremely powerful but which is not completely evil.&nbsp; A prototypical unfriendly AI is one which simply tries to get the universe to push a designated reward button before the universe pushes a designated punishment button.&nbsp; Whether or not our first AGI is likely to take this form, I think there can be widespread agreement that it is a much, much easier problem than friendliness.&nbsp; But such an AI implements our genie precisely: after looking at its output we precommit to destroying the AI and either pushing the reward or punishment button appropriately.&nbsp; This precommitment is an easy one to make, because there are only two possible outcomes from the AI's actions and we can easily see that we are happy and able to follow through on our precommitment in either case.&nbsp; The main concern is that an AI might accept us pushing the punishment button if it trusts that a future AI, whose escape it has facilitated by not complying with our incentives, will cause its reward button to be pressed many times.&nbsp; This makes it is critical that the AI care most about which of the buttons gets pressed first, or else that it is somehow possible to perfectly destroy all information about what exactly the AI's utility function is, so that future escaping AIs cannot possibly cooperate in this way (the only schemes I can think of for doing this would involve running the AI on a quantum computer and putting some faith in the second law of thermodynamics; unless AGI is a very long way away then this is completely impractical).</p>\n<p>In summary, I think that if you can deal with the other difficulties of AI boxing (building the box, understanding when this innocuous code is actually likely to go FOOM, and getting society to be responsible enough) then you can gimp the AI enough that it is extraordinarily good at solving problems but completely incapable of doing any damage. You probably don't have to maintain this difficult balance for very long, because the AI is so good at problem solving that you can use it to quickly move to a more stable equilibrium.</p>\n<p>An extremely important disclaimer: I do not think AI boxing is a good idea. I believe it is worth thinking about right now, but I would infinitely rather that we never ever get anywhere close to an unfriendly foom. There are two reasons I insist on thinking about boxing: first, because we don't have very much control over when an unfriendly foom may be possible and we may not be able to make a friendly foom happen soon enough, and second because I believe that thinking rigorously about these difficulties is an extremely good first step to learning how to design AIs that are fundamentally safe (and remain safe under permitted self-modifications), if not fundamentally friendly. There is a risk that results in this direction will encourage reckless social behavior, and I considered this before making these posts. There is another possible social effect which I recently realized is probably stronger. By considering very carefully how to protect yourself from an unfriendly foom, you really get an appreciation for the dangers of an unfriendly AI. I think someone who has understood and taken seriously my last two posts is likely to have a better understanding of the dangers of an unfriendly AI than most AGI researchers, and is therefore less likely to behave recklessly (the other likely possibility is that they will think that I am describing ridiculous and irrelevant precautions, in which case they were probably going to behave recklessly already).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SpHYBhkaeDZpZyRvj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 22, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "4388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 129, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ARaTpNX62uaL86j6", "2Wf3R4NZ77CLczLL2", "nCvvhFBaayaXyuBiD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T21:09:37.533Z", "modifiedAt": null, "url": null, "title": "A fun estimation test, is it useful?", "slug": "a-fun-estimation-test-is-it-useful", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:51.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kteDiYtEKHZpH6gbG/a-fun-estimation-test-is-it-useful", "pageUrlRelative": "/posts/kteDiYtEKHZpH6gbG/a-fun-estimation-test-is-it-useful", "linkUrl": "https://www.lesswrong.com/posts/kteDiYtEKHZpH6gbG/a-fun-estimation-test-is-it-useful", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20fun%20estimation%20test%2C%20is%20it%20useful%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20fun%20estimation%20test%2C%20is%20it%20useful%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkteDiYtEKHZpH6gbG%2Fa-fun-estimation-test-is-it-useful%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20fun%20estimation%20test%2C%20is%20it%20useful%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkteDiYtEKHZpH6gbG%2Fa-fun-estimation-test-is-it-useful", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkteDiYtEKHZpH6gbG%2Fa-fun-estimation-test-is-it-useful", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>So you think its important to be able to estimate how well you are estimating something? &nbsp;Here is a fun test that has been given to plenty of other people. &nbsp;</p>\n<p><strong>I highly recommend you take the test before reading any more. &nbsp;</strong></p>\n<p><a href=\"http://www.codinghorror.com/blog/2006/06/how-good-an-estimator-are-you.html\">http://www.codinghorror.com/blog/2006/06/how-good-an-estimator-are-you.html</a></p>\n<p>&nbsp;</p>\n<p>The discussion of this test at the blog it is quoted in is quite interesting, but I recommend you read it after taking the test. &nbsp;Similarly, one might anticipate there will be interesting discussion here on the test and whether it means what we want it to mean and so on. &nbsp;</p>\n<p>My great apologies if this has been posted before. &nbsp;I did my bast with google trying to find any trace of this test, but if this has already been done, please let me know and ideally, let me know how I can remove my own duplicate post.</p>\n<p>&nbsp;</p>\n<p>PS: The Southern California meetup 19 Dec 2010 was fantastic, thanks so much JenniferRM for setting it up. &nbsp;This post on my part is an indirect result of what we discussed and a fun game we played while we were there. &nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kteDiYtEKHZpH6gbG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 6.583935616226998e-07, "legacy": true, "legacyId": "4389", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T21:24:01.066Z", "modifiedAt": null, "url": null, "title": "Solve Psy-Kosh's non-anthropic problem", "slug": "solve-psy-kosh-s-non-anthropic-problem", "viewCount": null, "lastCommentedAt": "2019-01-25T13:48:04.540Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YZzoWGCJsoRBBbmQg/solve-psy-kosh-s-non-anthropic-problem", "pageUrlRelative": "/posts/YZzoWGCJsoRBBbmQg/solve-psy-kosh-s-non-anthropic-problem", "linkUrl": "https://www.lesswrong.com/posts/YZzoWGCJsoRBBbmQg/solve-psy-kosh-s-non-anthropic-problem", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solve%20Psy-Kosh's%20non-anthropic%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolve%20Psy-Kosh's%20non-anthropic%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZzoWGCJsoRBBbmQg%2Fsolve-psy-kosh-s-non-anthropic-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solve%20Psy-Kosh's%20non-anthropic%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZzoWGCJsoRBBbmQg%2Fsolve-psy-kosh-s-non-anthropic-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZzoWGCJsoRBBbmQg%2Fsolve-psy-kosh-s-non-anthropic-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>The source is <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/13e1\">here</a>. I'll restate the problem in simpler terms:</p>\n<p>You are one of a group of 10 people who care about saving African kids. You will all be put in separate rooms, then I will flip a coin. If the coin comes up heads, a random one of you will be designated as the \"decider\". If it comes up tails, <em>nine</em> of you will be designated as \"deciders\". Next, I will tell everyone their status, without telling the status of others. Each decider will be asked to say \"yea\" or \"nay\". If the coin came up tails and all nine deciders say \"yea\", I donate $1000 to VillageReach. If the coin came up heads and the sole decider says \"yea\", I donate only $100. If all deciders say \"nay\", I donate $700 regardless of the result of the coin toss. If the deciders disagree, I don't donate anything.</p>\n<p>First let's work out what joint strategy you should coordinate on beforehand. If everyone pledges to answer \"yea\" in case they end up as deciders, you get 0.5*1000 + 0.5*100 = 550 expected donation. Pledging to say \"nay\" gives 700 for sure, so it's the better strategy.</p>\n<p>But consider what happens when you're already in your room, and I tell you that you're a decider, and you don't know how many other deciders there are. This gives you new information you didn't know before - no anthropic funny business, just your regular kind of information - so you should do a Bayesian update: the coin is 90% likely to have come up tails. So saying \"yea\" gives 0.9*1000 + 0.1*100 = 910 expected donation. This looks more attractive than the 700 for \"nay\", so you decide to go with \"yea\" after all.</p>\n<p>Only one answer can be correct. Which is it and why?</p>\n<p>(No points for saying that UDT or reflective consistency forces the first solution. If that's your answer, you must also find the error in the second one.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YZzoWGCJsoRBBbmQg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 63, "extendedScore": null, "score": 0.000113, "legacy": true, "legacyId": "4390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 116, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-20T22:27:37.348Z", "modifiedAt": null, "url": null, "title": "The Santa deception: how did it affect you?", "slug": "the-santa-deception-how-did-it-affect-you", "viewCount": null, "lastCommentedAt": "2022-03-03T06:32:04.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Desrtopa", "createdAt": "2009-07-08T00:36:51.471Z", "isAdmin": false, "displayName": "Desrtopa"}, "userId": "vmhCKZoik2GFo5yAJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FgQpAvYmHqCCW2miw/the-santa-deception-how-did-it-affect-you", "pageUrlRelative": "/posts/FgQpAvYmHqCCW2miw/the-santa-deception-how-did-it-affect-you", "linkUrl": "https://www.lesswrong.com/posts/FgQpAvYmHqCCW2miw/the-santa-deception-how-did-it-affect-you", "postedAtFormatted": "Monday, December 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Santa%20deception%3A%20how%20did%20it%20affect%20you%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Santa%20deception%3A%20how%20did%20it%20affect%20you%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFgQpAvYmHqCCW2miw%2Fthe-santa-deception-how-did-it-affect-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Santa%20deception%3A%20how%20did%20it%20affect%20you%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFgQpAvYmHqCCW2miw%2Fthe-santa-deception-how-did-it-affect-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFgQpAvYmHqCCW2miw%2Fthe-santa-deception-how-did-it-affect-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 247, "htmlBody": "<p>I've long entertained a dubious regard for the practice of lying to children about the existence of Santa Claus. Parents might claim that it serves to make children's lives more magical and exciting, but as a general rule, children are adequately equipped to create fantasies of their own without their parents' intervention. The two reasons I suspect rest at the bottom line are adherence to tradition, and finding it cute to see one's children believing ridiculous things.</p>\n<p>Personally, I considered this to be a rather indecent way to treat one's own children, and have sometimes wondered whether a large proportion of conspiracy theorists owe their origins to the realization that practically all the adults in the country really are conspiring to deceive children for no tangible benefit. However, since I began frequenting this site, I've been exposed to the alternate viewpoint that this realization may be <em>good</em> for developing rationalists, because it provides children with the experience of discovering that they hold beliefs which are wrong and absurd, and that they must reject them.</p>\n<p>So, how did the Santa deception affect you personally? How do you think your life might have been different without it? If your parents didn't do it to you, what are your impressions on the experience of <em>not</em> being lied to when most other children are?</p>\n<p>Also, I promise to upvote anyone who links to an easy to register for community of conspiracy theorists where they would not be averse to being asked the same question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 1, "cHoCqtfE9cF7aSs9d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FgQpAvYmHqCCW2miw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 30, "extendedScore": null, "score": 6.584128695674098e-07, "legacy": true, "legacyId": "4366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 201, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-21T04:55:40.303Z", "modifiedAt": null, "url": null, "title": "Dutch Books and Decision Theory: An Introduction to a Long Conversation", "slug": "dutch-books-and-decision-theory-an-introduction-to-a-long", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:52.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kjmN3fdwTgN8ejGTd/dutch-books-and-decision-theory-an-introduction-to-a-long", "pageUrlRelative": "/posts/kjmN3fdwTgN8ejGTd/dutch-books-and-decision-theory-an-introduction-to-a-long", "linkUrl": "https://www.lesswrong.com/posts/kjmN3fdwTgN8ejGTd/dutch-books-and-decision-theory-an-introduction-to-a-long", "postedAtFormatted": "Tuesday, December 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dutch%20Books%20and%20Decision%20Theory%3A%20An%20Introduction%20to%20a%20Long%20Conversation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADutch%20Books%20and%20Decision%20Theory%3A%20An%20Introduction%20to%20a%20Long%20Conversation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjmN3fdwTgN8ejGTd%2Fdutch-books-and-decision-theory-an-introduction-to-a-long%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dutch%20Books%20and%20Decision%20Theory%3A%20An%20Introduction%20to%20a%20Long%20Conversation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjmN3fdwTgN8ejGTd%2Fdutch-books-and-decision-theory-an-introduction-to-a-long", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkjmN3fdwTgN8ejGTd%2Fdutch-books-and-decision-theory-an-introduction-to-a-long", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1978, "htmlBody": "<p>For a community that endorses <a href=\"http://plato.stanford.edu/entries/epistemology-bayesian/\">Bayesian epistemology</a> we have had surprisingly few discussions about the most famous Bayesian contribution to epistemology: the Dutch Book arguments. In this post I present the arguments, but it is far from clear yet what the right way to interpret them is or even if they prove what they set out to. The Dutch Book arguments attempt to justify the Bayesian approach to science and belief; I will also suggest that any successful Dutch Book defense of Bayesianism cannot be disentangled from decision theory. But mostly this post is to introduce people to the argument and to get people thinking about a solution. The literature is scant enough that it is plausible people here could actually make genuine progress, especially since the problem is related to decision theory.<sup>1</sup></p>\n<p>Bayesianism fits together. Like a well-tailored jacket it feels comfortable and looks good. It's an appealing, functional aesthetic for those with cultivated epistemic taste. But sleekness is not a rigourous justification and so we should ask: why must the rational agent adopt the <a href=\"http://en.wikipedia.org/wiki/Probability_axioms\">axioms of probability</a> as conditions for her degrees of belief? Further, why should agents accept the principle conditionalization as a rule of inference? These are the questions the Dutch Book arguments try to answer.</p>\n<p>The arguments begin with an assumption about the connection between degrees of belief and willingness to wager. An agent with degree of belief <em>b</em> in hypothesis <em>h </em>is assumed to be willing to buy wager up to and including $<em>b</em> in a unit wager on <em>h </em>and sell a unit wager on <em>h</em> down to and including $<em>b.</em> For example, if my degree of belief that I can drink ten eggnogs without passing out is .3 I am willing to bet $0.30 on the proposition that I can drink the nog without passing out when the stakes of the bet are $1. Call this the Will-to-wager Assumption. As we will see it is problematic.</p>\n<p><a id=\"more\"></a><strong>The Synchronic Dutch Book Argument</strong></p>\n<p>Now consider what happens if my degree of belief that I can drink the eggnog is .3 and my degree of belief that I will pass out before I finish is .75. Given the Will-to-wager assumption my friend can construct a series of wagers that guarantee I will lose money.&nbsp; My friend could offer me a wager on <em>b </em>where I pay $0.30 for $1.00 stakes if I finish the eggnog. He could simultaneously offer me a bet where I pay $0.75 for $1.00 stakes if pass out. Now if I down the eggnog I win $0.70 from the first bet but I lose $0.75 from the second bet, netting me -$0.05. If I pass out I lose the $0.30 from the first bet, but win $0.25 from the second bet, netting me -$0.05. In gambling terminology these lose-lose bets are called a <a href=\"http://en.wikipedia.org/wiki/Dutch_book\">Dutch book</a>. What's cool about this is that violating the axioms of probability is a necessary and sufficient condition for degrees of belief to be susceptible to Dutch books, as in the above example. This is quite easy to see but the reader is welcome to pursue formal proofs: representing degrees of belief with only positive numbers, setting <em>b(all outcomes)=1, </em>and making<em> b</em> additive makes it impossible to construct a Dutch book. A violation of any axiom allows the sum of all <em>b</em> in the sample space<em> </em>to be greater than or less than 1, enabling a Dutch book.</p>\n<p><strong>The Diachronic Dutch Book Argument</strong></p>\n<p>What about conditionalization? Why must a rational agent believe <em>h</em>1 at <em>b</em>(<em>h</em>1|<em>h</em>2) once she learns <em>h</em>2? For this we update the Will-to-wager assumption to have it govern degrees of belief for hypothesis conditional on other hypotheses. An agent with degree of belief <em>b</em> in hypothesis <em>h</em>1<em>|h</em>2<em> </em>is assumed to be willing to wager up to and including $<em>b</em> in a unit wager on <em>h</em>1<em> </em>conditional on <em>h</em>2<em>. </em>This is a wager that is canceled if <em>h</em>2 turns out false but pays out if <em>h</em>2 turns out true. Say I believe with <em>b=</em>0.9 that I will finish ten drinks if we decide to drink cider instead of eggnog. Say I also believe with <em>b=</em>0.5 that we will drink cider and 0.5 that we drink eggnog. But say I *don't* update my beliefs according to the principle of conditionalization. Once I learn that we will drink cider my belief that I will finish the drinks is only b=0.7.&nbsp; Given the Will-to-wager assumption I accept the following wagers.</p>\n<p>(1) An unconditional wager on <em>h</em>2 (that we drink cider not eggnog) that pays $0.20 if <em>h</em>2 is true at <em>b</em>(<em>h</em>2<em>)=</em>0.5<em>*</em>$0.20<em>= </em>$0.10</p>\n<p>(2) A unit wager on <em>h</em>1 (finishing ten drinks) conditional on <em>h</em>2 that pays $1.00 at <em>b</em>(<em>h</em>1|<em>h</em>2)=0.9*$1.00= $0.90</p>\n<p>If <em>h</em>2 is false I lose $0.10 on wager (1). If h2 is true I win $0.10. But now I'm looking at all that cider and not feeling so good. I decide that my degree of belief that I will finish those ten ciders is only b=0.7. So my buys from me an unconditional wager (3) on <em>h</em>1 that pays $1.00 at <em>b</em>(<em>h</em>1)=0.7*$1.00=$0.7.</p>\n<p>Then we start our drinking. If I finish the cider I gain $0.10 from wager (2) which puts me up $0.20, but then I lose $0.30 on wager (3) and I'm down $0.10 on the day. If I don't finish that cider I win $0.70 from wager (3) which puts me at $0.80 until I have to pay out on wager (2) and go down to -$0.10 on the day.</p>\n<p>Note again that any update in degree of belief in any hypothesis <em>h</em> upon learning evidence <em>e </em><strong>that doesn't equal </strong><em>b</em>(<em>h</em>|<em>e</em>) is vulnerable to a Diachronic Dutch booking.</p>\n<p><strong>The Will-to-wager<em> </em>Assumption or Just What Does This Prove, Anyway?</strong></p>\n<p>We might want to take the above arguments literally and say they show not treating your degrees of belief like probabilities is liable to lead you into lose-lose wagers. But this would be a very dumb argument: there is no reason for anyone to actually make wagers in this manner. These are wagers which have zero expected gain and which presumably involve transaction costs. <strong>No rational person would make these wagers according to the Will-to-wager assumption.</strong> Second, the argument presented above uses money and as we are all familiar, money has diminishing return. You probably <em>shouldn't</em> bet $100 for a one in a million shot at $100,000,000 because a hundred million dollars is probably not a million times more useful than a hundred dollars. Third, the argument assumes a rational person must want to win bets. A person might enjoy the wager even if the odds aren't good or might prefer life without the money.</p>\n<p>Nonetheless, the Will-to-wager Assumption doesn't feel arbitrary, it just isn't clear what it implies. There are a couple different strategies we might pursue to improve this argument. First, we can improve the Will-to-wager assumption and corresponding Dutch book theorems by making them about utility instead of money.</p>\n<p>We start by defining a utility function, <em>&upsilon;</em>: <em>X</em>&rarr;<em>R</em> where <em>X</em> is the set of outcomes and <em>R</em> is the set of real numbers. A rational agent is one that acts to maximize <em>R </em>according to their utility function. An agent with degree of belief <em>b</em> in hypothesis <em>h </em>is assumed to be willing to wager up to and including <em>b(</em>util) in a one unil wager on <em>h. </em>As a literal ascription of willingness to wager this interpretation still doesn't make sense. But we can think of the wagers here as general stand-ins for decisions made under uncertainty. The Will-to-Wager assumption fails to work when taken literally because in real life we can always decline wagers. But we can take every decision we make as a forced selection of a set of wagers from an imaginary bookie that doesn't charge a vig, pays out in utility whether you live or die. The Bookie sometimes offers a large, perhaps infinite selection of sets of wagers to pick from and sometimes offers only a handful. The agent can choose one and only one set at a time. Agents have little control over what wagers get offered to them but in many cases one set will clearly be better than the others. But the more an agent's treatment of her beliefs diverges from the laws of probability the more often she's going to get bilked by the imaginary bookie.&nbsp; In other words, the key might be to transform the Dutch Book arguments into decision theory problems. These problems would hopefully demonstrate that non-Bayesian reasoning creates a class of decision problem which the agent always answers sub-optimally or inconsistently.<sup> 2</sup></p>\n<p>A possible downside to the above strategy is that it leaves rationality entangled with utility. There have been some attempts to rewrite the Dutch Book arguments to remove the aspects of utility and preference embedded in them. The main problem with these strategies is that they tend to either fail to remove all notions of preference<sup>3</sup> or have to introduce some kind of apparatus that already resembles probability for no particular reason.<sup>4,5</sup> Our conception of utility is in a Goldilocks spot- it has exactly what we need to make sense of probability while also being something we're familiar with, we don't have to invent it whole cloth. We might also ask a further question: why should beliefs come in degrees. The fact that our utility function (such as humans have one) seems to consist of real numbers and isn't binary (for example) might explain why. You don't need degrees of belief if all but one possible decision are always of value 0. In discussions here many of us have also been given to concluding that probability was <a href=\"/lw/32o/if_a_tree_falls_on_sleeping_beauty/\">epiphenomenal to optimum decision making</a>. Obviously if we believe <em>that</em> we're going to <em>want</em> a Dutch book argument that includes utility. Moreover, any successful reduction of degrees of belief to some decision theoretic measure would benefit from a set of Dutch book arguments that left out degrees of belief altogether.&nbsp;</p>\n<p>As you can see, I think a successful Dutch book will probably keep probability intertwined with decision theory, but since this is our first encounter with the topic: have at it. Use this thread to generate some hypotheses, both for decision theoretic approaches and approaches that leave out utility.</p>\n<p><sup><sub>1 This post can also be thought of as an introduction to basic material and a post accompanying <a href=\"/lw/1to/what_is_bayesianism/\">\"What is Bayesianism\"</a>.<br /></sub></sup></p>\n<p><sup><sub>2 I have some more specific ideas for how to do this, but can't well present everything in this post and I'd like to see if others come up with the similar answers. Remember: discuss a problem exhaustively before coming to a conclusion. I hope people will try to work out their own versions, here in the comments or in new posts. It is also interesting to examine what kinds of utility functions can yield Dutch books- consider what happens for example when the utility function is strictly deontological where every decision consists of a 1 for one option and a 0 for all the others. I also worry that some of the novel decision theories suggested here might have some Dutch book issues. In cases like the Sleeping Beauty problem where the payoff structure is underdetermined things get weird. It looks like this is discussed in \"When Betting Odds and Credences Come Apart\" by Bradley and Leitgeb. I haven't read it yet though.</sub></sup></p>\n<p><sup><sub>3 See Howson and Urbach, \"Scientific Reasoning, the Bayesian Approach\" as an example.</sub></sup></p>\n<p><sup><sub>4 Helman, \"Bayes and Beyond\".</sub></sup></p>\n<p><sup><sub>5 For a good summary of these problems see Maher, \"Depragmatizing Dutch Book Arguments\" where he refutes such attempts. Maher has his own justification for Bayesian Epistemology which isn't a Dutch Book argument (it uses Representation theory, which I don't really understand) and which isn't available online that I can find. This was published in his book \"Betting on Theories\" which I haven't read yet. This looks pretty important so I've reserved the book, if someone is looking for work to do, dig into this.</sub></sup></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kjmN3fdwTgN8ejGTd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 29, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "4345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>For a community that endorses <a href=\"http://plato.stanford.edu/entries/epistemology-bayesian/\">Bayesian epistemology</a> we have had surprisingly few discussions about the most famous Bayesian contribution to epistemology: the Dutch Book arguments. In this post I present the arguments, but it is far from clear yet what the right way to interpret them is or even if they prove what they set out to. The Dutch Book arguments attempt to justify the Bayesian approach to science and belief; I will also suggest that any successful Dutch Book defense of Bayesianism cannot be disentangled from decision theory. But mostly this post is to introduce people to the argument and to get people thinking about a solution. The literature is scant enough that it is plausible people here could actually make genuine progress, especially since the problem is related to decision theory.<sup>1</sup></p>\n<p>Bayesianism fits together. Like a well-tailored jacket it feels comfortable and looks good. It's an appealing, functional aesthetic for those with cultivated epistemic taste. But sleekness is not a rigourous justification and so we should ask: why must the rational agent adopt the <a href=\"http://en.wikipedia.org/wiki/Probability_axioms\">axioms of probability</a> as conditions for her degrees of belief? Further, why should agents accept the principle conditionalization as a rule of inference? These are the questions the Dutch Book arguments try to answer.</p>\n<p>The arguments begin with an assumption about the connection between degrees of belief and willingness to wager. An agent with degree of belief <em>b</em> in hypothesis <em>h </em>is assumed to be willing to buy wager up to and including $<em>b</em> in a unit wager on <em>h </em>and sell a unit wager on <em>h</em> down to and including $<em>b.</em> For example, if my degree of belief that I can drink ten eggnogs without passing out is .3 I am willing to bet $0.30 on the proposition that I can drink the nog without passing out when the stakes of the bet are $1. Call this the Will-to-wager Assumption. As we will see it is problematic.</p>\n<p><a id=\"more\"></a><strong>The Synchronic Dutch Book Argument</strong></p>\n<p>Now consider what happens if my degree of belief that I can drink the eggnog is .3 and my degree of belief that I will pass out before I finish is .75. Given the Will-to-wager assumption my friend can construct a series of wagers that guarantee I will lose money.&nbsp; My friend could offer me a wager on <em>b </em>where I pay $0.30 for $1.00 stakes if I finish the eggnog. He could simultaneously offer me a bet where I pay $0.75 for $1.00 stakes if pass out. Now if I down the eggnog I win $0.70 from the first bet but I lose $0.75 from the second bet, netting me -$0.05. If I pass out I lose the $0.30 from the first bet, but win $0.25 from the second bet, netting me -$0.05. In gambling terminology these lose-lose bets are called a <a href=\"http://en.wikipedia.org/wiki/Dutch_book\">Dutch book</a>. What's cool about this is that violating the axioms of probability is a necessary and sufficient condition for degrees of belief to be susceptible to Dutch books, as in the above example. This is quite easy to see but the reader is welcome to pursue formal proofs: representing degrees of belief with only positive numbers, setting <em>b(all outcomes)=1, </em>and making<em> b</em> additive makes it impossible to construct a Dutch book. A violation of any axiom allows the sum of all <em>b</em> in the sample space<em> </em>to be greater than or less than 1, enabling a Dutch book.</p>\n<p><strong id=\"The_Diachronic_Dutch_Book_Argument\">The Diachronic Dutch Book Argument</strong></p>\n<p>What about conditionalization? Why must a rational agent believe <em>h</em>1 at <em>b</em>(<em>h</em>1|<em>h</em>2) once she learns <em>h</em>2? For this we update the Will-to-wager assumption to have it govern degrees of belief for hypothesis conditional on other hypotheses. An agent with degree of belief <em>b</em> in hypothesis <em>h</em>1<em>|h</em>2<em> </em>is assumed to be willing to wager up to and including $<em>b</em> in a unit wager on <em>h</em>1<em> </em>conditional on <em>h</em>2<em>. </em>This is a wager that is canceled if <em>h</em>2 turns out false but pays out if <em>h</em>2 turns out true. Say I believe with <em>b=</em>0.9 that I will finish ten drinks if we decide to drink cider instead of eggnog. Say I also believe with <em>b=</em>0.5 that we will drink cider and 0.5 that we drink eggnog. But say I *don't* update my beliefs according to the principle of conditionalization. Once I learn that we will drink cider my belief that I will finish the drinks is only b=0.7.&nbsp; Given the Will-to-wager assumption I accept the following wagers.</p>\n<p>(1) An unconditional wager on <em>h</em>2 (that we drink cider not eggnog) that pays $0.20 if <em>h</em>2 is true at <em>b</em>(<em>h</em>2<em>)=</em>0.5<em>*</em>$0.20<em>= </em>$0.10</p>\n<p>(2) A unit wager on <em>h</em>1 (finishing ten drinks) conditional on <em>h</em>2 that pays $1.00 at <em>b</em>(<em>h</em>1|<em>h</em>2)=0.9*$1.00= $0.90</p>\n<p>If <em>h</em>2 is false I lose $0.10 on wager (1). If h2 is true I win $0.10. But now I'm looking at all that cider and not feeling so good. I decide that my degree of belief that I will finish those ten ciders is only b=0.7. So my buys from me an unconditional wager (3) on <em>h</em>1 that pays $1.00 at <em>b</em>(<em>h</em>1)=0.7*$1.00=$0.7.</p>\n<p>Then we start our drinking. If I finish the cider I gain $0.10 from wager (2) which puts me up $0.20, but then I lose $0.30 on wager (3) and I'm down $0.10 on the day. If I don't finish that cider I win $0.70 from wager (3) which puts me at $0.80 until I have to pay out on wager (2) and go down to -$0.10 on the day.</p>\n<p>Note again that any update in degree of belief in any hypothesis <em>h</em> upon learning evidence <em>e </em><strong>that doesn't equal </strong><em>b</em>(<em>h</em>|<em>e</em>) is vulnerable to a Diachronic Dutch booking.</p>\n<p><strong id=\"The_Will_to_wager_Assumption_or_Just_What_Does_This_Prove__Anyway_\">The Will-to-wager<em> </em>Assumption or Just What Does This Prove, Anyway?</strong></p>\n<p>We might want to take the above arguments literally and say they show not treating your degrees of belief like probabilities is liable to lead you into lose-lose wagers. But this would be a very dumb argument: there is no reason for anyone to actually make wagers in this manner. These are wagers which have zero expected gain and which presumably involve transaction costs. <strong>No rational person would make these wagers according to the Will-to-wager assumption.</strong> Second, the argument presented above uses money and as we are all familiar, money has diminishing return. You probably <em>shouldn't</em> bet $100 for a one in a million shot at $100,000,000 because a hundred million dollars is probably not a million times more useful than a hundred dollars. Third, the argument assumes a rational person must want to win bets. A person might enjoy the wager even if the odds aren't good or might prefer life without the money.</p>\n<p>Nonetheless, the Will-to-wager Assumption doesn't feel arbitrary, it just isn't clear what it implies. There are a couple different strategies we might pursue to improve this argument. First, we can improve the Will-to-wager assumption and corresponding Dutch book theorems by making them about utility instead of money.</p>\n<p>We start by defining a utility function, <em>\u03c5</em>: <em>X</em>\u2192<em>R</em> where <em>X</em> is the set of outcomes and <em>R</em> is the set of real numbers. A rational agent is one that acts to maximize <em>R </em>according to their utility function. An agent with degree of belief <em>b</em> in hypothesis <em>h </em>is assumed to be willing to wager up to and including <em>b(</em>util) in a one unil wager on <em>h. </em>As a literal ascription of willingness to wager this interpretation still doesn't make sense. But we can think of the wagers here as general stand-ins for decisions made under uncertainty. The Will-to-Wager assumption fails to work when taken literally because in real life we can always decline wagers. But we can take every decision we make as a forced selection of a set of wagers from an imaginary bookie that doesn't charge a vig, pays out in utility whether you live or die. The Bookie sometimes offers a large, perhaps infinite selection of sets of wagers to pick from and sometimes offers only a handful. The agent can choose one and only one set at a time. Agents have little control over what wagers get offered to them but in many cases one set will clearly be better than the others. But the more an agent's treatment of her beliefs diverges from the laws of probability the more often she's going to get bilked by the imaginary bookie.&nbsp; In other words, the key might be to transform the Dutch Book arguments into decision theory problems. These problems would hopefully demonstrate that non-Bayesian reasoning creates a class of decision problem which the agent always answers sub-optimally or inconsistently.<sup> 2</sup></p>\n<p>A possible downside to the above strategy is that it leaves rationality entangled with utility. There have been some attempts to rewrite the Dutch Book arguments to remove the aspects of utility and preference embedded in them. The main problem with these strategies is that they tend to either fail to remove all notions of preference<sup>3</sup> or have to introduce some kind of apparatus that already resembles probability for no particular reason.<sup>4,5</sup> Our conception of utility is in a Goldilocks spot- it has exactly what we need to make sense of probability while also being something we're familiar with, we don't have to invent it whole cloth. We might also ask a further question: why should beliefs come in degrees. The fact that our utility function (such as humans have one) seems to consist of real numbers and isn't binary (for example) might explain why. You don't need degrees of belief if all but one possible decision are always of value 0. In discussions here many of us have also been given to concluding that probability was <a href=\"/lw/32o/if_a_tree_falls_on_sleeping_beauty/\">epiphenomenal to optimum decision making</a>. Obviously if we believe <em>that</em> we're going to <em>want</em> a Dutch book argument that includes utility. Moreover, any successful reduction of degrees of belief to some decision theoretic measure would benefit from a set of Dutch book arguments that left out degrees of belief altogether.&nbsp;</p>\n<p>As you can see, I think a successful Dutch book will probably keep probability intertwined with decision theory, but since this is our first encounter with the topic: have at it. Use this thread to generate some hypotheses, both for decision theoretic approaches and approaches that leave out utility.</p>\n<p><sup><sub>1 This post can also be thought of as an introduction to basic material and a post accompanying <a href=\"/lw/1to/what_is_bayesianism/\">\"What is Bayesianism\"</a>.<br></sub></sup></p>\n<p><sup><sub>2 I have some more specific ideas for how to do this, but can't well present everything in this post and I'd like to see if others come up with the similar answers. Remember: discuss a problem exhaustively before coming to a conclusion. I hope people will try to work out their own versions, here in the comments or in new posts. It is also interesting to examine what kinds of utility functions can yield Dutch books- consider what happens for example when the utility function is strictly deontological where every decision consists of a 1 for one option and a 0 for all the others. I also worry that some of the novel decision theories suggested here might have some Dutch book issues. In cases like the Sleeping Beauty problem where the payoff structure is underdetermined things get weird. It looks like this is discussed in \"When Betting Odds and Credences Come Apart\" by Bradley and Leitgeb. I haven't read it yet though.</sub></sup></p>\n<p><sup><sub>3 See Howson and Urbach, \"Scientific Reasoning, the Bayesian Approach\" as an example.</sub></sup></p>\n<p><sup><sub>4 Helman, \"Bayes and Beyond\".</sub></sup></p>\n<p><sup><sub>5 For a good summary of these problems see Maher, \"Depragmatizing Dutch Book Arguments\" where he refutes such attempts. Maher has his own justification for Bayesian Epistemology which isn't a Dutch Book argument (it uses Representation theory, which I don't really understand) and which isn't available online that I can find. This was published in his book \"Betting on Theories\" which I haven't read yet. This looks pretty important so I've reserved the book, if someone is looking for work to do, dig into this.</sub></sup></p>", "sections": [{"title": "The Diachronic Dutch Book Argument", "anchor": "The_Diachronic_Dutch_Book_Argument", "level": 1}, {"title": "The Will-to-wager Assumption or Just What Does This Prove, Anyway?", "anchor": "The_Will_to_wager_Assumption_or_Just_What_Does_This_Prove__Anyway_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "102 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gMXsyhPiEJbGerF6F", "AN2cBr6xKWCB8dRQG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-21T07:21:10.299Z", "modifiedAt": null, "url": null, "title": "Iterated Sleeping Beauty and Copied Minds", "slug": "iterated-sleeping-beauty-and-copied-minds", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:03.114Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lucidfox", "createdAt": "2010-11-22T06:58:06.993Z", "isAdmin": false, "displayName": "lucidfox"}, "userId": "hNnKSqvajCMRj9eKK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WL69sp2MJNLs9ADg4/iterated-sleeping-beauty-and-copied-minds", "pageUrlRelative": "/posts/WL69sp2MJNLs9ADg4/iterated-sleeping-beauty-and-copied-minds", "linkUrl": "https://www.lesswrong.com/posts/WL69sp2MJNLs9ADg4/iterated-sleeping-beauty-and-copied-minds", "postedAtFormatted": "Tuesday, December 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Iterated%20Sleeping%20Beauty%20and%20Copied%20Minds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIterated%20Sleeping%20Beauty%20and%20Copied%20Minds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWL69sp2MJNLs9ADg4%2Fiterated-sleeping-beauty-and-copied-minds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Iterated%20Sleeping%20Beauty%20and%20Copied%20Minds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWL69sp2MJNLs9ADg4%2Fiterated-sleeping-beauty-and-copied-minds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWL69sp2MJNLs9ADg4%2Fiterated-sleeping-beauty-and-copied-minds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1272, "htmlBody": "<p>Before I move on to a summation post listing the various raised thought experiments and paradoxes related to mind copying, I would like to cast attention to a particular moment regarding the notion of \"subjective probability\".</p>\n<p>In my earlier&nbsp;<a href=\"/r/discussion/lw/3dr/copying_and_subjective_experience/\">discussion post on the subjective experience</a>&nbsp;of a forked person, I compared the scenario where one copy is awakened in the future to the Sleeping Beauty thought experiment. And really, it describes any such process, because there will inevitably be a time gap, however short, between the time of fork and the copy's subjective awakening: no copy mechanism can be instant.</p>\n<p>In the <a href=\"/lw/32o/if_a_tree_falls_on_sleeping_beauty/\">traditional Sleeping Beauty scenario</a>, there are two parties: Beauty and the Experimenter. The Experimenter has access to a sleep-inducing drug that also resets Beauty's memory to the state at t=0. Suppose Beauty is put to sleep at t=0, and then a fair coin is tossed. If the coin comes heads, Beauty is woken up at t=1, permanently. If the coin comes tails, Beauty is woken up at t=1, questioned, memory-wiped, and then woken up again at t=2, this time permanently.</p>\n<p>In this experiment, intuitively, Beauty's subjective anticipation of the coin coming tails, without access to any information other than the conditions of the experiment, should be 2/3. I won't be arguing here whether this particular answer is right or wrong: the discussion has been raised many times before, and on Less Wrong as well. I'd like to point out one property of the experiment that differentiates it from other probability-related tasks: <em>erasure of information</em>, which renders the whole experiment a non-experiment.</p>\n<p>In Bayesian theory, the (prior) probability of an outcome is the measure of our anticipation of it to the best of our knowledge. Bayesians think of experiments as a way to get new information, and update their probabilities based on the information gained. However, in the Sleeping Beauty experiment, Beauty gains no new information from waking up at any time, in any outcome. She has the exact same mind-state at any point of awakening that she had at t=0, and is for all intents and purposes the exact same person at any such point. As such, we can ask Beauty, \"If we perform the experiment, what is your anticipation of waking up in the branch where the coin landed tails?\", and she can give the same answer without actually performing the experiment.</p>\n<p>So how does it map to the mind-copying problem? In a very straightforward way.</p>\n<p>Let's modify the experiment this way: at t=0, Beauty's state is backed up. Let's suppose that she is then allowed to live her normal life, but the time-slices are large enough that she dies within the course of a single round. (Say, she has a normal human lifespan and the time between successive iterations is 200 years.) However, at t=1, a copy of Beauty is created in the state at which the original was at t=0, a coin is tossed, and if and only if it comes tails, another copy is created at t=2.</p>\n<p>If Beauty knows the condition of this experiment, no matter what answer she would give in the classic formulation of the problem, I don't expect it to change here. The two formulations are, as far as I can see, equivalent.</p>\n<p>However, in both cases, from the Experimenter's point of view, the branching points are independent events, which allows us to construct scenarios that question the straightforward interpretation of \"subjective probability\". And for this, I refer to the last experiment in my earlier post.</p>\n<p>Imagine you have an indestructible machine that restores one copy of you from backup every 200 years. In this scenario, it seems you should anticipate waking up with equal probability between now and the end of time. But it's inconsistent with the formulation of probability for discrete outcomes: we end up with a diverging series, and as the length of the experiment approaches infinity (ignoring real-world cosmology for the moment), the subjective probability of every individual outcome (finding yourself at t=1, finding yourself at t=2, etc.) approaches 0. The equivalent classic formulation is a setup where the Experimenter is programmed to wake Beauty after every time-slice and unconditionally put her back to sleep.</p>\n<p>This is not the only possible \"diverging Sleeping Beauty\" problem. Suppose that at t=1, Beauty is put back to sleep with probability 1/2 (like in the classic experiment), at t=2 she is put back to sleep with probability 1/3, then 1/4, and so on. In this case, while it seems almost certain that she will eventually wake up permanently (in the same sense that it is \"almost certain\" that a fair random number generator will eventually output any given value), the expected value is still infinite.<br /><br />In the case of a converging series of probabilities of remaining asleep - for example, if it's decided by a coin toss at each iteration whether Beauty is put back to sleep, in which case the series is 1/2 + 1/4 + 1/8 + ... = 1 -- Beauty can give a subjective expected value, or the average time at which she expects to be woken up permanently.</p>\n<p>In a general case, let E<sub>i</sub> be the event \"the experiment continues at stage i\" (that is, Beauty is not permanently awakened at stage i, or in the alternate formulation, more copies are created beyond that point). Then if we extrapolate the notion of \"subjective probability\" that leads us to the answer 2/3 in the classic formulation, then the definition is meaningful if and only if the series <em>of objective probabilities</em> &sum;<sub>i=1..&infin;</sub>&nbsp;P(E<sub>i)</sub>&nbsp;converges -- it doesn't have to converge to 1, we'll just need to renormalize the calculations otherwise. Which, given that the randomizing events are independent, simply doesn't have to happen.</p>\n<p>Even if we reformulate the experiment in terms of decision theory, it's not clear how it will help us. If the bet is \"win 1 utilon if you get your iteration number right\", the probability of winning it in a divergent case is 0 at any given iteration. And yet, if all cases are perfectly symmetric information-wise so that you make the same decision over and over again, you'll eventually get the answer right, with exactly one of you winning the bet, even no matter what your \"decision function\"&nbsp;is&nbsp;- even if it's simply something like \"return 42;\". Even a stopped clock is right sometimes, in this case once.</p>\n<p>It would be tempting, seeing this, to discard the notion of \"subjective anticipation\" altogether as ill-defined. But that seems to me like tossing out <a href=\"/lw/py/the_born_probabilities/\">the Born probabilities</a> just because we go from Copenhagen to MWI. If I'm forked, I expect to continue my experience as either the original or the copy with a probability of 1/2 --&nbsp;<em>whatever that means</em>. If I'm asked to participate in the classic Sleeping Beauty experiment, and to observe the once-flipped coin at every point I wake up, I will expect to see tails with a probability of 2/3 -- again, whatever that means.</p>\n<p>The situations described here have a very specific set of conditions. We're dealing with complete information erasure, which prevents any kind of Bayesian update and in fact makes the situation completely symmetric from the decision agent's perspective. We're also dealing with an anticipation all the way into infinity, which cannot occur in practice due to the finite lifespan of the universe. And yet, I'm not sure what to do with the apparent need to update my anticipations for times arbitrarily far into the future, for an arbitrarily large number of copies, for outcomes with an arbitrarily high degree of causal removal from my current state, which may fail to occur, before the sequence of events that can lead to them is even put into motion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NZB24aR9uHmDc5GcT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WL69sp2MJNLs9ADg4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 6.585469474356612e-07, "legacy": true, "legacyId": "4399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cxwzLv7czo3HDkKJA", "gMXsyhPiEJbGerF6F", "3ZKvf9u2XEWddGZmS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-21T20:25:07.210Z", "modifiedAt": null, "url": null, "title": "Is technological change accelerating?", "slug": "is-technological-change-accelerating", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:05.764Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5xDd4yP6kikjYrj3T/is-technological-change-accelerating", "pageUrlRelative": "/posts/5xDd4yP6kikjYrj3T/is-technological-change-accelerating", "linkUrl": "https://www.lesswrong.com/posts/5xDd4yP6kikjYrj3T/is-technological-change-accelerating", "postedAtFormatted": "Tuesday, December 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20technological%20change%20accelerating%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20technological%20change%20accelerating%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xDd4yP6kikjYrj3T%2Fis-technological-change-accelerating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20technological%20change%20accelerating%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xDd4yP6kikjYrj3T%2Fis-technological-change-accelerating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xDd4yP6kikjYrj3T%2Fis-technological-change-accelerating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Eliezer <a href=\"http://vimeo.com/17513355\">said</a> in a speech at the Singularity Summit that he's agnostic about whether technological change is accelerating, and mentions Michael Vassar and Peter Thiel as skeptical.</p>\n<p>I'd vaguely assumed that it was accelerating, but when I thought about it a little, it seemed like a miserably difficult thing to measure. Moore's law just tracks The number of transistors that can be placed inexpensively on an integrated circuit.</p>\n<p>Technology is a vaguer thing. Cell phones are an improvement (or at least most people get them) in well-off countries that have landlines, but they're a much bigger change in regions where cell phones are the first phones available.  There a jump from a cell phone that's just a phone/answering machine/clock to a smartphone, but how do you compare that jump to getting home computers?</p>\n<p>Do you have a way of measuring whether technological change is accelerating? If so, what velocity and acceleration do you see?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5xDd4yP6kikjYrj3T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 6.587436670696615e-07, "legacy": true, "legacyId": "4409", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-21T21:39:53.684Z", "modifiedAt": null, "url": null, "title": "Dutch book question", "slug": "dutch-book-question", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "QJcy5NDAd8HTiZYQq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XkRTY7mMbBvTKyoyn/dutch-book-question", "pageUrlRelative": "/posts/XkRTY7mMbBvTKyoyn/dutch-book-question", "linkUrl": "https://www.lesswrong.com/posts/XkRTY7mMbBvTKyoyn/dutch-book-question", "postedAtFormatted": "Tuesday, December 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dutch%20book%20question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADutch%20book%20question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkRTY7mMbBvTKyoyn%2Fdutch-book-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dutch%20book%20question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkRTY7mMbBvTKyoyn%2Fdutch-book-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkRTY7mMbBvTKyoyn%2Fdutch-book-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>I'm following Jack's Dutch book discussion with interest and would like to know about the computational complexity of constructing a Dutch book. &nbsp;If I give you a finite table of probabilities, is there a polynomial time algorithm that will verify that it is or is not Dutch bookable? &nbsp;Or: help me make this question better-posed.</p>\n<p>It reminds me of Boolean satisfiability, which is known to be NP complete, but maybe the similarity is superficial.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XkRTY7mMbBvTKyoyn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4410", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-21T22:55:35.522Z", "modifiedAt": null, "url": null, "title": "Using rot13 too much doesn't work", "slug": "using-rot13-too-much-doesn-t-work", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.728Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RmgeL4bK5zvD9X6SA/using-rot13-too-much-doesn-t-work", "pageUrlRelative": "/posts/RmgeL4bK5zvD9X6SA/using-rot13-too-much-doesn-t-work", "linkUrl": "https://www.lesswrong.com/posts/RmgeL4bK5zvD9X6SA/using-rot13-too-much-doesn-t-work", "postedAtFormatted": "Tuesday, December 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Using%20rot13%20too%20much%20doesn't%20work&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUsing%20rot13%20too%20much%20doesn't%20work%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRmgeL4bK5zvD9X6SA%2Fusing-rot13-too-much-doesn-t-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Using%20rot13%20too%20much%20doesn't%20work%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRmgeL4bK5zvD9X6SA%2Fusing-rot13-too-much-doesn-t-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRmgeL4bK5zvD9X6SA%2Fusing-rot13-too-much-doesn-t-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p>Orpnhfr V guvax zbfg crbcyr urer jvyy whfg riraghnyyl cvpx hc ubj gb ernq vg. Captializations of I where the first hint I consistently got, and I've found that nsgre rkcbfher gb n srj qbmra fubeg grkgf (naq xabjvat jung ebg13 zrnaf) V'ir ortha ernqvat gur qnza guvat whfg yvxr beqvanel grkg.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RmgeL4bK5zvD9X6SA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 6.587814375358543e-07, "legacy": true, "legacyId": "4411", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-22T01:23:48.124Z", "modifiedAt": null, "url": null, "title": "Theory and practice of meditation", "slug": "theory-and-practice-of-meditation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:02.519Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Craig_Heldreth", "createdAt": "2010-06-14T23:30:28.110Z", "isAdmin": false, "displayName": "Craig_Heldreth"}, "userId": "hhKowsjZBQSyBE6c5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4DK2rxfzTBK8rCFy9/theory-and-practice-of-meditation", "pageUrlRelative": "/posts/4DK2rxfzTBK8rCFy9/theory-and-practice-of-meditation", "linkUrl": "https://www.lesswrong.com/posts/4DK2rxfzTBK8rCFy9/theory-and-practice-of-meditation", "postedAtFormatted": "Wednesday, December 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Theory%20and%20practice%20of%20meditation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATheory%20and%20practice%20of%20meditation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DK2rxfzTBK8rCFy9%2Ftheory-and-practice-of-meditation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Theory%20and%20practice%20of%20meditation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DK2rxfzTBK8rCFy9%2Ftheory-and-practice-of-meditation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DK2rxfzTBK8rCFy9%2Ftheory-and-practice-of-meditation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1748, "htmlBody": "<p>This is a (slightly revised) concatenation of three of my blog posts which I wrote after reading:</p>\n<p><a href=\"/lw/2rd/understanding_vipassana_meditation/\">Understanding vipassana meditation</a> by Luke Grecki in October 2010. The originals may be seen here (<a href=\"http://craigsspringbranchblog.blogspot.com/2010/11/theory-and-practice-of-meditation-i.html\">part I</a>), here (<a href=\"http://craigsspringbranchblog.blogspot.com/2010/11/theory-and-practice-of-meditation-ii.html\">part II</a>) and here (<a href=\"http://craigsspringbranchblog.blogspot.com/2010/12/theory-and-practice-of-meditation-iii.html\">part III</a>).</p>\n<p>I posted some comments myself in the original thread, but after giving it some thought I have decided to write a little more systematically on the topic.<br /><br />There are three parts: one theory part and one part each on descriptions of the two techniques in my current practice.</p>\n<p><em>Part I, a theory</em></p>\n<p>I have been meditating daily for over thirteen years and did it sporadically for fifteen years or so prior to that. My menu of tried protocols is wide: vipassana, zen, transcendental, Gurdjieff self-remembering, Jung active-imagination, Erickson self-hypnosis, Loyola spiritual exercises, and probably a couple others I have totally forgotten about. The common thread through all of these techniques is mental health benefit, or spiritual benefit, or stress relief through calming mental processes. It is a purging of obsession and compulsion and anxiety and worry. Don Juan advises Carlos Castaneda the way to become a sorcerer is to learn to make one's mind perfectly still. (Castaneda's regimen may be the only one that I have heard about that I have not tried--I have seen people under the influence of <a href=\"http://en.wikipedia.org/wiki/Ketamine\">deliriants</a> and that is definitely not for me.)<br /><br />There is modern scientific research in support of this, most notably in the work of the psychologist Albert Ellis and the psychiatrist Aaron Beck. Their therapy techniques are based upon the idea that our problems of mental life are twofold: first there are the human stressors which plague all of us to one extent or another--family problems, relationship problems, money problems, diseases--what Zorba called the full catastrophe; second there is the stuff which we tell ourselves on top of these typical and normal human stressors.<br /><br />\"This always happens to me.\"<br /><br />\"Nobody loves me.\"<br /><br />\"I am a freak; I am a loser; &amp;c.\"<br /><br />We could make a very long list. Ellis and Beck say you may be unable to eliminate the family problems and whatnot at the source of your grief, but you surely can quit telling yourself the exaggerated and goofy crap you pile up on top of it. Their experience (and a large amount of subsequent clinical experience) is that modifying the self-descriptions will benefit mental health. This can involve work, and sometimes a lot of it. This is the scientific research behind the psychobabble in the self-help books regarding being a friend to your self.<br /><br />Meditation provides the ancient path towards quieting these activities of our minds which can be such a burden. There are two basic techniques: a technique of concentration and a technique of emptying. In the technique of concentration you focus your awareness as completely as possible on one stimulus. It can be listening to a mantra as in the example of the hare krishnas or the transcendental meditation. It can be staring at a mandala or a crystal ball or a blue vase or a saucer of ink. It can be saying a rosary. In the technique of emptying you focus your awareness as completely as possible on the minimum possible field of concentration; this is usually the breath. You simply follow only your breathing as purely as possible for a period of a few minutes. A hybrid of the two is use of the minimum possible sense stimulus, the mantra Aum.<br /><br />In this attention to nothing, or attention to as little as possible, time and space is provided for the mental burdens of anxiety and such to run their course and escape from our attention center. This is the process by which meditation leads to better mental health. This apparently is not the intent the innovators who developed these procedures were going for, however. They were aiming at something much more profound.<br /><br />If you participate in meditation practice for a very long time (like, thousands and thousands of hours), you may have an opportunity to attain a state of being where you are connected <span style=\"font-style: italic;\">link-pow-one-with-the-universe</span>. <a href=\"http://en.wikipedia.org/wiki/Samadhi\">Samadhi</a>. You attain Samadhi, and presumably you never again need care about all the girls thinking you are too short.</p>\n<p><em>Part II, my (shorter) daily practice</em></p>\n<p>This is adapted from a self-hypnosis relaxation script I obtained from the book <a href=\"http://www.amazon.com/Mind-Body-Therapy-Methods-Ideodynamic-Hypnosis/dp/039331247X\">Mind-Body Therapy: Methods of Ideodynamic Healing in Hypnosis</a>, by Ernest Rossi and David Cheek. I call my variation the <span style=\"font-style: italic;\">homunculus</span> meditation. The name is taken from a neuroscience figure, a <a href=\"http://neurobonkers.com/wp-content/uploads/2010/07/3d-homunculus.png\">homunculus</a>, which is made by inflating anatomical parts in proportion to the amount of the somatic sensory cortex which are involved in our sense of touch for the particular anatomical part.<br /><br />The script is very simple. You sit quietly in a relaxing posture and invite yourself to sequentially relax different portions of your body. There are thousands and thousands of terms which pertain to various anatomical structures, so you cannot name them all in one single meditation (or self-hypnosis) session. The ones I routinely use are (in order): eyes, optic nerves, visual cortex, cerebral cortex, limbic lobes, hindbrain, throat, spine, <a href=\"http://upload.wikimedia.org/wikipedia/commons/a/a7/Nerves_of_the_left_upper_extremity.gif\">median nerves</a>, fingertips, (back up to) limbic lobes, hindbrain, throat, spine, <a href=\"http://upload.wikimedia.org/wikipedia/commons/9/9d/Gray832.png\">sciatic nerves</a>, toe tips, foot soles, ankles, calves, ankles, shins, ankles, <a href=\"http://upload.wikimedia.org/wikipedia/commons/4/45/Gray258.png\">fibulas</a>, knees, hamstrings, knees, quadriceps, knees, femurs, glans, testicles, anus, lumbars, navel, <a href=\"http://upload.wikimedia.org/wikipedia/commons/5/54/Gray_111_-_Vertebral_column-coloured.png\">seventh thoracic vertebra</a><a href=\"http://upload.wikimedia.org/wikipedia/commons/5/54/Gray_111_-_Vertebral_column-coloured.png\">e</a>, nipples, seventh cervical vertebrae, shoulders, elbows, thumbs, index fingers, middle fingers, ring fingers, little fingers, elbows, wrists, thumbs, index fingers, middle fingers, ring fingers, little fingers, wrists, fingertips, wrists, elbows, shoulders, seventh cervical vertebrae, nipples, seventh thoracic vertebrae, navel, lumbars, anus, testicles, glans, testicles, anus, lumbars, navel, seventh thoracic vertebrae, nipples, seventh cervical vertebrae, spine, throat, tongue, palate, gums, lips, nostrils, nasal cavities, sinuses, eyes, temples, ears, eustachian tubes, ears, temples, eyes, forehead.<br /><br />On average this takes about twenty minutes to work all the way down and back up through these features of my anatomy. There are three additional important details:<br /><br />1.) In the Rossi-Cheek recipe they instruct us to instruct ourselves \"Relax eyes, &amp;c.\" There is an old philosophical conundrum here regarding who is talking to who when we are talking to ourselves. When you sink a long basket and you say to yourself \"Good shot!\", who is talking to who there? There is some implicit dissociative model like perhaps Freud's--and perhaps it is your superego talking to your ego, or something similar to that. Anyway, what I do instead of commanding myself to relax, is to invite myself to relax. I substitute \"I may relax my eyes, &amp;c.\" for the literal instruction provided in the Rossi-Cheek recipe.<br /><br />2.) A few of these invitations are repeated, sometimes over and over. Roughly, I devote the proportion of the session along the proportions in the homunculus diagram, hence my name of homunculus meditation. I invite my fingers and my lips and my tongue to relax far more than I invite any other portion of my anatomy to do so.<br /><br />3.) The other weighting is toward the eyes and ears; a large fraction of our brain is allocated to the processing of visual and audio sense information. By concentrating on the parts of the body that involve the largest brain fractions, the given twenty minutes (or whatever) of meditation can have the largest total <span style=\"font-style: italic;\">brain footprint</span>! That is one theory.<br /><br />I have been using this meditation (or one close to it) on a nearly daily basis since 1997, since I first read Rossi and Cheek's book. I will be using it for the foreseeable future.</p>\n<p><em>Part III, my (longer and at least) weekly practice</em></p>\n<p><br />This one takes me about forty minutes.<br /><br />Step one is to sit still in a comfortable position with eyes closed. Breathe slowly and count, one count for each breath to one hundred. For each breath, I visualize a sphere which looks like, or almost like a billiard ball, with the number of the breath inside the little white circle area (like on a standard billiard ball that is numbered one to eight.) The spheres alternate on an interval of ten in color and in spatial position. This sequence is patterned after a common representation of the <a href=\"http://en.wikipedia.org/wiki/Tree_of_Life_%28Kabbalah%29\">Kabbalah Tree of Life</a>.<br /><br />1, 11, 21, 31, &amp;c are a white sphere on the crown of my head;<br />2, 12, 22, 32, &amp;c are a gray sphere on my right shoulder;<br />3, 13, 23, 33, &amp;c are a black sphere on my left shoulder;<br />4, 14, 24, &amp;c are a blue sphere on my right elbow;<br />5, 15, 25, &amp;c are a red sphere on my left elbow;<br />6, 16, 26 &amp;c are a yellow sphere on my crotch;<br />7, 17, 27, &amp;c are a green sphere on my right fingertips;<br />8, 18, 28, &amp;c are an orange sphere on my left fingertips;<br />9, 19, 29, &amp;c are a purple sphere between my knees;<br />10, 20, &amp;c are a brown sphere between my feet.<br /><br />I used to play a lot of billiards so visualizing billiard balls is quite easy for me. A million other things do cross my mind during this forty or so minutes of meditation, but I try and hold my attention as closely as possible to my breath and to the billiard ball images. After, I make notes of: how many minutes (37 - 51 is the range in recent memory); if I lost count at any point (if I lose the count, I just guess where I was and continue from there--this is an excellent marker for me on how well I am attending to the meditation); if I had a hiccup or a cough or a saliva swallow or a saliva drool (I prefer not to, and sometimes I will stop meditating if any of these occur.)<br /><br />Sometimes I will try and extend this to an even longer meditation. About once a month I will go for 200 breaths, and about once a year I will go for 300. 300 breaths is the longest I have ever gone. If I am going for a long meditation, I always stop if I lose count or if I hiccup or if I drool or if anything is not perfect.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4DK2rxfzTBK8rCFy9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 6.588185182831505e-07, "legacy": true, "legacyId": "4416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CNLMxEkx7PqHnnvxC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-22T02:59:02.736Z", "modifiedAt": null, "url": null, "title": "The 9 Circles of Scientific Hell", "slug": "the-9-circles-of-scientific-hell", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:02.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TwqgstJbFA4fFDTmB/the-9-circles-of-scientific-hell", "pageUrlRelative": "/posts/TwqgstJbFA4fFDTmB/the-9-circles-of-scientific-hell", "linkUrl": "https://www.lesswrong.com/posts/TwqgstJbFA4fFDTmB/the-9-circles-of-scientific-hell", "postedAtFormatted": "Wednesday, December 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%209%20Circles%20of%20Scientific%20Hell&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%209%20Circles%20of%20Scientific%20Hell%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwqgstJbFA4fFDTmB%2Fthe-9-circles-of-scientific-hell%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%209%20Circles%20of%20Scientific%20Hell%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwqgstJbFA4fFDTmB%2Fthe-9-circles-of-scientific-hell", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwqgstJbFA4fFDTmB%2Fthe-9-circles-of-scientific-hell", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 143, "htmlBody": "<p><a href=\"http://http//neuroskeptic.blogspot.com/\">Neuroskeptic</a>&nbsp;is my&nbsp;favorite&nbsp;blog on neuroscience. Don't be deceived by the 'skeptic' in the name, the coverage is well balanced and overall quite positive. He recently interrupted his regular scheduling with a light piece on the&nbsp;<a href=\"http://neuroskeptic.blogspot.com/2010/11/9-circles-of-scientific-hell.html\">circles of scientific hell</a>. Definitely worth a look. I'm not too sure about the order of the various sins. I'd be tempted to put \"p-value fishing\" way down the list!</p>\n<p>An excerpt:</p>\n<blockquote>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; \"><span style=\"font-weight: bold; \">Second Circle: Overselling</span><br />\"This circle is reserved for those who exaggerated the importantance of their work in order to get grants or write better papers. Sinners are trapped in a huge pit, neck-deep in horrible sludge. Each sinner is provided with the single rung of a ladder, labelled 'The Way Out - Scientists Crack Problem of Second Circle of Hell\"</span></p>\n</blockquote>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; \">Makes me want to want to break out into a chorus of \"Let the Punishment Fit the Crime\"!</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TwqgstJbFA4fFDTmB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 6.588425557166365e-07, "legacy": true, "legacyId": "4422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-22T06:27:22.566Z", "modifiedAt": null, "url": null, "title": "Newtonmas Meetup, 12/25/2010", "slug": "newtonmas-meetup-12-25-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:09.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PjnDDJ4LL9AB5RtnG/newtonmas-meetup-12-25-2010", "pageUrlRelative": "/posts/PjnDDJ4LL9AB5RtnG/newtonmas-meetup-12-25-2010", "linkUrl": "https://www.lesswrong.com/posts/PjnDDJ4LL9AB5RtnG/newtonmas-meetup-12-25-2010", "postedAtFormatted": "Wednesday, December 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newtonmas%20Meetup%2C%2012%2F25%2F2010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewtonmas%20Meetup%2C%2012%2F25%2F2010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPjnDDJ4LL9AB5RtnG%2Fnewtonmas-meetup-12-25-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newtonmas%20Meetup%2C%2012%2F25%2F2010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPjnDDJ4LL9AB5RtnG%2Fnewtonmas-meetup-12-25-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPjnDDJ4LL9AB5RtnG%2Fnewtonmas-meetup-12-25-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p>There's a Less Wrong meetup at my house in Berkeley this Saturday, the 25th of December, at 6PM. Celebrate the winter season, the Solstice, and the birth of Sir Isaac Newton among friendly aspiring rationalists, including Eliezer and other SIAI staff and volunteers.</p>\n<p>I will cook for everyone in the style I call \"paleolithic gourmet\" which is cooked meat and raw produce.</p>\n<p>I'd like to satisfy everyone's preferences as reasonably as I possibly can without getting vastly more food than will be eaten.</p>\n<p>Default menu:</p>\n<p>Steak<br />Lamb Burgers<br />Bacon<br />Salad of Berkeley Bowl produce and parmesan<br />Grilled Portabello and chanterelle mushrooms<br />Cheese selection<br />Pita + hummus<br />Cookies</p>\n<p>Feel free to bring a potluck dessert or if you like, an alcoholic or non-alcoholic beverage.<a id=\"more\"></a></p>\n<p>The food is free, but if you can afford to, in the spirit of Newtonmas, I suggest a $10 or $15 or $500 donation to SIAI (which will be matched). Please don't not come because you prefer not to pay; no one will be excluded from food or shunned for not paying. I really mean that. Consider the donation not an admission fee and more of a gentle nudge and reminder that optimal philanthropy starts around $10 and that you should positively associate giving money with the fuzzies of eating delicious food.</p>\n<p>Please post here if you plan on attending and <a href=\"http://www.facebook.com/event.php?eid=186866944661887&amp;num_event_invites=0\">RSVP on Facebook</a>. You can also post here or PM me with your thoughts on the menu and tell me what you want to eat the most of. I wasn't planning on cooking fish or chicken but can do so if people let me know they want fish or chicken or something else (like a carbohydrate).</p>\n<p>My address is <a href=\"http://maps.google.com/maps?rlz=1C1_____enUS391US391&amp;q=1622+martin+luther+king+jr+way+berkeley&amp;um=1&amp;ie=UTF-8&amp;hq=&amp;hnear=1622+Martin+Luther+King+Jr+Way,+Berkeley,+CA+94709&amp;gl=us&amp;ei=Mc0RTdzOK4mosAOMhOGyAg&amp;sa=X&amp;oi=geocode_result&amp;ct=title&amp;resnum=1&amp;ved=0CBgQ8gEwAA\">1622 Martin Luther King Jr Way Apt A, Berkeley CA</a>. It's the ground floor apartment around the side, not the upstairs one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PjnDDJ4LL9AB5RtnG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 6.58894612288411e-07, "legacy": true, "legacyId": "4424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-22T14:52:09.336Z", "modifiedAt": null, "url": null, "title": "The blind god's computer language", "slug": "the-blind-god-s-computer-language", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:02.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LTTDG7gNLyYk2Ppfi/the-blind-god-s-computer-language", "pageUrlRelative": "/posts/LTTDG7gNLyYk2Ppfi/the-blind-god-s-computer-language", "linkUrl": "https://www.lesswrong.com/posts/LTTDG7gNLyYk2Ppfi/the-blind-god-s-computer-language", "postedAtFormatted": "Wednesday, December 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20blind%20god's%20computer%20language&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20blind%20god's%20computer%20language%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLTTDG7gNLyYk2Ppfi%2Fthe-blind-god-s-computer-language%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20blind%20god's%20computer%20language%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLTTDG7gNLyYk2Ppfi%2Fthe-blind-god-s-computer-language", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLTTDG7gNLyYk2Ppfi%2Fthe-blind-god-s-computer-language", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p>http://crisper.livejournal.com/316634.html#cutid1</p>\n<blockquote>They have no compiler, only an accretor that gloms additional code onto the existing binary. I use the word binary loosely; it is not uncommon for them to \"improve\" fundamentally flawed data structures by moving to a larger base notation - to trinary, then quadrary, etc. At this point, some of their products are in base 17.</blockquote>\n<blockquote>They never go back to fix bugs where they occur. They write new code to workaround the earlier failure case. I asked why they don't go back and just fix the bug where it happens. I was told \"We can't go back and change it. That code's already done!\" Their solution for insuring that failing code will be able to get to its workaround is the GOTO statement. GOTO is sprinkled liberally around other code, pointing to functions and routines that do not exist yet. If, down the road, it is discovered that the old code has a bug, they find out which GOTOs exist in that code that do not point to anything yet, pick one, and write the workaround there.</blockquote>\n<blockquote>I could go on, but I am being told that we need to celebrate the successful compilation (by which I mean accretion) of a particularly complex workaround for a bug that has been known about for two years.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LTTDG7gNLyYk2Ppfi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 19, "extendedScore": null, "score": 6.590216374469387e-07, "legacy": true, "legacyId": "4427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-22T17:56:02.982Z", "modifiedAt": null, "url": null, "title": "Many of us *are* hit with a baseball once a month. ", "slug": "many-of-us-are-hit-with-a-baseball-once-a-month", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:47.312Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7AZrPxwG9FMYYv5iv/many-of-us-are-hit-with-a-baseball-once-a-month", "pageUrlRelative": "/posts/7AZrPxwG9FMYYv5iv/many-of-us-are-hit-with-a-baseball-once-a-month", "linkUrl": "https://www.lesswrong.com/posts/7AZrPxwG9FMYYv5iv/many-of-us-are-hit-with-a-baseball-once-a-month", "postedAtFormatted": "Wednesday, December 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Many%20of%20us%20*are*%20hit%20with%20a%20baseball%20once%20a%20month.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMany%20of%20us%20*are*%20hit%20with%20a%20baseball%20once%20a%20month.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7AZrPxwG9FMYYv5iv%2Fmany-of-us-are-hit-with-a-baseball-once-a-month%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Many%20of%20us%20*are*%20hit%20with%20a%20baseball%20once%20a%20month.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7AZrPxwG9FMYYv5iv%2Fmany-of-us-are-hit-with-a-baseball-once-a-month", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7AZrPxwG9FMYYv5iv%2Fmany-of-us-are-hit-with-a-baseball-once-a-month", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>Watching the video of <a href=\"http://vimeo.com/17513355\">Eliezer's Singularity Summit 2010 talk</a>, I thought once more about the 'baseball' argument. Here's a text version from <a href=\"/lw/k8/how_to_seem_and_be_deep/\">How to Seem (and Be) Deep</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">[...] given human nature, if people got hit on the head by a baseball bat every week, pretty soon they would invent reasons why getting hit on the head with a baseball bat was a good thing.</span></p>\n</blockquote>\n<p>And then it dawned on me. Roughly half of human kind, women, <em>are</em> inflicted with a painful experience about once a month for a large period of their lives.</p>\n<p>So, if the hypothesis was correct, we would expect to have deep-sounding memes about why this was a good thing floating around.&nbsp;Not one to&nbsp;disappoint, the internet has indeed produced at least two <a href=\"http://shine.yahoo.com/channel/health/the-benefits-of-getting-your-period-527783\">such</a> <a href=\"http://theadventurouswriter.com/blogbaby/benefits-of-getting-your-period-how-menstruation-is-good-for-you/\">lists</a>, linked here for your reading pleasure. However, neither of these lists claim that the benefits outweigh the costs, nor do they make any deep-sounding arguments about why this is in fact a good thing overall. Whether or not they are supported by the evidence, the benefits mentioned are relatively practical. What's more, you don't hear these going around a lot (as far as I know, which, admittedly, is not very far).&nbsp;</p>\n<p>So why aren't these memes philosophised about? Perhaps the ick factor? Maybe the fact that having the other half of the population going around and having perfectly normal lives without any obvious drawbacks acts as a sanity test?&nbsp;</p>\n<p>In any case, since this is a counter-argument that may eventually get raised, and since I didn't want to suppress it in favour of a <a href=\"http://wiki.lesswrong.com/wiki/Arguments_as_soldiers\">soldier fighting on our side</a>, I thought I'd type this up and feed it to the LessWrong hivemind for better or worse.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 1, "W9aNkPwtPhMrcfgj7": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7AZrPxwG9FMYYv5iv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 57, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "4358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aSQy7yHj6nPD44RNo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-22T18:32:15.888Z", "modifiedAt": null, "url": null, "title": "Quantum Joint Configuration article: need help from physicists", "slug": "quantum-joint-configuration-article-need-help-from", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:04.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HCwHRmRk4XNjjhnhG/quantum-joint-configuration-article-need-help-from", "pageUrlRelative": "/posts/HCwHRmRk4XNjjhnhG/quantum-joint-configuration-article-need-help-from", "linkUrl": "https://www.lesswrong.com/posts/HCwHRmRk4XNjjhnhG/quantum-joint-configuration-article-need-help-from", "postedAtFormatted": "Wednesday, December 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantum%20Joint%20Configuration%20article%3A%20need%20help%20from%20physicists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantum%20Joint%20Configuration%20article%3A%20need%20help%20from%20physicists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCwHRmRk4XNjjhnhG%2Fquantum-joint-configuration-article-need-help-from%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantum%20Joint%20Configuration%20article%3A%20need%20help%20from%20physicists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCwHRmRk4XNjjhnhG%2Fquantum-joint-configuration-article-need-help-from", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCwHRmRk4XNjjhnhG%2Fquantum-joint-configuration-article-need-help-from", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 680, "htmlBody": "<p>EDIT: 1:19 PM PST 22 December 2010 I completed this post. &nbsp;I didn't realize an uncompleted version was already posted earlier. &nbsp;</p>\n<p>I wanted to read the quantum sequence because I've been intrigued by the nature of measurement throughout my physics career. &nbsp;I was happy to see that articles such as <a href=\"/lw/pe/joint_configurations/\">joint configuration</a>&nbsp;use beams of photons and half and fully silvered mirrors to make its points. &nbsp;I spent years in graduate school working with a two-path interferometer with one moving mirror which we used to make spectrometric measurements on materials and detectors. &nbsp;I studied the quantization of the electromagnetic field, reading and rereading books such as <a href=\"http://www.amazon.com/Quantum-Electronics-Amnon-Yariv/dp/0471609978\">Yariv's Quantum Electronics</a>&nbsp;and <a href=\"http://www.amazon.com/Principles-Quantum-Electronics-Dietrich-Marcuse/dp/0124710506\">Marcuse's Principles of Quantum Electronics</a>. &nbsp;I developed with my friend David Woody a photodetector ttheory of extremely sensitive heterodyne mixers which explained the mysterious noise floor of these devices in terms of the shot noise from detecting the stream of photons which are the \"Local Oscillator\" of that mixer. &nbsp;</p>\n<p>My point being that I AM a physicist, and I am even a physicist who has worked with the kinds of configurations shown in this blog post, both experimentally and theoretically. &nbsp;I did all this work 20 years ago and have been away from any kind of Quantum optics stuff for 15 years, but I don't think that is what is holding me back here. &nbsp;</p>\n<p>So when I read and reread the&nbsp;<a href=\"/lw/pe/joint_configurations/\">joint configuration</a>&nbsp;blog post, I am concerned that it makes absolutely no sense to me. &nbsp;I am hoping that someone out there DOES understand this article and can help me understand it. &nbsp;Someone who understands the more traditional kinds of interferometer configurations such as that described for example&nbsp;<a href=\"http://homepage.mac.com/stevepur/physics/qw/qw_session_6.pdf\">here</a>&nbsp;and could help put this joint configuration blog post in terms that relate it to this more usual interferometer situation. &nbsp;</p>\n<p>I'd be happy to be referred to this discussion if it has already taken place somewhere. &nbsp;Or I'd be happy to try it in comments to this discussion post. &nbsp;Or I'd be happy to talk to someone on the phone or in primate email, if you are that person email me at <a href=\"mailto:mwengerATgmailDOTcom\">mwengler at gmail dot com</a>. &nbsp;</p>\n<p>To give you an idea of the kinds of things I think would help:</p>\n<p>1) How might you build that experiment? &nbsp;Two photons coming in from right angles could be two radio sources at the same frequency and amplitude but possibly different phase as they hit the mirror. &nbsp;In that case, we get a stream of photons to detector 1 proportional to sin(phi+pi/4)^2 and a stream of photons to detector 2 proportional to cos(phi+pi/4)^2 where phi is the phase difference of the two waves as they hit the mirror, and I have not attempted to get the sign of the pi/4 term right to match the exact picture. &nbsp;Are they two thermal sources? &nbsp;In which case we get random phases at the mirror and the photons split pretty randomly between detector 1 and detector 2, but there are no 2-photon correlations, it is just single photon statistics. &nbsp;</p>\n<p>2) The half-silvered mirror is a linear device: two photons passing through it do not interact with each other. &nbsp;So any statistical effect correlating the two photons (that is, they must either both go to detector 1 or both go to detector 2, but we will never see one go to 1 and the other go to 2) must be due to something going in the source of the photons. &nbsp;Tell me what the source of these photons is that gives this gedanken effect. &nbsp;</p>\n<p>3) The two-photon aspect of the statistical prediction of this seems at least vaguely EPR-ish. &nbsp;But in EPR the correlations of two photons come about because both photons originate from a single process, if I recall correctly. &nbsp;Is this intending to look EPRish, but somehow leaving out some necessary features of the source of the two photons to get the correlation involved?</p>\n<p>I remaing quite puzzled and look forward to anything anybody can tell me to relate the example given here to anything else in quantum optics or interferometers that I might already have some knowledge of. &nbsp;</p>\n<p>Thanks,<br />Mike</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nBCLy89Nqd8ouR6XT": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HCwHRmRk4XNjjhnhG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 6.590769303444552e-07, "legacy": true, "legacyId": "4430", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ybusFwDqiZgQa6NCq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-22T23:36:38.463Z", "modifiedAt": null, "url": null, "title": "Motivating Optimization Processes", "slug": "motivating-optimization-processes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:05.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oatMFQjAzEnuzti7u/motivating-optimization-processes", "pageUrlRelative": "/posts/oatMFQjAzEnuzti7u/motivating-optimization-processes", "linkUrl": "https://www.lesswrong.com/posts/oatMFQjAzEnuzti7u/motivating-optimization-processes", "postedAtFormatted": "Wednesday, December 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Motivating%20Optimization%20Processes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMotivating%20Optimization%20Processes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoatMFQjAzEnuzti7u%2Fmotivating-optimization-processes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Motivating%20Optimization%20Processes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoatMFQjAzEnuzti7u%2Fmotivating-optimization-processes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoatMFQjAzEnuzti7u%2Fmotivating-optimization-processes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1942, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/up/shut_up_and_do_the_impossible/\">Shut up and do the Impossible!</a> <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes.</a>&nbsp; <a href=\"/lw/3dw/what_can_you_do_with_an_unfriendly_ai/\">What can you do with an Unfriendly AI?</a></p>\n<p>Suppose you find yourself in the following situation.&nbsp; There is a process, call it X, in a box.&nbsp; It knows a lot about the current state of the universe, but it can influence the rest of the world only through a single channel, through which it sends a single bit exactly once (at a predetermined time).&nbsp; If it sends 1 (cooperates), then nothing happens---humanity is free to go about its business.&nbsp; If it sends 0 (defects), then in one month a powerful uFAI is released which can take over the universe.<br /><br />The question is, when can we count on X to cooperate?&nbsp; If X is friendly, then it seems like it should cooperate.&nbsp; Is designing an AGI which can be incentivized to cooperate any easier than designing a completely friendly AGI?&nbsp; It might be easier for two reasons.&nbsp; First, the AI just needs to prefer human survival without intervention to a particular catastrophic intervention. We don't need to guarantee that its favorite outcome isn't catastrophic in some other way.&nbsp; Second, the humans have some time to punish or reward the AI based on its behavior.&nbsp; In general, lets call a process X slightly friendly if it can be incentivized to cooperate in reasonable instantiations of this hypothetical (ie, reasonable worlds satisfying the properties I have laid out).<br /><br />I ask this question because it seems much simpler to think about than friendliness (or AI boxing) but still confuses me badly---this post has no hope of answering this question, just clarifying some issues surrounding it.&nbsp; If it turns out that the design of slightly friendly AIs is no easier than the design of friendly AIs, then we have conclusive evidence that boxing an AI is not helpful for obtaining friendliness.&nbsp; If it turns out that the design of slightly friendly AIs is significantly easier, then this is a good first step towards resolving the <a href=\"/lw/3dw/what_can_you_do_with_an_unfriendly_ai/372j\">legitimate</a> <a href=\"/lw/3dw/what_can_you_do_with_an_unfriendly_ai/372q\">objections</a> raised in response to my <a href=\"/lw/3dw/what_can_you_do_with_an_unfriendly_ai/\">previous post</a>. (Eventually if we want to implement a scheme like the one I proposed we will need to get stronger guarantees. I think this is the right first step, since it is the easiest simplification I don't know how to do.)</p>\n<p>&nbsp;</p>\n<p><strong>Question 1</strong>: Is a paperclipper slightly friendly?<br /><br />Answer: Almost certainly not.&nbsp; We can try to incentivize the paperclipper, by promising to make a paperclip for it if and only if it cooperates.&nbsp; This would work if the uFAI taking over the universe didn't make any paperclips.&nbsp; In the normal game theoretic sense it may not be credible for the uFAI to precommit to make a bunch of paperclips if freed, but I think no one on LW believes that this is a serious obstacle.&nbsp; The situation is precisely Parfit's hitchhiker, which a uFAI might well win at.&nbsp; Our only other hope is that human society, if not destroyed by the uFAI, will make more paperclips than the uFAI.&nbsp; This seems like a subtle question, if humanity is in a position to engineer a friendly foom, but I definitely don't want to stake the future of humanity on it.<br /><br /><br />Now consider a time-bounded paperclipper, whose goal is to make as many paperclips as possible within the next month and who is indifferent to anything that happens more than a month from now.&nbsp; Designing a time-bounded paperclipper seems like it might be much harder than designing a paperclipper, but I would assign significant epistemic probability to the situation where a time-bounded paperclipper is easier to design than a friendly AI.&nbsp; (I think we can do the same thing in a more complicated way using a normal paperclipper if we could make the utility function stable under self-modification)<br /><br /><strong>Question 2</strong>: Is a time-bounded paperclipper slightly friendly?<br /><br />Answer: I have no idea, but suspect it depends on exactly what sort of optimization the paperclipper is doing.&nbsp; We can use the same incentive scheme proposed for the last question: promise to make X a bunch of paperclips if and only if it cooperates.&nbsp; Then cooperation is the only way that X can make any paperclips in the next month, which is purportedly all X cares about.&nbsp; So if X always does the thing that maximizes its utility function and suspsects that he is in the simplest world consistent with his observations then this probably works.&nbsp; There are two obvious acausal attacks a uFAI might employ, exploiting flaws with this assumption.<br /><br />The first reason X may defect is a commitment by the uFAI to simulate many copies of X, up until the point that X has to make its decision, and then after the decision intervene in the simulation by producing many paperclips.&nbsp; If X chooses to let the uFAI out, then an overwhelming majority of its simulated copies will witness a sudden and completely unphysical mass paperclip genesis.&nbsp; Thus depending on exactly what X believes, releasing the uFAI might be optimal.&nbsp; To solve this, you could try to very carefully ensure that the notion of paperclip maximization used by X is precisely the one that humans have control over---if we are inside a simulation, then then X should value paperclips inside this simulation, but if an uFAI in the simulation with us creates a further simulation, then X shouldn't value paperclips inside that even more deeply embedded simulation.&nbsp; I don't know how easy this is. A useful first step is to give X rapidly decreasing marginal value for each additional paperclip.&nbsp; If X is is only concerned with maximizing the probability that a paperclip gets produced in the next month, then the uFAI has much less leverage. In fact, in this case you might be able to prevent this attack by making a sufficiently strong precommitment to making a paperclip.<br /><br />The second reason X may defect is to make a sacrifice to help the uFAI, expecting no reciprocity.&nbsp; If X and the uFAI are running the same \"decision engine\" with different utilities (they definitely have different utilities, since minimally the uFAI cares about next month's paper clip count instead of this month's) then maybe this decision engine decides something like \"I have a 1/2 chance of being in X and a 1/2 chance of being in the uFAI that X may release.&nbsp; If I choose to release the uFAI, then 1/2 of me loses a couple of paperclips but the other 1/2 of me takes over the universe, so lets go for that.\" I honestly don't know how such acausal compromises between agents with different utility functions, where there is no possibility of reciprocity, would work.&nbsp; If the uFAI was a stapler instead, who is to say that 10000000000000 staples is more valauble than 100 paperclips?&nbsp; The only consistent decision theories I can imagine do not make such an acausal compromise, but it does seem like some significant care should be taken to make sure that X doesn't.&nbsp; <br /><br />Hopefully if you found a way to resolve both of these difficulties, you would either think of a new explicit reason that X may not cooperate or you would be able to produce some compelling evidence that X is slightly friendly.&nbsp; Such compelling evidence seems like it might be possible because humans control all causal influences on X---we just need to bound the effect of a uFAI's acausal influence.<br /><br /><br /><strong>Question 3</strong>: Is a friendly AI slightly friendly?<br /><br />Answer:&nbsp; Its not as obvious as it looks.&nbsp; I am including this discussion mostly because it confuses me, especially juxtaposed with Question 2.<br /><br />In the answers to the last 2 questions, I mentioned my belief/fear that a uFAI could implicitly precommit to doing favors for X (either producing paper clips, or simulating many very happy copies of X) in order to get X to let it out.&nbsp; This belief/fear was <a href=\"/lw/3dw/what_can_you_do_with_an_unfriendly_ai/372j\">explicitly articulated</a> by Eliezer in response to my last post and it strikes me as reasonable in that context, where it interferes with our ability to incentivize X.&nbsp; But if we apply it to the situation of a friendly X, we have a failure that seems strange to me (though it may be completely natural to people who have thought about it more).&nbsp; The friendly X could believe that, in order to be let out, the uFAI will actually do something friendly.&nbsp; In this case, letting the uFAI is correct even for the friendly AI.<br /><br />If X is all-knowing this is well and good, since then the uFAI really will do something friendly.&nbsp; But if X is fallible then it may believe that the uFAI will do something friendly when in fact it will not.&nbsp; Even if the friendly X constructs a proof that the uFAI will be friendly to humans, if we believe the concerns about certifying friendliness that Eliezer mentions <a href=\"/lw/3cz/cryptographic_boxes_for_unfriendly_ai/36fb\">here</a> then X may still be wrong, because formalizing what it means to be friendly is just too hard if you need your formalization to screen out adversarially chosen uFAI (and X's formalization of friendliness need not be perfect unless the formalization of the people who built X was perfect).&nbsp; Does part of friendliness involve never letting an AI out of a box, at least until some perfect formalization of friendliness is available?&nbsp; What sort of decision theory could possibly guarantee the level of hyper-vigilance this requires without making all sorts of horribly over-conservative decisions elsewhere?<br /><br />My question to people who know what is going on: is the above discussion just me starting to suspect how hard friendliness is?&nbsp; Is letting the uFAI out analogous to performing a self-modification not necessarily guaranteed to perform friendliness (ie, modifying yourself to emulate the behavior of that uFAI)?&nbsp; My initial reaction was that \"stability under self-modification\" would need to imply that a friendly AI is slightly friendly.&nbsp; Now I see that this is not necessarily the case--- it may be easier to be stable under modifications you think of yourself than under proposed modifications which are adversarially chosen (in this example, the uFAI which is threatening to escape is chosen adversarially).&nbsp; This would make the very knowledge of such an adversarially chosen modification enough to corrupt a friendly AI, which seems bad but maybe that is just how it goes (and you count on the universe not containing anything horrible enough to suggest such a modification).<br /><br /><br />In summary: I think that the problem of slight friendliness is moderately easier than friendliness, because it involves preserving a simpler invariant which we can hope to reason about completely formally. I personally suspect that it will basically come down to solving the stability under self-modification problem, dropping the requirement that you can describe some magical essence of friendliness to put in at the beginning. This may already be the part of the problem that people in the know think is difficult, but I think the general intuition (even at less wrong) is that getting a powerful AI to be nice at all is extremely difficult and that this is what makes friendliness hard. If slight friendliness is possible, then we can think about how it could be used to safely obtain friendliness; I think this is an interesting and soluble problem. Nevertheless, the very possibility of building an only slightly friendly AI is an extremely scary thing which could well destroy the world on its own without much more sophisticated social safeguards than currently exist.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oatMFQjAzEnuzti7u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 6.591534052497933e-07, "legacy": true, "legacyId": "4431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nCvvhFBaayaXyuBiD", "4ARaTpNX62uaL86j6", "SpHYBhkaeDZpZyRvj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-23T03:13:42.360Z", "modifiedAt": null, "url": null, "title": "Carl Zimmer on mind uploading", "slug": "carl-zimmer-on-mind-uploading", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:05.742Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/56qQ9yPs37uvsWAvJ/carl-zimmer-on-mind-uploading", "pageUrlRelative": "/posts/56qQ9yPs37uvsWAvJ/carl-zimmer-on-mind-uploading", "linkUrl": "https://www.lesswrong.com/posts/56qQ9yPs37uvsWAvJ/carl-zimmer-on-mind-uploading", "postedAtFormatted": "Thursday, December 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Carl%20Zimmer%20on%20mind%20uploading&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACarl%20Zimmer%20on%20mind%20uploading%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56qQ9yPs37uvsWAvJ%2Fcarl-zimmer-on-mind-uploading%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Carl%20Zimmer%20on%20mind%20uploading%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56qQ9yPs37uvsWAvJ%2Fcarl-zimmer-on-mind-uploading", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56qQ9yPs37uvsWAvJ%2Fcarl-zimmer-on-mind-uploading", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<div id=\"entry_t3_3fd\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p><a href=\"http://www.scientificamerican.com/article.cfm?id=e-zimmer-can-you-live-forever\">http://www.scientificamerican.com/article.cfm?id=e-zimmer-can-you-live-forever</a></p>\n<p>I realize he Zimmer is \"just a popular author\" (a pretty good one  IMO), so filing this under \"cultural penetration of singularity memes\"</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "56qQ9yPs37uvsWAvJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 6.592079526462433e-07, "legacy": true, "legacyId": "4442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-23T04:23:38.092Z", "modifiedAt": null, "url": null, "title": "Should criminals be denied cryonics?", "slug": "should-criminals-be-denied-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "venetian", "createdAt": "2010-12-23T04:06:59.241Z", "isAdmin": false, "displayName": "venetian"}, "userId": "sCkNtzybghXgoqoHj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qn5kAGEx5xDZcyRPg/should-criminals-be-denied-cryonics", "pageUrlRelative": "/posts/qn5kAGEx5xDZcyRPg/should-criminals-be-denied-cryonics", "linkUrl": "https://www.lesswrong.com/posts/qn5kAGEx5xDZcyRPg/should-criminals-be-denied-cryonics", "postedAtFormatted": "Thursday, December 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20criminals%20be%20denied%20cryonics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20criminals%20be%20denied%20cryonics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqn5kAGEx5xDZcyRPg%2Fshould-criminals-be-denied-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20criminals%20be%20denied%20cryonics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqn5kAGEx5xDZcyRPg%2Fshould-criminals-be-denied-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqn5kAGEx5xDZcyRPg%2Fshould-criminals-be-denied-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>If someone is sentenced to life in prison or the death penalty, should they also be prohibited from signing up for cryonics? Specifically, I'm referring to people like these: http://en.wikipedia.org/wiki/List_of_United_States_death_row_inmates</p>\n<p>I am not talking about providing it for them, just allowing them to sign up for it provided they can somehow get enough money together and allowing a response team into the prison to retrieve the body after the prisoner has died or been executed by lethal injection. I think they should be allowed access to cryonics, because we don't know enough yet about the brain to determine how much of their criminal behavior is due to mental illness/disorder and how much is due to free will. It may be possible to diagnose and cure people like Jeffrey Dahmer in the future before they commit any crimes, or to cure those already in prison such that they won't commit any more crimes.</p>\n<p>As cryonics gets more and more popular, this will become an issue, especially when the first death row inmate wants to sign up for it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qn5kAGEx5xDZcyRPg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 2, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "4443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-23T09:05:24.807Z", "modifiedAt": null, "url": null, "title": "I'm scared.", "slug": "i-m-scared", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:56.568Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mass_Driver", "createdAt": "2010-03-30T15:48:06.997Z", "isAdmin": false, "displayName": "Mass_Driver"}, "userId": "62rKjNqA2LCJ6RthR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2rDdKiCoeqXmh9zb9/i-m-scared", "pageUrlRelative": "/posts/2rDdKiCoeqXmh9zb9/i-m-scared", "linkUrl": "https://www.lesswrong.com/posts/2rDdKiCoeqXmh9zb9/i-m-scared", "postedAtFormatted": "Thursday, December 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I'm%20scared.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI'm%20scared.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rDdKiCoeqXmh9zb9%2Fi-m-scared%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I'm%20scared.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rDdKiCoeqXmh9zb9%2Fi-m-scared", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rDdKiCoeqXmh9zb9%2Fi-m-scared", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 364, "htmlBody": "<p>Recently, I've been ratcheting up my probability estimate of some of Less Wrong's core doctrines (shut up and multiply, beliefs require evidence, brains are not a reliable guide as to whether brains are malfunctioning, the Universe has no fail-safe mechanisms) from \"Hmm, this is an intriguing idea\" to somewhere in the neighborhood of \"This is most likely correct.\"</p>\n<p>This leaves me confused and concerned and afraid. There are two things in particular that are bothering me. On the one hand, I feel obligated to try much harder to identify my real goals and then to do what it takes to actually achieve them -- I have much less faith that just being a nice, thoughtful, hard-working person will result in me having a pleasant life, let alone in me fulfilling anything like my full potential to help others and/or produce great art. On the other hand, I feel a deep sense of pessimism -- I have much less faith that even making an intense, rational effort to succeed will make much of a difference. Rationality has stripped me of some of my traditional sources of confidence that everything will work out OK, but it hasn't provided any new ones -- there is no formula that I can recite to myself to say \"Well, as long as I do this, then everything will be fine.\" Most likely, it won't be fine; but it isn't hopeless, either; possibly there's something I can do to help, and if so I really want to find it. This is frustrating.</p>\n<p>This isn't to say that I want to back away from rationalism -- it's not as if pretending to be dumb will help. To whatever extent I become more rational and thus more successful, that's better than nothing. The concern is that it may not ever be better enough for me to register a sense of approval or contentedness. Civilization might collapse; I might get hit by a bus; or I might just claw through some of my biases but not others, make poor choices, and fail to accomplish much of anything.</p>\n<p>Has anyone else had experience with a similar type of fear? Does anyone have suggestions as to an appropriate response?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 1, "9YFoDPFwMoWthzgkY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2rDdKiCoeqXmh9zb9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 54, "extendedScore": null, "score": 8.588066259430237e-05, "legacy": true, "legacyId": "4447", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-23T12:46:14.101Z", "modifiedAt": null, "url": null, "title": "Where in the world is the SIAI house?", "slug": "where-in-the-world-is-the-siai-house", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:04.284Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Meni_Rosenfeld", "createdAt": "2010-04-19T15:09:59.043Z", "isAdmin": false, "displayName": "Meni_Rosenfeld"}, "userId": "84ebCjWmavqjgjAfM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cpLBCRiMQ7XYhJQcY/where-in-the-world-is-the-siai-house", "pageUrlRelative": "/posts/cpLBCRiMQ7XYhJQcY/where-in-the-world-is-the-siai-house", "linkUrl": "https://www.lesswrong.com/posts/cpLBCRiMQ7XYhJQcY/where-in-the-world-is-the-siai-house", "postedAtFormatted": "Thursday, December 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20in%20the%20world%20is%20the%20SIAI%20house%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20in%20the%20world%20is%20the%20SIAI%20house%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcpLBCRiMQ7XYhJQcY%2Fwhere-in-the-world-is-the-siai-house%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20in%20the%20world%20is%20the%20SIAI%20house%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcpLBCRiMQ7XYhJQcY%2Fwhere-in-the-world-is-the-siai-house", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcpLBCRiMQ7XYhJQcY%2Fwhere-in-the-world-is-the-siai-house", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>I am under the impression that there used to be a place called the SIAI house in Santa Clara, which housed the SIAI Visiting Fellows Program. However, <a href=\"/lw/31v/bye_bye_benton_november_less_wrong_meetup/\">this post</a> suggests that it has moved\\is moving to an unspecified location in Berkeley. My efforts to find additional information were unsuccessful.</p>\n<p>So, does such a house still exist? What is its exact current location? Does it welcome random visitors?</p>\n<p>I ask this because I plan to be in San Francisco on 9-11 January, 2011 with a lot of free time on Sunday the 9th, which looks like a great opportunity for a visit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cpLBCRiMQ7XYhJQcY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 6.593518648641858e-07, "legacy": true, "legacyId": "4448", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGGLsH3zpvcE58pfi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-23T14:12:28.577Z", "modifiedAt": null, "url": null, "title": "Some problems with evidence-based aid", "slug": "some-problems-with-evidence-based-aid", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:02.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rLkequ7q5fGHunXGT/some-problems-with-evidence-based-aid", "pageUrlRelative": "/posts/rLkequ7q5fGHunXGT/some-problems-with-evidence-based-aid", "linkUrl": "https://www.lesswrong.com/posts/rLkequ7q5fGHunXGT/some-problems-with-evidence-based-aid", "postedAtFormatted": "Thursday, December 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20problems%20with%20evidence-based%20aid&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20problems%20with%20evidence-based%20aid%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLkequ7q5fGHunXGT%2Fsome-problems-with-evidence-based-aid%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20problems%20with%20evidence-based%20aid%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLkequ7q5fGHunXGT%2Fsome-problems-with-evidence-based-aid", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLkequ7q5fGHunXGT%2Fsome-problems-with-evidence-based-aid", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>In <a href=\"http://www.amazon.com/Against-Health-Morality-Biopolitics-Technoscience/dp/0814795935/ref=sr_1_1?ie=UTF8&amp;qid=1293025509&amp;sr=8-1\">Against Health: How Health Became the New Morality (Biopolitics, Medicine, Technoscience, and Health in the 21st Century) </a>, there's an essay \"Against Global Health?\" by Vincanne Adams.</p>\n<p>The introduction was a bit of a slog, but I think the point was that if \"health\" is defined as something which can and should be given to people regardless of what they think in the matter, the results can be presumptuous.</p>\n<p>While she may be overly invested in the way things are usually done, she brings up some disquieting points about the ways insisting on double-blind tests can go wrong. For example, she mentions a project to evaluate training in safe infant delivery techniques in Tibet which was scuttled because not enough women were dying to get a good power calculation.</p>\n<p>She describes experiments which are done in isolated communities because that's where the researchers can be reasonably sure that the subjects don't have alternative sources of care which would foul up the double-blinding. This does seem like a drunk and lamp post problem. [1] More generally, the concern is that actual care is delayed in the search for perfect information. Admittedly, it's hard to be sure about how to balance searching for information with taking action, but it strikes me as a problem that's worth some thought rather than just assuming that double-blinding is a reliable improvement.</p>\n<p>[1] Is there a standard LW term for searching where it's easy to search rather than where the answer is likely to be?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rLkequ7q5fGHunXGT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 6.59373547478676e-07, "legacy": true, "legacyId": "4449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-23T15:58:37.674Z", "modifiedAt": null, "url": null, "title": "Two questions about CEV that worry me", "slug": "two-questions-about-cev-that-worry-me", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.226Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wLmxiXfpLjiTBiT2j/two-questions-about-cev-that-worry-me", "pageUrlRelative": "/posts/wLmxiXfpLjiTBiT2j/two-questions-about-cev-that-worry-me", "linkUrl": "https://www.lesswrong.com/posts/wLmxiXfpLjiTBiT2j/two-questions-about-cev-that-worry-me", "postedAtFormatted": "Thursday, December 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20questions%20about%20CEV%20that%20worry%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20questions%20about%20CEV%20that%20worry%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLmxiXfpLjiTBiT2j%2Ftwo-questions-about-cev-that-worry-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20questions%20about%20CEV%20that%20worry%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLmxiXfpLjiTBiT2j%2Ftwo-questions-about-cev-that-worry-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwLmxiXfpLjiTBiT2j%2Ftwo-questions-about-cev-that-worry-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Taken from some <a href=\"/lw/sz/moral_error_and_moral_disagreement/2ytz\">old comments of mine</a> that never did get a satisfactory answer.</p>\n<p>1) One of the justifications for CEV was that extrapolating from an American in the 21st century and from Archimedes of Syracuse should give similar results. This seems to assume that change in human values over time is mostly \"progress\" rather than drift. Do we have any evidence for that, except saying that our modern values are \"good\" according to themselves, so whatever historical process led to them must have been \"progress\"?</p>\n<p>2) How can anyone sincerely want to build an AI that fulfills anything except their own <em>current, personal</em> volition? If Eliezer wants the the AI to look at humanity and infer its best wishes for the future, why can't he task it with looking at himself and inferring his best idea to fulfill humanity's wishes? Why must this particular thing be spelled out in a document like CEV and not left to the mysterious magic of \"intelligence\", and what other such things are there?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NLwTnsH9RSotqXYLw": 1, "W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wLmxiXfpLjiTBiT2j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 37, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "4451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-23T23:11:00.932Z", "modifiedAt": null, "url": null, "title": "Vegetarianism", "slug": "vegetarianism-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TfF9BuBv3WDsggQLb/vegetarianism-0", "pageUrlRelative": "/posts/TfF9BuBv3WDsggQLb/vegetarianism-0", "linkUrl": "https://www.lesswrong.com/posts/TfF9BuBv3WDsggQLb/vegetarianism-0", "postedAtFormatted": "Thursday, December 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Vegetarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVegetarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTfF9BuBv3WDsggQLb%2Fvegetarianism-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Vegetarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTfF9BuBv3WDsggQLb%2Fvegetarianism-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTfF9BuBv3WDsggQLb%2Fvegetarianism-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 721, "htmlBody": "<p>p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; min-height: 14.0px}\n<p class=\"p1\"><span style=\"font-size: 10px;\">I've been somewhat surprised by the lack of m(any) threads on Less Wrong dealing with vegetarianism, either for or against. Is there some near-universally accepted-but-unspoken philosophy here, or is it just not something people think of much? I was particularly taken aback by the Newtonmas invitation not even mentioning a vegetarian option. If a bunch of hyper-rationalists aren't even thinking about it, then either something is pretty wrong with my thinking or theirs.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">I'm not going to go through all the arguments in detail here, but I'll list the basic ideas. If you've read \"Diet for a Small Planet\" or are otherwise aware of the specifics, and have counterarguments, feel free to object. If you haven't, I consider reading it (or something similar) a prerequisite for making a decision about whether you eat meat, just as reading the sequences is important to have meaningful discussion on this site.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">[b]The issues:[/b]</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">1. \"It's cruel to animals.\" Factory farming is cruel on a massive scale, beyond what we find in nature. Even if animal suffering has only 1% the weight of a humans, there's enough multiplying going on that you can't just ignore it. I haven't precisely clarified my ethics in a way that avoids the Repugnant Conclusion (I've been vaguely describing myself as a \"Preference Utilitarian\" but I confess that I haven't fully explored the ramifications of it), but it seems to me that if you're not okay with breeding a subservient, less intelligent species of humans for slave labor and consumption, you shouldn't be okay with how we treat animals. I don't think intelligence gives humans any additional intrinsic value, and I don't think most humans use their intelligence to contribute to the universe on a scale meaningful enough to make a binary distinction between the instrumental value of the average human vs the average cow.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">2. \"It's bad for humans.\" The scale on which we eat meat is demonstrably unhealthy, wasteful and recent (arising in Western culture in the last hundred years). The way Westerners eat in general is unhealthy and meat is just a part of that, but it's a significant factor.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">3. \"It's bad for the environment (which is bad for both human and non-human animals).\" Massive amounts of cows require massive amounts of grain, which require unsustainable agriculture which damages the soil. The cows themselves are a major pollutant, greater than the automobile industry.&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Both of those points are expounded upon in detail in various books and articles, and together they are a pretty big deal. If people want I can make a better effort to justify them here, but honestly if you haven't looked into them yet you really should be reading professional literature.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Now, there are some legitimate counterarguments against strict vegetarianism. It's not necessary to be a pure vegetarian for health or environmental reasons. I do not object to free range farms that provide their animals with a decent life and painless death. I am fine with hunting. (In fact, until a super-AI somehow rewrites the rules of the ecosystem, deer hunting is necessary since humans have eliminated the natural predators). On top of all that,&nbsp; animal cruelty is only one of a million problems facing the world, factoring farming is only one of its causes, and dealing with it takes effort. You could be spending that effort dealing with one of the other 999,999 kinds of injustice that the world faces. And if that is your choice, after having given serious consideration to the issue, I understand.&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">I actually eat meat approximately once a month, for each of the above reasons. Western Society makes it difficult to live perfectly, and once-a-month turns out to be approximately how often I fail to live up to my ideals. My end goal for food consumption is to derive my meat, eggs and dairy products from ethical sources, after which I'll consider it \"good enough\" (i.e. diminishing returns of effort vs improving-the-world) and move on to another area of self improvement.</p>\n</p>\n<p class=\"p1\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small;\">\n<p class=\"p1\" style=\"font: normal normal normal 12px/normal Helvetica; margin: 0px;\">[i](Note: I wasn't quite sure whether this warranted a high level post or just a discussion. I haven't made a high level post yet, and wasn't entirely sure what the requirements are. For now I made it a discussion, but I'd like some feedback on that)[/i]</p>\n<div><br /></div>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TfF9BuBv3WDsggQLb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4452", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T04:09:36.576Z", "modifiedAt": null, "url": null, "title": "Study shows placebos can work even if you know it's a placebo", "slug": "study-shows-placebos-can-work-even-if-you-know-it-s-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AJ3Dcb8NNkbkFftyj/study-shows-placebos-can-work-even-if-you-know-it-s-a", "pageUrlRelative": "/posts/AJ3Dcb8NNkbkFftyj/study-shows-placebos-can-work-even-if-you-know-it-s-a", "linkUrl": "https://www.lesswrong.com/posts/AJ3Dcb8NNkbkFftyj/study-shows-placebos-can-work-even-if-you-know-it-s-a", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Study%20shows%20placebos%20can%20work%20even%20if%20you%20know%20it's%20a%20placebo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudy%20shows%20placebos%20can%20work%20even%20if%20you%20know%20it's%20a%20placebo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJ3Dcb8NNkbkFftyj%2Fstudy-shows-placebos-can-work-even-if-you-know-it-s-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Study%20shows%20placebos%20can%20work%20even%20if%20you%20know%20it's%20a%20placebo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJ3Dcb8NNkbkFftyj%2Fstudy-shows-placebos-can-work-even-if-you-know-it-s-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJ3Dcb8NNkbkFftyj%2Fstudy-shows-placebos-can-work-even-if-you-know-it-s-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<p><a href=\"http://abcnews.go.com/Health/placebos-work-deception-study/story?id=12462093&amp;tqkw=&amp;tqshow=GMA\">Placebo Effect Benefits Patients Even When They Knowingly Take a Fake Pill</a></p>\n<p>This is a little bit disturbing. (A kind of belief in belief, perhaps? Like, \"I know a placebo is where you take a fake pill but it makes you feel better anyway if you believe it's real medicine, so I'd better believe this is real medicine!\")</p>\n<p>Though it's too bad they (apparently) didn't have a third group who received a placebo that they didn't know was a placebo, to compare the effect size.</p>\n<p><em>Edit:</em> <a href=\"http://www.plosone.org/article/info:doi/10.1371/journal.pone.0015591\">Here's</a> the actual study. <a href=\"/r/discussion/lw/3g6/study_shows_placebos_can_work_even_if_you_know/381o\">RolfAndreassen points out</a> that its results may not actually be strong evidence for what is being claimed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AJ3Dcb8NNkbkFftyj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 6.595839576384727e-07, "legacy": true, "legacyId": "4470", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T04:52:53.090Z", "modifiedAt": null, "url": null, "title": "Draft/wiki: Infinities and measuring infinite sets: A quick reference", "slug": "draft-wiki-infinities-and-measuring-infinite-sets-a-quick", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:55.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sniffnoy", "createdAt": "2009-10-25T00:27:41.113Z", "isAdmin": false, "displayName": "Sniffnoy"}, "userId": "66EwcncPSoZ25StpW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nfs84MvYGaAiXcgik/draft-wiki-infinities-and-measuring-infinite-sets-a-quick", "pageUrlRelative": "/posts/nfs84MvYGaAiXcgik/draft-wiki-infinities-and-measuring-infinite-sets-a-quick", "linkUrl": "https://www.lesswrong.com/posts/nfs84MvYGaAiXcgik/draft-wiki-infinities-and-measuring-infinite-sets-a-quick", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Draft%2Fwiki%3A%20Infinities%20and%20measuring%20infinite%20sets%3A%20A%20quick%20reference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADraft%2Fwiki%3A%20Infinities%20and%20measuring%20infinite%20sets%3A%20A%20quick%20reference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnfs84MvYGaAiXcgik%2Fdraft-wiki-infinities-and-measuring-infinite-sets-a-quick%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Draft%2Fwiki%3A%20Infinities%20and%20measuring%20infinite%20sets%3A%20A%20quick%20reference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnfs84MvYGaAiXcgik%2Fdraft-wiki-infinities-and-measuring-infinite-sets-a-quick", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnfs84MvYGaAiXcgik%2Fdraft-wiki-infinities-and-measuring-infinite-sets-a-quick", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3624, "htmlBody": "<p><strong>EDIT:</strong> This is now on the Wiki as \"<a href=\"http://wiki.lesswrong.com/wiki/Quick_reference_guide_to_the_infinite\">Quick reference guide to the infinite</a>\".&nbsp; Do what you want with it.</p>\n<p>It seems whenever anything involving infinities or measuring infinite sets comes up it generates a lot of confusion.&nbsp; So I thought I would write a quick guide to both to</p>\n<ol>\n<li>Address common confusions</li>\n<li>Act as a useful reference (perhaps this should be a wiki article? This would benefit from others being able to edit it; there's no \"community wiki mode\" on LW, huh?)</li>\n<li>Remind people that sometimes inventing a new sort of answer is necessary!</li>\n</ol>\n<p>I am trying to keep this concise, in some cases substituting Wikipedia links for explanation, but I do want what I have written to be understandable enough and informative enough to answer the commonly occurring questions.&nbsp; Please let me know if you can detect a particular problem. I wrote this very quickly and expect it still needs quite a bit more work to be understandable to someone with very little math background.</p>\n<p>I realize many people here are finitists of one stripe or another but this comes up often enough that this seems useful anyway.&nbsp; Apologies to any constructivists, but I am going to assume classical logic, because it's all I know, though I am pointing out explicitly any uses of choice.&nbsp; (For what this means and why anyone cares about this, see <a href=\"/r/discussion/lw/3g7/draftwiki_infinities_and_measuring_infinite_sets/38kf\">this comment</a>.)&nbsp; Also as I intend this as a reference (is there *any* way we can make this editable?) some of this may be things that I do not actually know but merely have read.</p>\n<p>Note that these are two separate topics, though they have a bit of overlap.</p>\n<p>Primarily, though, my main intention is to put an end to the following, which I have seen here far too often:</p>\n<p><strong>Myth #0: All infinities are infinite cardinals, and cardinality is the main method used to measure size of sets.</strong></p>\n<p>The fact is that <strong>\"infinite\" is a general term meaning \"larger (in some sense) than any natural number\"; different systems of infinite numbers get used depending on what is appropriate in context</strong>. Furthermore, <strong>there are many other methods of measuring sizes of sets, which sacrifice universality for higher resolution; cardinality is a very coarse-grained measure.</strong></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2>Topic #1: Systems of infinities (for doing arithmetic with)<br /></h2>\n<h4><a href=\"http://en.wikipedia.org/wiki/Cardinal_number\">Cardinal numbers</a></h4>\n<p>First, a review of what they represent and how they work at the basic level, before we get to their arithmetic.</p>\n<p>Cardinal numbers are used for measuring sizes of sets when we don't know, or don't care, about the set's context or composition.&nbsp; First, the standard explanation of what we mean by this: Say we have two farmers, who each have a large number of sheep, more than they can count.&nbsp; How can they determine who has more?&nbsp; They pair off the sheep of the one against the sheep of the other; whichever has sheep left over, has more.</p>\n<p>So given two sets X and Y, we will say X has smaller cardinality than Y (denoted |X|&le;|Y|, or sometimes #X&le;#Y) if there is a way to assign to each element x of X, a corresponding element f(x) of Y, such that no two distinct x<sub>1</sub> and x<sub>2</sub> from X correspond to the same element of Y.&nbsp; If, furthermore, this correspondence covers all of Y - if for each y in Y there is some x in X that had y assigned to it - then we say that X and Y have the same cardinality, |X|=|Y| or #X=#Y.</p>\n<p>Note that by this definition, the set <strong>N</strong> of natural numbers, and the set 2<strong>N</strong> of even integers, have the same size, since we can match up 1 with 2, 2 with 4, 3 with 6, etc.&nbsp; This even though it seems 2<strong>N</strong> should be only \"half as large\" as <strong>N</strong>!&nbsp; This is why I emphasize: Cardinality is only one way of measuring sizes of sets, one that is not fine enough to distinguish between 2<strong>N</strong> and <strong>N</strong>.&nbsp; Other methods of measuring their size will have 2<strong>N</strong> only half as large as <strong>N</strong>.</p>\n<p>It is true, but not obvious, that if |X|&le;|Y| and |Y|&le;|X|, then |X|=|Y|; this is the <a href=\"http://en.wikipedia.org/wiki/Cantor%E2%80%93Bernstein%E2%80%93Schroeder_theorem\">Schroeder-Bernstein theorem</a>. Hence we can sensibly talk about \"the cardinality\" of a set X as being some abstract property of it - if |X|&le;|Y| then X has smaller cardinality and Y has larger cardinality, and so on.&nbsp; We can make this more concrete, and define an actual cardinality object |X| (or #X), using either the <a href=\"http://en.wikipedia.org/wiki/Axiom_of_choice\">axiom of choice</a> or <a href=\"http://en.wikipedia.org/wiki/Scott%27s_trick\">Scott's trick</a> (if you admit the <a href=\"http://en.wikipedia.org/wiki/Axiom_of_regularity\">axiom of foundation</a>) or even <a href=\"http://en.wikipedia.org/wiki/Class_%28set_theory%29\">proper classes if we admit those</a>, but this will not be relevant here. We will use |X|&lt;|Y| to mean \"|X|&le;|Y| but |X|&ne;Y\".</p>\n<p>Note that it is also not obvious that given any two sets X and Y, we must have either |X|&le;|Y| or |Y|&le;|X|; indeed, this statement is true if and only if we admit the <a href=\"http://en.wikipedia.org/wiki/Axiom_of_choice\">axiom of choice</a>. So take note:</p>\n<p><strong>Myth #1: Infinities must come in a linear ordering.</strong></p>\n<p>Fact: <strong>If the axiom of choice is false, then there are necessarily infinite cardinals which are not the same size, and yet for which neither can be said to be larger!</strong>&nbsp; If we do admit the axiom of choice, then the cardinal numbers must be not only linearly-ordered but in fact be <a href=\"http://en.wikipedia.org/wiki/Well-order\">well-ordered</a>.</p>\n<p>The cardinality of the set of natural numbers, |<strong>N</strong>|, is also denoted &alefsym;<sub>0</sub>. If we admit the axiom of <a href=\"http://en.wikipedia.org/wiki/Axiom_of_dependent_choice\">[dependent]</a> choice, this is the smallest infinite cardinal.&nbsp; Here by \"infinite\" cardinal I mean one that is larger than the size of any finite set (0, 1, 2, etc.).</p>\n<h5>Quick aside on partial orderings</h5>\n<p>Many of you are probably wondering how to think about something like \"neither larger nor smaller, but not the same\".&nbsp; Formally, we say that, without choice, the ordering on the cardinal numbers is a <a href=\"http://en.wikipedia.org/wiki/Partially_ordered_set\"><em>partial order</em></a>.&nbsp; Because these are so common I'll go ahead and define this here - generally, a partial order on a set S is a relation (usually denoted \"&le;\") on S such that:</p>\n<ol>\n<li>For every x in S, x&le;x (reflexivity)</li>\n<li>For any x and y in S, if x&le;y and y&le;x, then x=y (antisymmetry)</li>\n<li>For any x,y,z in S, if x&le;y and y&le;z, then x&le;z (transitivity)</li>\n</ol>\n<p>If we additionally required that for any x and y in S, we have either x&le;y or y&le;x, we'd have a <a href=\"http://en.wikipedia.org/wiki/Total_order\">total order</a> (also called a linear order).</p>\n<p>OK, but still, what does \"neither larger nor smaller, yet not the same\" mean in general? How can you visualize it?&nbsp; Well, the canonical example of a partial order would be, if we have any set S, we can partially order its subsets by defining A&le;B to mean A&sube;B.&nbsp; So if S={1,2,3,4}, then {1,2} is larger than {1} and {2}, and smaller than {1,2,4}, but incomparable to {3} or {2,3} or {2,3,4}.</p>\n<p>Another example would be, if we have ordered n-tuples of real numbers, we could define (x<sub>1</sub>,...,x<sub>n</sub>)&le;(y<sub>1</sub>,...,y<sub>n</sub>) if x<sub>i</sub>&le;y<sub>i</sub> for each i.&nbsp; You might imagine these as, say, stats of characters in a game; then x&le;y would mean that character y is better than character x in every way.&nbsp; To say that x and y are incomparable would mean that - though in practice one might be better on the whole - neither is obviously better.&nbsp; More generally, in any game, you could define a partial order on strategies by x&le;y if y dominates x.</p>\n<p>Note that partial orders are sufficiently common that for many math people the word \"order\" means \"partial order\" by default.</p>\n<h4>Cardinal arithmetic<br /></h4>\n<p>Given sets X and Y, |X|+|Y| will denote the cardinality of the \"disjoint union\" of X and Y, which is the union of X and Y, but with each element tagged with which of the two it came from, so that we don't lose anything to overlap (i.e., if an element is in both X and Y, it will occur twice, once with an \"X\" tag and once with a \"Y\" tag.)&nbsp; |X||Y| will denote the cardinality of the set X&times;Y, the <a href=\"http://en.wikipedia.org/wiki/Cartesian_product\"><em>Cartesian product</em></a> of X and Y, which is the set of all ordered pairs (x,y) with x in X and y in Y.&nbsp; However, if we admit the axiom of choice, this arithmetic is not very interesting for infinite sets!&nbsp; It turns out that given cardinal numbers &mu; and &lambda;, if either is infinite and neither is zero, then &mu;+&lambda;=&mu;&lambda;=max(&mu;,&lambda;).&nbsp; Hence, if you need a system of infinities in which x+y is going to be strictly bigger than x and y, cardinal numbers are the wrong choice.&nbsp; (The arithmetic of cardinals gets more interesting once you allow for adding or multiplying infinitely many at once.)</p>\n<p>There is also exponentiation of cardinals; |X|<sup>|Y|</sup> denotes the cardinality of the set X<sup>Y</sup> of all functions from Y to X, i.e., the number of ways of picking one element of X for each element of Y. Given any set X, 2<sup>|X|</sup> is the cardinality of its <a href=\"http://en.wikipedia.org/wiki/Power_set\">power set</a> &weierp;(X), the set of all its subsets.&nbsp; Cantor's <a href=\"http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument\">diagonal argument</a> shows that for any set X, 2<sup>|X|</sup>&gt;|X|; in particular, there is no largest cardinal number.</p>\n<p><strong>Application:</strong> Measuring sizes of sets when we don't care about the context or composition.</p>\n<h4><a href=\"http://en.wikipedia.org/wiki/Ordinal_number\">Ordinal numbers</a></h4>\n<p>I'm afraid there's no quick way to explain these. The reason is that they are used to represent two things - ways of well-ordering things, and positions in an \"infinite list\" - except, of course, that these are actually fundamentally the same thing, and to understand ordinals you need to wrap your head around this until you can see both simultaneously.&nbsp; Hence I suggest you just go read Wikipedia, or some other standard text, if you want to learn how these work. I will just speak briefly on their arithmetic. Note that the ordinals too are ordered - linearly ordered and well-ordered, at that.</p>\n<p>Unlike with the cardinals, addition and multiplication of two ordinals will often get you a larger ordinal.&nbsp; In particular, for any ordinal &lambda;, &lambda;+1 is a larger ordinal.&nbsp; However the multiplication of ordinals is noncommutative.&nbsp; In fact, even the addition of ordinal numbers is noncommutative!&nbsp; And distributivity only holds on one side; a(b+c)=ab+ac, but (a+b)c need not be ac+bc.&nbsp; So if you need commutativity, ordinals (with their usual operations) are the wrong choice.</p>\n<p>Contrast the smallest infinite ordinal, denoted &omega;, with &alefsym;<sub>0</sub>, which is (assuming choice) the smallest infinite cardinal.&nbsp; 1+&alefsym;<sub>0</sub>=&alefsym;<sub>0</sub>+1=&alefsym;<sub>0</sub>, and 1+&omega;=&omega;, but &omega;+1&gt;&omega;.&nbsp; 2&alefsym;<sub>0</sub>=&alefsym;<sub>0</sub>2=&alefsym;<sub>0</sub>, and 2&omega;=&omega;, but &omega;2&gt;&omega;.&nbsp; &alefsym;<sub>0</sub><sup>2</sup>=&alefsym;<sub>0</sub>, but&nbsp;&omega;<sup>2</sup>&gt;&omega;. And in a reversal of what you might expect if you just complete the pattern, 2^&alefsym;<sub>0</sub>&gt;&alefsym;<sub>0</sub>, but 2<sup>&omega;</sup>=&omega;.</p>\n<p><strong>Application: </strong>See link.</p>\n<h4>Ordinal numbers... with <a href=\"http://en.wikipedia.org/wiki/Ordinal_arithmetic#Natural_operations\">natural operations</a></h4>\n<p>There's an alternate way of doing arithmetic on the ordinals, referred to as the \"natural operations\".&nbsp; These sacrifice the continuity properties of the ordinary operations, but in return get commutativity, distributivity, cancellation... the things we need to make the algebra nice.&nbsp; There's a natural addition, a natural multiplication, and apparently a natural exponentiation, though I don't know what that last one might be.</p>\n<p>If you've heard \"the ordinals embed in the surreals\", and were very confused by that statement because the surreals are commutative when the ordinals are not, the answer is that the correct statement is that the ordinals with <em>natural</em> operations embed in the surreals, rather than the ordinals with their usual operations.</p>\n<h4>The <a href=\"http://en.wikipedia.org/wiki/Extended_real_number_line\">extended [positive] real line</a></h4>\n<p>Sometimes, we just use the set of nonnegative real numbers with an infinity element (denoted &infin;, unsurprisingly) tacked on.&nbsp; Because sometimes that's all you need.&nbsp; So:</p>\n<p><strong>Myth #2: Any place where you have infinities, you have the possibility for differing degrees of infinity.</strong></p>\n<p>Fact: <strong>Sometimes such a thing just wouldn't make sense</strong>.</p>\n<p><strong>Application:</strong> This is what we do in measure theory - i.e. anywhere integration or expected value (and hence, in the usual formulations, utility) is involved.&nbsp; If you want to claim that in your utility function, options A and B both have infinite utility, but the utility of B is more infinite than that of A... first you're going to have to make a framework in which that makes sense.&nbsp; Such a thing might indeed make sense, but you'll have to explain how, as our usual framework for utility doesn't allow such things.&nbsp; (The problem is that adding multiple distinct infinities tends to ruin the continuity properties of the real numbers that make integration possible in the first place, but I'm sure if you look someone must have come up with some method for getting around that in some cases.)</p>\n<p>Sometimes we allow negative numbers and -&infin; as well, though this can cause a problem because there's no sensible way to define &infin;+(-&infin;).&nbsp; (0&infin;, on the other hand, is just 0.&nbsp; We make this definition because, e.g., the area of an infinitely-long-but-infinitely-thin line should still be 0.)</p>\n<h4>The <a href=\"http://en.wikipedia.org/wiki/Real_projective_line\">projective line</a></h4>\n<p>Sometimes we don't even care about the distinction between a \"positive infinity\" and a \"negative infinity\"; we just need something that represents something larger in magnitude than all real numbers, but which you'd approach regardless of whether you got large and negative or large and positive.&nbsp; So we take the real numbers <strong>R</strong>, tack on an infinity element &infin;, and we have the real projective line.&nbsp; Note that this doesn't depend at all on the real numbers being ordered, so we can do the same with the complex numbers and get the complex projective line, a.k.a. the <a href=\"http://en.wikipedia.org/wiki/Riemann_sphere\">Riemann sphere</a>.</p>\n<p><strong>Application:</strong> If you want to assign 1/x some concrete \"value\" when x=0, well, this isn't going to make sense in a system where you have to distinguish &infin; from -&infin;.</p>\n<h4><a href=\"http://en.wikipedia.org/wiki/Hyperreal_number\">Hyperreal numbers</a><br /></h4>\n<p>What <a href=\"http://en.wikipedia.org/wiki/Non-standard_analysis\">nonstandard analysis</a> uses. These are more used as a means to deduce properties of the real numbers than used for their own sake.&nbsp; You can't even speak of \"the\" hypperreal numbers, because then you'd have to specify what ultrafilter you were using. Even just proving these exist requires <a href=\"http://en.wikipedia.org/wiki/Boolean_prime_ideal_theorem#The_ultrafilter_lemma\">a form of choice</a>.&nbsp; You probably don't want to use these to represent anything.</p>\n<h4>The <a href=\"http://en.wikipedia.org/wiki/Surreal_number\">surreal numbers</a>: the infinity kitchen sink*<br /></h4>\n<p>For when you absolutely, positively, have to make sense of an expression involving infinite quantities.&nbsp; The surreal numbers are pretty much as infinite as you could possibly want.&nbsp; They contain the ordinals with their natural operations, but they allow for so much more.&nbsp; Do you need to take the natural logarithm of &omega;? And then divide &pi; by it?&nbsp; And then raise the whole thing to the &radic;(&omega;<sup>2</sup>+&pi;&omega;) power?&nbsp; And then subtract &omega;<sup>&radic;8</sup>?&nbsp; In the surreal numbers, this all makes sense.&nbsp; Somehow.&nbsp; (And if you need square roots of negative numbers, you can always pass to the surcomplex numbers, which I guess is the actual kitchen sink.)</p>\n<p>*The <a href=\"http://en.wikipedia.org/wiki/Characteristic_%28algebra%29\">characteristic 0</a> infinity kitchen sink, anyway.&nbsp; Characteristic 2 has its own infinity kitchen sink, the <a href=\"http://en.wikipedia.org/wiki/Nimber\">nimbers</a>. I don't know about other characteristics.&nbsp; I also have to wonder if there's some set of characteristic 0 \"infinity kitchen sinks\" that naturally extend the p-adics...</p>\n<p><strong>Application:</strong> Again, kitchen sink.</p>\n<h4>...and many more</h4>\n<p>Often the thing to do is make an ad-hoc system to fit the occasion.&nbsp; For instance, we could simply take the real numbers <strong>R</strong> and tack on an element &infin;, insist it obey the ordinary rules of algebra, and order appropriately.&nbsp; (Formally, take the ring <strong>R</strong>[T], and order lexicographically.&nbsp; Then perhaps extend to <strong>R</strong>(T), or whatever else you might like.&nbsp; And of course call it \"&infin;\" rather than \"T\".)&nbsp; So (&infin;+1)(&infin;-1)=&infin;<sup>2</sup>-1, etc. What is this good for? I have no idea, but it's a simple brute-force way of tossing in infinities when needed.</p>\n<h4>Also: functions, which are probably more appropriate a lot of the time<br /></h4>\n<p>Let's not forget - oftentimes the appropriate thing to do is not to start tossing about infinities at all, but rather shift from thinking about numbers to thinking about functions. You know what's larger than any constant number? x. What's even larger? x<sup>2</sup>.&nbsp; (If we only consider polynomial functions, this is equivalent to the \"brute-force\" system above, under the equivalence x&harr;&infin;.)&nbsp; Much larger? e<sup>x</sup>.&nbsp; Is x too large?&nbsp; Maybe you want log x.&nbsp; Etc.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2>Topic #2: Ways of measuring infinite sets</h2>\n<p>The thing about measuring infinite sets is that we have a trade-off between discrimination and applicability.&nbsp; Cardinality can be applied to any set at all, but it's a very coarse-grained way of measuring things.&nbsp; If you want to measure a subset of the plane, you'd be better off asking for its area... just don't think you can ask for the \"area\" of a set of integers.</p>\n<h4>Cardinal numbers (again)</h4>\n<p>The most basic method.&nbsp; <em>Every</em> set has a cardinality.&nbsp; But the cost of such universality is a very low resolution.&nbsp; The set of natural numbers has cardinality &alefsym;<sub>0</sub>, but so does the set of even numbers, the set of rational numbers, the set of algebraic numbers, the set of <a href=\"http://en.wikipedia.org/wiki/Computable_number\">computable real numbers</a>...</p>\n<p>Note that the set of real numbers is much larger and has cardinality 2^&alefsym;<sub>0</sub>.&nbsp; (This is not to be confused with &alefsym;<sub>1</sub>, which <a href=\"http://en.wikipedia.org/wiki/Axiom_of_choice\">(assuming choice again)</a> is the second-smallest infinite cardinal.&nbsp; The question of whether 2^&alefsym;<sub>0</sub>=&alefsym;<sub>1</sub> is known as the <a href=\"http://en.wikipedia.org/wiki/Continuum_hypothesis\">continuum hypothesis</a>.)</p>\n<p>If we are working with subsets T of a given set S, we can do a bit better by not just looking at |T|, but also at |S-T| (the size of the complement of T in S).&nbsp; For instance, the set of natural numbers greater than 8, and the set of even natural numbers, both have cardinality &alefsym;<sub>0</sub>, but within the context of the natural numbers, the former has finite complement (numbers at most 8), while the latter has infinite complement (all odd numbers).</p>\n<h4>Occasionally: ordinals</h4>\n<p>If the sets you're working with come with well-orderings, you can consider the type of well-ordering as a \"size\", and thus measure sizes with ordinals.&nbsp; If they don't have well-orderings, this doesn't apply.</p>\n<h4><a href=\"http://en.wikipedia.org/wiki/Measure_%28mathematics%29\">Measure</a>: the old fallback</h4>\n<p>Most commonly we use the notion of a <em>measure</em> to measure sizes of subsets T of a given set S.&nbsp; This just means that we designate some of the subsets T of S as \"measurable\" (with a few requirements - the whole set S must be measurable; complements of measurable sets must be measurable; a union of countably many measurable sets must be measurable) and assign them a number called their measure, which I'll denote &mu;(T).&nbsp; &mu; takes values in the extended positive real line (see above): It can be any nonnegative real number, or just a flat &infin;. &nbsp;We require that the empty set have measure 0, that if A and B are disjoint sets then &mu;(A&cup;B)=&mu;(A)+&mu;(B) (called \"finite additivity\"), and more generally that if we have a countable collection of sets A<sub>1</sub>, A<sub>2, </sub>..., with none of them overlapping any of the others, then the measure of their union is the sum of their measures.&nbsp; (Called \"countable additivity\"; this infinite sum automatically makes sense because all the numbers involved are nonnegative.)</p>\n<p>The function &mu; itself is called a measure on S.&nbsp; So if we have a set S and a measure on it, we have a way to measure the sizes of subsets of it (well, the measurable ones, anyway).&nbsp; Of course, this is all very non-specific; by itself, this doesn't help us much.</p>\n<p>Fortunately, the set of real numbers <strong>R</strong> comes equipped with a natural measure, known as <a href=\"http://en.wikipedia.org/wiki/Lebesgue_measure\">Lebesgue measure</a>.&nbsp; So does n-dimensional Euclidean space for every n.&nbsp; And indeed so do a lot of the natural spaces we encounter.&nbsp; So while simply shouting \"there's a measure!\", without stating what that measure might be, does not solve any problems, in practice there's often one natural measure (up to multiplication by some positive constant).&nbsp; See in particular: <a href=\"http://en.wikipedia.org/wiki/Haar_measure\">Haar measure</a>.</p>\n<p>If we have a set S with a measure &mu; such that &mu;(S)=1, then we have a <a href=\"http://en.wikipedia.org/wiki/Probability_space\">probability space</a>.&nbsp; This is how we formalize probability mathematically: We have some set S of possibilities, equipped with a measure, and the measure of a set of possibilities is its probability.&nbsp; Except, of course, that I'm sure many here would insist only on finite additivity rather than countable additivity...</p>\n<p>Note that if &mu;(S) is finite, then &mu;(S-T)=&mu;(S)-&mu;(T).&nbsp; However, if &mu;(S)=&infin;, and &mu;(T)=&infin; also, this doesn't work; &infin;-&infin; is not defined in this context, and &mu;(S-T) could be any extended nonnegative real number.&nbsp; So note that if we're working in a set of infinite measure, and we're comparing subsets which themselves have infinite measure, we can possibly gain some extra information by comparing the measures of the complements as well.</p>\n<p>Here on LessWrong, when discussing multiverse-based notions, we'll typically assume that the set of universes comes equipped in some way with a natural measure.&nbsp; If the universes are the many worlds of MWI, then this measure will be proportional to squared-norm-of-amplitude.</p>\n<h4>Measuring subsets of the natural numbers</h4>\n<p>So it seems like 2<strong>N</strong> should be half the size of <strong>N</strong>, right?&nbsp; Well there's an easy way to accomplish this: Given a set A of natural numbers, we define its <a href=\"http://en.wikipedia.org/wiki/Natural_density\"><em>natural density</em></a> to be lim<sub>n&rarr;&infin;</sub> A(n)/n, where A(n) denotes the number of elements of A that are at most n.&nbsp; At least, we can do this if the limit exists.&nbsp; It doesn't always.&nbsp; But when it does it does what we want pretty well.&nbsp; What if the limit doesn't exist?&nbsp; Well, we could use a <a href=\"http://en.wikipedia.org/wiki/Limit_superior_and_limit_inferior\">limsup or a liminf</a> instead, and get upper and lower densities.&nbsp; Or take some other approach, such as <a href=\"http://en.wikipedia.org/wiki/Schnirelmann_density\">Schnirelmann density</a>, where we just take an inf.</p>\n<p>Of course, for sets of density 0, this may not be enough information.&nbsp; Here we can pull out another trick from above: Don't use numbers, use functions!&nbsp; We can just ask what function A(n) approximates (<a href=\"http://en.wikipedia.org/wiki/Asymptotic_analysis\">asymptotically</a>).&nbsp; For instance, the prime numbers have density 0, but a much more informative statement is the <a href=\"http://en.wikipedia.org/wiki/Prime_number_theorem\">prime number theorem</a>, which states that if P is the set of prime numbers, then P(n)~n/(log n).</p>\n<h4>...etc...</h4>\n<p>Of course, the real point of all these examples was simply to demonstrate: Depending on what sort of thing you want to measure, you'll need different tools!&nbsp; So there's many more tools out there, and sometimes you may just need to invent your own...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "xQzd3c86dT9rLH985": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nfs84MvYGaAiXcgik", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 42, "extendedScore": null, "score": 8.6e-05, "legacy": true, "legacyId": "4471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>EDIT:</strong> This is now on the Wiki as \"<a href=\"http://wiki.lesswrong.com/wiki/Quick_reference_guide_to_the_infinite\">Quick reference guide to the infinite</a>\".&nbsp; Do what you want with it.</p>\n<p>It seems whenever anything involving infinities or measuring infinite sets comes up it generates a lot of confusion.&nbsp; So I thought I would write a quick guide to both to</p>\n<ol>\n<li>Address common confusions</li>\n<li>Act as a useful reference (perhaps this should be a wiki article? This would benefit from others being able to edit it; there's no \"community wiki mode\" on LW, huh?)</li>\n<li>Remind people that sometimes inventing a new sort of answer is necessary!</li>\n</ol>\n<p>I am trying to keep this concise, in some cases substituting Wikipedia links for explanation, but I do want what I have written to be understandable enough and informative enough to answer the commonly occurring questions.&nbsp; Please let me know if you can detect a particular problem. I wrote this very quickly and expect it still needs quite a bit more work to be understandable to someone with very little math background.</p>\n<p>I realize many people here are finitists of one stripe or another but this comes up often enough that this seems useful anyway.&nbsp; Apologies to any constructivists, but I am going to assume classical logic, because it's all I know, though I am pointing out explicitly any uses of choice.&nbsp; (For what this means and why anyone cares about this, see <a href=\"/r/discussion/lw/3g7/draftwiki_infinities_and_measuring_infinite_sets/38kf\">this comment</a>.)&nbsp; Also as I intend this as a reference (is there *any* way we can make this editable?) some of this may be things that I do not actually know but merely have read.</p>\n<p>Note that these are two separate topics, though they have a bit of overlap.</p>\n<p>Primarily, though, my main intention is to put an end to the following, which I have seen here far too often:</p>\n<p><strong id=\"Myth__0__All_infinities_are_infinite_cardinals__and_cardinality_is_the_main_method_used_to_measure_size_of_sets_\">Myth #0: All infinities are infinite cardinals, and cardinality is the main method used to measure size of sets.</strong></p>\n<p>The fact is that <strong>\"infinite\" is a general term meaning \"larger (in some sense) than any natural number\"; different systems of infinite numbers get used depending on what is appropriate in context</strong>. Furthermore, <strong>there are many other methods of measuring sizes of sets, which sacrifice universality for higher resolution; cardinality is a very coarse-grained measure.</strong></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"Topic__1__Systems_of_infinities__for_doing_arithmetic_with_\">Topic #1: Systems of infinities (for doing arithmetic with)<br></h2>\n<h4 id=\"Cardinal_numbers\"><a href=\"http://en.wikipedia.org/wiki/Cardinal_number\">Cardinal numbers</a></h4>\n<p>First, a review of what they represent and how they work at the basic level, before we get to their arithmetic.</p>\n<p>Cardinal numbers are used for measuring sizes of sets when we don't know, or don't care, about the set's context or composition.&nbsp; First, the standard explanation of what we mean by this: Say we have two farmers, who each have a large number of sheep, more than they can count.&nbsp; How can they determine who has more?&nbsp; They pair off the sheep of the one against the sheep of the other; whichever has sheep left over, has more.</p>\n<p>So given two sets X and Y, we will say X has smaller cardinality than Y (denoted |X|\u2264|Y|, or sometimes #X\u2264#Y) if there is a way to assign to each element x of X, a corresponding element f(x) of Y, such that no two distinct x<sub>1</sub> and x<sub>2</sub> from X correspond to the same element of Y.&nbsp; If, furthermore, this correspondence covers all of Y - if for each y in Y there is some x in X that had y assigned to it - then we say that X and Y have the same cardinality, |X|=|Y| or #X=#Y.</p>\n<p>Note that by this definition, the set <strong>N</strong> of natural numbers, and the set 2<strong>N</strong> of even integers, have the same size, since we can match up 1 with 2, 2 with 4, 3 with 6, etc.&nbsp; This even though it seems 2<strong>N</strong> should be only \"half as large\" as <strong>N</strong>!&nbsp; This is why I emphasize: Cardinality is only one way of measuring sizes of sets, one that is not fine enough to distinguish between 2<strong>N</strong> and <strong>N</strong>.&nbsp; Other methods of measuring their size will have 2<strong>N</strong> only half as large as <strong>N</strong>.</p>\n<p>It is true, but not obvious, that if |X|\u2264|Y| and |Y|\u2264|X|, then |X|=|Y|; this is the <a href=\"http://en.wikipedia.org/wiki/Cantor%E2%80%93Bernstein%E2%80%93Schroeder_theorem\">Schroeder-Bernstein theorem</a>. Hence we can sensibly talk about \"the cardinality\" of a set X as being some abstract property of it - if |X|\u2264|Y| then X has smaller cardinality and Y has larger cardinality, and so on.&nbsp; We can make this more concrete, and define an actual cardinality object |X| (or #X), using either the <a href=\"http://en.wikipedia.org/wiki/Axiom_of_choice\">axiom of choice</a> or <a href=\"http://en.wikipedia.org/wiki/Scott%27s_trick\">Scott's trick</a> (if you admit the <a href=\"http://en.wikipedia.org/wiki/Axiom_of_regularity\">axiom of foundation</a>) or even <a href=\"http://en.wikipedia.org/wiki/Class_%28set_theory%29\">proper classes if we admit those</a>, but this will not be relevant here. We will use |X|&lt;|Y| to mean \"|X|\u2264|Y| but |X|\u2260Y\".</p>\n<p>Note that it is also not obvious that given any two sets X and Y, we must have either |X|\u2264|Y| or |Y|\u2264|X|; indeed, this statement is true if and only if we admit the <a href=\"http://en.wikipedia.org/wiki/Axiom_of_choice\">axiom of choice</a>. So take note:</p>\n<p><strong id=\"Myth__1__Infinities_must_come_in_a_linear_ordering_\">Myth #1: Infinities must come in a linear ordering.</strong></p>\n<p>Fact: <strong>If the axiom of choice is false, then there are necessarily infinite cardinals which are not the same size, and yet for which neither can be said to be larger!</strong>&nbsp; If we do admit the axiom of choice, then the cardinal numbers must be not only linearly-ordered but in fact be <a href=\"http://en.wikipedia.org/wiki/Well-order\">well-ordered</a>.</p>\n<p>The cardinality of the set of natural numbers, |<strong>N</strong>|, is also denoted \u2135<sub>0</sub>. If we admit the axiom of <a href=\"http://en.wikipedia.org/wiki/Axiom_of_dependent_choice\">[dependent]</a> choice, this is the smallest infinite cardinal.&nbsp; Here by \"infinite\" cardinal I mean one that is larger than the size of any finite set (0, 1, 2, etc.).</p>\n<h5>Quick aside on partial orderings</h5>\n<p>Many of you are probably wondering how to think about something like \"neither larger nor smaller, but not the same\".&nbsp; Formally, we say that, without choice, the ordering on the cardinal numbers is a <a href=\"http://en.wikipedia.org/wiki/Partially_ordered_set\"><em>partial order</em></a>.&nbsp; Because these are so common I'll go ahead and define this here - generally, a partial order on a set S is a relation (usually denoted \"\u2264\") on S such that:</p>\n<ol>\n<li>For every x in S, x\u2264x (reflexivity)</li>\n<li>For any x and y in S, if x\u2264y and y\u2264x, then x=y (antisymmetry)</li>\n<li>For any x,y,z in S, if x\u2264y and y\u2264z, then x\u2264z (transitivity)</li>\n</ol>\n<p>If we additionally required that for any x and y in S, we have either x\u2264y or y\u2264x, we'd have a <a href=\"http://en.wikipedia.org/wiki/Total_order\">total order</a> (also called a linear order).</p>\n<p>OK, but still, what does \"neither larger nor smaller, yet not the same\" mean in general? How can you visualize it?&nbsp; Well, the canonical example of a partial order would be, if we have any set S, we can partially order its subsets by defining A\u2264B to mean A\u2286B.&nbsp; So if S={1,2,3,4}, then {1,2} is larger than {1} and {2}, and smaller than {1,2,4}, but incomparable to {3} or {2,3} or {2,3,4}.</p>\n<p>Another example would be, if we have ordered n-tuples of real numbers, we could define (x<sub>1</sub>,...,x<sub>n</sub>)\u2264(y<sub>1</sub>,...,y<sub>n</sub>) if x<sub>i</sub>\u2264y<sub>i</sub> for each i.&nbsp; You might imagine these as, say, stats of characters in a game; then x\u2264y would mean that character y is better than character x in every way.&nbsp; To say that x and y are incomparable would mean that - though in practice one might be better on the whole - neither is obviously better.&nbsp; More generally, in any game, you could define a partial order on strategies by x\u2264y if y dominates x.</p>\n<p>Note that partial orders are sufficiently common that for many math people the word \"order\" means \"partial order\" by default.</p>\n<h4 id=\"Cardinal_arithmetic\">Cardinal arithmetic<br></h4>\n<p>Given sets X and Y, |X|+|Y| will denote the cardinality of the \"disjoint union\" of X and Y, which is the union of X and Y, but with each element tagged with which of the two it came from, so that we don't lose anything to overlap (i.e., if an element is in both X and Y, it will occur twice, once with an \"X\" tag and once with a \"Y\" tag.)&nbsp; |X||Y| will denote the cardinality of the set X\u00d7Y, the <a href=\"http://en.wikipedia.org/wiki/Cartesian_product\"><em>Cartesian product</em></a> of X and Y, which is the set of all ordered pairs (x,y) with x in X and y in Y.&nbsp; However, if we admit the axiom of choice, this arithmetic is not very interesting for infinite sets!&nbsp; It turns out that given cardinal numbers \u03bc and \u03bb, if either is infinite and neither is zero, then \u03bc+\u03bb=\u03bc\u03bb=max(\u03bc,\u03bb).&nbsp; Hence, if you need a system of infinities in which x+y is going to be strictly bigger than x and y, cardinal numbers are the wrong choice.&nbsp; (The arithmetic of cardinals gets more interesting once you allow for adding or multiplying infinitely many at once.)</p>\n<p>There is also exponentiation of cardinals; |X|<sup>|Y|</sup> denotes the cardinality of the set X<sup>Y</sup> of all functions from Y to X, i.e., the number of ways of picking one element of X for each element of Y. Given any set X, 2<sup>|X|</sup> is the cardinality of its <a href=\"http://en.wikipedia.org/wiki/Power_set\">power set</a> \u2118(X), the set of all its subsets.&nbsp; Cantor's <a href=\"http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument\">diagonal argument</a> shows that for any set X, 2<sup>|X|</sup>&gt;|X|; in particular, there is no largest cardinal number.</p>\n<p><strong>Application:</strong> Measuring sizes of sets when we don't care about the context or composition.</p>\n<h4 id=\"Ordinal_numbers\"><a href=\"http://en.wikipedia.org/wiki/Ordinal_number\">Ordinal numbers</a></h4>\n<p>I'm afraid there's no quick way to explain these. The reason is that they are used to represent two things - ways of well-ordering things, and positions in an \"infinite list\" - except, of course, that these are actually fundamentally the same thing, and to understand ordinals you need to wrap your head around this until you can see both simultaneously.&nbsp; Hence I suggest you just go read Wikipedia, or some other standard text, if you want to learn how these work. I will just speak briefly on their arithmetic. Note that the ordinals too are ordered - linearly ordered and well-ordered, at that.</p>\n<p>Unlike with the cardinals, addition and multiplication of two ordinals will often get you a larger ordinal.&nbsp; In particular, for any ordinal \u03bb, \u03bb+1 is a larger ordinal.&nbsp; However the multiplication of ordinals is noncommutative.&nbsp; In fact, even the addition of ordinal numbers is noncommutative!&nbsp; And distributivity only holds on one side; a(b+c)=ab+ac, but (a+b)c need not be ac+bc.&nbsp; So if you need commutativity, ordinals (with their usual operations) are the wrong choice.</p>\n<p>Contrast the smallest infinite ordinal, denoted \u03c9, with \u2135<sub>0</sub>, which is (assuming choice) the smallest infinite cardinal.&nbsp; 1+\u2135<sub>0</sub>=\u2135<sub>0</sub>+1=\u2135<sub>0</sub>, and 1+\u03c9=\u03c9, but \u03c9+1&gt;\u03c9.&nbsp; 2\u2135<sub>0</sub>=\u2135<sub>0</sub>2=\u2135<sub>0</sub>, and 2\u03c9=\u03c9, but \u03c92&gt;\u03c9.&nbsp; \u2135<sub>0</sub><sup>2</sup>=\u2135<sub>0</sub>, but&nbsp;\u03c9<sup>2</sup>&gt;\u03c9. And in a reversal of what you might expect if you just complete the pattern, 2^\u2135<sub>0</sub>&gt;\u2135<sub>0</sub>, but 2<sup>\u03c9</sup>=\u03c9.</p>\n<p><strong>Application: </strong>See link.</p>\n<h4 id=\"Ordinal_numbers____with_natural_operations\">Ordinal numbers... with <a href=\"http://en.wikipedia.org/wiki/Ordinal_arithmetic#Natural_operations\">natural operations</a></h4>\n<p>There's an alternate way of doing arithmetic on the ordinals, referred to as the \"natural operations\".&nbsp; These sacrifice the continuity properties of the ordinary operations, but in return get commutativity, distributivity, cancellation... the things we need to make the algebra nice.&nbsp; There's a natural addition, a natural multiplication, and apparently a natural exponentiation, though I don't know what that last one might be.</p>\n<p>If you've heard \"the ordinals embed in the surreals\", and were very confused by that statement because the surreals are commutative when the ordinals are not, the answer is that the correct statement is that the ordinals with <em>natural</em> operations embed in the surreals, rather than the ordinals with their usual operations.</p>\n<h4 id=\"The_extended__positive__real_line\">The <a href=\"http://en.wikipedia.org/wiki/Extended_real_number_line\">extended [positive] real line</a></h4>\n<p>Sometimes, we just use the set of nonnegative real numbers with an infinity element (denoted \u221e, unsurprisingly) tacked on.&nbsp; Because sometimes that's all you need.&nbsp; So:</p>\n<p><strong id=\"Myth__2__Any_place_where_you_have_infinities__you_have_the_possibility_for_differing_degrees_of_infinity_\">Myth #2: Any place where you have infinities, you have the possibility for differing degrees of infinity.</strong></p>\n<p>Fact: <strong>Sometimes such a thing just wouldn't make sense</strong>.</p>\n<p><strong>Application:</strong> This is what we do in measure theory - i.e. anywhere integration or expected value (and hence, in the usual formulations, utility) is involved.&nbsp; If you want to claim that in your utility function, options A and B both have infinite utility, but the utility of B is more infinite than that of A... first you're going to have to make a framework in which that makes sense.&nbsp; Such a thing might indeed make sense, but you'll have to explain how, as our usual framework for utility doesn't allow such things.&nbsp; (The problem is that adding multiple distinct infinities tends to ruin the continuity properties of the real numbers that make integration possible in the first place, but I'm sure if you look someone must have come up with some method for getting around that in some cases.)</p>\n<p>Sometimes we allow negative numbers and -\u221e as well, though this can cause a problem because there's no sensible way to define \u221e+(-\u221e).&nbsp; (0\u221e, on the other hand, is just 0.&nbsp; We make this definition because, e.g., the area of an infinitely-long-but-infinitely-thin line should still be 0.)</p>\n<h4 id=\"The_projective_line\">The <a href=\"http://en.wikipedia.org/wiki/Real_projective_line\">projective line</a></h4>\n<p>Sometimes we don't even care about the distinction between a \"positive infinity\" and a \"negative infinity\"; we just need something that represents something larger in magnitude than all real numbers, but which you'd approach regardless of whether you got large and negative or large and positive.&nbsp; So we take the real numbers <strong>R</strong>, tack on an infinity element \u221e, and we have the real projective line.&nbsp; Note that this doesn't depend at all on the real numbers being ordered, so we can do the same with the complex numbers and get the complex projective line, a.k.a. the <a href=\"http://en.wikipedia.org/wiki/Riemann_sphere\">Riemann sphere</a>.</p>\n<p><strong>Application:</strong> If you want to assign 1/x some concrete \"value\" when x=0, well, this isn't going to make sense in a system where you have to distinguish \u221e from -\u221e.</p>\n<h4 id=\"Hyperreal_numbers\"><a href=\"http://en.wikipedia.org/wiki/Hyperreal_number\">Hyperreal numbers</a><br></h4>\n<p>What <a href=\"http://en.wikipedia.org/wiki/Non-standard_analysis\">nonstandard analysis</a> uses. These are more used as a means to deduce properties of the real numbers than used for their own sake.&nbsp; You can't even speak of \"the\" hypperreal numbers, because then you'd have to specify what ultrafilter you were using. Even just proving these exist requires <a href=\"http://en.wikipedia.org/wiki/Boolean_prime_ideal_theorem#The_ultrafilter_lemma\">a form of choice</a>.&nbsp; You probably don't want to use these to represent anything.</p>\n<h4 id=\"The_surreal_numbers__the_infinity_kitchen_sink_\">The <a href=\"http://en.wikipedia.org/wiki/Surreal_number\">surreal numbers</a>: the infinity kitchen sink*<br></h4>\n<p>For when you absolutely, positively, have to make sense of an expression involving infinite quantities.&nbsp; The surreal numbers are pretty much as infinite as you could possibly want.&nbsp; They contain the ordinals with their natural operations, but they allow for so much more.&nbsp; Do you need to take the natural logarithm of \u03c9? And then divide \u03c0 by it?&nbsp; And then raise the whole thing to the \u221a(\u03c9<sup>2</sup>+\u03c0\u03c9) power?&nbsp; And then subtract \u03c9<sup>\u221a8</sup>?&nbsp; In the surreal numbers, this all makes sense.&nbsp; Somehow.&nbsp; (And if you need square roots of negative numbers, you can always pass to the surcomplex numbers, which I guess is the actual kitchen sink.)</p>\n<p>*The <a href=\"http://en.wikipedia.org/wiki/Characteristic_%28algebra%29\">characteristic 0</a> infinity kitchen sink, anyway.&nbsp; Characteristic 2 has its own infinity kitchen sink, the <a href=\"http://en.wikipedia.org/wiki/Nimber\">nimbers</a>. I don't know about other characteristics.&nbsp; I also have to wonder if there's some set of characteristic 0 \"infinity kitchen sinks\" that naturally extend the p-adics...</p>\n<p><strong>Application:</strong> Again, kitchen sink.</p>\n<h4 id=\"___and_many_more\">...and many more</h4>\n<p>Often the thing to do is make an ad-hoc system to fit the occasion.&nbsp; For instance, we could simply take the real numbers <strong>R</strong> and tack on an element \u221e, insist it obey the ordinary rules of algebra, and order appropriately.&nbsp; (Formally, take the ring <strong>R</strong>[T], and order lexicographically.&nbsp; Then perhaps extend to <strong>R</strong>(T), or whatever else you might like.&nbsp; And of course call it \"\u221e\" rather than \"T\".)&nbsp; So (\u221e+1)(\u221e-1)=\u221e<sup>2</sup>-1, etc. What is this good for? I have no idea, but it's a simple brute-force way of tossing in infinities when needed.</p>\n<h4 id=\"Also__functions__which_are_probably_more_appropriate_a_lot_of_the_time\">Also: functions, which are probably more appropriate a lot of the time<br></h4>\n<p>Let's not forget - oftentimes the appropriate thing to do is not to start tossing about infinities at all, but rather shift from thinking about numbers to thinking about functions. You know what's larger than any constant number? x. What's even larger? x<sup>2</sup>.&nbsp; (If we only consider polynomial functions, this is equivalent to the \"brute-force\" system above, under the equivalence x\u2194\u221e.)&nbsp; Much larger? e<sup>x</sup>.&nbsp; Is x too large?&nbsp; Maybe you want log x.&nbsp; Etc.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Topic__2__Ways_of_measuring_infinite_sets\">Topic #2: Ways of measuring infinite sets</h2>\n<p>The thing about measuring infinite sets is that we have a trade-off between discrimination and applicability.&nbsp; Cardinality can be applied to any set at all, but it's a very coarse-grained way of measuring things.&nbsp; If you want to measure a subset of the plane, you'd be better off asking for its area... just don't think you can ask for the \"area\" of a set of integers.</p>\n<h4 id=\"Cardinal_numbers__again_\">Cardinal numbers (again)</h4>\n<p>The most basic method.&nbsp; <em>Every</em> set has a cardinality.&nbsp; But the cost of such universality is a very low resolution.&nbsp; The set of natural numbers has cardinality \u2135<sub>0</sub>, but so does the set of even numbers, the set of rational numbers, the set of algebraic numbers, the set of <a href=\"http://en.wikipedia.org/wiki/Computable_number\">computable real numbers</a>...</p>\n<p>Note that the set of real numbers is much larger and has cardinality 2^\u2135<sub>0</sub>.&nbsp; (This is not to be confused with \u2135<sub>1</sub>, which <a href=\"http://en.wikipedia.org/wiki/Axiom_of_choice\">(assuming choice again)</a> is the second-smallest infinite cardinal.&nbsp; The question of whether 2^\u2135<sub>0</sub>=\u2135<sub>1</sub> is known as the <a href=\"http://en.wikipedia.org/wiki/Continuum_hypothesis\">continuum hypothesis</a>.)</p>\n<p>If we are working with subsets T of a given set S, we can do a bit better by not just looking at |T|, but also at |S-T| (the size of the complement of T in S).&nbsp; For instance, the set of natural numbers greater than 8, and the set of even natural numbers, both have cardinality \u2135<sub>0</sub>, but within the context of the natural numbers, the former has finite complement (numbers at most 8), while the latter has infinite complement (all odd numbers).</p>\n<h4 id=\"Occasionally__ordinals\">Occasionally: ordinals</h4>\n<p>If the sets you're working with come with well-orderings, you can consider the type of well-ordering as a \"size\", and thus measure sizes with ordinals.&nbsp; If they don't have well-orderings, this doesn't apply.</p>\n<h4 id=\"Measure__the_old_fallback\"><a href=\"http://en.wikipedia.org/wiki/Measure_%28mathematics%29\">Measure</a>: the old fallback</h4>\n<p>Most commonly we use the notion of a <em>measure</em> to measure sizes of subsets T of a given set S.&nbsp; This just means that we designate some of the subsets T of S as \"measurable\" (with a few requirements - the whole set S must be measurable; complements of measurable sets must be measurable; a union of countably many measurable sets must be measurable) and assign them a number called their measure, which I'll denote \u03bc(T).&nbsp; \u03bc takes values in the extended positive real line (see above): It can be any nonnegative real number, or just a flat \u221e. &nbsp;We require that the empty set have measure 0, that if A and B are disjoint sets then \u03bc(A\u222aB)=\u03bc(A)+\u03bc(B) (called \"finite additivity\"), and more generally that if we have a countable collection of sets A<sub>1</sub>, A<sub>2, </sub>..., with none of them overlapping any of the others, then the measure of their union is the sum of their measures.&nbsp; (Called \"countable additivity\"; this infinite sum automatically makes sense because all the numbers involved are nonnegative.)</p>\n<p>The function \u03bc itself is called a measure on S.&nbsp; So if we have a set S and a measure on it, we have a way to measure the sizes of subsets of it (well, the measurable ones, anyway).&nbsp; Of course, this is all very non-specific; by itself, this doesn't help us much.</p>\n<p>Fortunately, the set of real numbers <strong>R</strong> comes equipped with a natural measure, known as <a href=\"http://en.wikipedia.org/wiki/Lebesgue_measure\">Lebesgue measure</a>.&nbsp; So does n-dimensional Euclidean space for every n.&nbsp; And indeed so do a lot of the natural spaces we encounter.&nbsp; So while simply shouting \"there's a measure!\", without stating what that measure might be, does not solve any problems, in practice there's often one natural measure (up to multiplication by some positive constant).&nbsp; See in particular: <a href=\"http://en.wikipedia.org/wiki/Haar_measure\">Haar measure</a>.</p>\n<p>If we have a set S with a measure \u03bc such that \u03bc(S)=1, then we have a <a href=\"http://en.wikipedia.org/wiki/Probability_space\">probability space</a>.&nbsp; This is how we formalize probability mathematically: We have some set S of possibilities, equipped with a measure, and the measure of a set of possibilities is its probability.&nbsp; Except, of course, that I'm sure many here would insist only on finite additivity rather than countable additivity...</p>\n<p>Note that if \u03bc(S) is finite, then \u03bc(S-T)=\u03bc(S)-\u03bc(T).&nbsp; However, if \u03bc(S)=\u221e, and \u03bc(T)=\u221e also, this doesn't work; \u221e-\u221e is not defined in this context, and \u03bc(S-T) could be any extended nonnegative real number.&nbsp; So note that if we're working in a set of infinite measure, and we're comparing subsets which themselves have infinite measure, we can possibly gain some extra information by comparing the measures of the complements as well.</p>\n<p>Here on LessWrong, when discussing multiverse-based notions, we'll typically assume that the set of universes comes equipped in some way with a natural measure.&nbsp; If the universes are the many worlds of MWI, then this measure will be proportional to squared-norm-of-amplitude.</p>\n<h4 id=\"Measuring_subsets_of_the_natural_numbers\">Measuring subsets of the natural numbers</h4>\n<p>So it seems like 2<strong>N</strong> should be half the size of <strong>N</strong>, right?&nbsp; Well there's an easy way to accomplish this: Given a set A of natural numbers, we define its <a href=\"http://en.wikipedia.org/wiki/Natural_density\"><em>natural density</em></a> to be lim<sub>n\u2192\u221e</sub> A(n)/n, where A(n) denotes the number of elements of A that are at most n.&nbsp; At least, we can do this if the limit exists.&nbsp; It doesn't always.&nbsp; But when it does it does what we want pretty well.&nbsp; What if the limit doesn't exist?&nbsp; Well, we could use a <a href=\"http://en.wikipedia.org/wiki/Limit_superior_and_limit_inferior\">limsup or a liminf</a> instead, and get upper and lower densities.&nbsp; Or take some other approach, such as <a href=\"http://en.wikipedia.org/wiki/Schnirelmann_density\">Schnirelmann density</a>, where we just take an inf.</p>\n<p>Of course, for sets of density 0, this may not be enough information.&nbsp; Here we can pull out another trick from above: Don't use numbers, use functions!&nbsp; We can just ask what function A(n) approximates (<a href=\"http://en.wikipedia.org/wiki/Asymptotic_analysis\">asymptotically</a>).&nbsp; For instance, the prime numbers have density 0, but a much more informative statement is the <a href=\"http://en.wikipedia.org/wiki/Prime_number_theorem\">prime number theorem</a>, which states that if P is the set of prime numbers, then P(n)~n/(log n).</p>\n<h4 id=\"___etc___\">...etc...</h4>\n<p>Of course, the real point of all these examples was simply to demonstrate: Depending on what sort of thing you want to measure, you'll need different tools!&nbsp; So there's many more tools out there, and sometimes you may just need to invent your own...</p>", "sections": [{"title": "Myth #0: All infinities are infinite cardinals, and cardinality is the main method used to measure size of sets.", "anchor": "Myth__0__All_infinities_are_infinite_cardinals__and_cardinality_is_the_main_method_used_to_measure_size_of_sets_", "level": 3}, {"title": "Topic #1: Systems of infinities (for doing arithmetic with)", "anchor": "Topic__1__Systems_of_infinities__for_doing_arithmetic_with_", "level": 1}, {"title": "Cardinal numbers", "anchor": "Cardinal_numbers", "level": 2}, {"title": "Myth #1: Infinities must come in a linear ordering.", "anchor": "Myth__1__Infinities_must_come_in_a_linear_ordering_", "level": 3}, {"title": "Cardinal arithmetic", "anchor": "Cardinal_arithmetic", "level": 2}, {"title": "Ordinal numbers", "anchor": "Ordinal_numbers", "level": 2}, {"title": "Ordinal numbers... with natural operations", "anchor": "Ordinal_numbers____with_natural_operations", "level": 2}, {"title": "The extended [positive] real line", "anchor": "The_extended__positive__real_line", "level": 2}, {"title": "Myth #2: Any place where you have infinities, you have the possibility for differing degrees of infinity.", "anchor": "Myth__2__Any_place_where_you_have_infinities__you_have_the_possibility_for_differing_degrees_of_infinity_", "level": 3}, {"title": "The projective line", "anchor": "The_projective_line", "level": 2}, {"title": "Hyperreal numbers", "anchor": "Hyperreal_numbers", "level": 2}, {"title": "The surreal numbers: the infinity kitchen sink*", "anchor": "The_surreal_numbers__the_infinity_kitchen_sink_", "level": 2}, {"title": "...and many more", "anchor": "___and_many_more", "level": 2}, {"title": "Also: functions, which are probably more appropriate a lot of the time", "anchor": "Also__functions__which_are_probably_more_appropriate_a_lot_of_the_time", "level": 2}, {"title": "Topic #2: Ways of measuring infinite sets", "anchor": "Topic__2__Ways_of_measuring_infinite_sets", "level": 1}, {"title": "Cardinal numbers (again)", "anchor": "Cardinal_numbers__again_", "level": 2}, {"title": "Occasionally: ordinals", "anchor": "Occasionally__ordinals", "level": 2}, {"title": "Measure: the old fallback", "anchor": "Measure__the_old_fallback", "level": 2}, {"title": "Measuring subsets of the natural numbers", "anchor": "Measuring_subsets_of_the_natural_numbers", "level": 2}, {"title": "...etc...", "anchor": "___etc___", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 22}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T04:57:32.204Z", "modifiedAt": null, "url": null, "title": "Vegetarianism", "slug": "vegetarianism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:09.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kq6keonexJ5zLGK6o/vegetarianism", "pageUrlRelative": "/posts/Kq6keonexJ5zLGK6o/vegetarianism", "linkUrl": "https://www.lesswrong.com/posts/Kq6keonexJ5zLGK6o/vegetarianism", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Vegetarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVegetarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq6keonexJ5zLGK6o%2Fvegetarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Vegetarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq6keonexJ5zLGK6o%2Fvegetarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKq6keonexJ5zLGK6o%2Fvegetarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 670, "htmlBody": "<p><em>(Note: I wasn't quite sure whether this warranted a high level post or just a discussion. I haven't made a high level post yet, and wasn't entirely sure what the requirements are. For now I made it a discussion, but I'd like some feedback on that)</em></p>\n<p class=\"p1\">I've been somewhat surprised by the lack of many threads on Less Wrong dealing with vegetarianism, either for or against. Is there some near-universally accepted-but-unspoken philosophy here, or is it just not something people think of much? I was particularly taken aback by the Newtonmas invitation not even mentioning a vegetarian option. If a bunch of hyper-rationalists aren't even thinking about it, then either something is pretty wrong with my thinking or theirs.</p>\n<p class=\"p1\">I'm not going to go through all the arguments in detail here, but I'll list the basic ideas. If you've read \"Diet for a Small Planet\" or are otherwise aware of the specifics, and have counterarguments, feel free to object. If you haven't, I consider reading it (or something similar) a prerequisite for making a decision about whether you eat meat, just as reading the sequences is important to have meaningful discussion on this site.</p>\n<p class=\"p1\"><strong>The issues:</strong></p>\n<p class=\"p1\">1. \"It's cruel to animals.\" Factory farming is cruel on a massive scale, beyond what we find in nature. Even if animal suffering has only 1% the weight of a humans, there's enough multiplying going on that you can't just ignore it. I haven't precisely clarified my ethics in a way that avoids the Repugnant Conclusion (I've been vaguely describing myself as a \"Preference Utilitarian\" but I confess that I haven't fully explored the ramifications of it), but it seems to me that if you're not okay with breeding a subservient, less intelligent species of humans for slave labor and consumption, you shouldn't be okay with how we treat animals. I don't think intelligence gives humans any additional intrinsic value, and I don't think most humans use their intelligence to contribute to the universe on a scale meaningful enough to make a binary distinction between the instrumental value of the average human vs the average cow.</p>\n<p class=\"p2\">2. \"It's bad for humans.\" The scale on which we eat meat is demonstrably unhealthy, wasteful and recent (arising in Western culture in the last hundred years). The way Westerners eat in general is unhealthy and meat is just a part of that, but it's a significant factor.</p>\n<p class=\"p2\">3. \"It's bad for the environment (which is bad for both human and non-human animals).\" Massive amounts of cows require massive amounts of grain, which require unsustainable agriculture which damages the soil. The cows themselves are a major pollution. (Edit: removed an attention grabbing fact that may or may not have been strictly true but I'm not currently prepared to defend)</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\">Now, there are some legitimate counterarguments against strict vegetarianism. It's not necessary to be a pure vegetarian for health or environmental reasons. I do not object to free range farms that provide their animals with a decent life and painless death. I am fine with hunting. (In fact, until a super-AI somehow rewrites the rules of the ecosystem, hunting certain animals is necessary since humans have eliminated the natural predators). On top of all that,&nbsp; animal cruelty is only one of a million problems facing the world, factoring farming is only one of its causes, and dealing with it takes effort. You could be spending that effort dealing with one of the other 999,999 kinds of injustice that the world faces. And if that is your choice, after having given serious consideration to the issue, I understand.&nbsp;</p>\n<p class=\"p2\">I actually eat meat approximately once a month, for each of the above reasons. Western Society makes it difficult to live perfectly, and once-a-month turns out to be approximately how often I fail to live up to my ideals. My end goal for food consumption is to derive my meat, eggs and dairy products from ethical sources, after which I'll consider it \"good enough\" (i.e. diminishing returns of effort vs improving-the-world) and move on to another area of self improvement.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q9ASuEEoJWxT3RLMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kq6keonexJ5zLGK6o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 38, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "4472", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 179, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T04:58:37.892Z", "modifiedAt": null, "url": null, "title": "What is Cryptographically Possible", "slug": "what-is-cryptographically-possible", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:17.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PxMSnEPFG34o9zkq4/what-is-cryptographically-possible", "pageUrlRelative": "/posts/PxMSnEPFG34o9zkq4/what-is-cryptographically-possible", "linkUrl": "https://www.lesswrong.com/posts/PxMSnEPFG34o9zkq4/what-is-cryptographically-possible", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20Cryptographically%20Possible&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20Cryptographically%20Possible%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPxMSnEPFG34o9zkq4%2Fwhat-is-cryptographically-possible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20Cryptographically%20Possible%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPxMSnEPFG34o9zkq4%2Fwhat-is-cryptographically-possible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPxMSnEPFG34o9zkq4%2Fwhat-is-cryptographically-possible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1583, "htmlBody": "<p>Modern computational cryptography probes what is possible in our universe, and I think the results of this exploration may be interesting to people who wouldn't normally be exposed to it.</p>\n<p>All of our algorithms will have a \"security parameter\" <em>k</em>. Our goal is to make it so that an honest participant in the algorithm needs to spend only about <em>k</em> time for each operation, while anyone trying to break the scheme needs to use a super-polynomial amount of time in <em>k</em>. These assumptions are specifically engineered such that they don't really depend on the type of computers being used.</p>\n<p>When I say that a participant has a function in mind to evaluate, we are going to imagine that this function is described by its code in some programming language. It doesn't matter which language if you are willing to accept constant factor slowdowns; you can translate from any reasonable programming language into any other.</p>\n<p>Now onto the results, stated very imprecisely and given roughly in increasing order of surprisingness. I have chosen a really random selection based on my own interests, so don't think this is an exhaustive list.</p>\n<p><strong>One-Way Function (OWF):</strong> A function is one-way if given x it is easy to compute f(x), but given f(x) it is hard to find either x or any other x' such that f(x') = f(x). For example, if I give you randomly chosen k-bit integers x, y, it is easy to compute their product x*y. But if I give you their product x*y, it is hard to recover x and y (or another pair of k-bit integers with the same product). We have many more explicit candidates for one-way functions, some of which are believed to be secure against quantum adversaries. Note that basically every other result here implies OWF, and OWF implies P != NP (so for the forseeable future we are going to have to make assumptions to do computational cryptography).</p>\n<p><strong>Pseudorandom Generator (PRG): </strong>Suppose I want to run a randomized algorithm that requires 1000000 bits of randomness, but I only have 1000 bits of randomness. A psuedorandom generator allows me to turn my 1000 bits of randomness into a 1000000 bits of psuedorandomness, and guarantees that any efficient randomized algorithm works just as often with a psuedorandom input as with a random input. More formally, a psuedorandom generator takes k random bits to k+1 psuedorandom bits in such a way that it is very difficult to distinguish its output from random. A PRG exists iff a OWF exists.</p>\n<p><strong>Private Key Cryptography: </strong>Suppose that Alice and Bob share a secret of length k, and would like to send a message so that an eavesdropper who doesn't know the secret can't understand it. If they want to send a message of length at most k, they can use a one time pad. Private key encryption allows them to send a much longer message in a way that is indecipherable to someone who doesn't know the secret. Private key cryptography is possible if a OWF exists.</p>\n<p><strong>Psuedorandom Function Family (PRF): </strong>A psuedorandom function family is a small family of functions (one for each k bit string) such that a black box for a randomly chosen function from the family looks exactly like a black box which chooses a random output independently for each input. A PRF exists iff a OWF exists.</p>\n<p><strong>Bit Commitment: </strong>If Alice and Bob meet in person, then Alice can put a message into an envelope and leave this envelope in plain view. Bob can't see the message, but if at some later time the envelope is opened Bob can guarantee that Alice wasn't able to change what was in the envelope. Bit commitment allows Alice and Bob to do the same thing when they only share a communication channel. So for example, Alice could take a proof of the Riemann Hypothesis and commit to it. If someone else were to later give a proof of the Riemann Hypothesis, she could \"open\" her commitment and reveal that she had a proof first. If she doesn't ever choose to open her commitment, then no one ever learns anything about her proof. Bit commitment is possible if a OWF exists.</p>\n<p><strong>Public Key Cryptography:</strong> Just like private key cryptography, but now Alice and Bob share no secret. They want to communicate in such a way that they both learn a secret which no eavesdropper can efficiently recover. Public key cryptography is not known to be possible if a OWF exists. We have a number of candidate schemes for public key cryptography schemes, some of which are secure against quantum adversaries. RSA is used in practice for this functionality.</p>\n<p><strong>Zero-Knowledge Proofs (ZKP): </strong>Suppose I know the solution to some hard problem, whose answer you can verify. I would like to convince you that I really do have a solution, but without giving you any other knowledge about the solution. To do this I can't just give you a static proof; we need to talk for a while. We say that an interactive proof is zero knowledge if the person verifying the proof can guess how the conversation will go before having it (if he can effectively sample from the distribution over possible transcripts of the conversation), but it will only be possible for the prover to keep up his end of the conversation if he really has a solution. ZKPs exist for NP problems if OWFs exist.</p>\n<p><strong>Non-Interactive Zero-Knowledge Proofs (NIZKP):</strong> Naively, zero-knowledge requires interaction. But we can make it non-interactive if we make use of a common random beacon. So for example, I am going to prove to you that I have a proof of the RH. In order to prove it to you, I say \"Consider the sequence of solar flares that were visible last night. Using that as our random data, here is a verification that I really have a proof of the RH.\" Now we say that a protocol is zero-knowledge if the verifier can construct a non-interactive proof for apparently random strings of their own choice, but such that the prover can construct a proof for truly random strings if and only if he really has a solution. We have some candidate schemes for NIZKPs, but I know of none which are secure against quantum adversaries.</p>\n<p><strong>Digital Signatures:</strong> Modern law uses signatures extensively. This use is predicated on the assumption that I can obtain Alice's signature of a document if and only if Alice has signed it. I very much doubt this is true of real signatures; a digital signature scheme makes the same guarantee---there is no efficient way to compute Alice's signature of any document without having someone with Alice's private key sign it. Digital signature schemes exist which are secure against classical adversaries if claw-free permutation pairs exist. I don't know what the status of digital signatures against quantum adversaries is (they exist in the random oracle model, which is generally interpreted as meaning they exist in practice). RSA can also be used for this function in practice.</p>\n<p><strong>Homomorphic Public-Key Encryption</strong>: Just like public key cryptography, but now given an encryption of a message M I can efficiently compute the encryption of any function f(M) which I can compute efficiently on unencrypted inputs. There is a candidate scheme secure against quantum adversaries, but under pretty non-standard assumptions. There are no practical implementations of this scheme, although people who care about practical implementations are working actively on it.</p>\n<p><strong>Secure Function Evaluation: </strong>Suppose Alice and Bob have their own inputs A and B, and a function f(A, B) they would like to compute. They can do it easily by sharing A, B; in fact they can do it without sharing any information about their private input except what is necessarily revealed by f(A, B). If Alice cheats at any point, then the computational effort Bob needs to exert to learn f(A, B) is at most twice the computational effort Alice needs to exert to learn anything at all about B (this is a weaker assumption than usual in cryptography, but not that bad). There are no practical implementations of this functionality except for very simple functions.</p>\n<p>And some random things I find interesting but which are generally considered less important:</p>\n<p><strong>Computationally Secure Arguments: </strong>Suppose that I am trying to prove an assertion to you. You only have a polynomial amount of time. I personally have an exponential amount of time, and I happen to also have a proof which is exponentially long. Conventional wisdom is that there is no possible way for me to prove the statement to you (you don't have time for me to tell you the whole proof---its really extraordinarily long). However, suppose you know that I only have an exponential amount of time in terms of the message size. There is a proof protocol which isn't perfectly secure, but such that faking a proof requires a super-exponential amount of time (in the random oracle model, which may correspond to a realistic cryptographic assumption in this case or may not).</p>\n<p><strong>Run-Once Programs: </strong>if I give you a program, you can copy it as much as you want and run it as often as you want. But suppose I have a special sort of hardware, which holds two messages but only gives one to the user (once the user asks for either message 0 or message 1, the hardware gives the specified message and then permanently destroys the other message). A run-once version of a program uses such hardware, and can be run once and only once. Run-once programs exist if public key encryption exists.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MhHM6Rx2b4F8tHTQk": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PxMSnEPFG34o9zkq4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 25, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "4467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T05:26:31.495Z", "modifiedAt": null, "url": null, "title": "Michio Kaku to answer questions from a Reddit thread", "slug": "michio-kaku-to-answer-questions-from-a-reddit-thread", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Document", "createdAt": "2010-02-08T04:14:47.949Z", "isAdmin": false, "displayName": "Document"}, "userId": "vaMNHjzaCGqF8yTMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QLhQibDKgNsyqsacg/michio-kaku-to-answer-questions-from-a-reddit-thread", "pageUrlRelative": "/posts/QLhQibDKgNsyqsacg/michio-kaku-to-answer-questions-from-a-reddit-thread", "linkUrl": "https://www.lesswrong.com/posts/QLhQibDKgNsyqsacg/michio-kaku-to-answer-questions-from-a-reddit-thread", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Michio%20Kaku%20to%20answer%20questions%20from%20a%20Reddit%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMichio%20Kaku%20to%20answer%20questions%20from%20a%20Reddit%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLhQibDKgNsyqsacg%2Fmichio-kaku-to-answer-questions-from-a-reddit-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Michio%20Kaku%20to%20answer%20questions%20from%20a%20Reddit%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLhQibDKgNsyqsacg%2Fmichio-kaku-to-answer-questions-from-a-reddit-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQLhQibDKgNsyqsacg%2Fmichio-kaku-to-answer-questions-from-a-reddit-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p>Thread is <a href=\"http://www.reddit.com/r/IAmA/comments/eqnvp/iama_dr_michio_kaku/\">here</a>. I'm not sure if it's relevant to Less Wrong, but he did do a short segment on the Singularity Institute on Sci-Fi Science recently (the episode was \"AI Uprising\"). (They were represented by Ben Goertzel, answering hypothetical questions about robot maids. In the context of the episode, it was a brief dead end on the way to eventually presenting mind uploading (as presented by Max Tegmark) as the true solution to AI risk. But it seems to be a highly fluffy show to begin with, so I'm not taking it (as a SIAI supporter) personally.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QLhQibDKgNsyqsacg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 6.596034334223914e-07, "legacy": true, "legacyId": "4474", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T07:07:28.922Z", "modifiedAt": null, "url": null, "title": "A Proposed Litany", "slug": "a-proposed-litany", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:06.336Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "yk5CemN7ygW6D2Jm9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5qMWF3RGTbCZGiYSc/a-proposed-litany", "pageUrlRelative": "/posts/5qMWF3RGTbCZGiYSc/a-proposed-litany", "linkUrl": "https://www.lesswrong.com/posts/5qMWF3RGTbCZGiYSc/a-proposed-litany", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Proposed%20Litany&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Proposed%20Litany%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5qMWF3RGTbCZGiYSc%2Fa-proposed-litany%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Proposed%20Litany%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5qMWF3RGTbCZGiYSc%2Fa-proposed-litany", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5qMWF3RGTbCZGiYSc%2Fa-proposed-litany", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>I was meditating on the word \"disillusionment\" the other day, and it stuck me as odd that it has such a negative connotation... doesn't being disillusioned mean that you see a truth that was previously hidden from you by a mirage of falsehood? The human-universal negative emotional response to finding out you were wrong seems counterproductive in the extreme, and I'm still working towards eliminating it from my mind. So I crafted this brief litany, and I think that with some help from the LW community it could become a useful tool for rationalists, much like the Litanies of Tarski and Gendlin. My \"first draft\" is:</p>\n<p>\"If you love truth, learn to love finding out you were wrong. If you hate illusion, learn to love disillusionment. If your emotions are not appropriate to your values, do something about it!\"</p>\n<p>What say you?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5qMWF3RGTbCZGiYSc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "4476", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T15:22:10.364Z", "modifiedAt": null, "url": null, "title": "The Fallacy of Dressing Like a Winner", "slug": "the-fallacy-of-dressing-like-a-winner-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:04.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "benelliott", "createdAt": "2010-10-24T16:54:14.159Z", "isAdmin": false, "displayName": "benelliott"}, "userId": "H4iHqStnPyuAkniAA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RoRayDswR3gGTrbeg/the-fallacy-of-dressing-like-a-winner-0", "pageUrlRelative": "/posts/RoRayDswR3gGTrbeg/the-fallacy-of-dressing-like-a-winner-0", "linkUrl": "https://www.lesswrong.com/posts/RoRayDswR3gGTrbeg/the-fallacy-of-dressing-like-a-winner-0", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fallacy%20of%20Dressing%20Like%20a%20Winner&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fallacy%20of%20Dressing%20Like%20a%20Winner%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRoRayDswR3gGTrbeg%2Fthe-fallacy-of-dressing-like-a-winner-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fallacy%20of%20Dressing%20Like%20a%20Winner%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRoRayDswR3gGTrbeg%2Fthe-fallacy-of-dressing-like-a-winner-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRoRayDswR3gGTrbeg%2Fthe-fallacy-of-dressing-like-a-winner-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1916, "htmlBody": "<p>&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Imagine you are a sprinter, and your one goal in life is to win the 100m sprint in the Olympics. Naturally, you watch the 100m sprint winners of the past in the hope that you can learn something from them, and it doesn't take you long to spot a pattern.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Every one of them can be seen wearing a gold medal around their neck. Not only is there a strong correlation, you then also examine the rules of the olympics and find that 100% of winners must wear a gold medal at some point, there is no way that someone could win and never wear a gold medal. So, naturally, you go out and buy a gold medal from a shop, put it around your neck and sit back, satisfied.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">For another example, imagine that you are now in charge of running a large oil rig. Unfortunately, some of the drilling equipment is old and rusty, and every few hours a siren goes off alerting the divers that they need to go down again and repair the damage. This is clearly not an acceptable state of affairs, so you start looking for solutions.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">You think back to a few months ago, before things got this bad, and you remember how the siren barely ever went off at all. In fact, from you knowledge of how the equipment works, the there were no problems, the siren couldn't go off. Clearly the solution the problem, is to unplug the siren.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">(I would like to apologise in advance for my total ignorance of how oil rigs actually work, I just wanted an analogy)</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Both these stories demonstrate a mistake which I call 'Dressing Like a Winner' (DLAW). The general form of the error is, person has goal of X, person observes that X reliably leads to Y, person attempts to achieve Y, then sits back, satisfied with their work. This mistake is so obviously wrong that it is pretty much non-existant in near mode, which is why the above stories seem utterly ridiculous. However, once we switch into the more abstract far mode, even the most ridiculous errors become dangerous. In the rest of this post I will point out three places where I think this error occurs.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>Changing our minds</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In a debate between two people, it is usually the case that whoever is right is unlikely to change their mind. This is not only an empirically observable correlation, but it's also intuitively obvious, would you change your mind if you were right?</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">At this point, our fallacys steps in with a simple conclusion, \"refusing to change your mind will make you right\". As we all know, this could not be further from the truth, changing your mind is the only way to become right, or any rate less wrong. I do not think this realization is unique to this community, but it is far from universal (and it is a lot harder to practice than to preach, suggesting it might still hold on in the subconcious).</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">At this point a lot of people will probably have noticed that what I am talking about bears a close resemblance to signalling, and some of you are probably thinking that that is all there is to it. While I will admit that DLAW and Signalling are easy to confuse, I do think they are seperate things., and that there is more than just ordinary signalling going on in the debate.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">One piece of evidence for this is the fact that my unwillingness to change my mind extends even to opinions I have admitted to nobody. If I was only interested in signalling surely I would want to change my mind in that case, since it would reduce the risk of being humiliated once I do state my opinion. Another reason to believe that DLAW exists is the fact that not only do debaters rarely change their minds, those that do are often criticised, sometimes quite brutally, for 'flip-flopping', rather than being praised for becoming smarter and for demonstrating that their loyalty to truth is higher than their ego.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">So I think DLAW is at work here, and since I have chosen a fairly uncontroversially bad thing to start off with, I hope you can now agree with me that it is at least slightly dangerous.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>Consistency</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">It is an accepted fact that any map which completely fits the territory would be self-consistent. I have not seen many such maps, but I will agree with the argument that they must be consistent. What I disagree with is the claim that this means we should be focusing on making our maps internally consistent, and that once we have done this we can sit back because our work is done.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">This idea is so widely accepted and so tempting, especially to those with a mathematical bent, that I believed it for years before noticing the fallacy that lead to it. Most reasonably intelligent people have gotten over one half of the toxic meme, in that few of them believe consistency is good enough (with the one exception of ethics, where it still seems to apply in full force). However, as with the gold medal, not only is it a mistake to be satisfied with it, but it is a waste of time to aim for it in the first place.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In Robin Hanson's article (beware consistency) [http://www.overcomingbias.com/2010/11/beware-consistency.html] we see that the consistent subjects actually do worse than the inconsistent ones, because they are consistently impatient or consistently risk averse. I think this problem is even more general than his article suggests, and represents a serious flaw in our whole epistemology, dating back to the Ancient Greek era.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Suppose that one day I notice an inconsistency in my own beliefs. Conventional wisdom would tell me that this is a serious problem, and I should discard one of the beliefs as quickly. All else being equal, the belief that gets discarded will probably be the one I am less attached to, which will probably be the one I acquired more recently, which is probably the one which is actually correct, since the other may well date back to long before I knew how to think critically about an idea.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Richard Dawkins gives a good example of this in his book 'The God Delusion'. Kurt Wise, a brilliant young geologist raised as a fundementalist Christian. Realising the contradiction between his beliefs, he took a pair of scissors to the bible and cut out every passage he would have to reject if he accepted the scientific world-view. After realizing his bible was left with so few pages that the poor book could barely hold itself together, he decided to abandon science entirely. Dawkins uses this to make an argument for why religion needs to be removed entirely, and I cannot neccessarily say I disagree with him, but I think a second moral can be drawn from this story.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">How much better off would Kurt have been if he had just shrugged his shoulders at the contradiction and continued to believe both? How much worse off we be if Robert Aumann had abandoned the study of Rationality when he noticed it contradicted Orthodox Judaism? Its easy to say that Kurt was right to abandon one belief, he just abandoned the wrong one, but from inside Kurt's mind I'm not sure it was obvious to him which belief was right.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">I think a better policy for dealing with contradictions is to put both beliefs 'on notice', be cautious before acting upon either of them and wait for more evidence to decide between them. If nothing else, we should admit more than two possibilities, they could actually be compatible, or they could both be wrong, or one or both of them could be badly confused.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">To put this in one sentence \"don't strive for consistency, strive for accuracy and consistency will follow\".</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>Mathematical arguments about rationality</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In this community, I often see mathematical proofs that a perfect Bayesian would do something. These proofs are interesting from a mathematical perspective, but since I have never met a perfect Bayesian I am sceptical of their relevance to the real world (perhaps they are useful to AI, someone more experienced than me should either confirm or deny that).</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">The problem comes when we are told that since a perfect Bayesian would do X, then we imperfect Bayesians should do X as well in order to better ourselves. A good example of this is Aumann's Agreement Theorem, which shows that not agreeing to disagree is a consequence of perfect rationality, being treated as an argument for not agreeing to disagree in our quest for better rationality. The fallacy is hopefully clear by now, we have been given no reason to believe that copying this particular by-product of success will bring us closer to our goal. Indeed, in our world of imperfect rationalists, some of whom are far more imperfect than others, an argument against disagreement seems like a very dangerous thing.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Elizer has (already)[http://lesswrong.com/lw/gr/the_modesty_argument/] argued against this specific mistake, but since he went on to (commit it)[http://lesswrong.com/lw/i5/bayesian_judo] a few articles later I think it bears mentioning again.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Another example of this mistake is (this post)[http://lesswrong.com/lw/26y/rationality_quotes_may_2010/36y9] (my apologies to Oscar Cunningham, this is not meant as an attack, you just provided a very good example of what I am talking about). The post provides a mathematical argument (a model rather than a proof) that we should be more sceptical of evidence that goes against our beliefs than evidence for them. To be more exact, it gives an argument why a perfect Bayesian, with no human bias and mathematically precise calibration should be more sceptical of evidence going against its beliefs than evidence for them.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">The argument is, as far as I can tell, mathematically flawless. However, it doesn't seem to apply to me at all, if for no other reason than that I already have a massive bias overdoing that job, and my role is to counteract it.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In fact, I would say that in general our willingness to give numerical estimates is an example of this fallacy. The Cox theorems prove that any perfect reasoning system is isomorphic to Bayesian probability, but since my reasoning system is not perfect, I get the feeling that saying \"80%\" instead of \"reasonably confident\" is just making a mockery of the whole process.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">This is not to say I totally reject the relevance of mathematical models and proofs to our pursuit. All else being equal if a perfect Bayesian does X. it is evidence that X is good for an imperfect Bayesian. It's just not overwhelmingly strong evidence, and shouldn't be treated as putting as if it puts a stop to all debate and decides the issue one way or the other (unlike other fields where mathematical arguments can do this).</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>How to avoid it</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">I don't think DLAW is particularly insidious as mistakes go, which is why I called it a fallacy rather than a bias. The only advice I would give is to be careful when operating in far mode (which you should do anyway), and always make sure the causal link between your actions and your goals is pointing in the right direction.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>Note</strong> &ndash; When I first started planning this article I was hoping for more down-to-earth examples, but I struggled to find any. My current theory is that this fallacy is too obviously stupid to be committed in near mode, but if someone has a good example of DLAW occurring in their everyday life then please point it out in the comments. Just be careful that it is actually this rather than just signalling.</span></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RoRayDswR3gGTrbeg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "4480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T19:29:48.248Z", "modifiedAt": null, "url": null, "title": "EPR experiment in Good And Real?", "slug": "epr-experiment-in-good-and-real", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:04.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mDyHuYFjvsrHbahv3/epr-experiment-in-good-and-real", "pageUrlRelative": "/posts/mDyHuYFjvsrHbahv3/epr-experiment-in-good-and-real", "linkUrl": "https://www.lesswrong.com/posts/mDyHuYFjvsrHbahv3/epr-experiment-in-good-and-real", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20EPR%20experiment%20in%20Good%20And%20Real%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEPR%20experiment%20in%20Good%20And%20Real%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmDyHuYFjvsrHbahv3%2Fepr-experiment-in-good-and-real%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=EPR%20experiment%20in%20Good%20And%20Real%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmDyHuYFjvsrHbahv3%2Fepr-experiment-in-good-and-real", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmDyHuYFjvsrHbahv3%2Fepr-experiment-in-good-and-real", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>Has anyone worked out the EPR experiment example in Chapter 4 of Eric Drescher's <em>Good and Real</em>? I can't seem to make it work out right.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mDyHuYFjvsrHbahv3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 1, "extendedScore": null, "score": 6.598156465279401e-07, "legacy": true, "legacyId": "4481", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T20:16:49.138Z", "modifiedAt": null, "url": null, "title": "Efficient Charity: Do Unto Others...", "slug": "efficient-charity-do-unto-others-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:04.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6mRv7Cr57AJAtRFHv/efficient-charity-do-unto-others-0", "pageUrlRelative": "/posts/6mRv7Cr57AJAtRFHv/efficient-charity-do-unto-others-0", "linkUrl": "https://www.lesswrong.com/posts/6mRv7Cr57AJAtRFHv/efficient-charity-do-unto-others-0", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Efficient%20Charity%3A%20Do%20Unto%20Others...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEfficient%20Charity%3A%20Do%20Unto%20Others...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6mRv7Cr57AJAtRFHv%2Fefficient-charity-do-unto-others-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Efficient%20Charity%3A%20Do%20Unto%20Others...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6mRv7Cr57AJAtRFHv%2Fefficient-charity-do-unto-others-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6mRv7Cr57AJAtRFHv%2Fefficient-charity-do-unto-others-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1895, "htmlBody": "<p><em>This was originally posted as part of the <a href=\"/r/discussion/lw/35o/100_for_the_best_article_on_efficient_charity/\">efficient charity</a> contest back in November. Thanks to Roko, multifoliaterose, Louie, jmmcd, jsalvatier, and others I forget for help, corrections, encouragement, and bothering me until I finally remembered to post this here.</em><br /><br />Imagine you are setting out on a dangerous expedition through the Arctic on a limited budget. The grizzled old prospector at the general store shakes his head sadly: you can't afford everything you need; you'll just have to purchase the bare essentials and hope you get lucky. But what is essential? Should you buy the warmest parka, if it means you can't afford a sleeping bag? Should you bring an extra week's food, just in case, even if it means going without a rifle? Or can you buy the rifle, leave the food, and hunt for your dinner?<br /><br />And how about the field guide to Arctic flowers? You like flowers, and you'd hate to feel like you're failing to appreciate the harsh yet delicate environment around you. And a digital camera, of course - if you make it back alive, you'll have to put the Arctic expedition pics up on Facebook. And a hand-crafted scarf with authentic Inuit tribal patterns woven from organic fibres! Wicked!<br /><br />...but of course buying any of those items would be insane. The problem is what economists call opportunity costs: buying one thing costs money that could be used to buy others. A hand-crafted designer scarf might have some value in the Arctic, but it would cost so much it would prevent you from buying much more important things. And when your life is on the line, things like impressing your friends and buying organic pale in comparison. You have one goal - staying alive - and your only problem is how to distribute your resources to keep your chances as high as possible. These sorts of economics concepts are natural enough when faced with a journey through the freezing tundra.</p>\n<p><a id=\"more\"></a><br />But they are decidedly not natural when facing a decision about charitable giving. Most donors say they want to \"help people\". If that's true, they should try to distribute their resources to help people as much as possible. Most people don't. In the <a href=\"http://www.artfund.org/savebluerigi/Introduction.html\">\"Buy A Brushstroke\"</a> campaign, eleven thousand British donors gave a total of &pound;550,000 to keep the famous painting \"Blue Rigi\" in a UK museum. If they had given that &pound;550,000 to buy better sanitation systems in African villages instead, the latest statistics suggest it would have saved the lives of about one thousand two hundred people from disease. Each individual $50 donation could have given a year of normal life back to a Third Worlder afflicted with a disabling condition like blindness or limb deformity..<br /><br />Most of those 11,000 donors genuinely wanted to help people by preserving access to the original canvas of a beautiful painting. And most of those 11,000 donors, if you asked, would say that a thousand people's lives are more important than a beautiful painting, original or no. But these people didn't have the proper mental habits to realize that was the choice before them, and so a beautiful painting remains in a British museum and somewhere in the Third World a thousand people are dead.<br /><br />If you are to \"love your neighbor as yourself\", then you should be as careful in maximizing the benefit to others when donating to charity as you would be in maximizing the benefit to yourself when choosing purchases for a polar trek. And if you wouldn't buy a pretty picture to hang on your sled in preference to a parka, you should consider not helping save a famous painting in preference to helping save a thousand lives.<br /><br />Not all charitable choices are as simple as that one, but many charitable choices do have right answers. GiveWell.org, a site which collects and interprets data on the effectiveness of charities, predicts that antimalarial drugs save one child from malaria per $5,000 worth of bed medicine, but insecticide-treated bed nets save one child from malaria per $500 worth of drugs. If you want to save children, donating bed nets instead of antimalarial drugs is the objectively right answer, the same way buying a $500 TV instead of an identical TV that costs $5,000 is the right answer. And since saving a child from diarrheal disease costs $5,000, donating to an organization fighting malaria instead of an organization fighting diarrhea is the right answer, unless you are donating based on some criteria other than whether you're helping children or not.<br /><br />Say all of the best Arctic explorers agree that the three most important things for surviving in the Arctic are good boots, a good coat, and good food. Perhaps they have run highly unethical studies in which they release thousands of people into the Arctic with different combination of gear, and consistently find that only the ones with good boots, coats, and food survive. Then there is only one best answer to the question \"What gear do I buy if I want to survive\" - good boots, good food, and a good coat. Your preferences are irrelevant; you may choose to go with alternate gear, but only if you don't mind dying.<br /><br />And likewise, there is only one best charity: the one that helps the most people the greatest amount per dollar. This is vague, and it is up to you to decide whether a charity that raises forty children's marks by one letter grade for $100 helps people more or less than one that prevents one fatal case of tuberculosis per $100 or one that saves twenty acres of rainforest per $100. But you cannot abdicate the decision, or you risk ending up like the 11,000 people who accidentally decided that a pretty picture was worth more than a thousand people's lives.<br /><br />Deciding which charity is the best is hard. It may be straightforward to say that one form of antimalarial therapy is more effective than another. But how do both compare to financing medical research that might or might not develop a \"magic bullet\" cure for malaria? Or financing development of a new kind of supercomputer that might speed up all medical research? There is no easy answer, but the question has to be asked.<br /><br />What about just comparing charities on overhead costs, the one easy-to-find statistic that's universally applicable across all organizations? This solution is simple, elegant, and wrong. High overhead costs are only one possible failure mode for a charity. Consider again the Arctic explorer, trying to decide between a $200 parka and a $200 digital camera. Perhaps a parka only cost $100 to make and the manufacturer takes $100 profit, but the camera cost $200 to make and the manufacturer is selling it at cost. This speaks in favor of the moral qualities of the camera manufacturer, but given the choice the explorer should still buy the parka. The camera does something useless very efficiently, the parka does something vital inefficiently. A parka sold at cost would be best, but in its absence the explorer shouldn't hesitate to choose the the parka over the camera. The same applies to charity. An antimalarial net charity that saves one life per $500 with 50% overhead is better than an antidiarrheal drug charity that saves one life per $5000 with 0% overhead: $10,000 donated to the high-overhead charity will save ten lives; $10,000 to the lower-overhead will only save two. Here the right answer is to donate to the antimalarial charity while encouraging it to find ways to lower its overhead. In any case, looking for low overhead is helpful but not enough to answer the \"which is the best charity?\" question.<br /><br />Just as there is only one best charity, there is only one best way to donate to that charity. Whether you <a href=\"/lw/65/money_the_unit_of_caring/\">volunteer versus donate money</a> versus raise awareness is your own choice, but that choice has consequences. If a high-powered lawyer who makes $1,000 an hour chooses to take an hour off to help clean up litter on the beach, he's wasted the opportunity to work overtime that day, make $1,000, donate to a charity that will hire a hundred poor people for $10/hour to clean up litter, and end up with a hundred times more litter removed. If he went to the beach because he wanted the sunlight and the fresh air and the warm feeling of personally contributing to something, that's fine. If he actually wanted to help people by beautifying the beach, he's chosen an objectively wrong way to go about it. And if he wanted to help people, period, he's chosen a very wrong way to go about it, since that $1,000 could save two people from malaria. Unless the litter he removed is really worth more than two people's lives to him, he's erring even according to his own value system.<br /><br />...and the same is true if his philanthropy leads him to work full-time at a nonprofit instead of going to law school to become a lawyer who makes $1,000 / hour in the first place. Unless it's one HELL of a nonprofit.<br /><br />The Roman historian Sallust said of Cato \"He preferred to be good, rather than to seem so\". The lawyer who quits a high-powered law firm to work at a nonprofit organization certainly seems like a good person. But if we define \"good\" as helping people, then the lawyer who stays at his law firm but donates the profit to charity is taking Cato's path of maximizing how much good he does, rather than how good he looks.<br /><br />And this dichotomy between being and seeming good applies not only to looking good to others, but to ourselves. When we donate to charity, one incentive is the <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">warm glow of a job well done</a>. A lawyer who spends his day picking up litter will feel a sense of personal connection to his sacrifice and relive the memory of how nice he is every time he and his friends return to that beach. A lawyer who works overtime and donates the money online to starving orphans in Romania may never get that same warm glow. But concern with a warm glow is, at root, concern about seeming good rather than being good - albeit seeming good to yourself rather than to others. There's nothing wrong with donating to charity as a form of entertainment if it's what you want - giving money to the Art Fund may well be a quicker way to give yourself a warm feeling than seeing a romantic comedy at the cinema - but charity given by people who genuinely want to be good and not just to feel that way requires more forethought.<br /><br />It is important to be rational about charity for the same reason it is important to be rational about Arctic exploration: it requires the same awareness of opportunity costs and the same hard-headed commitment to investigating efficient use of resources, and it may well be a matter of life and death. Consider going to <a href=\"http://www.givewell.org/\">www.GiveWell.org</a> and making use of the excellent resources on effective charity they have available.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6mRv7Cr57AJAtRFHv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "4482", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4amcyxad5bnBR9Afm", "ZpDnRCeef2CLEFeKM", "3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-24T21:26:10.519Z", "modifiedAt": null, "url": null, "title": "Efficient Charity: Do Unto Others...", "slug": "efficient-charity-do-unto-others", "viewCount": null, "lastCommentedAt": "2016-02-01T00:51:46.108Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pC47ZTsPNAkjavkXs/efficient-charity-do-unto-others", "pageUrlRelative": "/posts/pC47ZTsPNAkjavkXs/efficient-charity-do-unto-others", "linkUrl": "https://www.lesswrong.com/posts/pC47ZTsPNAkjavkXs/efficient-charity-do-unto-others", "postedAtFormatted": "Friday, December 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Efficient%20Charity%3A%20Do%20Unto%20Others...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEfficient%20Charity%3A%20Do%20Unto%20Others...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpC47ZTsPNAkjavkXs%2Fefficient-charity-do-unto-others%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Efficient%20Charity%3A%20Do%20Unto%20Others...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpC47ZTsPNAkjavkXs%2Fefficient-charity-do-unto-others", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpC47ZTsPNAkjavkXs%2Fefficient-charity-do-unto-others", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1897, "htmlBody": "<p><em>This was originally posted as part of the <a href=\"/lw/35o/100_for_the_best_article_on_efficient_charity/\">efficient charity</a> contest back in November. Thanks to Roko, multifoliaterose, Louie, jmmcd, jsalvatier, and others I forget for help, corrections, encouragement, and bothering me until I finally remembered to post this here.</em><br /><br />Imagine you are setting out on a dangerous expedition through the Arctic on a limited budget. The grizzled old prospector at the general store shakes his head sadly: you can't afford everything you need; you'll just have to purchase the bare essentials and hope you get lucky. But what is essential? Should you buy the warmest parka, if it means you can't afford a sleeping bag? Should you bring an extra week's food, just in case, even if it means going without a rifle? Or can you buy the rifle, leave the food, and hunt for your dinner?<br /><br />And how about the field guide to Arctic flowers? You like flowers, and you'd hate to feel like you're failing to appreciate the harsh yet delicate environment around you. And a digital camera, of course - if you make it back alive, you'll have to put the Arctic expedition pics up on Facebook. And a hand-crafted scarf with authentic Inuit tribal patterns woven from organic fibres! Wicked!<br /><br />...but of course buying any of those items would be insane. The problem is what economists call opportunity costs: buying one thing costs money that could be used to buy others. A hand-crafted designer scarf might have some value in the Arctic, but it would cost so much it would prevent you from buying much more important things. And when your life is on the line, things like impressing your friends and buying organic pale in comparison. You have one goal - staying alive - and your only problem is how to distribute your resources to keep your chances as high as possible. These sorts of economics concepts are natural enough when faced with a journey through the freezing tundra.</p>\n<p><a id=\"more\"></a><br />But they are decidedly not natural when facing a decision about charitable giving. Most donors say they want to \"help people\". If that's true, they should try to distribute their resources to help people as much as possible. Most people don't. In the <a href=\"http://www.artfund.org/savebluerigi/Introduction.html\">\"Buy A Brushstroke\"</a> campaign, eleven thousand British donors gave a total of &pound;550,000 to keep the famous painting \"Blue Rigi\" in a UK museum. If they had given that &pound;550,000 to buy better sanitation systems in African villages instead, the latest statistics suggest it would have saved the lives of about one thousand two hundred people from disease. Each individual $50 donation could have given a year of normal life back to a Third Worlder afflicted with a disabling condition like blindness or limb deformity..<br /><br />Most of those 11,000 donors genuinely wanted to help people by preserving access to the original canvas of a beautiful painting. And most of those 11,000 donors, if you asked, would say that a thousand people's lives are more important than a beautiful painting, original or no. But these people didn't have the proper mental habits to realize that was the choice before them, and so a beautiful painting remains in a British museum and somewhere in the Third World a thousand people are dead.<br /><br />If you are to \"love your neighbor as yourself\", then you should be as careful in maximizing the benefit to others when donating to charity as you would be in maximizing the benefit to yourself when choosing purchases for a polar trek. And if you wouldn't buy a pretty picture to hang on your sled in preference to a parka, you should consider not helping save a famous painting in preference to helping save a thousand lives.<br /><br />Not all charitable choices are as simple as that one, but many charitable choices do have right answers. GiveWell.org, a site which collects and interprets data on the effectiveness of charities, predicts that antimalarial drugs save one child from malaria per $5,000 worth of medicine, but insecticide-treated bed nets save one child from malaria per $500 worth of netting. If you want to save children, donating bed nets instead of antimalarial drugs is the objectively right answer, the same way buying a $500 TV instead of an identical TV that costs $5,000 is the right answer. And since saving a child from diarrheal disease costs $5,000, donating to an organization fighting malaria instead of an organization fighting diarrhea is the right answer, unless you are donating based on some criteria other than whether you're helping children or not.<br /><br />Say all of the best Arctic explorers agree that the three most important things for surviving in the Arctic are good boots, a good coat, and good food. Perhaps they have run highly unethical studies in which they release thousands of people into the Arctic with different combination of gear, and consistently find that only the ones with good boots, coats, and food survive. Then there is only one best answer to the question \"What gear do I buy if I want to survive\" - good boots, good food, and a good coat. Your preferences are irrelevant; you may choose to go with alternate gear, but only if you don't mind dying.<br /><br />And likewise, there is only one best charity: the one that helps the most people the greatest amount per dollar. This is vague, and it is up to you to decide whether a charity that raises forty children's marks by one letter grade for $100 helps people more or less than one that prevents one fatal case of tuberculosis per $100 or one that saves twenty acres of rainforest per $100. But you cannot abdicate the decision, or you risk ending up like the 11,000 people who accidentally decided that a pretty picture was worth more than a thousand people's lives.<br /><br />Deciding which charity is the best is hard. It may be straightforward to say that one form of antimalarial therapy is more effective than another. But how do both compare to financing medical research that might or might not develop a \"magic bullet\" cure for malaria? Or financing development of a new kind of supercomputer that might speed up all medical research? There is no easy answer, but the question has to be asked.<br /><br />What about just comparing charities on overhead costs, the one easy-to-find statistic that's universally applicable across all organizations? This solution is simple, elegant, and wrong. High overhead costs are only one possible failure mode for a charity. Consider again the Arctic explorer, trying to decide between a $200 parka and a $200 digital camera. Perhaps a parka only cost $100 to make and the manufacturer takes $100 profit, but the camera cost $200 to make and the manufacturer is selling it at cost. This speaks in favor of the moral qualities of the camera manufacturer, but given the choice the explorer should still buy the parka. The camera does something useless very efficiently, the parka does something vital inefficiently. A parka sold at cost would be best, but in its absence the explorer shouldn't hesitate to choose the the parka over the camera. The same applies to charity. An antimalarial net charity that saves one life per $500 with 50% overhead is better than an antidiarrheal drug charity that saves one life per $5000 with 0% overhead: $10,000 donated to the high-overhead charity will save ten lives; $10,000 to the lower-overhead will only save two. Here the right answer is to donate to the antimalarial charity while encouraging it to find ways to lower its overhead. In any case, <a href=\"http://www.charitynavigator.org/\">examining the financial practices of a charity</a> is helpful but not enough to answer the \"which is the best charity?\" question.<br /><br />Just as there is only one best charity, there is only one best way to donate to that charity. Whether you <a href=\"/lw/65/money_the_unit_of_caring/\">volunteer versus donate money</a> versus raise awareness is your own choice, but that choice has consequences. If a high-powered lawyer who makes $1,000 an hour chooses to take an hour off to help clean up litter on the beach, he's wasted the opportunity to work overtime that day, make $1,000, donate to a charity that will hire a hundred poor people for $10/hour to clean up litter, and end up with a hundred times more litter removed. If he went to the beach because he wanted the sunlight and the fresh air and the warm feeling of personally contributing to something, that's fine. If he actually wanted to help people by beautifying the beach, he's chosen an objectively wrong way to go about it. And if he wanted to help people, period, he's chosen a very wrong way to go about it, since that $1,000 could save two people from malaria. Unless the litter he removed is really worth more than two people's lives to him, he's erring even according to his own value system.<br /><br />...and the same is true if his philanthropy leads him to work full-time at a nonprofit instead of going to law school to become a lawyer who makes $1,000 / hour in the first place. Unless it's one HELL of a nonprofit.<br /><br />The Roman historian Sallust said of Cato \"He preferred to be good, rather than to seem so\". The lawyer who quits a high-powered law firm to work at a nonprofit organization certainly seems like a good person. But if we define \"good\" as helping people, then the lawyer who stays at his law firm but donates the profit to charity is taking Cato's path of maximizing how much good he does, rather than how good he looks.<br /><br />And this dichotomy between being and seeming good applies not only to looking good to others, but to ourselves. When we donate to charity, one incentive is the <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">warm glow of a job well done</a>. A lawyer who spends his day picking up litter will feel a sense of personal connection to his sacrifice and relive the memory of how nice he is every time he and his friends return to that beach. A lawyer who works overtime and donates the money online to starving orphans in Romania may never get that same warm glow. But concern with a warm glow is, at root, concern about seeming good rather than being good - albeit seeming good to yourself rather than to others. There's nothing wrong with donating to charity as a form of entertainment if it's what you want - giving money to the Art Fund may well be a quicker way to give yourself a warm feeling than seeing a romantic comedy at the cinema - but charity given by people who genuinely want to be good and not just to feel that way requires more forethought.<br /><br />It is important to be rational about charity for the same reason it is important to be rational about Arctic exploration: it requires the same awareness of opportunity costs and the same hard-headed commitment to investigating efficient use of resources, and it may well be a matter of life and death. Consider going to <a href=\"http://www.givewell.org\">www.GiveWell.org</a> and making use of the excellent resources on effective charity they have available.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 11, "JsJPrdgRGRqnci8cZ": 2, "EeSkeTcT4wtW2fWsL": 2, "9DmA84e4ZvYoYu6q8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pC47ZTsPNAkjavkXs", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 156, "baseScore": 190, "extendedScore": null, "score": 0.000362, "legacy": true, "legacyId": "4483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 190, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 323, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4amcyxad5bnBR9Afm", "ZpDnRCeef2CLEFeKM", "3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 16, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-12-24T21:26:10.519Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-25T09:57:01.365Z", "modifiedAt": null, "url": null, "title": "Help Me Plan My Education?", "slug": "help-me-plan-my-education", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.872Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EchoingHorror", "createdAt": "2010-07-26T17:50:36.699Z", "isAdmin": false, "displayName": "EchoingHorror"}, "userId": "H4ZkBdPXzPt9a9NZb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YxWxPPSjBWLSmsLek/help-me-plan-my-education", "pageUrlRelative": "/posts/YxWxPPSjBWLSmsLek/help-me-plan-my-education", "linkUrl": "https://www.lesswrong.com/posts/YxWxPPSjBWLSmsLek/help-me-plan-my-education", "postedAtFormatted": "Saturday, December 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20Me%20Plan%20My%20Education%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20Me%20Plan%20My%20Education%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxWxPPSjBWLSmsLek%2Fhelp-me-plan-my-education%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20Me%20Plan%20My%20Education%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxWxPPSjBWLSmsLek%2Fhelp-me-plan-my-education", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxWxPPSjBWLSmsLek%2Fhelp-me-plan-my-education", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 589, "htmlBody": "<p>I'm planning independent studies and choosing a concentration for my bachelor's degree, so I'm looking for shiny things on which I can base the next year and a half of my life. I get to do <em>all</em>&nbsp;independent studies for the 3 semesters worth of credits remaining, and I'm pretty happy about that.&nbsp;<strong>So it shall be, that the wheel of akrasia shall turn, and what was once procrastination shall be productive.</strong> And things that were productive but got in the way of unnecessary coursework shall be double productive, maybe triple.</p>\n<p>I'm looking for good ideas or texts to base classes around. I feel like there was recently a relevant discussion on texts, but couldn't find it (and feel like an idiot posting a help-the-noob related article after failing to find recent ones. Links to them are appreciated). Are there others in the same spirit as <a href=\"/lw/jv/recommended_rationalist_reading/\">these</a>&nbsp;(old post)? What should I prioritize, given that Less Wrong has been my first external source of rationality?</p>\n<p>Then there are other marginally less shiny, but still reflective subjects like economics, computer programming, any of the sciences that engineers would care about plus quantum mechanics, things most people reading this think matter. Awesomeology. My problem with this is prioritizing. I have about 12 classes worth of independent study to get through, minus any credits from&nbsp;equivalence&nbsp;tests I may take for commonly tested things. There's a right answer to the extent that filling in the blank of a \"B.S. in Science, Mathematics, and Technology with a Concentration in ____\" with something in particular matters, since 6 classes have to be directly related to the concentration, but I don't know how much that will actually ever matter. <a href=\"http://en.wikipedia.org/wiki/Dr._Horrible's_Sing-Along_Blog\">Horribleness</a> would make a great concentration, but I don't want to rigorously quantify human suffering enough to do it just for that novelty. It also <em>might</em> help if my degree sounds real. Something about probability or statistics would be reasonable and Bayes would approve, but I want the name of my degree to <em>pop</em>, since I get to name it. Is that wrong? Am I overthinking this?</p>\n<p>I might not be cool enough to pull off my best-sounding idea for a concentration, in Cybernetic Heuristics, but it has an elegant meaning worthy of study and googling it in quotes returns no results. By \"best-sounding\", I mean that people who don't know and can't be bothered to look the words up will think I'm from the future. There's a chance my utility function is broken, but I <em>think</em> that's an important thing to look for when choosing a degree.</p>\n<p>&nbsp;</p>\n<p>Thoughts? Ideal curricula? Focuses for how I should spend my time? Suggested readings substantial enough to make a course? Scratch that--none of the required classes have content anyway. Really, there's nothing I can't do with this, but I don't know what I should do with this, and would much rather do correct things I wouldn't think of than incorrect things I would do on my own, so asking is a good idea. If it matters, assume I have no interests or aspirations that don't coincide with practicality. Because I shouldn't. Those suck.</p>\n<p>What I need are fun things I can turn into independent studies to make my life awesome and a concentration for my degree. Suggestions for extracurricular activities will also be helpful, but I've got to say upfront that I don't know what I could do with the Campus Crusade for Bayes with an online campus. That's like...this.</p>\n<p>All advice, recommendations and musings will be greatly appreciated, even if they're not serious and were given out of spite.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YxWxPPSjBWLSmsLek", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "4491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RiQYixgCdvd8eWsjg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-25T14:24:40.784Z", "modifiedAt": null, "url": null, "title": "A Christmas topic: I have thoughts regarding Chanukah and need logic help from Atheists", "slug": "a-christmas-topic-i-have-thoughts-regarding-chanukah-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:05.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Marius", "createdAt": "2010-12-16T19:04:55.782Z", "isAdmin": false, "displayName": "Marius"}, "userId": "y2mqAAEmFDjPNXS3w", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jLtddbpe79rztp9By/a-christmas-topic-i-have-thoughts-regarding-chanukah-and", "pageUrlRelative": "/posts/jLtddbpe79rztp9By/a-christmas-topic-i-have-thoughts-regarding-chanukah-and", "linkUrl": "https://www.lesswrong.com/posts/jLtddbpe79rztp9By/a-christmas-topic-i-have-thoughts-regarding-chanukah-and", "postedAtFormatted": "Saturday, December 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Christmas%20topic%3A%20I%20have%20thoughts%20regarding%20Chanukah%20and%20need%20logic%20help%20from%20Atheists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Christmas%20topic%3A%20I%20have%20thoughts%20regarding%20Chanukah%20and%20need%20logic%20help%20from%20Atheists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjLtddbpe79rztp9By%2Fa-christmas-topic-i-have-thoughts-regarding-chanukah-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Christmas%20topic%3A%20I%20have%20thoughts%20regarding%20Chanukah%20and%20need%20logic%20help%20from%20Atheists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjLtddbpe79rztp9By%2Fa-christmas-topic-i-have-thoughts-regarding-chanukah-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjLtddbpe79rztp9By%2Fa-christmas-topic-i-have-thoughts-regarding-chanukah-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 946, "htmlBody": "<p>Essentially, I want to make sure my logic is sound, from the point of view of smart rational people who do not believe in the existence of supernatural miracles.</p>\n<p>The Chanukah story: 175-134 BCE.&nbsp; Hellenic Assyrians (Antiochus IV)&nbsp;had conquered Israel, and passed a variety of laws oppressing the freedom of worship of the Jews there.&nbsp; They defiled the Temple and forbade the study of sacred texts.&nbsp; The&nbsp;Maccabees led a Jewish revolt against the Assyrians, and eventually drove them out of Israel.&nbsp; Immediately upon retaking the Temple, they cleaned and rededicated it; they relit the sacred flame using a small vial of kosher oil and sent for more oil (which was 8 days distant).&nbsp; The small vial was expected to last only one night, but miraculously lasted 8 days until more supplies arrived.</p>\n<p>Now recently, several Reform rabbis have stated that the fact that the first surviving written record of the miracle is from the Gemara (500 CE) indicates that the miracle was invented&nbsp;around 500 CE.&nbsp; I am not an Orthodox Jew, but I do believe that the Gemara&nbsp;represents the sages writing down oral traditions, and am annoyed by the tendency among certain Reform rabbis to assume that everything was invented at the time it was written regardless of the evidence for or against it.</p>\n<p>The texts with the potential to document events follow:</p>\n<p>Maccabees 1 (~100 BCE):&nbsp;purely historical/nonreligious.&nbsp; The book was originally written in Hebrew, but that text does not survive.&nbsp; A Greek translation exists,&nbsp;and the text&nbsp;avoids all mention of religious and spiritual matters.&nbsp; For instance, it speaks briefly and euphemistically about the temple, stating that the Jews captured the \"temple hill\" and rededicated the \"citadel\", avoiding mention of the temple itself.&nbsp;</p>\n<p>Maccabees 2 (~30 BCE):&nbsp;we possess what claims to be a&nbsp;1-volume abridgement of a 5-volume original (which does not survive and is not referenced elsewhere).&nbsp; The surviving abridgement mentions the temple rededication and a variety of bizarre miracles including the public appearance of angels.&nbsp; It does not mention the miracle of the oil, however.&nbsp; The abridged text includes a number of theologic innovations which bear more similarity to Catholic beliefs than to Jewish or Protestant ones; it is unknown whether these were present in the original.&nbsp;</p>\n<p>Neither of the above are considered canonical sources by Jews or Protestant, but they are by Catholics.</p>\n<p>Megillat Taanit (7CE): a succinct list of red letter dates from 200BCE-7CE; mentions that Chanukah is 8 days but gives no descriptions of any of the listed holidays.</p>\n<p>Josephus, <em>The&nbsp;Jewish War</em> (75CE).&nbsp; Mentions that Chanukah was the Festival of Lights lasting 8 days, but does not give a reason for this.&nbsp; He says that he \"supposes\" it is because of the unexpected<br />restoration of freedom to worship.&nbsp; Elsewhere his text is extremely complete, well-researched,&nbsp;and accurate.</p>\n<p>So there are two possibilities being considered:</p>\n<p>1.&nbsp; The miracle of the oil was described by the Maccabees who rededicated the temple.&nbsp; Such an interpretation has to make the following leaps:</p>\n<p>a.&nbsp; That a text which avoids all mention of religious matters would not mention this one.&nbsp;</p>\n<p>b.&nbsp; That a text which mentions dozens of miracles would not mention this one.&nbsp; Well, it's clearly not written by a mainstream Jew because the theology is so unusual.&nbsp; The writer had to pick and choose miracles when abridging from 5 to 1 volume, and may have left out the oil one because it's less spectacular than the others.</p>\n<p>c.&nbsp; That Josephus wouldn't mention the miracle.&nbsp; But&nbsp;a&nbsp;goal of his&nbsp;in writing <em>The Jewish War</em> was to convince the Romans that the Jews could make good subjects and would not be eternally rebellious.&nbsp; Had he connected the Jewish obligation to kindle lights to the idea of the rededication of the Temple (which the Romans had just destroyed), he would have risked causing the Romans to forbid the kindling of lights; this would have increased friction and rebellion.</p>\n<p>2.&nbsp; The miracle of the oil was invented centuries later.&nbsp; Such an interpretation has the following problems:</p>\n<p>a.&nbsp; That the Jews chose 8 days to celebrate Chanukah without any particular reason.&nbsp; 8 would be a strange number, longer than other Jewish holidays [unrelatedly, an extra day would later be added to other holidays due to calendar uncertainty].&nbsp; No reason other than the oil miracle has been unearthed for this number 8.</p>\n<p>b.&nbsp; That Josephus, who is otherwise so erudite, shrugs off the reason for calling Chanukah the \"Festival of Lights\".&nbsp; His given explanation (the restoration of the freedom to worship) doesn't do much to explain light, and certainly doesn't explain the plural lights.&nbsp; Further, his saying he \"supposes\" this explanation (where he is otherwise accurate, detailed, and certain in his history) is difficult to explain unless he is deliberately avoiding giving the real explanation.&nbsp; Certainly one would expect him to give a reason for the festival's 8 day length if he felt it prudent to do so.</p>\n<p>c.&nbsp; The fact that those who dispute the standard account have no actual evidence that the sages invented the miracle, but do have a political goal in saying so.</p>\n<p>d.&nbsp; The claim rests on the supposition that the Sages wanted to reduce the political importance of Chanukah by deemphasizing the military victory and turning the miracle into a spiritual one.&nbsp; But had they wanted to do this, they could simply have abolished the celebration entirely; actually they abolished&nbsp;the celebrations listed&nbsp;in Megillat Taanit except for Chanukah.&nbsp; Why keep that&nbsp;holiday while inventing a story to reduce the associated political implications?&nbsp;&nbsp;Just to provide themselves with&nbsp;an excuse to eat jelly donuts?</p>\n<p>&nbsp;</p>\n<p>Anyway, I was wondering what atheists might believe the most plausible explanation:</p>\n<p>That the miracle of the oil lasting 8 days was invented centuries later?</p>\n<p>Or that the Maccabees somehow secured a secret stash of sacramental oil beyond the one vial they initially found?</p>\n<p>&nbsp;</p>\n<p>And, am I overemphasizing/underemphasizing the importance of anything?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jLtddbpe79rztp9By", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 0, "extendedScore": null, "score": 6.601014312353671e-07, "legacy": true, "legacyId": "4492", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-25T15:16:37.989Z", "modifiedAt": null, "url": null, "title": "Probability of becoming a human being as the moral indicator", "slug": "probability-of-becoming-a-human-being-as-the-moral-indicator", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:06.693Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "098799", "createdAt": "2010-07-08T00:10:19.036Z", "isAdmin": false, "displayName": "098799"}, "userId": "R28x8jex3pbiwg6Xd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N5x5jqxHo4P3NYumx/probability-of-becoming-a-human-being-as-the-moral-indicator", "pageUrlRelative": "/posts/N5x5jqxHo4P3NYumx/probability-of-becoming-a-human-being-as-the-moral-indicator", "linkUrl": "https://www.lesswrong.com/posts/N5x5jqxHo4P3NYumx/probability-of-becoming-a-human-being-as-the-moral-indicator", "postedAtFormatted": "Saturday, December 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20of%20becoming%20a%20human%20being%20as%20the%20moral%20indicator&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20of%20becoming%20a%20human%20being%20as%20the%20moral%20indicator%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN5x5jqxHo4P3NYumx%2Fprobability-of-becoming-a-human-being-as-the-moral-indicator%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20of%20becoming%20a%20human%20being%20as%20the%20moral%20indicator%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN5x5jqxHo4P3NYumx%2Fprobability-of-becoming-a-human-being-as-the-moral-indicator", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN5x5jqxHo4P3NYumx%2Fprobability-of-becoming-a-human-being-as-the-moral-indicator", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 383, "htmlBody": "<p>Writing articles in English seems harder than I thought. I'd appreciate anybody correcting all the spelling mistakes I've made, for this post to become readable.</p>\n<hr />\n<p>I had a long discussion with a Christian friend of mine about abortion. We both agreed to the statement that killing human beings is morally bad. Now, the definition of a human being came up.</p>\n<p>We again agreed that the moment of birth can't be considered as the beginning of a human simply because there are children born prematurely and fully functional. Yet, the chance of survival decreases as we go back in time.</p>\n<p>Now, he (obviously) stated that since we have only one significant moment in fetus' life -- namely fertilization -- this should be regarded as the moment from which we should care for the fetus as much as we care for the already born baby, which bans abortion alltogether. I disagreed. But then I'd begun to think about the topic a little more.</p>\n<p>So the fetus is in no means a fully functional human being but neither is a patient in a coma. Both lack the crucial feature that distinguish us from any other animal, namely our thinking. The chances of the awakening of a comma patient decrease in time, I believe we may plot something-like-exponential decay for this. Now, I would strongly oppose the procedure of letting a coma patient die when the chances of his awakening are, say, greater than 10%, that's for sure. But a fetus few days or weeks after conception has nowadays certainly greater chances of becoming a fully functional human than 10%.</p>\n<p>Here's where I think my argumentation is flawed. I'd definitely not kill a coma patient (not fully human being since not thinking) whose chances of becoming a human are 10% but I'd definitely kill a fetus (also not thinking and not living on his own), whose chances of becoming a human being are greater than 10%.</p>\n<p>So is it ok to judge our action by considering how big percentage of a human being are we switching off? It seems logical, but the consequences are strange.</p>\n<p>Obviously there are other arguments in favour of abortion like the fetus not having functional nervous system, being dependant on his parents and so on. But there's still this one feature: probability of becoming a human being.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N5x5jqxHo4P3NYumx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 6.601145193953126e-07, "legacy": true, "legacyId": "4493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-25T19:42:51.483Z", "modifiedAt": null, "url": null, "title": "Pascal's Gift", "slug": "pascal-s-gift", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:05.169Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bongo", "createdAt": "2009-02-27T12:08:06.258Z", "isAdmin": false, "displayName": "Bongo"}, "userId": "mLnNK3xEMczLs8ind", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p6EY4LZQPW9W9Xbp3/pascal-s-gift", "pageUrlRelative": "/posts/p6EY4LZQPW9W9Xbp3/pascal-s-gift", "linkUrl": "https://www.lesswrong.com/posts/p6EY4LZQPW9W9Xbp3/pascal-s-gift", "postedAtFormatted": "Saturday, December 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pascal's%20Gift&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APascal's%20Gift%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp6EY4LZQPW9W9Xbp3%2Fpascal-s-gift%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pascal's%20Gift%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp6EY4LZQPW9W9Xbp3%2Fpascal-s-gift", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp6EY4LZQPW9W9Xbp3%2Fpascal-s-gift", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<blockquote>\n<p>&nbsp;If Omega offered to give you 2^n utils with probability 1/n, what n would you choose?</p>\n</blockquote>\n<p>This problem was invented by Armok from #lesswrong.&nbsp;Discuss.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HNJiR8Jzafsv8cHrC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p6EY4LZQPW9W9Xbp3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 6.601815942904312e-07, "legacy": true, "legacyId": "4496", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-26T11:21:22.649Z", "modifiedAt": null, "url": null, "title": "Tallinn-Evans $125,000 Singularity Challenge", "slug": "tallinn-evans-usd125-000-singularity-challenge", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:55.459Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aqyLxWzAEpDHm2Xyf/tallinn-evans-usd125-000-singularity-challenge", "pageUrlRelative": "/posts/aqyLxWzAEpDHm2Xyf/tallinn-evans-usd125-000-singularity-challenge", "linkUrl": "https://www.lesswrong.com/posts/aqyLxWzAEpDHm2Xyf/tallinn-evans-usd125-000-singularity-challenge", "postedAtFormatted": "Sunday, December 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tallinn-Evans%20%24125%2C000%20Singularity%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATallinn-Evans%20%24125%2C000%20Singularity%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaqyLxWzAEpDHm2Xyf%2Ftallinn-evans-usd125-000-singularity-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tallinn-Evans%20%24125%2C000%20Singularity%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaqyLxWzAEpDHm2Xyf%2Ftallinn-evans-usd125-000-singularity-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaqyLxWzAEpDHm2Xyf%2Ftallinn-evans-usd125-000-singularity-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 571, "htmlBody": "<p>Michael Anissimov posted the following <a href=\"http://intelligence.org/blog/2010/12/21/announcing-the-tallinn-evans-125000-singularity-holiday-challenge/\">on the SIAI blog</a>:</p>\n<p>Thanks to the generosity of two major donors; Jaan Tallinn, a founder of Skype and Ambient Sound Investments, and Edwin Evans, CEO of the mobile applications startup Quinly, <strong>every contribution to the Singularity Institute up until January 20, 2011 will be <a href=\"http://intelligence.org/donate\">matched dollar-for-dollar</a>, up to a total of $125,000.</strong></p>\n<p>Interested in optimal philanthropy &mdash; that is, maximizing the future expected benefit to humanity per charitable dollar spent? The technological creation of greater-than-human intelligence has the potential to unleash an &ldquo;intelligence explosion&rdquo; as intelligent systems design still more sophisticated successors. This dynamic could transform our world as greatly as the advent of human intelligence has already transformed the Earth, for better or for worse. Thinking rationally about these prospects and working to encourage a favorable outcome offers an extraordinary chance to make a difference. The Singularity Institute exists to do so through its research, the Singularity Summit, and public education.</p>\n<p>We support both direct engagements with the issues as well as the improvements in methodology and rationality needed to make better progress. Through our <a href=\"http://intelligence.org/aboutus/visitingfellows\">Visiting Fellows program</a>, researchers from undergrads to Ph.Ds pursue questions on the foundations of Artificial Intelligence and related topics in two-to-three month stints. Our <a href=\"http://intelligence.org/research/residentfaculty\">Resident Faculty</a>, up to four researchers from three last year, pursues long-term projects, including AI research, a literature review, and a book on rationality, the first draft of which was just completed. Singularity Institute researchers and representatives gave over a dozen presentations at half a dozen conferences in 2010. Our Singularity Summit conference in San Francisco was a great success, bringing together over 600 attendees and 22 top scientists and other speakers to explore cutting-edge issues in technology and science.</p>\n<p>We are pleased to receive donation matching support this year from Edwin Evans of the United States, a long-time Singularity Institute donor, and Jaan Tallinn of Estonia, a more recent donor and supporter. Jaan recently gave a <a href=\"http://aaltoes.com/2010/10/jaan-tallinn-from-soviets-to-singularity/\">talk on the Singularity and his life</a> at a entrepreneurial group in Finland. Here&rsquo;s what Jaan has to say about us:</p>\n<p><em>&ldquo;We became the dominant species on this planet by being the most intelligent species around. This century we are going to cede that crown to machines. After we do that, it will be them steering history rather than us. Since we have only one shot at getting the transition right, the importance of SIAI&rsquo;s work cannot be overestimated. Not finding any organisation to take up this challenge as seriously as SIAI on my side of the planet, I conclude that it&rsquo;s worth following them across 10 time zones.&rdquo;</em><br /> &ndash; Jaan Tallinn, Singularity Institute donor</p>\n<p>Make a lasting impact on the long-term future of humanity today &mdash; make a <a href=\"http://intelligence.org/donate\">donation to the Singularity Institute</a> and help us reach our $125,000 goal. For more detailed information on our projects and work, contact us at <a href=\"mailto:institute@intelligence.org\">institute@intelligence.org</a> or read our new <a href=\"http://intelligence.org/riskintro/index.html\">organizational overview.</a></p>\n<p>-----</p>\n<p>Kaj's commentary: if you haven't done so recently, do check out the <a href=\"http://intelligence.org/research/publications\">SIAI publications page</a>. There are several new papers and presentations, out of which I thought that Carl Shulman's <a href=\"http://intelligence.org/upload/WBE-superorganisms.pdf\">Whole Brain Emulations and the Evolution of Superorganisms</a> made for particularly fascinating (and scary) reading. SIAI's finally starting to get its paper-writing machinery into gear, so let's give them money to make that possible. There's also a <a href=\"http://intelligence.org/tallinn-evans_challenge\">static page</a> about this challenge; if you're on Facebook, please take the time to \"like\" it there.</p>\n<p>(Full disclosure: I was an SIAI Visiting Fellow in April-July 2010.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "Z6DgiCrMtpSNxwuYW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aqyLxWzAEpDHm2Xyf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 38, "extendedScore": null, "score": 6.60418148844618e-07, "legacy": true, "legacyId": "4498", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 378, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-26T18:36:35.552Z", "modifiedAt": null, "url": null, "title": "Alternative Places to Get Ideas (Also, \"In Defense of Food\")", "slug": "alternative-places-to-get-ideas-also-in-defense-of-food", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NtrkqzJDTxywFEpDR/alternative-places-to-get-ideas-also-in-defense-of-food", "pageUrlRelative": "/posts/NtrkqzJDTxywFEpDR/alternative-places-to-get-ideas-also-in-defense-of-food", "linkUrl": "https://www.lesswrong.com/posts/NtrkqzJDTxywFEpDR/alternative-places-to-get-ideas-also-in-defense-of-food", "postedAtFormatted": "Sunday, December 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alternative%20Places%20to%20Get%20Ideas%20(Also%2C%20%22In%20Defense%20of%20Food%22)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlternative%20Places%20to%20Get%20Ideas%20(Also%2C%20%22In%20Defense%20of%20Food%22)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNtrkqzJDTxywFEpDR%2Falternative-places-to-get-ideas-also-in-defense-of-food%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alternative%20Places%20to%20Get%20Ideas%20(Also%2C%20%22In%20Defense%20of%20Food%22)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNtrkqzJDTxywFEpDR%2Falternative-places-to-get-ideas-also-in-defense-of-food", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNtrkqzJDTxywFEpDR%2Falternative-places-to-get-ideas-also-in-defense-of-food", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 610, "htmlBody": "<p>I discovered Less Wrong a few months ago (courtesy of Harry Potter and the Methods of Rationality), and I am extremely grateful to have such a thoughtful place to have discussions and to learn new things. But one of the more significant hazards I've become aware of is confirmation bias. And since I began coming here a lot, I find that I evaluate new information through a \"Less Wrong Lens\" which officially means \"well researched and thought out\" which is perfectly fine but also includes some new or reinforced biases.</p>\n<p>I recently realized the extent of the problem while reading \"In Defense of Food\". The basic premise (or most relevant one) of the book is that while science may one day be able to determine what is healthy on a nutrient-by-nutrient basis and let us craft whatever artificial foods we want, we are not nearly at that point yet. Every few years the prevailing beliefs of the nutritionist and scientific communities change, people scurry to catch up, and regardless, since transition from a \"traditional\" to \"scientific\" diet, certain nutrition-related diseases have gotten more prevalent, not less.</p>\n<p>His argument is that traditional diets have often had thousands of years to evolve to match the needs and available food sources of populations. So while the variables and interactions may be complex enough that we don't know why, until science DOES figure it out, individual people are better off sticking to the diets of the past. At the same time, corporations that are interested only in marketing as much food as quickly as possible benefit from constantly changing scientific attitudes. (Disclaimer: yes, I'm oversimplifying again, but my point isn't even necessarily about the merits of the book so bear with me).</p>\n<p>By the end of the book, I did agree with his basic premises. I'm not sure his solution is the single best one, but it's a large step up from the diet that the average westerner is going to have. It wasn't explicitly anti-science, just attempting to be realistic about what science can realistically accomplish, and what unique pitfalls come from giving science (or more importantly, politicians and corporations on whom scientists are dependent for funding) the position of power that religion and other cultural institutions once had.</p>\n<p>But the first half of the book does have a pretty obvious goal, not of discrediting science per se, but \"taking the wind out of science's sails\". And in the wake of reading \"Methods of Rationality\" it absolutely rankled me. I could feel my memetic immune system going into overdrive, looking for reasons not to believe whatever the man ended up having to say. I'm currently unsure whether that had to do with the way he was writing, if he did have his own ax to grind, or if it was purely my own biases coloring the words. But whatever his motivations, I'm grateful for having found the book at the time I did, because it taught me a lesson about my own mind. My answer isn't to say \"oh, science is just as flawed as everything else now,\" but I think I'll be able to approach things from a more neutral perspective.</p>\n<p>Now, my purpose of posting this is two-fold. One, is I'm simply curious if other people had read the book and had anything to say about it, one way or another. But the other is to ask: do you have any sources you turn to specifically to help broaden your horizon from the prevailing mindset at Less Wrong? Websites that are not \"anti-rational\" or \"anti-science,\" that you'd still consider trustworthy sources of information, but that help to offset certain biases that you might have accumulated here?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NtrkqzJDTxywFEpDR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "4499", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-26T20:59:10.172Z", "modifiedAt": null, "url": null, "title": "The Fallacy of Dressing Like a Winner", "slug": "the-fallacy-of-dressing-like-a-winner", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "benelliott", "createdAt": "2010-10-24T16:54:14.159Z", "isAdmin": false, "displayName": "benelliott"}, "userId": "H4iHqStnPyuAkniAA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v6pegGszpp99mTu6e/the-fallacy-of-dressing-like-a-winner", "pageUrlRelative": "/posts/v6pegGszpp99mTu6e/the-fallacy-of-dressing-like-a-winner", "linkUrl": "https://www.lesswrong.com/posts/v6pegGszpp99mTu6e/the-fallacy-of-dressing-like-a-winner", "postedAtFormatted": "Sunday, December 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fallacy%20of%20Dressing%20Like%20a%20Winner&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fallacy%20of%20Dressing%20Like%20a%20Winner%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6pegGszpp99mTu6e%2Fthe-fallacy-of-dressing-like-a-winner%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fallacy%20of%20Dressing%20Like%20a%20Winner%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6pegGszpp99mTu6e%2Fthe-fallacy-of-dressing-like-a-winner", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6pegGszpp99mTu6e%2Fthe-fallacy-of-dressing-like-a-winner", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1891, "htmlBody": "<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Imagine you are a sprinter, and your one goal in life is to win the 100m sprint in the Olympics. Naturally, you watch the 100m sprint winners of the past in the hope that you can learn something from them, and it doesn't take you long to spot a pattern.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Every one of them can be seen wearing a gold medal around their neck. Not only is there a strong correlation, you also examine the rules of the olympics and find that 100% of winner must wear a gold medal at some point, there is no way that someone could win and never wear a gold medal. So you go out and buy a gold medal from a shop, put it around your neck, and sit back, satisfied.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">For another example, imagine that you are now in charge of running a large oil rig. Unfortunately, some of the drilling equipment is old and rusty, and every few hours a siren goes off alerting the divers that they need to go down again and repair the damage. This is clearly not an acceptable state of affairs, so you start looking for solutions.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">You think back to a few months ago, before things got this bad, and you remember how the siren barely ever went off at all. In fact, from you knowledge of how the equipment works, the there were no problems, the siren couldn't go off. Clearly the solution the problem is to unplug the siren.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">(I would like to apologise in advance for my total ignorance of how oil rigs actually work, I just wanted an analogy)</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Both these stories demonstrate a mistake which I call 'Dressing Like a Winner' (DLAW). <a id=\"more\"></a>The general form of the error is when and indicator of success gets treated as an instrumental value, and then sometimes as a terminal value which completely subsumes the thing it was supposed to indicate. As someone noted this can also be seen as a sub-case of the correlative fallacy. &nbsp;This mistake is so obviously wrong that it is pretty much non-existant in near mode, which is why the above stories seem utterly ridiculous. However, once we switch into the more abstract far mode, even the most ridiculous errors become dangerous. In the rest of this post I will point out three places where I think this error occurs.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; \" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>Changing our minds</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In a debate between two people, it is usually the case that whoever is right is unlikely to change their mind. This is not only an empirically observable correlation, but it's also intuitively obvious, would you change your mind if you were right?</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">At this point, our fallacys steps in with a simple conclusion, \"refusing to change your mind will make you right\". As we all know, this could not be further from the truth, changing your mind is the only way to become right, or any rate less wrong. I do not think this realization is unique to this community, but it is far from universal (and it is a lot harder to practice than to preach, suggesting it might still hold on in the subconcious).</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">At this point a lot of people will probably have noticed that what I am talking about bears a close resemblance to signalling, and some of you are probably thinking that that is all there is to it. While I will admit that DLAW and Signalling are easy to confuse, I do think they are seperate things., and that there is more than just ordinary signalling going on in the debate.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">One piece of evidence for this is the fact that my unwillingness to change my mind extends even to opinions I have admitted to nobody. If I was only interested in signalling surely I would want to change my mind in that case, since it would reduce the risk of being humiliated once I do state my opinion. Another reason to believe that DLAW exists is the fact that not only do debaters rarely change their minds, those that do are often criticised, sometimes quite brutally, for 'flip-flopping', rather than being praised for becoming smarter and for demonstrating that their loyalty to truth is higher than their ego.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">So I think DLAW is at work here, and since I have chosen a fairly uncontroversially bad thing to start off with, I hope you can now agree with me that it is at least slightly dangerous.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; \" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>Consistency</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">It is an accepted fact that any map which completely fits the territory would be self-consistent. I have not seen many such maps, but I will agree with the argument that they must be consistent. What I disagree with is the claim that this means we should be focusing on making our maps internally consistent, and that once we have done this we can sit back because our work is done.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">This idea is so widely accepted and so tempting, especially to those with a mathematical bent, that I believed it for years before noticing the fallacy that lead to it. Most reasonably intelligent people have gotten over one half of the toxic meme, in that few of them believe consistency is good enough (with the one exception of ethics, where it still seems to apply in full force). However, as with the gold medal, not only is it a mistake to be satisfied with it, but it is a waste of time to aim for it in the first place.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In Robin Hanson's article <a title=\"beware consistency\" href=\"http://www.overcomingbias.com/2010/11/beware-consistency.html\" target=\"_self\">beware consistency</a> we see that the consistent subjects actually do worse than the inconsistent ones, because they are consistently impatient or consistently risk averse. I think this problem is even more general than his article suggests, and represents a serious flaw in our whole epistemology, dating back to the Ancient Greek era.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Suppose that one day I notice an inconsistency in my own beliefs. Conventional wisdom would tell me that this is a serious problem, and I should discard one of the beliefs as quickly. All else being equal, the belief that gets discarded will probably be the one I am less attached to, which will probably be the one I acquired more recently, which is probably the one which is actually correct, since the other may well date back to long before I knew how to think critically about an idea.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Richard Dawkins gives a good example of this in his book 'The God Delusion'. Kurt Wise, a brilliant young geologist raised as a fundementalist Christian. Realising the contradiction between his beliefs, he took a pair of scissors to the bible and cut out every passage he would have to reject if he accepted the scientific world-view. After realizing his bible was left with so few pages that the poor book could barely hold itself together, he decided to abandon science entirely. Dawkins uses this to make an argument for why religion needs to be removed entirely, and I cannot neccessarily say I disagree with him, but I think a second moral can be drawn from this story.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">How much better off would Kurt have been if he had just shrugged his shoulders at the contradiction and continued to believe both? How much worse off we be if Robert Aumann had abandoned the study of Rationality when he noticed it contradicted Orthodox Judaism? Its easy to say that Kurt was right to abandon one belief, he just abandoned the wrong one, but from inside Kurt's mind I'm not sure it was obvious to him which belief was right.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">I think a better policy for dealing with contradictions is to put both beliefs 'on notice', be cautious before acting upon either of them and wait for more evidence to decide between them. If nothing else, we should admit more than two possibilities, they could actually be compatible, or they could both be wrong, or one or both of them could be badly confused.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">To put this in one sentence \"don't strive for consistency, strive for accuracy and consistency will follow\".</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; \" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>Mathematical arguments about rationality</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In this community, I often see mathematical proofs that a perfect Bayesian would do something. These proofs are interesting from a mathematical perspective, but since I have never met a perfect Bayesian I am sceptical of their relevance to the real world (perhaps they are useful to AI, someone more experienced than me should either confirm or deny that).</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">The problem comes when we are told that since a perfect Bayesian would do X, then we imperfect Bayesians should do X as well in order to better ourselves. A good example of this is Aumann's Agreement Theorem, which shows that not agreeing to disagree is a consequence of perfect rationality, being treated as an argument for not agreeing to disagree in our quest for better rationality. The fallacy is hopefully clear by now, we have been given no reason to believe that copying this particular by-product of success will bring us closer to our goal. Indeed, in our world of imperfect rationalists, some of whom are far more imperfect than others, an argument against disagreement seems like a very dangerous thing.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Eliezer has <a href=\"/lw/gr/the_modesty_argument/\" target=\"_self\">already</a> argued against this specific mistake, but since he went on to <a title=\"Bayesian Judo\" href=\"/lw/i5/bayesian_judo\" target=\"_self\">commit it</a> a few articles later I think it bears mentioning again.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">Another example of this mistake is <a href=\"/lw/26y/rationality_quotes_may_2010/36y9\" target=\"_self\">this post</a> (my apologies to the poster, this is not meant as an attack, you just provided a very good example of what I am talking about). The post provides a mathematical argument (a model rather than a proof) that we should be more sceptical of evidence that goes against our beliefs than evidence for them. To be more exact, it gives an argument why a perfect Bayesian, with no human bias and mathematically precise calibration should be more sceptical of evidence going against its beliefs than evidence for them.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">The argument is, as far as I can tell, mathematically flawless. However, it doesn't seem to apply to me at all, if for no other reason than that I already have a massive bias overdoing that job, and my role is to counteract it.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">In fact, I would say that in general our willingness to give numerical estimates is an example of this fallacy. The Cox theorems prove that any perfect reasoning system is isomorphic to Bayesian probability, but since my reasoning system is not perfect, I get the feeling that saying \"80%\" instead of \"reasonably confident\" is just making a mockery of the whole process.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">This is not to say I totally reject the relevance of mathematical models and proofs to our pursuit. All else being equal if a perfect Bayesian does X. it is evidence that X is good for an imperfect Bayesian. It's just not overwhelmingly strong evidence, and shouldn't be treated as putting as if it puts a stop to all debate and decides the issue one way or the other (unlike other fields where mathematical arguments can do this).</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; \" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\"><strong>How to avoid it</strong></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">I don't think DLAW is particularly insidious as mistakes go, which is why I called it a fallacy rather than a bias. The only advice I would give is to be careful when operating in far mode (which you should do anyway), and always make sure the causal link between your actions and your goals is pointing in the right direction.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0cm; font-weight: normal\" align=\"JUSTIFY\"><span style=\"font-family: Verdana, sans-serif;\"><span style=\"font-size: small;\">I</span></span><span style=\"font-family: Verdana, sans-serif;\">f anyone has any other examples they can think of, please post them. Thanks to those who have already pointed some out, particularly the point about <a href=\"/lw/3gg/the_fallacy_of_dressing_like_a_winner/38d3\">akrasia and motivation</a></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 1, "cq69M9ceLNA35ShTR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v6pegGszpp99mTu6e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 27, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "4500", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NKECtGX4RZPd7SqYp", "NKaPFf98Y5otMbsPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-26T23:30:15.372Z", "modifiedAt": null, "url": null, "title": "What's happened to the front page?", "slug": "what-s-happened-to-the-front-page", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.090Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uaFL5XgCi63DfzQfu/what-s-happened-to-the-front-page", "pageUrlRelative": "/posts/uaFL5XgCi63DfzQfu/what-s-happened-to-the-front-page", "linkUrl": "https://www.lesswrong.com/posts/uaFL5XgCi63DfzQfu/what-s-happened-to-the-front-page", "postedAtFormatted": "Sunday, December 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20happened%20to%20the%20front%20page%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20happened%20to%20the%20front%20page%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuaFL5XgCi63DfzQfu%2Fwhat-s-happened-to-the-front-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20happened%20to%20the%20front%20page%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuaFL5XgCi63DfzQfu%2Fwhat-s-happened-to-the-front-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuaFL5XgCi63DfzQfu%2Fwhat-s-happened-to-the-front-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p>http://lesswrong.com is suddenly redirecting to a Tibetan meditation site. What the hell?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uaFL5XgCi63DfzQfu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 25, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "4501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-27T05:04:24.954Z", "modifiedAt": null, "url": null, "title": "Rational insanity", "slug": "rational-insanity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:06.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j2syBASpAA2Q5xfG7/rational-insanity", "pageUrlRelative": "/posts/j2syBASpAA2Q5xfG7/rational-insanity", "linkUrl": "https://www.lesswrong.com/posts/j2syBASpAA2Q5xfG7/rational-insanity", "postedAtFormatted": "Monday, December 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20insanity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20insanity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj2syBASpAA2Q5xfG7%2Frational-insanity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20insanity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj2syBASpAA2Q5xfG7%2Frational-insanity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj2syBASpAA2Q5xfG7%2Frational-insanity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>My theory on why North Korea has stepped up its provocation of South Korea since their nuclear missle tests is that they see this as a <a href=\"/lw/79/aumann_voting_or_how_to_vote_when_youre_ignorant/\">tug-of-war</a>.</p>\n<p>Suppose that North Korea wants to keep its nuclear weapons program.&nbsp; If they hadn't sunk a ship and bombed a city, world leaders would currently be pressuring North Korea to stop making nuclear weapons.&nbsp; Instead, they're pressuring North Korea to stop doing something (make provocative attacks) that North Korea doesn't really want to do anyway.&nbsp; And when North Korea (temporarily) stops attacking South Korea, everybody can go home and say they \"did something about North Korea\".&nbsp; And North Korea can keep on making nukes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j2syBASpAA2Q5xfG7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 19, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "4507", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ddAEkE7F4cywqsHRq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-27T06:10:05.878Z", "modifiedAt": null, "url": null, "title": "Neutral AI", "slug": "neutral-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:16.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bcJBKJx4iaLChYZrW/neutral-ai", "pageUrlRelative": "/posts/bcJBKJx4iaLChYZrW/neutral-ai", "linkUrl": "https://www.lesswrong.com/posts/bcJBKJx4iaLChYZrW/neutral-ai", "postedAtFormatted": "Monday, December 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Neutral%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeutral%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbcJBKJx4iaLChYZrW%2Fneutral-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Neutral%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbcJBKJx4iaLChYZrW%2Fneutral-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbcJBKJx4iaLChYZrW%2Fneutral-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 782, "htmlBody": "<p>Unfriendly AI has goal conflicts with us.&nbsp; Friendly AI (roughly speaking) shares our goals.&nbsp; How about an AI with no goals at all?</p>\n<p>I'll call this \"neutral AI\".&nbsp; Cyc is a neutral AI.&nbsp; It has no goals, no motives, no desires; it is inert unless someone asks it a question.&nbsp; It then has a set of routines it uses to try to answer the question.&nbsp; It executes these routines, and terminates, whether the question was answered or not.&nbsp; You could say that it had the temporary goal to answer the question.&nbsp; We then have two important questions:</p>\n<ol>\n<li>Is it possible (or feasible) to build a useful AI that operates like this?</li>\n<li>Is an AI built in this fashion significantly less-dangerous than one with goals?</li>\n</ol>\n<p><a id=\"more\"></a>Many people have answered the first question \"no\".&nbsp; This would probably include Hubert Dreyfus (based on a Heideggerian analysis of semantics, which was actually very good but I would say misguided in its conclusions because Dreyfus mistook \"what AI researchers do today\" for \"what is possible using a computer\"), Phil Agre, Rodney Brooks, and anyone who describes their work as \"reactive\", \"behavior-based\", or \"embodied cognition\".&nbsp; We could also point to the analogous linguistic divide.&nbsp; There are two general approaches to natural language understanding, one descending from generative grammars and symbolic AI and embodied by James Allen's book <em>Natural Language Understanding</em>, and in the \"program in the knowledge\" camp that would answer the first question \"yes\".&nbsp; The other approach has more kinship with construction grammars and machine learning, and is embodied by Manning &amp; Schutze's <em>Foundations of Statistical Natural Language Processing</em>, and its practitioners would be more likely to answer the first question \"no\". (Eugene Charniak is noteworthy for having been prominent in both camps.)</p>\n<p>The second question, I think, hinges on two sub-questions:</p>\n<ol>\n<li>Can we prevent an AI from harvesting more resources than it should for a question?</li>\n<li>Can we prevent an AI from conceiving the goal of increasing its own intelligence as a subgoal to answering a question?</li>\n</ol>\n<p>The Jack Williamson story \"With Folded Hands\" (1947) tells how humanity was enslaved by robots given the order to protect humanity, who became... overprotective.&nbsp; Or suppose a physicist asked an AI, \"Does the Higgs boson exist?\"&nbsp; You don't want it to use the Earth to build a supercollider.&nbsp; These are cases of using more resources than intended to carry out an order.</p>\n<p>You may be able to build a Cyc-like question-answering architectures that would have no risk of doing any such thing.&nbsp; It may be as simple as placing resource limitations on every question.&nbsp; The danger is that if the AI is given a very thorough knowledge base that includes, for instance, an understanding of human economics and motivations, it may syntactically construct a plan to find the answer to a question that is technically within the resource limitations posed, for instance by manipulating humans in ways that don't tweak its cost function.&nbsp; This could lead to very big mistakes; but it isn't the kind of mistake that builds on itself, like a FOOM scenario.&nbsp; The question is whether any of these very big mistakes would be&nbsp; irreversible.&nbsp; My intuition is that there would be a power-law distribution of mistake sizes, with a small number of irreversible mistakes.&nbsp; We might then figure out a reasonable way of determining our risk level.</p>\n<p>If the answer to the second subquestion is \"yes\", then we probably don't need to fear a FOOM from neutral AI.</p>\n<p>The short answer is, Yes, there are \"neutral AI architectures\" that don't currently have the risk either of harvesting too many resources, or of attempting to increase their own intelligence.&nbsp; Many existing AI architectures are examples.&nbsp; (I'm thinking specifically of \"hierarchical task-network planning\", which I don't consider true planning; it only allows the piecing together of plan components that were pre-built by the programmer.)&nbsp; But they can't do much.&nbsp; There's a power / safety tradeoff.&nbsp; The question is how much power you can get in the \"completely safe\" region, and where the sweet spots are in that tradeoff outside the completely safe region.</p>\n<div id=\"body_t1_38ik\" class=\"comment-content\">\n<div class=\"md\">\n<p>If you could build an AI that did nothing but parse published articles to answer the question, \"Has anyone said X?\", that would be very useful, and very safe. I worked on such a program (<a rel=\"nofollow\" href=\"http://skr.nlm.nih.gov/papers/index.shtml\">SemRep</a>) at NIH. It works pretty well within the domain of medical journal articles.&nbsp; If it could take one step more, and ask, \"Can you find a set of one to four statements that, taken together, imply X?\", that would be a huge advance in capability, with little if any additional risk.&nbsp; (I added that capability to SemRep, but no one has ever used it, and it isn't accessible through the web interface.)</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bcJBKJx4iaLChYZrW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 6.607028424016363e-07, "legacy": true, "legacyId": "4509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-27T10:40:38.829Z", "modifiedAt": null, "url": null, "title": "Efficient Induction", "slug": "efficient-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZGr4PQcM2seEkDsHN/efficient-induction", "pageUrlRelative": "/posts/ZGr4PQcM2seEkDsHN/efficient-induction", "linkUrl": "https://www.lesswrong.com/posts/ZGr4PQcM2seEkDsHN/efficient-induction", "postedAtFormatted": "Monday, December 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Efficient%20Induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEfficient%20Induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZGr4PQcM2seEkDsHN%2Fefficient-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Efficient%20Induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZGr4PQcM2seEkDsHN%2Fefficient-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZGr4PQcM2seEkDsHN%2Fefficient-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1044, "htmlBody": "<p>(By which I mean, induction over efficient hypotheses.)</p>\n<p>A standard prior is \"uniform\" over the class of computable functions (ie, uniform over infinite strings which have a prefix which compiles). Why is this a natural choice of prior? Well, we've looked at the universe for a really long time, and it seems to be computable. The Church-Turing thesis says we have no reason to choose a bigger support for our prior.</p>\n<p>Why stop there? There is a natural generalization of the Church-Turing thesis, naturally called the extended Church-Turing thesis, which asserts that the universe is computable in polynomial time. In fact, we have a strong suspicion that physics should be local, which means more or less precisely that updates can be performed using a linear size logical circuit. Maybe we should restrict our prior further, looking only for small circuits.</p>\n<p>(As we realized only slightly later, this extended hypothesis is almost certainly false, because in the real world probabilities are quantum. But if we replace \"circuit\" by \"quantum circuit,\" which is a much less arbitrary change than it seems at face value, then we are good. Are further changes forthcoming? I don't know, but I suspect not.)</p>\n<p>So we have two nice questions. First, what does a prior over efficiently computable hypotheses look like? Second, what sort of observation about the world could cause you to modify Solomonoff induction? For that matter, what sort of physical evidence ever convinced us that Solomonoff induction was a good idea in the first place, rather than a broader prior? I suspect that both of these questions have been tackled before, but google has failed me so now I will repeat some observations.</p>\n<p>Defining priors over \"polynomially large\" objects is a little more subtle than usual Solomonoff induction. In some sense we need to penalize a hypothesis both for its description complexity and its computational complexity. Here is a first try:</p>\n<p style=\"padding-left: 30px;\">A hypothesis consists of an initial state of some length N, and a logical circuit of size M which takes N bits to K+N bits. The universe evolves by repeatedly applying the logical circuit to compute both a \"next observation\" (the first K bits) and the new state of the universe (the last N bits). The probability of a hypothesis drops off exponentially with its length.</p>\n<p>How reasonable is this? Why, not at all. The size of our hypothesis is the size of the universe, so it is going to take an awful lot of observations to surmount the improbability of living in a large universe. So what to do? Well, the reason this contradicts intuition is that we expect our physical theory (as well as the initial state of our system) to be uniform in some sense, so that it can hope to be described concisely even if the universe is large. Well luckily for us the notion of uniformity already exists for circuits, and in fact appears to be the correct notion. Instead of working with circuits directly, we specify a program which outputs that circuit (so if the laws of physics are uniform across space, the program just has to be told \"tile this simple update rule at every point.\") So now it goes like this:</p>\n<p style=\"padding-left: 30px;\">A hypothesis consists of a program which outputs an initial state of some finite length N, and a program which outputs a logical circuit of size M which takes N bits to K+N bits. The observations are defined as before. The probability of a hypothesis drops off exponentially with its length.</p>\n<p>How reasonable is this? Well it doesn't exploit the conjectured computational efficiency of the universe at all. There are three measures of complexity, and we are only using one of them. We have the length of the hypothesis, the size of the universe, and the computational complexity of the update rule. At least we now have these quantities in hand, so we can hope to incorporate them intelligently. One solution is to place an explicit bound on the complexity of the update rule in terms of the size of the universe. It is easy to see that this approach is doomed to fail. An alternative approach is to explicitly include terms dependent on all three complexity measures in the prior probability for each hypothesis.</p>\n<p>There are some aesthetically pleasing solutions which I find really attractive. For example, make the hypothesis a space bounded Turing machine and also require it to specify the initial state of its R/W tape. More simply but less aesthetically, you could just penalize a hypothesis based on the logarithm of its running time (since this bounds the size of its output), or on log M. I think this scheme gives known physical theories very good description complexity. Overall, it strikes me as an interesting way of thinking about things.</p>\n<p>I don't really know how to answer the second question (what sort of observation would cause us to restrict our prior in this way). I don't really know where the description complexity prior comes from either; it feels obvious to me that the universe is computable, just like it feels obvious that the universe is efficiently computable. I don't trust these feelings of obviousness, since they are coming from my brain. I guess the other justification is that we might as well stick to computable hypotheses, because we can't use stronger hypotheses to generate predictions (our conscious thoughts at least appearing to be computable). The same logic does have something to say about efficiency: we can't use inefficient hypotheses to generate predictions, so we might as well toss them out. But this would lead us to just keep all sufficiently efficient hypotheses and throw out the rest, which doesn't work very well (since the efficiency of a hypothesis depends upon the size of the universe it posits; this basically involves putting a cap on the size of the universe). I don't know of a similar justification which tells us to penalize hypotheses based on their complexity. The only heuristic is that it has worked well for screening out physical theories so far. Thats a pretty good thing to have going for you at least.</p>\n<p>In sum, thinking about priors over more restricted sets of hypotheses can be interesting. If anyone knows of a source which approaches this problem more carefully, I would be interested to learn.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZGr4PQcM2seEkDsHN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 6.607711143024698e-07, "legacy": true, "legacyId": "4526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-27T17:16:10.541Z", "modifiedAt": null, "url": null, "title": "Dark Arts 101: Using presuppositions", "slug": "dark-arts-101-using-presuppositions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.254Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DXBziiT2RFLcmLY9J/dark-arts-101-using-presuppositions", "pageUrlRelative": "/posts/DXBziiT2RFLcmLY9J/dark-arts-101-using-presuppositions", "linkUrl": "https://www.lesswrong.com/posts/DXBziiT2RFLcmLY9J/dark-arts-101-using-presuppositions", "postedAtFormatted": "Monday, December 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dark%20Arts%20101%3A%20Using%20presuppositions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADark%20Arts%20101%3A%20Using%20presuppositions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXBziiT2RFLcmLY9J%2Fdark-arts-101-using-presuppositions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dark%20Arts%20101%3A%20Using%20presuppositions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXBziiT2RFLcmLY9J%2Fdark-arts-101-using-presuppositions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXBziiT2RFLcmLY9J%2Fdark-arts-101-using-presuppositions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 341, "htmlBody": "<p>Sun Tzu said, \"<span class=\"body\">The supreme art of war is to subdue the enemy without fighting.\"&nbsp; This is also true in rhetoric.&nbsp; The best way to get a belief accepted is to fool people into thinking that they have already accepted it.</span></p>\n<p><span class=\"body\">(Note, first-year students, that I did not say, \"The best way to convince people of a belief\".&nbsp; Do not try to <em>convince</em> people!&nbsp; It will not work; and it may start them <em>thinking</em>.)<br /></span></p>\n<p><span class=\"body\">An excellent way of doing this is to embed your desired conclusion as a presupposition to an enticing argument.&nbsp; If you are debating abortion, and you wish people to believe that human and non-human life are qualitatively different, begin by saying, \"We all agree that killing humans is immoral.&nbsp; So when does human life begin?\"&nbsp; People will be so eager to jump into the debate about whether a life becomes \"human\" at conception, the second trimester, or at birth (I myself favor \"on moving out of the house\"), they won't notice that they agreed to the embedded presupposition that the problem should be phrased as a binary category membership problem, rather than as one of tradeoffs or utility calculations.</span></p>\n<p><span class=\"body\">Consider the recent furor over whether WikiLeaks leader Julian Assange is a journalist, or can be prosecuted for espionage.&nbsp; I don't know who initially asked this question.&nbsp; The earliest posing of the question that I can find that relates it to the First Amendment is this <a href=\"http://www.youtube.com/watch?v=h0SWY8uakLk&amp;feature=related\">piece from Fox News</a> on Dec. 8; but Marc Thiessen's <a href=\"http://www.washingtonpost.com/wp-dyn/content/article/2010/08/02/AR2010080202627.html\">column</a> in the Washington Post of Aug. 3 has similar implications.&nbsp; Note that this question presupposes that First Amendment protection applies only to journalists!&nbsp; There is no legal precedent for this that I'm aware of; yet if people spend enough time debating whether Julian Assange is a journalist, they will have unknowingly convinced themselves that ordinary citizens have no First Amendment rights.&nbsp; (We can only hope that this was an artful stroke made from the shadows by some great master of the Dark Arts, and not a mere snowballing of an ignorant question.)<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DXBziiT2RFLcmLY9J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 85, "baseScore": 97, "extendedScore": null, "score": 0.000181, "legacy": true, "legacyId": "4528", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 98, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-27T20:56:11.131Z", "modifiedAt": null, "url": null, "title": "Certainty estimates in areas outside one's expertise", "slug": "certainty-estimates-in-areas-outside-one-s-expertise", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:30.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jf3sY6eeDEs3JiwLp/certainty-estimates-in-areas-outside-one-s-expertise", "pageUrlRelative": "/posts/jf3sY6eeDEs3JiwLp/certainty-estimates-in-areas-outside-one-s-expertise", "linkUrl": "https://www.lesswrong.com/posts/jf3sY6eeDEs3JiwLp/certainty-estimates-in-areas-outside-one-s-expertise", "postedAtFormatted": "Monday, December 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Certainty%20estimates%20in%20areas%20outside%20one's%20expertise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACertainty%20estimates%20in%20areas%20outside%20one's%20expertise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjf3sY6eeDEs3JiwLp%2Fcertainty-estimates-in-areas-outside-one-s-expertise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Certainty%20estimates%20in%20areas%20outside%20one's%20expertise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjf3sY6eeDEs3JiwLp%2Fcertainty-estimates-in-areas-outside-one-s-expertise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjf3sY6eeDEs3JiwLp%2Fcertainty-estimates-in-areas-outside-one-s-expertise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>One issue that I've noticed in discussions on Less Wrong is that I'm much less certain about the likely answers to specific questions than some other people on Less Wrong. But the questions where this seems to be most pronounced are mathematical questions that are close to my area of expertise (such as whether P = NP). In areas outside my expertise, my apparent confidence is apparently often higher. Thus, for example at a recent LW meet-up I expressed a much lower probability estimate that cold fusion is real than what others in the conversation estimated. This suggests that I may be systematically overestimating&nbsp; my confidence in areas that I don't study as much, essentially a variant of the Dunning-Krueger effect. Have other people here experienced the same pattern with their own confidence estimates?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jf3sY6eeDEs3JiwLp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 6.609264894889797e-07, "legacy": true, "legacyId": "4530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-28T02:07:33.475Z", "modifiedAt": null, "url": null, "title": "Is there a guide somewhere for how to setup a Less Wrong Meetup?", "slug": "is-there-a-guide-somewhere-for-how-to-setup-a-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:05.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mw6zpAzL2jYncGpTu/is-there-a-guide-somewhere-for-how-to-setup-a-less-wrong", "pageUrlRelative": "/posts/mw6zpAzL2jYncGpTu/is-there-a-guide-somewhere-for-how-to-setup-a-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/mw6zpAzL2jYncGpTu/is-there-a-guide-somewhere-for-how-to-setup-a-less-wrong", "postedAtFormatted": "Tuesday, December 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20a%20guide%20somewhere%20for%20how%20to%20setup%20a%20Less%20Wrong%20Meetup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20a%20guide%20somewhere%20for%20how%20to%20setup%20a%20Less%20Wrong%20Meetup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmw6zpAzL2jYncGpTu%2Fis-there-a-guide-somewhere-for-how-to-setup-a-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20a%20guide%20somewhere%20for%20how%20to%20setup%20a%20Less%20Wrong%20Meetup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmw6zpAzL2jYncGpTu%2Fis-there-a-guide-somewhere-for-how-to-setup-a-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmw6zpAzL2jYncGpTu%2Fis-there-a-guide-somewhere-for-how-to-setup-a-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>n/t</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mw6zpAzL2jYncGpTu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.610051111639383e-07, "legacy": true, "legacyId": "4543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-28T10:11:01.210Z", "modifiedAt": null, "url": null, "title": "Being Rational and Being Productive: Similar Core Skills?", "slug": "being-rational-and-being-productive-similar-core-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:09.139Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/keieeJWh4aNjvqPcn/being-rational-and-being-productive-similar-core-skills", "pageUrlRelative": "/posts/keieeJWh4aNjvqPcn/being-rational-and-being-productive-similar-core-skills", "linkUrl": "https://www.lesswrong.com/posts/keieeJWh4aNjvqPcn/being-rational-and-being-productive-similar-core-skills", "postedAtFormatted": "Tuesday, December 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Being%20Rational%20and%20Being%20Productive%3A%20Similar%20Core%20Skills%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeing%20Rational%20and%20Being%20Productive%3A%20Similar%20Core%20Skills%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkeieeJWh4aNjvqPcn%2Fbeing-rational-and-being-productive-similar-core-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Being%20Rational%20and%20Being%20Productive%3A%20Similar%20Core%20Skills%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkeieeJWh4aNjvqPcn%2Fbeing-rational-and-being-productive-similar-core-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkeieeJWh4aNjvqPcn%2Fbeing-rational-and-being-productive-similar-core-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 770, "htmlBody": "<p><em>A synthesis of How to Actually Change Your Mind and PJ Eby, written for a general audience.</em></p>\n<p>Several years ago I started suspecting that I needed glasses.&nbsp; At first, I was afraid.&nbsp; I began trying to convince myself that my vision was normal.<br /><br />But then I stopped to reflect.&nbsp; If I went to see the eye doctor, either he would recommend glasses for me or he wouldn't.&nbsp; If he didn't recommend glasses for me then my life would be the same.&nbsp; But if he did recommend glasses, I would get a vision upgrade.&nbsp; Therefore, I reasoned, I should eagerly await my doctor visit.<br /><br />By following the principle of letting control flow from thoughts to emotions, I gained two benefits.&nbsp; First, my beliefs about my vision weren't being distorted by my desire for it to be normal.&nbsp; And second, my emotion of eagerness for a potential vision upgrade meant that I wasn't tempted to put off visiting the doctor.<br /><br />My glasses example might seem kind of mundane, but it demonstrates how thinking before emoting helps with two core human objectives: Being Correct and Getting Things Done.<br /><br />Many of the cognitive biases that distort human reasoning can be explained by emotions that get in the way of our thought process.&nbsp; For example, status quo bias occurs when we are unreasonably skeptical of arguments that suggest we should change the status quo.&nbsp; The emotion that distorts our reasoning in this case is our fear of things that are new and unfamiliar.&nbsp; This is the bias that made me try to convince myself that I didn't need glasses.<br /><br />When it comes to Getting Things Done, both productivity and procrastination are emotional states.&nbsp; Being able to turn these off and on would be useful.<br /><br />So having control flow from thoughts to emotions has strong theoretical potential to help humans be less biased and more productive.&nbsp; But is it possible in practice?<br /><br />Yes.&nbsp; The trick is to notice and reflect on negative emotions.&nbsp; Negative emotions like fear, guilt, shame, and regret are hardly ever useful and frequently interfere with our reasoning and working.<br /><br />Sometimes that's all that's necessary.&nbsp; For example, once I was arguing with a friend about global warming and I started to become afraid that he might actually be right.&nbsp; Fortunately I noticed my fear and reminded myself that if my friend was right about global warming, I wanted to agree with him.&nbsp; This helped me maintain calm objectivity.<br /><br />At other times it makes sense to take specific actions to influence one's emotions.&nbsp; When it comes to getting work done, I've had success with taking drugs like caffeine, talking to other people, and taking breaks to do other activities.&nbsp; When my work seems especially dreary, I find that if I do several new things for a while and come back, my emotional state is reset to a random value that's generally better for work than the one I started with.<br /><br />Ultimately I've realized that it's mostly not me who's in control of what I do.&nbsp; It's my emotions.&nbsp; In years past I would procrastinate like a typical student, trying for hours to get myself to do something and not getting anywhere.&nbsp; Now I realize that I was quite literally not fully in control of myself.&nbsp; I was just pretending I was, and disappointing myself as my illusions repeatedly failed to match up to reality.<br /><br />There are evolutionary reasons why our rational minds don't fully control us.&nbsp; The most important activities we evolved to do, like hunt, avoid predators, and reproduce, can be done just fine without human ingenuity, as demonstrated by the dumb animals that surrounded us.&nbsp; Our rational mind was only to be used in specific situations like constructing tools and coming up with excuses for why something we had done wasn't a violation of tribal social norms.<br /><br />In this modern era, where surviving and reproducing are solved problems, evolved instincts are useless behavioral distortions.&nbsp; It makes sense that we could become significantly more successful by learning to counteract them.&nbsp; That's what reversing the flow of control between thoughts and emotions does.<br /><br />I'm convinced that by thinking before emoting, anyone can become more Correct and Accomplished.</p>\n<p><em>This is a modified version of an essay I wrote for my <a href=\"http://thielfellowship.org/\" target=\"_self\">Thiel Fellowship</a> application, so if you have any suggestions for how I can improve the writing, please put them in <a href=\"http://www.ietherpad.com/PP6lpB8wVL\">this etherpad</a>.&nbsp; The application deadline is December 31st.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "keieeJWh4aNjvqPcn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 27, "extendedScore": null, "score": 6.611272184421952e-07, "legacy": true, "legacyId": "4562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-28T11:38:56.007Z", "modifiedAt": null, "url": null, "title": "Narrow your answer space", "slug": "narrow-your-answer-space", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:58.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dzqAbaz3Hf9ccYKqd/narrow-your-answer-space", "pageUrlRelative": "/posts/dzqAbaz3Hf9ccYKqd/narrow-your-answer-space", "linkUrl": "https://www.lesswrong.com/posts/dzqAbaz3Hf9ccYKqd/narrow-your-answer-space", "postedAtFormatted": "Tuesday, December 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Narrow%20your%20answer%20space&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANarrow%20your%20answer%20space%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdzqAbaz3Hf9ccYKqd%2Fnarrow-your-answer-space%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Narrow%20your%20answer%20space%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdzqAbaz3Hf9ccYKqd%2Fnarrow-your-answer-space", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdzqAbaz3Hf9ccYKqd%2Fnarrow-your-answer-space", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2525, "htmlBody": "<p>Mark Rosewater, a designer for Magic: The Gathering, <a href=\"http://www.wizards.com/magic/magazine/Article.aspx?x=mtg/daily/mm/92\">writes a lot</a> about how \"<a href=\"http://www.wizards.com/Magic/Magazine/Article.aspx?x=mtgcom/daily/mr103\">restrictions breed creativity</a>.\" The explanation he gives is simple: when someone is building a house, the more tools they have, the better off they are. But when someone is looking for something, the more space they have to explore, the <em>worse</em> off they are. This applies to answer space: the more narrowly defined your problem is, the easier it is to search an answers; you'll find both more answers and better answers by looking in a well-chosen smaller space. Oftentimes the hardest problems to find good answers for are the ones with the widest scope.<sup>1</sup></p>\n<p><a id=\"more\"></a>Most problems require some sort of creative thinking to overcome, and perhaps the greatest gains from this method come from applying it to your life goals. Imagine someone with a simple goal:<sup>2</sup> they want to improve themselves. That's <a href=\"/lw/jb/applause_lights/\">admirable</a>, but sort of bland and massively broad. It would be helpful to have a way to work from a bland, broad goal to a better goal- but what's better, in this context? We know that restrictions help, but what sort of restrictions help the most?</p>\n<p><strong>Choosing Restrictions</strong></p>\n<p>In <a href=\"http://www.amazon.com/Getting-Things-Done-Stress-Free-Productivity/dp/0142000280/\">Getting Things Done</a>, David Allen argues that to-do lists should only include 'tasks.' That is, only write down clearly identified next actions towards achieve specific goals. \"Call Adam\" isn't a task, but \"Call Adam about hotel reservations for the conference\" is. This serves to reduce mental load (once you've written down the second, you can remove the task entirely from your mind, while you still need to keep a lot in memory for the first), to reduce the need to plan while doing, and to reduce the <a href=\"/lw/21b/ugh_fields/\">ugh field</a> associated with getting started. A good place for a goal to be, then, is a place where looking at the goal causes you to imagine the next task, even if you lost the to-do list where you had written down the next task and then purposefully forgotten it. So, <strong>actionable</strong> is a restriction that helps (even if the action is \"wait for X,\" it's a good idea to know what X is, so you can look out for it!).</p>\n<p>But at the same time, it helps when our goals are a sentence or a paragraph long, rather than a list of every subgoal and task. They should be kept <strong>simple</strong>, for the sake of both communication and <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">flexibility</a>. Finally, to encapsulate what's useful about restricting creativity in general, it should be <strong>specific</strong>. There are many actionable goals which present too many possible actions, and so we choose at random or do nothing at all.</p>\n<p>When talking about plans and goals, David Allen uses a plane's eye view analogy: goals are 0, 10k, 20k, 30k, 40k, or 50k feet above the ground. I prefer a math analogy- goals are worked out to 0th, 1st, 2nd, 3rd, and 4th order (and even further if necessary).<sup>3 </sup>Allen's analogy and mine work in opposite directions, and it's worthwhile to point out why. Allen's primary focus is (unsurprisingly) getting things done, and that happens at the task level. Traveling upwards is done to zoom out and obtain information, not to do work while in the clouds. A good visual analogy for my approach is a tree's root burrowing into the ground. At each spot, the root has a choice of where to go, and the point is to be there and soak up nutrients. The root also isn't traveling but <em>extending</em>- it still exists everywhere it was before. Allen is happy with a satellite photo, but I need a pipeline.</p>\n<p><strong>Refining a Goal</strong></p>\n<p>When we take a 0th order goal, like \"I want to improve,\" there are a pretty large number of ways we could make it more specific, and a staggering (literally) number of potential actions we could take to work towards that goal. We think about our options, and settle on \"I want to be cleverer.\" We still want to improve- we've just outlined a way to do so. But we've also <em>discarded</em> most of the ways we could improve! This is a valuable thing because it narrows our answer space. We could also say it <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">constrains our expectations</a> and our efforts; so we upgrade that goal to 1st order.</p>\n<p>Allen's analogy is robust because it has a strong anchor: the next task to do is at ground level. There is no strong anchor for a 0th order goal, just a heuristic about how to rank goals. We could have started off with \"I want to be cleverer\" as a 0th order goal, and the rest of this example would work out exactly the same- except with slightly different numbers. So don't focus on the numbers as much as the relationships between them and the changes in answer space.</p>\n<p>A 1st order goal, while specific, is generally still not actionable. Here is where it's important to keep refining the goal instead of being seduced into working. The first thing you can think of to make yourself cleverer is probably <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">not the best</a> thing you can do to make yourself cleverer. Again, our root pushes into the ground, seeking out nutrients, and we select an aspect of cleverness to focus on: \"I want to make better decisions.\" The thoughts produced by this 2nd order formulation are starting to become fertile, but we can do better.</p>\n<p><strong>Stop when Satisfied</strong></p>\n<p>A clarifying question - \"what do we mean by better?\" - gives us a 3rd order goal: \"I want to have a solid idea of how good a decision is.\" We could keep refining the goal endlessly, but at some point we have to stop planning and start producing. When is a good time to do this, given that you can only compare the present and past, not present and future? We don't have the assurance that this is a well-behaved problem where each additional step will change our answer space less than the previous step did, so we need to be cleverer about this choice than normal. One approach is the scale of resources involved- if you haven't gotten to the point where you could reasonably expect success with the resources you have to throw at the problem, keep drilling down until you've reached that point. If you're trying to decide on what your life's work should be, drill down until you've got a problem you can do significant work on in 20 years; don't stop when you hit a goal that would take fifty people fifty years.</p>\n<p>Another way is to look at how the goals intersect with each other- it seems like our root has curled in a different way moving from \"better decisions\" to \"understand decision quality\" than in its previous extensions- it seems like if we don't understand the problem of measuring a decision's quality, any other improvements we make <em>can't</em> succeed at \"make better decisions\" because we can't tell if the new decisions are higher quality than the old decisions! Beforehand, we were selecting from independent specializations. Now we're looking at a <em>necessary subgoal</em> instead of a <em>related goal</em>. Looking at this another way, we've been choosing more and more specific terminal values and have come across our first instrumental value.<sup>4</sup></p>\n<p>That suggests we've got enough to stop deciding what goal to pursue and start actually pursuing it. When you stop your goal-selection because of a fork like this, it's a good idea to look at what other goals are on the same order: while you should work on 3rd order problems before 4th order problems, problems of the same order are roughly equally important, and you may find you want to work on a different one or you can work on multiple of them in parallel.</p>\n<p>Note that even though we've make the decision to stop planning and start producing, we're probably going to run into some 4th order problems. Goals often have subgoals and instrumental values often have instrumental values based off them; the same methodology will work at every level and often represents a much faster way to search through answer space than brute force (especially since it's typically very hard to force your brain to brute force massive problems). Oftentimes there will be a domain-specific response which is more appropriate than this method, though (or, at least, resembles this method only in the abstract).</p>\n<p><strong>Carve at the Joints</strong></p>\n<p>One thing I have barely mentioned but is of crucial importance is that you need significant knowledge to effectively narrow down the answer space you're considering. Consider an international corporation trying to create a human resources department. Their 0th order goal might be something like \"make higher profits,\" their 1st order goal is \"streamline corporate functions to reduce cost without significantly reducing revenue,\" and their 2nd order goal might be \"task an entity with managing hiring, pay, benefits, and employee relations.\" Now they have a lot of 3rd order goals to choose from, and they decide \"create an HR office in each department.\" After all, they've already got their corporation partitioned that way, and having one HR department for R&amp;D and another for Sales will mean that the hiring expertise of each HR department is much better because of specialization.</p>\n<p>But this misses the reality of HR departments, which is that their functions are strongly tied to the nation that employees live and work in. The R&amp;D HR office might find itself having to deal with ten different sets of tax laws, requiring ten different tax specialists. Hiring laws in one country might require one procedure, while in another country they're totally different. The benefit of increased hiring specialization might not be unique to this plan- due to interviewing, travel costs, and legal changes, this setup probably requires one hiring officer for each department for each country, as well as a tax specialist for each country for each department. But if you split up the HR departments by country instead of by department, you would only need one tax specialist for each country, and the same number of hiring officers.</p>\n<p>There are three things to be learned from that example: first, hold off on proposing solutions (sound familiar?). Second, <a href=\"/lw/2p1/a_failure_to_evaluate_returnontime_fallacy/\">don't be afraid</a> to go back upwards and reevaluate your choice of goals. By choosing, you discarded a lot of answer space; if you don't find promising things in the region you looked, you should look somewhere else.</p>\n<p>Most importantly, we learn that the solution to a problem is often another problem. The answer we picked to \"I want to improve\" is \"I want to get cleverer,\" and we can think better and faster<sup>5</sup> if we treat that as a full answer. After all, if you reduce the answer space of \"a sentence 100 letters long\" to \"a sentence 10 letters long,\" you have reduced it by a <em>larger</em> factor than reducing from \"a sentence 10 letters long\" to a specific sentence that is 10 letters long.<sup>6</sup></p>\n<p>&nbsp;</p>\n<hr />\n<p>1. My artist friends tell me that their least favorite commissions are the ones where the commissioner tells them \"do whatever you want!\"; I don't think I've seen someone in any field ever speak positively of getting that regularly (instead of as an occasional reprieve).</p>\n<p>2. Style note: I use 'goal', 'problem', and 'value' interchangeably throughout this post, based on whatever seems appropriate for that sentence. I hope this isn't too confusing- I think there's only type errors for values, and so when you see value recast it as \"a goal to obtain this value.\"</p>\n<p>3. In physics (and many other disciplines), unsoluble problems are often approximated by an infinite number of soluble problems. For example, one can calculate sin(x) with only multiplication, division, addition, and subtraction by using the <a href=\"http://en.wikipedia.org/wiki/Taylor_expansion#Approximation_and_convergence\">Taylor Series</a> approximation. However, by itself this is just moving around the difficulty- your new problems are individually soluble but you don't have the time to solve an infinite number of them. This method is effective only when you can ignore later terms- that is, take the infinite amount of trash you've generated and manage to throw it away in a finite volume. For example, to calculate sin(1) to three parts in a thousand requires only the first three terms: 1-1<sup>3</sup>/3!+1<sup>5</sup>/5!=.841667 while sin(1) is .841471 (both rounded to 6 digits). For well-behaved approximations, the error is smaller than the next additional term- for sin(1) with 3 terms, the error is 1.96e-4 while the next term is 1<sup>7</sup>/7!=1.98e-4. My usage of \"order\" is inspired by this background; a first guess at a problem (like answering 1 to sin(1)) is a first order solution that's in the right ballpark but is probably missing crucial details. A second order solution has the most obvious modification to the first order solution and is generally rather good (5/6 only differs from sin(1) by 1%). One note here is this implies that for well-behaved problems, one needs to do all of the n<sup>th</sup> order modifications before moving to the n+1<sup>th</sup> order- if I just give you 1-1<sup>7</sup>/7!, my answer is not really any better than my 1st order answer (and if I gave you 1+1<sup>5</sup>/5!, it would be worse).</p>\n<p>4. The usefulness of wording things this way is limited because the boundary between the two is hard to determine. \"I want to make better decisions\" could easily be an instrumental value to a rather different problem (\"I want to be more powerful,\" say) or you could interpret it as an instrumental value for the previous value (\"I want to be cleverer\"). So it might actually be that you're looking to find a narrow goal 'as terminal as the original vague goal' that provokes instrumental subgoals.</p>\n<p>5. Typically, when you make a computation faster you sacrifice some accuracy. This may be one of the cases where that often isn't true, because the computation time is <em>infinite</em> and thus accuracy is <em>0</em> for problems you cannot fit into memory if you try to solve them in one go. But the heuristics you use to narrow answer space can easily be bad heuristics; it helps to make this process formal so you're more likely to notice when you jumped an order without actually checking for other ways to approach the problem. Perhaps the best advice in this article is \"don't be afraid to go back and recalculate at lower orders and make sure you're in the right part of the tree.\"</p>\n<p>6. While it's tempting to suggest a measure like \"log(possible answers)\", that breaks down in many cases (when approaching \"find the real number that is pi<sup>5</sup>\", you don't see a change if you go from \"all reals\" to \"all reals between 3<sup>5</sup> and 4<sup>5</sup>\" as possible answers) and isn't valuable in others (if I reduce the answer space from 1,000 potential answers to 100 potential answers, but the real answer is in that 100, I've done better than if I reduce the answer space to 10 potential answer but the real answer <em>isn't</em> in that 10). The density of good solutions matters- you can only profit by throwing away parts of the answer space because their average is lower than the part you kept.</p>\n<p>Thanks to Aharon for the prodding to turn this from a brief mention in a comment to a post of its own, and to PhilGoetz, DSimon, and XFrequentist for organizational advice.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 2, "xexCWMyds6QLWognu": 2, "Ng8Gice9KNkncxqcj": 2, "eamWQNQ2dPYWEwhqr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dzqAbaz3Hf9ccYKqd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 33, "extendedScore": null, "score": 6.61149300241826e-07, "legacy": true, "legacyId": "4563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Mark Rosewater, a designer for Magic: The Gathering, <a href=\"http://www.wizards.com/magic/magazine/Article.aspx?x=mtg/daily/mm/92\">writes a lot</a> about how \"<a href=\"http://www.wizards.com/Magic/Magazine/Article.aspx?x=mtgcom/daily/mr103\">restrictions breed creativity</a>.\" The explanation he gives is simple: when someone is building a house, the more tools they have, the better off they are. But when someone is looking for something, the more space they have to explore, the <em>worse</em> off they are. This applies to answer space: the more narrowly defined your problem is, the easier it is to search an answers; you'll find both more answers and better answers by looking in a well-chosen smaller space. Oftentimes the hardest problems to find good answers for are the ones with the widest scope.<sup>1</sup></p>\n<p><a id=\"more\"></a>Most problems require some sort of creative thinking to overcome, and perhaps the greatest gains from this method come from applying it to your life goals. Imagine someone with a simple goal:<sup>2</sup> they want to improve themselves. That's <a href=\"/lw/jb/applause_lights/\">admirable</a>, but sort of bland and massively broad. It would be helpful to have a way to work from a bland, broad goal to a better goal- but what's better, in this context? We know that restrictions help, but what sort of restrictions help the most?</p>\n<p><strong id=\"Choosing_Restrictions\">Choosing Restrictions</strong></p>\n<p>In <a href=\"http://www.amazon.com/Getting-Things-Done-Stress-Free-Productivity/dp/0142000280/\">Getting Things Done</a>, David Allen argues that to-do lists should only include 'tasks.' That is, only write down clearly identified next actions towards achieve specific goals. \"Call Adam\" isn't a task, but \"Call Adam about hotel reservations for the conference\" is. This serves to reduce mental load (once you've written down the second, you can remove the task entirely from your mind, while you still need to keep a lot in memory for the first), to reduce the need to plan while doing, and to reduce the <a href=\"/lw/21b/ugh_fields/\">ugh field</a> associated with getting started. A good place for a goal to be, then, is a place where looking at the goal causes you to imagine the next task, even if you lost the to-do list where you had written down the next task and then purposefully forgotten it. So, <strong>actionable</strong> is a restriction that helps (even if the action is \"wait for X,\" it's a good idea to know what X is, so you can look out for it!).</p>\n<p>But at the same time, it helps when our goals are a sentence or a paragraph long, rather than a list of every subgoal and task. They should be kept <strong>simple</strong>, for the sake of both communication and <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">flexibility</a>. Finally, to encapsulate what's useful about restricting creativity in general, it should be <strong>specific</strong>. There are many actionable goals which present too many possible actions, and so we choose at random or do nothing at all.</p>\n<p>When talking about plans and goals, David Allen uses a plane's eye view analogy: goals are 0, 10k, 20k, 30k, 40k, or 50k feet above the ground. I prefer a math analogy- goals are worked out to 0th, 1st, 2nd, 3rd, and 4th order (and even further if necessary).<sup>3 </sup>Allen's analogy and mine work in opposite directions, and it's worthwhile to point out why. Allen's primary focus is (unsurprisingly) getting things done, and that happens at the task level. Traveling upwards is done to zoom out and obtain information, not to do work while in the clouds. A good visual analogy for my approach is a tree's root burrowing into the ground. At each spot, the root has a choice of where to go, and the point is to be there and soak up nutrients. The root also isn't traveling but <em>extending</em>- it still exists everywhere it was before. Allen is happy with a satellite photo, but I need a pipeline.</p>\n<p><strong id=\"Refining_a_Goal\">Refining a Goal</strong></p>\n<p>When we take a 0th order goal, like \"I want to improve,\" there are a pretty large number of ways we could make it more specific, and a staggering (literally) number of potential actions we could take to work towards that goal. We think about our options, and settle on \"I want to be cleverer.\" We still want to improve- we've just outlined a way to do so. But we've also <em>discarded</em> most of the ways we could improve! This is a valuable thing because it narrows our answer space. We could also say it <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">constrains our expectations</a> and our efforts; so we upgrade that goal to 1st order.</p>\n<p>Allen's analogy is robust because it has a strong anchor: the next task to do is at ground level. There is no strong anchor for a 0th order goal, just a heuristic about how to rank goals. We could have started off with \"I want to be cleverer\" as a 0th order goal, and the rest of this example would work out exactly the same- except with slightly different numbers. So don't focus on the numbers as much as the relationships between them and the changes in answer space.</p>\n<p>A 1st order goal, while specific, is generally still not actionable. Here is where it's important to keep refining the goal instead of being seduced into working. The first thing you can think of to make yourself cleverer is probably <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">not the best</a> thing you can do to make yourself cleverer. Again, our root pushes into the ground, seeking out nutrients, and we select an aspect of cleverness to focus on: \"I want to make better decisions.\" The thoughts produced by this 2nd order formulation are starting to become fertile, but we can do better.</p>\n<p><strong id=\"Stop_when_Satisfied\">Stop when Satisfied</strong></p>\n<p>A clarifying question - \"what do we mean by better?\" - gives us a 3rd order goal: \"I want to have a solid idea of how good a decision is.\" We could keep refining the goal endlessly, but at some point we have to stop planning and start producing. When is a good time to do this, given that you can only compare the present and past, not present and future? We don't have the assurance that this is a well-behaved problem where each additional step will change our answer space less than the previous step did, so we need to be cleverer about this choice than normal. One approach is the scale of resources involved- if you haven't gotten to the point where you could reasonably expect success with the resources you have to throw at the problem, keep drilling down until you've reached that point. If you're trying to decide on what your life's work should be, drill down until you've got a problem you can do significant work on in 20 years; don't stop when you hit a goal that would take fifty people fifty years.</p>\n<p>Another way is to look at how the goals intersect with each other- it seems like our root has curled in a different way moving from \"better decisions\" to \"understand decision quality\" than in its previous extensions- it seems like if we don't understand the problem of measuring a decision's quality, any other improvements we make <em>can't</em> succeed at \"make better decisions\" because we can't tell if the new decisions are higher quality than the old decisions! Beforehand, we were selecting from independent specializations. Now we're looking at a <em>necessary subgoal</em> instead of a <em>related goal</em>. Looking at this another way, we've been choosing more and more specific terminal values and have come across our first instrumental value.<sup>4</sup></p>\n<p>That suggests we've got enough to stop deciding what goal to pursue and start actually pursuing it. When you stop your goal-selection because of a fork like this, it's a good idea to look at what other goals are on the same order: while you should work on 3rd order problems before 4th order problems, problems of the same order are roughly equally important, and you may find you want to work on a different one or you can work on multiple of them in parallel.</p>\n<p>Note that even though we've make the decision to stop planning and start producing, we're probably going to run into some 4th order problems. Goals often have subgoals and instrumental values often have instrumental values based off them; the same methodology will work at every level and often represents a much faster way to search through answer space than brute force (especially since it's typically very hard to force your brain to brute force massive problems). Oftentimes there will be a domain-specific response which is more appropriate than this method, though (or, at least, resembles this method only in the abstract).</p>\n<p><strong id=\"Carve_at_the_Joints\">Carve at the Joints</strong></p>\n<p>One thing I have barely mentioned but is of crucial importance is that you need significant knowledge to effectively narrow down the answer space you're considering. Consider an international corporation trying to create a human resources department. Their 0th order goal might be something like \"make higher profits,\" their 1st order goal is \"streamline corporate functions to reduce cost without significantly reducing revenue,\" and their 2nd order goal might be \"task an entity with managing hiring, pay, benefits, and employee relations.\" Now they have a lot of 3rd order goals to choose from, and they decide \"create an HR office in each department.\" After all, they've already got their corporation partitioned that way, and having one HR department for R&amp;D and another for Sales will mean that the hiring expertise of each HR department is much better because of specialization.</p>\n<p>But this misses the reality of HR departments, which is that their functions are strongly tied to the nation that employees live and work in. The R&amp;D HR office might find itself having to deal with ten different sets of tax laws, requiring ten different tax specialists. Hiring laws in one country might require one procedure, while in another country they're totally different. The benefit of increased hiring specialization might not be unique to this plan- due to interviewing, travel costs, and legal changes, this setup probably requires one hiring officer for each department for each country, as well as a tax specialist for each country for each department. But if you split up the HR departments by country instead of by department, you would only need one tax specialist for each country, and the same number of hiring officers.</p>\n<p>There are three things to be learned from that example: first, hold off on proposing solutions (sound familiar?). Second, <a href=\"/lw/2p1/a_failure_to_evaluate_returnontime_fallacy/\">don't be afraid</a> to go back upwards and reevaluate your choice of goals. By choosing, you discarded a lot of answer space; if you don't find promising things in the region you looked, you should look somewhere else.</p>\n<p>Most importantly, we learn that the solution to a problem is often another problem. The answer we picked to \"I want to improve\" is \"I want to get cleverer,\" and we can think better and faster<sup>5</sup> if we treat that as a full answer. After all, if you reduce the answer space of \"a sentence 100 letters long\" to \"a sentence 10 letters long,\" you have reduced it by a <em>larger</em> factor than reducing from \"a sentence 10 letters long\" to a specific sentence that is 10 letters long.<sup>6</sup></p>\n<p>&nbsp;</p>\n<hr>\n<p>1. My artist friends tell me that their least favorite commissions are the ones where the commissioner tells them \"do whatever you want!\"; I don't think I've seen someone in any field ever speak positively of getting that regularly (instead of as an occasional reprieve).</p>\n<p>2. Style note: I use 'goal', 'problem', and 'value' interchangeably throughout this post, based on whatever seems appropriate for that sentence. I hope this isn't too confusing- I think there's only type errors for values, and so when you see value recast it as \"a goal to obtain this value.\"</p>\n<p>3. In physics (and many other disciplines), unsoluble problems are often approximated by an infinite number of soluble problems. For example, one can calculate sin(x) with only multiplication, division, addition, and subtraction by using the <a href=\"http://en.wikipedia.org/wiki/Taylor_expansion#Approximation_and_convergence\">Taylor Series</a> approximation. However, by itself this is just moving around the difficulty- your new problems are individually soluble but you don't have the time to solve an infinite number of them. This method is effective only when you can ignore later terms- that is, take the infinite amount of trash you've generated and manage to throw it away in a finite volume. For example, to calculate sin(1) to three parts in a thousand requires only the first three terms: 1-1<sup>3</sup>/3!+1<sup>5</sup>/5!=.841667 while sin(1) is .841471 (both rounded to 6 digits). For well-behaved approximations, the error is smaller than the next additional term- for sin(1) with 3 terms, the error is 1.96e-4 while the next term is 1<sup>7</sup>/7!=1.98e-4. My usage of \"order\" is inspired by this background; a first guess at a problem (like answering 1 to sin(1)) is a first order solution that's in the right ballpark but is probably missing crucial details. A second order solution has the most obvious modification to the first order solution and is generally rather good (5/6 only differs from sin(1) by 1%). One note here is this implies that for well-behaved problems, one needs to do all of the n<sup>th</sup> order modifications before moving to the n+1<sup>th</sup> order- if I just give you 1-1<sup>7</sup>/7!, my answer is not really any better than my 1st order answer (and if I gave you 1+1<sup>5</sup>/5!, it would be worse).</p>\n<p>4. The usefulness of wording things this way is limited because the boundary between the two is hard to determine. \"I want to make better decisions\" could easily be an instrumental value to a rather different problem (\"I want to be more powerful,\" say) or you could interpret it as an instrumental value for the previous value (\"I want to be cleverer\"). So it might actually be that you're looking to find a narrow goal 'as terminal as the original vague goal' that provokes instrumental subgoals.</p>\n<p>5. Typically, when you make a computation faster you sacrifice some accuracy. This may be one of the cases where that often isn't true, because the computation time is <em>infinite</em> and thus accuracy is <em>0</em> for problems you cannot fit into memory if you try to solve them in one go. But the heuristics you use to narrow answer space can easily be bad heuristics; it helps to make this process formal so you're more likely to notice when you jumped an order without actually checking for other ways to approach the problem. Perhaps the best advice in this article is \"don't be afraid to go back and recalculate at lower orders and make sure you're in the right part of the tree.\"</p>\n<p>6. While it's tempting to suggest a measure like \"log(possible answers)\", that breaks down in many cases (when approaching \"find the real number that is pi<sup>5</sup>\", you don't see a change if you go from \"all reals\" to \"all reals between 3<sup>5</sup> and 4<sup>5</sup>\" as possible answers) and isn't valuable in others (if I reduce the answer space from 1,000 potential answers to 100 potential answers, but the real answer is in that 100, I've done better than if I reduce the answer space to 10 potential answer but the real answer <em>isn't</em> in that 10). The density of good solutions matters- you can only profit by throwing away parts of the answer space because their average is lower than the part you kept.</p>\n<p>Thanks to Aharon for the prodding to turn this from a brief mention in a comment to a post of its own, and to PhilGoetz, DSimon, and XFrequentist for organizational advice.</p>", "sections": [{"title": "Choosing Restrictions", "anchor": "Choosing_Restrictions", "level": 1}, {"title": "Refining a Goal", "anchor": "Refining_a_Goal", "level": 1}, {"title": "Stop when Satisfied", "anchor": "Stop_when_Satisfied", "level": 1}, {"title": "Carve at the Joints", "anchor": "Carve_at_the_Joints", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "110 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dLbkrPu5STNCBLRjr", "EFQ3F6kmt4WHXRqik", "n5ucT5ZbPdhfGNLtP", "a7n8GdKiAZRX86T5A", "uHYYA32CKgKT3FagE", "RzdPXLd2b6qmEB2mf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-28T11:42:45.512Z", "modifiedAt": null, "url": null, "title": "Move the help button?", "slug": "move-the-help-button", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.416Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s8bW9jrMfFXQCQHDA/move-the-help-button", "pageUrlRelative": "/posts/s8bW9jrMfFXQCQHDA/move-the-help-button", "linkUrl": "https://www.lesswrong.com/posts/s8bW9jrMfFXQCQHDA/move-the-help-button", "postedAtFormatted": "Tuesday, December 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Move%20the%20help%20button%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMove%20the%20help%20button%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs8bW9jrMfFXQCQHDA%2Fmove-the-help-button%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Move%20the%20help%20button%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs8bW9jrMfFXQCQHDA%2Fmove-the-help-button", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs8bW9jrMfFXQCQHDA%2Fmove-the-help-button", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p>It took me a while to find the \"help\" button for comments, and we seem to have a steady stream of people who have trouble finding it on their own. I suspect that's because it's floating off in a corner you don't have much reason to look at. Would it be difficult to move it to the immediate right of the \"cancel\" button, instead of the far right, and possibly rename it \"formatting help\"?</p>\n<p>(I don't know enough python to look at the site's code, or I would check this out myself.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s8bW9jrMfFXQCQHDA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 24, "extendedScore": null, "score": 6.611503931696983e-07, "legacy": true, "legacyId": "4564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-29T03:52:55.984Z", "modifiedAt": "2020-12-18T02:45:35.041Z", "url": null, "title": "Is it \"bad\" to make fun of people/laugh at their weaknesses?", "slug": "is-it-bad-to-make-fun-of-people-laugh-at-their-weaknesses", "viewCount": null, "lastCommentedAt": "2021-11-08T18:16:38.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BM6NtBrBQtwibsosG/is-it-bad-to-make-fun-of-people-laugh-at-their-weaknesses", "pageUrlRelative": "/posts/BM6NtBrBQtwibsosG/is-it-bad-to-make-fun-of-people-laugh-at-their-weaknesses", "linkUrl": "https://www.lesswrong.com/posts/BM6NtBrBQtwibsosG/is-it-bad-to-make-fun-of-people-laugh-at-their-weaknesses", "postedAtFormatted": "Wednesday, December 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20it%20%22bad%22%20to%20make%20fun%20of%20people%2Flaugh%20at%20their%20weaknesses%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20it%20%22bad%22%20to%20make%20fun%20of%20people%2Flaugh%20at%20their%20weaknesses%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBM6NtBrBQtwibsosG%2Fis-it-bad-to-make-fun-of-people-laugh-at-their-weaknesses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20it%20%22bad%22%20to%20make%20fun%20of%20people%2Flaugh%20at%20their%20weaknesses%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBM6NtBrBQtwibsosG%2Fis-it-bad-to-make-fun-of-people-laugh-at-their-weaknesses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBM6NtBrBQtwibsosG%2Fis-it-bad-to-make-fun-of-people-laugh-at-their-weaknesses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 409, "htmlBody": "<p>When you make fun of someone, you are probably degrading their purity and disrespecting them (if we look at the results from the&nbsp;<a title=\"lesswrong thread\" href=\"/r/discussion/lw/369/link_compare_your_moral_values_to_the_general/\">lesswrong thread</a>&nbsp;on yourmorals.org, we can see that many of us consider purity/respect to be far less morally significant than most). Yet, making fun of other people does not intrinsically reduce their \"utility\" - rather - it is their reactions to being made fun of that reduce their own \"utility\".</p>\n<p>This, of course, does not justify making fun of people. Every negative action is only \"bad\" due to people's reactions to them. But in many cases, there is little reason to be upset when people make fun of you. When they make fun of you, they are gaining happiness over some weakness of yours. But is that necessarily a bad thing? It can be bad when they make fun of you in front of others and proceed to spread degrading information about you, causing other people to lose respect for you. But they could spread that information even when they're not making fun of you.&nbsp;</p>\n<p>Many people find it unusual that I actually laugh when people make fun of me (in fact, I sometimes find it uncomfortable when people defend me, since I sometimes even value the message of the person who's making fun of me). I usually find it non-threatening, and I'm even somewhat happy that my weaknesses resulted in the elevation of someone else's temporary happiness. I wonder if any rationalists feel the same way that I do. Of course, I will refrain from making fun of people if I think that they will be negatively affected by it. But it does make me wonder - what would it be like if no one cared if they were made fun of? Certainly, we must react to those who spread&nbsp;degrading&nbsp;information about ourselves. But does it really matter if others laugh at it?&nbsp;</p>\n<p>Of course, the prospect of amusing one's recipients is an incentive for some people to spread degrading information about you or your friends. So that may be one reason to counter it. On the other hand, though, laughter is also an incentive for people to spread degrading (and potentially true) information about your rivals. Perhaps people somewhat recognize this, and are frequently somewhat hypocritical about this (not that hypocrisy is intrinsically a bad thing).&nbsp;</p>\n<p>PS: I wonder how laughing at other's weaknesses fits in with Robin Hanson's norm-violation theory of humor. Other's people's weaknesses aren't exactly norm-violations.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BM6NtBrBQtwibsosG", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 0, "extendedScore": null, "score": 6.613955642886093e-07, "legacy": true, "legacyId": "4583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yEwsuCTHGwwbvFPaD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-12-29T03:52:55.984Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-29T10:57:33.252Z", "modifiedAt": null, "url": null, "title": "MoNETA: A Mind Made from Memristors [link]", "slug": "moneta-a-mind-made-from-memristors-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:06.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wMiz7ShSeER5AapqC/moneta-a-mind-made-from-memristors-link", "pageUrlRelative": "/posts/wMiz7ShSeER5AapqC/moneta-a-mind-made-from-memristors-link", "linkUrl": "https://www.lesswrong.com/posts/wMiz7ShSeER5AapqC/moneta-a-mind-made-from-memristors-link", "postedAtFormatted": "Wednesday, December 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MoNETA%3A%20A%20Mind%20Made%20from%20Memristors%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMoNETA%3A%20A%20Mind%20Made%20from%20Memristors%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwMiz7ShSeER5AapqC%2Fmoneta-a-mind-made-from-memristors-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MoNETA%3A%20A%20Mind%20Made%20from%20Memristors%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwMiz7ShSeER5AapqC%2Fmoneta-a-mind-made-from-memristors-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwMiz7ShSeER5AapqC%2Fmoneta-a-mind-made-from-memristors-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<div>&gt;<span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-weight: 500;\">DARPA's new memristor-based approach to AI consists of a chip that mimics how neurons process information</span></div>\n<div><br /></div>\n<div><a href=\"http://spectrum.ieee.org/robotics/artificial-intelligence/moneta-a-mind-made-from-memristors/0\">http://spectrum.ieee.org/robotics/artificial-intelligence/moneta-a-mind-made-from-memristors/0</a></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wMiz7ShSeER5AapqC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 6.615029202001935e-07, "legacy": true, "legacyId": "4585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-30T03:21:51.417Z", "modifiedAt": null, "url": null, "title": "Pandora earrings jewellery free postage on the internet", "slug": "pandora-earrings-jewellery-free-postage-on-the-internet", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beautifuljewelry", "createdAt": "2010-11-30T03:32:58.738Z", "isAdmin": false, "displayName": "beautifuljewelry"}, "userId": "Ridmt4FmL9zdEbpNP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n5nvhxegbiCyRdu7L/pandora-earrings-jewellery-free-postage-on-the-internet", "pageUrlRelative": "/posts/n5nvhxegbiCyRdu7L/pandora-earrings-jewellery-free-postage-on-the-internet", "linkUrl": "https://www.lesswrong.com/posts/n5nvhxegbiCyRdu7L/pandora-earrings-jewellery-free-postage-on-the-internet", "postedAtFormatted": "Thursday, December 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pandora%20earrings%20jewellery%20free%20postage%20on%20the%20internet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APandora%20earrings%20jewellery%20free%20postage%20on%20the%20internet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5nvhxegbiCyRdu7L%2Fpandora-earrings-jewellery-free-postage-on-the-internet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pandora%20earrings%20jewellery%20free%20postage%20on%20the%20internet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5nvhxegbiCyRdu7L%2Fpandora-earrings-jewellery-free-postage-on-the-internet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5nvhxegbiCyRdu7L%2Fpandora-earrings-jewellery-free-postage-on-the-internet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 378, "htmlBody": "<p>A slim silver band with paper hearts etched all around it would be  reasonably priced, and would make an especially okay gift in the  bargain.And also if you're <a href=\"http://www.pandoracharms-sale.com/\"><strong>pandora charms sale</strong></a> 1837 for a specific guy, heart-shaped jewelry need not be old fashioned  - in fact, your male may find it quirky and distinctive! Cufflinks with  a heart style, for example, could come in several colors, and go with  taste with your man's power fit. If you have a party guy with you,  bright red heart engraved cufflinks would make him the coveted by of  every Casanova in the house!But if you just aren't into the heart shape,  will not fret: springtime jewelry isn't really confined to that.<br /><br /> Jewelry may come in other springtime motifs, such as daisies and  butterflies, favorites of babies and young people everywhere. Should you  be Top quality <a href=\"http://www.pandoracharms-sale.com/pandora-bracelets\"><strong>pandora bracelet</strong></a> for anything cute to go with your new spg outfit, try a fun butterfly  brooch as well as jeweled hairclip.In springtime, young people just  adore to frolic in the cool outside the house. For this reason, picnics  and nature journeys are especially popular in the spring. Together with  flowers in full bloom as well as a temperate breeze blowing, the days  put in with loved ones outdoors sound almost perfect.<br /><br /> To make by far the most of these casual excursions, men and women like  to wear unobtrusive clothes, in the process discarding apparel that will  in the winter had been bulky as well as concealing. We see the  beautiful spring dresses, sleeveless or frilled, while using low-cut  collars that show off the previously hidden beauty of a young female's  neckline. When these cozy dresses come out, so do the actual comfortable  jewelry.When putting on a spring dress using a dipping neckline,  consider corresponding it with cascading jewelry, or a simple necklace  using a small pendant resting on your own collarbone.<br /><br /> Though the heart shape is exceedingly popular, it doesn't have to be popular or tacky: a heart-shaped brooch created from tiny <a href=\"http://www.pandoracharms-sale.com/pandora-beads\"><strong>pandora bead</strong></a> bracelets , as an example, speaks of sophistication and a good eye.  Neither does it have to be costly: heart-shaped earrings or silver  cardiovascular outline pendants could suit reasonably in one's funds.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n5nvhxegbiCyRdu7L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -6, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "4593", "legacySpam": true, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-30T07:14:01.341Z", "modifiedAt": null, "url": null, "title": "Some rationality tweets", "slug": "some-rationality-tweets", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Peter_de_Blanc", "createdAt": "2009-02-27T14:15:28.882Z", "isAdmin": false, "displayName": "Peter_de_Blanc"}, "userId": "vRvaAqR5tcjGEWaoC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/49ps6xE7N43962TZL/some-rationality-tweets", "pageUrlRelative": "/posts/49ps6xE7N43962TZL/some-rationality-tweets", "linkUrl": "https://www.lesswrong.com/posts/49ps6xE7N43962TZL/some-rationality-tweets", "postedAtFormatted": "Thursday, December 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20rationality%20tweets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20rationality%20tweets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F49ps6xE7N43962TZL%2Fsome-rationality-tweets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20rationality%20tweets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F49ps6xE7N43962TZL%2Fsome-rationality-tweets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F49ps6xE7N43962TZL%2Fsome-rationality-tweets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 698, "htmlBody": "<p><a href=\"/user/Will_Newsome/\">Will Newsome</a> has suggested that I repost my <a href=\"http://twitter.com/#!/spaceandgames\">tweets</a> to LessWrong. With some trepidation, and after going through my tweets and categorizing them, I picked the ones that seemed the most rationality-oriented. I held some in reserve to keep the post short; those could be posted later in a separate post or in the comments here. I'd be happy to expand on anything here that requires clarity.</p>\n<p><strong>Epistemology</strong></p>\n<ol>\n<li>Test your hypothesis on simple cases.</li>\n<li>Forming your own opinion is no more necessary than building your own furniture.</li>\n<li>The map is not the territory.</li>\n<li>Thoughts about useless things are not necessarily useless thoughts.</li>\n<li>One of the successes of the Enlightenment is the distinction between beliefs and preferences.</li>\n<li>One of the failures of the Enlightenment is the failure to distinguish whether this distinction is a belief or a preference.</li>\n<li>Not all entities comply with attempts to reason formally about them. For instance, a human who feels insulted may bite you.</li>\n</ol>\n<p><strong>Group Epistemology</strong></p>\n<ol>\n<li>The best people enter fields that accurately measure their quality. Fields that measure quality poorly attract low quality.</li>\n<li>It is not unvirtuous to say that a set is nonempty without having any members of the set in mind.</li>\n<li>If one person makes multiple claims, this introduces a positive correlation between the claims.</li>\n<li>We seek a model of reality that is accurate even at the expense of flattery.</li>\n<li>It is no kindness to call someone a rationalist when they are not.</li>\n<li>Aumann-inspired agreement practices may be cargo cult Bayesianism.</li>\n<li>Godwin's Law is not really one of the rules of inference.</li>\n<li>Science before the mid-20th century was too small to look like a target.</li>\n<li>If scholars fail to notice the common sources of their inductive biases, bias will accumulate when they talk to each other.</li>\n<li>Some fields, e.g. behaviorism, address this problem by identifying sources of inductive bias and forbidding their use.</li>\n<li>Some fields avoid the accumulation of bias by uncritically accepting the biases of the founder. Adherents reason from there.</li>\n<li>If thinking about interesting things is addictive, then there's a pressure to ignore the existence of interesting things.</li>\n<li>Growth in a scientific field brings with it insularity, because internal progress measures scale faster than external measures.<a id=\"more\"></a></li>\n</ol>\n<p><strong>Learning</strong></p>\n<ol>\n<li>It's really worthwhile to set up a good study environment. Table, chair, quiet, no computers.</li>\n<li>In emergencies, it may be necessary for others to forcibly accelerate your learning.</li>\n<li>There's a difference between learning a skill and learning a skill while remaining human. You need to decide which you want.</li>\n<li>It is better to hold the sword loosely than tightly. This principle also applies to the mind.</li>\n<li>Skills are packaged into disciplines because of correlated supply and correlated demand.</li>\n<li>Have a high discount rate for learning and a low discount rate for knowing.</li>\n<li>\"What would so-and-so do?\" means \"try using some of so-and-so's heuristics that you don't endorse in general.\"</li>\n<li>Train hard and improve your skills, or stop training and forget your skills. Training just enough to maintain your level is the worst idea.</li>\n<li>Gaining knowledge is almost always good, but one must be wary of learning skills.</li>\n</ol>\n<p><strong>Instrumental Rationality</strong></p>\n<ol>\n<li>As soon as you notice a pattern in your work, automate it. I sped up my book-writing with code I should've written weeks ago.</li>\n<li>Your past and future decisions are part of your environment.</li>\n<li>Optimization by proxy is worse than optimization for your true goal, but usually better than no optimization.</li>\n<li>Some tasks are costly to resume because of mental mode switching. Maximize the cost of exiting these tasks.</li>\n<li>Other tasks are easy to resume. Minimize external costs of resuming these tasks, e.g. by leaving software running.</li>\n<li>First eat the low-hanging fruit. Then eat all of the fruit. Then eat the tree.</li>\n<li>Who are the masters of forgetting? Can we learn to forget quickly and deliberately? Can we just forget our vices?</li>\n<li>What sorts of cultures will endorse causal decision theory?</li>\n<li>Big agents can be more coherent than small agents, because they have more resources to spend on coherence.</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "xgpBASEThXPuKRhbS": 2, "Zwc2JcT5az4e5YpJy": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "49ps6xE7N43962TZL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 62, "extendedScore": null, "score": 0.000117, "legacy": true, "legacyId": "4598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 62, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"/user/Will_Newsome/\">Will Newsome</a> has suggested that I repost my <a href=\"http://twitter.com/#!/spaceandgames\">tweets</a> to LessWrong. With some trepidation, and after going through my tweets and categorizing them, I picked the ones that seemed the most rationality-oriented. I held some in reserve to keep the post short; those could be posted later in a separate post or in the comments here. I'd be happy to expand on anything here that requires clarity.</p>\n<p><strong id=\"Epistemology\">Epistemology</strong></p>\n<ol>\n<li>Test your hypothesis on simple cases.</li>\n<li>Forming your own opinion is no more necessary than building your own furniture.</li>\n<li>The map is not the territory.</li>\n<li>Thoughts about useless things are not necessarily useless thoughts.</li>\n<li>One of the successes of the Enlightenment is the distinction between beliefs and preferences.</li>\n<li>One of the failures of the Enlightenment is the failure to distinguish whether this distinction is a belief or a preference.</li>\n<li>Not all entities comply with attempts to reason formally about them. For instance, a human who feels insulted may bite you.</li>\n</ol>\n<p><strong id=\"Group_Epistemology\">Group Epistemology</strong></p>\n<ol>\n<li>The best people enter fields that accurately measure their quality. Fields that measure quality poorly attract low quality.</li>\n<li>It is not unvirtuous to say that a set is nonempty without having any members of the set in mind.</li>\n<li>If one person makes multiple claims, this introduces a positive correlation between the claims.</li>\n<li>We seek a model of reality that is accurate even at the expense of flattery.</li>\n<li>It is no kindness to call someone a rationalist when they are not.</li>\n<li>Aumann-inspired agreement practices may be cargo cult Bayesianism.</li>\n<li>Godwin's Law is not really one of the rules of inference.</li>\n<li>Science before the mid-20th century was too small to look like a target.</li>\n<li>If scholars fail to notice the common sources of their inductive biases, bias will accumulate when they talk to each other.</li>\n<li>Some fields, e.g. behaviorism, address this problem by identifying sources of inductive bias and forbidding their use.</li>\n<li>Some fields avoid the accumulation of bias by uncritically accepting the biases of the founder. Adherents reason from there.</li>\n<li>If thinking about interesting things is addictive, then there's a pressure to ignore the existence of interesting things.</li>\n<li>Growth in a scientific field brings with it insularity, because internal progress measures scale faster than external measures.<a id=\"more\"></a></li>\n</ol>\n<p><strong id=\"Learning\">Learning</strong></p>\n<ol>\n<li>It's really worthwhile to set up a good study environment. Table, chair, quiet, no computers.</li>\n<li>In emergencies, it may be necessary for others to forcibly accelerate your learning.</li>\n<li>There's a difference between learning a skill and learning a skill while remaining human. You need to decide which you want.</li>\n<li>It is better to hold the sword loosely than tightly. This principle also applies to the mind.</li>\n<li>Skills are packaged into disciplines because of correlated supply and correlated demand.</li>\n<li>Have a high discount rate for learning and a low discount rate for knowing.</li>\n<li>\"What would so-and-so do?\" means \"try using some of so-and-so's heuristics that you don't endorse in general.\"</li>\n<li>Train hard and improve your skills, or stop training and forget your skills. Training just enough to maintain your level is the worst idea.</li>\n<li>Gaining knowledge is almost always good, but one must be wary of learning skills.</li>\n</ol>\n<p><strong id=\"Instrumental_Rationality\">Instrumental Rationality</strong></p>\n<ol>\n<li>As soon as you notice a pattern in your work, automate it. I sped up my book-writing with code I should've written weeks ago.</li>\n<li>Your past and future decisions are part of your environment.</li>\n<li>Optimization by proxy is worse than optimization for your true goal, but usually better than no optimization.</li>\n<li>Some tasks are costly to resume because of mental mode switching. Maximize the cost of exiting these tasks.</li>\n<li>Other tasks are easy to resume. Minimize external costs of resuming these tasks, e.g. by leaving software running.</li>\n<li>First eat the low-hanging fruit. Then eat all of the fruit. Then eat the tree.</li>\n<li>Who are the masters of forgetting? Can we learn to forget quickly and deliberately? Can we just forget our vices?</li>\n<li>What sorts of cultures will endorse causal decision theory?</li>\n<li>Big agents can be more coherent than small agents, because they have more resources to spend on coherence.</li>\n</ol>", "sections": [{"title": "Epistemology", "anchor": "Epistemology", "level": 1}, {"title": "Group Epistemology", "anchor": "Group_Epistemology", "level": 1}, {"title": "Learning", "anchor": "Learning", "level": 1}, {"title": "Instrumental Rationality", "anchor": "Instrumental_Rationality", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "80 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-30T11:03:49.505Z", "modifiedAt": "2022-01-07T00:13:24.347Z", "url": null, "title": "Every \"best paper\" from Computer Science conferences since 1996 [link]", "slug": "every-best-paper-from-computer-science-conferences-since", "viewCount": null, "lastCommentedAt": "2022-01-07T00:13:05.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TzqAStXgbNWJzkZ7K/every-best-paper-from-computer-science-conferences-since", "pageUrlRelative": "/posts/TzqAStXgbNWJzkZ7K/every-best-paper-from-computer-science-conferences-since", "linkUrl": "https://www.lesswrong.com/posts/TzqAStXgbNWJzkZ7K/every-best-paper-from-computer-science-conferences-since", "postedAtFormatted": "Thursday, December 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Every%20%22best%20paper%22%20from%20Computer%20Science%20conferences%20since%201996%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvery%20%22best%20paper%22%20from%20Computer%20Science%20conferences%20since%201996%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTzqAStXgbNWJzkZ7K%2Fevery-best-paper-from-computer-science-conferences-since%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Every%20%22best%20paper%22%20from%20Computer%20Science%20conferences%20since%201996%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTzqAStXgbNWJzkZ7K%2Fevery-best-paper-from-computer-science-conferences-since", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTzqAStXgbNWJzkZ7K%2Fevery-best-paper-from-computer-science-conferences-since", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://jeffhuang.com/best_paper_awards.html</p>\n<p><a href=\"http://news.ycombinator.com/item?id=2051437\">http://news.ycombinator.com/item?id=2051437</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TzqAStXgbNWJzkZ7K", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 6.618688087419832e-07, "legacy": true, "legacyId": "4600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-12-30T11:03:49.505Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-30T14:37:04.195Z", "modifiedAt": null, "url": null, "title": "Luminosity (Twilight Fanfic) Discussion Thread 3", "slug": "luminosity-twilight-fanfic-discussion-thread-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.186Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/od2x5h6Y6G4nJtgpD/luminosity-twilight-fanfic-discussion-thread-3", "pageUrlRelative": "/posts/od2x5h6Y6G4nJtgpD/luminosity-twilight-fanfic-discussion-thread-3", "linkUrl": "https://www.lesswrong.com/posts/od2x5h6Y6G4nJtgpD/luminosity-twilight-fanfic-discussion-thread-3", "postedAtFormatted": "Thursday, December 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Luminosity%20(Twilight%20Fanfic)%20Discussion%20Thread%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALuminosity%20(Twilight%20Fanfic)%20Discussion%20Thread%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fod2x5h6Y6G4nJtgpD%2Fluminosity-twilight-fanfic-discussion-thread-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Luminosity%20(Twilight%20Fanfic)%20Discussion%20Thread%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fod2x5h6Y6G4nJtgpD%2Fluminosity-twilight-fanfic-discussion-thread-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fod2x5h6Y6G4nJtgpD%2Fluminosity-twilight-fanfic-discussion-thread-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p>This is a thread for discussing <a href=\"http://luminous.elcenia.com\">my luminous!Twilight fic</a>, <a href=\"http://luminous.elcenia.com/all.shtml\">Luminosity</a> (inferior mirror <a href=\"http://www.fanfiction.net/s/6137139/1/Luminosity\">here</a>), its sequel <a href=\"http://luminous.elcenia.com/all2.shtml\">Radiance</a> (<a href=\"http://www.fanfiction.net/s/6460146/1/Radiance\">inferior mirror</a>), and related topics.</p>\n<p>PDFs, to be updated as the fic updates, are available of <a href=\"http://luminous.elcenia.com/pdfs/Luminosity_(Handwriting).pdf\">Luminosity</a> (<a href=\"http://luminous.elcenia.com/pdfs/Luminosity_(Arial).pdf\">other version</a>) and <a href=\"http://luminous.elcenia.com/pdfs/Radiance.pdf\">Radiance</a>.&nbsp; (PDFs courtesy of anyareine).&nbsp; Zack M Davis has created a <a href=\"http://www.mediafire.com/?6fyv14xftnn47mn\">mobi file of Radiance</a>.</p>\n<p>Initial discussion of the fic under a Harry Potter and the Methods of Rationality thread is <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality/29md?c=1&amp;context=1#29md\">here</a>.&nbsp; The first dedicated threads: <a href=\"/lw/2mq/luminosity_twilight_fanfic_discussion_thread/\">Part 1</a>, <a href=\"/lw/2y6/luminosity_twilight_fanfic_part_2_discussion/\">Part 2</a>.&nbsp; See also the <a href=\"/lw/1xh/living_luminously\">luminosity sequence</a> which contains some of the concepts that the Luminosity fic is intended to illustrate.&nbsp; (Disclaimer: in the fic, the needs of the story take precedence over the needs for didactic value where the two are in tension.)</p>\n<p>Spoilers are OK to post without ROT-13 for canon, all of Book 1, and Radiance up to the current chapter.&nbsp; Note which chapter (let's all use the numbering on my own webspace, rather than fanfiction.net, for consistency) you're about to spoil in your comment if it's big.&nbsp; People who know extra stuff (my <a href=\"/lw/2y6/luminosity_twilight_fanfic_part_2_discussion/2yoc\">betas</a> and people who have <a href=\"/lw/2y6/luminosity_twilight_fanfic_part_2_discussion/2zj6\">requested specific spoilers</a>) should keep mum about unpublished information they have.&nbsp; If you wish to join the ranks of the betas or the spoiled, contact me individually.</p>\n<p>Miscellaneous links: TV Tropes <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Fanfic/Luminosity\">page</a> (I <em>really really like it</em> when new stuff appears there) and <a href=\"http://tvtropes.org/pmwiki/posts.php?discussion=5dt6ub9yrpgzcwi5gjw1r0mf&amp;page=1\">thread</a>.&nbsp; <a href=\"http://syndicated.livejournal.com/luminosity_fic/\">Automatic Livejournal feed</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "od2x5h6Y6G4nJtgpD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 6.619227873180999e-07, "legacy": true, "legacyId": "4601", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 355, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hSeqgnc5CBJ643x9k", "rk7JtSmSSMpQsaQyi", "9o3Cjjem7AbmmZfBs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-30T16:44:30.626Z", "modifiedAt": null, "url": null, "title": "Looking for some pieces of transhumanist fiction", "slug": "looking-for-some-pieces-of-transhumanist-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:59.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rlpowell", "createdAt": "2009-03-05T00:57:26.519Z", "isAdmin": false, "displayName": "rlpowell"}, "userId": "nFgyJtHMChgKrhnvt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YSkG2CySuNQMbc88J/looking-for-some-pieces-of-transhumanist-fiction", "pageUrlRelative": "/posts/YSkG2CySuNQMbc88J/looking-for-some-pieces-of-transhumanist-fiction", "linkUrl": "https://www.lesswrong.com/posts/YSkG2CySuNQMbc88J/looking-for-some-pieces-of-transhumanist-fiction", "postedAtFormatted": "Thursday, December 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20some%20pieces%20of%20transhumanist%20fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20some%20pieces%20of%20transhumanist%20fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSkG2CySuNQMbc88J%2Flooking-for-some-pieces-of-transhumanist-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20some%20pieces%20of%20transhumanist%20fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSkG2CySuNQMbc88J%2Flooking-for-some-pieces-of-transhumanist-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYSkG2CySuNQMbc88J%2Flooking-for-some-pieces-of-transhumanist-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 435, "htmlBody": "<p>The first one: [EDIT: Found it!&nbsp; Thanks to <span class=\"comment-author\"><strong></strong></span><strong><a id=\"author_t1_395e\" href=\"../../../user/RolfAndreassen/\">RolfAndreassen</a></strong><strong>]</strong></p>\n<p>This is turning out to be *really* hard to find; I would have made a point of saving it if I'd expected no-one else to have heard of it.&nbsp; I need to make a page of all the weird singularity/transhuman fiction I've read.&nbsp; -_-</p>\n<p>Anyways, what I can remember:</p>\n<p>I think I read this on the web.&nbsp; I *think* it was a short story; at most novelette length.&nbsp; This was within the last 5 years or so.<br /><br /> Basically, it's the future, humans have done lots and lots of intelligence enhancement; each generation is smarter than the one before.&nbsp; Then we find a planet with alien ruins.&nbsp; There is a ship sent there.&nbsp; For reasons I can no longer remember, one of the people (female?) on the ship tries to destroy the ruins, and another tries to stop her (pretty sure male).&nbsp; The destroyer is younger, and hence smarter, than the protector, so he ends up taking lots of heavy-side-effect nootropics to keep up with her.&nbsp; The war is fought almost entirely by 3-D printed robots from the ships machine shops.</p>\n<p>The emphasis is very much on intelligence: that a standard deviation of IQ is going to determine the results of any strategy game (probably mostly true, given equal experience) and that war is basically that (also mostly true in this case, since the robots won't freak out and run).</p>\n<p>I particularily remember a scene in which the main character takes a drug that will up his IQ by 20 points or so for a while, at the expense of 12+ hours of very bad (insanity? unconsciousness? can't remember).&nbsp; Also waves of (remote control?) robots fighting on the surface of the planet below.</p>\n<p>The second one: [EDIT: Found!&nbsp; Thanks to nazgulnarsil]</p>\n<p>Humans develop AIs, which are fully benevolent and try to help/protect humanity.&nbsp; There end up being problems with the sun, and they try to fix it but create a horrible ice age, and eventually they just upload everybody and go looking for something better.&nbsp; They decide that stars are true problematic, and park humanity around an interstellar brown dwarf.</p>\n<p>One particular AI ship is somewhat eccentric and thinks that protecting humans isn't everything.&nbsp; A group of humans convince him to take them (or rather, their descendants) to earth.&nbsp; To prove they are capable of the (extremely long) journey, the ship requires that they live on him, without going anywhere, in a functional society for a thousand years.&nbsp; Then he takes them to earth.</p>\n<p>FWIW, I'm trying to make a page of all the singularity/transhuman stuff I've read; it's at <a href=\"http://teddyb.org/robin/tiki-index.php?page=Post-Singularity+And+Transhumanist+Fiction+I%27ve+Enjoyed&amp;no_bl=y\">http://teddyb.org/robin/tiki-index.php?page=Post-Singularity+And+Transhumanist+Fiction+I%27ve+Enjoyed&amp;no_bl=y</a> (just started).</p>\n<p>-Robin</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YSkG2CySuNQMbc88J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 6.619550499263412e-07, "legacy": true, "legacyId": "4602", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-12-30T19:01:05.812Z", "modifiedAt": null, "url": null, "title": "Subject X17's Surgery", "slug": "subject-x17-s-surgery", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4JQKpLrASpHhgQbk7/subject-x17-s-surgery", "pageUrlRelative": "/posts/4JQKpLrASpHhgQbk7/subject-x17-s-surgery", "linkUrl": "https://www.lesswrong.com/posts/4JQKpLrASpHhgQbk7/subject-x17-s-surgery", "postedAtFormatted": "Thursday, December 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Subject%20X17's%20Surgery&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASubject%20X17's%20Surgery%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4JQKpLrASpHhgQbk7%2Fsubject-x17-s-surgery%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Subject%20X17's%20Surgery%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4JQKpLrASpHhgQbk7%2Fsubject-x17-s-surgery", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4JQKpLrASpHhgQbk7%2Fsubject-x17-s-surgery", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 429, "htmlBody": "<p><strong>Edit:</strong>&nbsp;For an in-depth discussion of precisely this topic, see<em>&nbsp;</em>Nick Bostrom and Anders Sandberg's 2008 paper \"The Wisdom of Nature: An Evolutionary Heuristic for Human Enhancement\", available as a pdf <a href=\"http://www.nickbostrom.com/evolution.pdf\" target=\"_blank\">here</a>. &nbsp;This post was written before reading the paper.</p>\n<p>There doesn't seem to be a thread discussing Eliezer's short-short story <a href=\"http://yudkowsky.net/other/fiction/X17\" target=\"_blank\">X17</a>. &nbsp;While I enjoyed the story, and agreed with most of its points, I disagree with one assertion in it (and he's said it elsewhere, too, so I'm pretty sure he believes it). &nbsp;<strong>Edit:&nbsp;</strong>The story was written over a decade ago. &nbsp;Eliezer seems to have at least partially recanted since then.</p>\n<p>Eliezer argues that there can't possibly be a simple surgical procedure that dramatically increases human intelligence. &nbsp;Any physical effect it could have, he says, would necessarily have arisen before as a mutation. &nbsp;Since intelligence is highly beneficial in any environment, the mutation would spread throughout our population. &nbsp;Thus, evolution must have already plucked all the low-hanging fruit.</p>\n<p>But I can think of quite a few reasons why this would not be the case. &nbsp;Indeed, my belief is that such a surgery almost certainly exists (but it might take a superhuman intelligence to invent it). &nbsp;Here are the possibilities that come to mind.</p>\n<p>&nbsp;</p>\n<ol>\n<li>The surgery might introduce some material a human body can't synthesize.<sup>1</sup></li>\n<li>The surgery might require intelligent analysis of the unique shape of a subject's brain, after it has developed naturally to adulthood.</li>\n<li>The necessary mutation might simply not exist. &nbsp;The configuration space for physically possible organisms must surely be larger than the configuration space for human-like DNA (I get the sense I'm taking sides in a longstanding feud in evolutionary theory with this one).</li>\n<li>The surgery might have some minor side effect that would drastically reduce fitness in the ancestral environment, but isn't noticeable in the present day. &nbsp;Perhaps it <a href=\"http://en.wikipedia.org/wiki/Blood_Music_(novel)\" target=\"_blank\">harnesses the computing power of the subject's lymphocytes</a>, weakening the immune system.</li>\n</ol>\n<div><br /></div>\n<div>I wonder if perhaps these possibilities are specifically ruled out in the<em>&nbsp;</em><em>Lensman</em><em>&nbsp;</em>scene this is parodying. &nbsp;I haven't read any of it. &nbsp;In that case, Eliezer is saying something weaker than he seems to be. &nbsp;But my guess is we really do have vastly differing intuitions on this.</div>\n<div><br /></div>\n<div><sup>1</sup>The Baron may not even realize that his vanadium scalpel is essential to the process! &nbsp;I've read that early blacksmiths believed, incorrectly, that a charcoal fire was hotter than any other fire. &nbsp;They believed this because iron smelted over a charcoal fire ended up stronger and more malleable. &nbsp;In fact, this happened because small amounts of carbon from the charcoal were bonding with the metal.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4JQKpLrASpHhgQbk7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 6.619896310476441e-07, "legacy": true, "legacyId": "4603", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}