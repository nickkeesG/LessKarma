{"results": [{"createdAt": null, "postedAt": "2014-01-24T17:55:50.501Z", "modifiedAt": null, "url": null, "title": "European Community Weekend in Berlin", "slug": "european-community-weekend-in-berlin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:30.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jSCEaE5TbYnbFCQYB/european-community-weekend-in-berlin", "pageUrlRelative": "/posts/jSCEaE5TbYnbFCQYB/european-community-weekend-in-berlin", "linkUrl": "https://www.lesswrong.com/posts/jSCEaE5TbYnbFCQYB/european-community-weekend-in-berlin", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20European%20Community%20Weekend%20in%20Berlin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEuropean%20Community%20Weekend%20in%20Berlin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjSCEaE5TbYnbFCQYB%2Feuropean-community-weekend-in-berlin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=European%20Community%20Weekend%20in%20Berlin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjSCEaE5TbYnbFCQYB%2Feuropean-community-weekend-in-berlin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjSCEaE5TbYnbFCQYB%2Feuropean-community-weekend-in-berlin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 532, "htmlBody": "<p>The Berlin Meetup Group is organizing the first European community meetup. We are planning a fun weekend with a focus on bringing the LessWrong community closer together. As a treat, some participants offer rationality exercises and workshops. <br /><br />If you like your local meetup we hope you will like this too. It is similar, but bigger: You will get to meet and exchange ideas with a diverse set of awesome people from all across Europe. And if you don&rsquo;t have a meetup nearby or didn&rsquo;t get around to participating yet, this is a great opportunity to get in touch with the rest of the community.<br /><br />The community weekend will take place April 11-13, from Friday evening to Sunday early afternoon, in the <a href=\"http://globetrotterhostel.de/\">Odyssee Hostel</a> in Berlin. The cost is 70 &euro; including accommodation and breakfast. A conference room with a projector and wifi will also be available during daytime. <br /><a id=\"more\"></a><br />The event is participant driven so you (yes, you) are very welcome to help with the content. Please contact Tristan (wegnertristan - at - googlemail - dot - com) for planning and scheduling.<br /><br />Friday evening we will welcome everyone around 17:00 and proceed to have dinner together. Sleeping quarters (shared rooms) will already be available from 16:00. The evening can be used for getting to know each other better or exploring the city in small groups. We also made sure that there are plenty of cultural, party and sightseeing offers available throughout the city.<br /><br />Saturday after breakfast, there will be a semi-structured program in the conference room. Offers will include a workshop for habit implementation, a science based introduction to meditation along with a practice session, and clicker training. We also encourage everyone to take part in lightning talks, where you are invited to give a short 5min presentation on any topic you are passionate about.<br /><br />Saturday evening can be further used to nerd out, explore the nightlife or participate in an exercise in social comfort zone expansion.<br /><br />Sunday will have room for additional projects. We will say goodbye in the early afternoon to give us enough time to head home. Of course everyone is free to prolong the stay in the greatest capital of Germany.<br /><br />By the end of the event we hope that we will have made new friends, exchanged ideas, received valuable feedback and started a joint world optimization project or two.<br /><br />To sign up, email John (johncryptfrink - at - gmail - dot - com), preferably before the first of February. The number of participants is limited, so register early to make sure you can participate. <br /><br />Looking forward to seeing you<br />John, Tristan, Alexander, Matthias, Christian&hellip; &amp; everyone else from the Berlin LessWrong meetup</p>\n<p><strong>UPDATE:</strong> We're now at slightly over 40 participants and can't accept any more people. Please mail John anyway if you'd have wanted to come! Things may still work out if someone else can't make it after all - and it will help us have a better idea of how many people to plan for next time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jSCEaE5TbYnbFCQYB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 50, "extendedScore": null, "score": 0.000136, "legacy": true, "legacyId": "25340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T20:22:00.187Z", "modifiedAt": null, "url": null, "title": "Gauging Interest: Santa Barbara, CA Meetup", "slug": "gauging-interest-santa-barbara-ca-meetup", "viewCount": null, "lastCommentedAt": "2014-01-24T20:22:00.187Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TylerJay", "createdAt": "2010-08-16T22:37:13.189Z", "isAdmin": false, "displayName": "TylerJay"}, "userId": "rR64xYGdnRFZ5MPQc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vEz5aYY2amcLnwLsk/gauging-interest-santa-barbara-ca-meetup", "pageUrlRelative": "/posts/vEz5aYY2amcLnwLsk/gauging-interest-santa-barbara-ca-meetup", "linkUrl": "https://www.lesswrong.com/posts/vEz5aYY2amcLnwLsk/gauging-interest-santa-barbara-ca-meetup", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gauging%20Interest%3A%20Santa%20Barbara%2C%20CA%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGauging%20Interest%3A%20Santa%20Barbara%2C%20CA%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEz5aYY2amcLnwLsk%2Fgauging-interest-santa-barbara-ca-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gauging%20Interest%3A%20Santa%20Barbara%2C%20CA%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEz5aYY2amcLnwLsk%2Fgauging-interest-santa-barbara-ca-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEz5aYY2amcLnwLsk%2Fgauging-interest-santa-barbara-ca-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p>I live in Santa Barbara. &nbsp;Los Angeles is a bit too far away for me to travel for LW meetups, so I wanted to see if anyone would be interested in a Santa Barbara meetup. &nbsp;I would be happy to arrange it and plan content.</p>\n<p>If anyone would be interested in attending *at least one* Santa Barbara meetup provided that it was on a day and time that worked for your schedule, please post a comment or send me a private message.</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vEz5aYY2amcLnwLsk", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5296402291026181e-06, "legacy": true, "legacyId": "25357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 0, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-01-24T20:22:00.187Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T20:30:25.950Z", "modifiedAt": null, "url": null, "title": "\"A Definition of Subjective Probability\" by Anscombe and Aumann", "slug": "a-definition-of-subjective-probability-by-anscombe-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.071Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zPCimXxSqzFPgezJJ/a-definition-of-subjective-probability-by-anscombe-and", "pageUrlRelative": "/posts/zPCimXxSqzFPgezJJ/a-definition-of-subjective-probability-by-anscombe-and", "linkUrl": "https://www.lesswrong.com/posts/zPCimXxSqzFPgezJJ/a-definition-of-subjective-probability-by-anscombe-and", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22A%20Definition%20of%20Subjective%20Probability%22%20by%20Anscombe%20and%20Aumann&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22A%20Definition%20of%20Subjective%20Probability%22%20by%20Anscombe%20and%20Aumann%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPCimXxSqzFPgezJJ%2Fa-definition-of-subjective-probability-by-anscombe-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22A%20Definition%20of%20Subjective%20Probability%22%20by%20Anscombe%20and%20Aumann%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPCimXxSqzFPgezJJ%2Fa-definition-of-subjective-probability-by-anscombe-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPCimXxSqzFPgezJJ%2Fa-definition-of-subjective-probability-by-anscombe-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 673, "htmlBody": "<p>In the course of studying how subjective probabilities can be defined, I read&nbsp;<a href=\"http://www.econ.ucsb.edu/~tedb/Courses/GraduateTheoryUCSB/anscombeaumann.pdf\">A Definition of Subjective Probability</a>&nbsp;(1963) by Anscombe and Aumann. My notes may be of interest to the Less Wrong community, and have pasted them below.</p>\n<p>The authors define two types of lotteries:</p>\n<ul>\n<li>A \"roulette lottery\" which is a game of chance with \"physical\" probabilities attached to outcomes, where each outcome is associated with a prize. The authors are vague about what they mean by \"physical\" probabilities, but they seem to mean probabilities that it's possible to generate via <a href=\"http://en.wikipedia.org/wiki/Frequentist_inference\">frequentist inference</a>.</li>\n<li>A \"horse lottery,\" which is a game of chance where physical probabilities are unavailable.</li>\n</ul>\n<p>The paper's goal is to give a definition of subjective probabilities attached to outcomes in a horse lottery.</p>\n<p>Intuitively, the idea seems to be as follows. Suppose that you have an event E, that you desire to happen, and a choice between the following options:</p>\n<ol>\n<li>A horse lottery occurs, and event E occurs if and only if the outcome of the horse lottery is O.</li>\n<li>A roulette lottery occurs, and event E occurs if and only if the outcome is O', where O' has probability q.</li>\n</ol>\n<p>Consider the set T of values of q such that you'd prefer #2 over #1. Then your subjective probability p of the horse lottery having outcome O is defined to be the greatest lower bound of T.</p>\n<p><a id=\"more\"></a></p>\n<p>The authors begin by assuming that one has a preference ordering over the prizes awarded in lotteries, with the best prize strictly favored over the worst prize. Here the prizes include tickets to other lotteries.</p>\n<p>The authors convert this preference ordering to a utility function u where the best prize is assigned utility 1 and the worst prize is assigned utility 0. The authors assume that the function u has the property that u of a roulette lottery is the expected utility (sum of utilities of the outcomes weighted by the probabilities of the outcomes). The authors also convert a preference ordering over horse lotteries to a utility function u<sup>*</sup>. We know that u<sup>*</sup>&nbsp;is 1 for the lottery that gives the best prize with probability 1 and u<sup>*</sup>&nbsp;is 0 for the horse lottery that gives the worst prize with probability 1. At this point it's not meaningful to say that that u<sup>*</sup>&nbsp;of a horse lottery is the expected utility, because the probabilities associated with outcomes of the horse lottery have not been defined.</p>\n<p>The authors then consider the set of horse lotteries h with the same <em>a priori</em> possible outcomes O<sub>i</sub>&nbsp;and the same actual outcome,&nbsp;<em>where the prizes are tickets for roulette lotteries&nbsp;</em>R<sub>i</sub>.The main theorem of the paper is that&nbsp;there exist nonnegative numbers p<sub>i</sub>&nbsp;summing to 1 such that&nbsp;u<sup>*</sup>&nbsp;of the horse lottery is given by the the sum of p<sub>i</sub>u(R<sub>i</sub>), independently of the roulette lotteries R<sub>i</sub>. Then p<sub>i</sub>&nbsp;is taken to be the definition of the subjective probability that one assigns to outcome O<sub>i</sub>.&nbsp;</p>\n<p>The candidate for p<sub>i</sub>&nbsp;is u<sup>*</sup>(k<sub>i</sub>), where k<sub>i</sub>&nbsp;is the horse lottery&nbsp;where outcome&nbsp;O<sub>i</sub>&nbsp;is associated with the best prize and the other outcomes are associated with the worst prize.&nbsp;</p>\n<p>To prove the theorem, let c be the sum of the numbers u(R<sub>i</sub>). For the sake of clarity, suppose that c &lt; 1 (where the idea of the proof is most evident). We have</p>\n<ul>\n<li><strong>Claim 1: </strong>u<sup>*</sup>(h) = cu<sup>*</sup>(h'), where h' is the horse race associating outcome&nbsp;O<sub>i</sub>&nbsp;to a roulette lottery&nbsp;S<sub>i</sub>&nbsp;with u(S<sub>i</sub>) = u(R<sub>i</sub>)/c. (These utilities are admissible since u(R<sub>i</sub>)/c is no larger than 1, from the definition of c).</li>\n<li><strong>Claim 2:</strong>&nbsp;u<sup>*</sup>(h') = u(S), where S is the roulette lottery that with probability&nbsp;u(S<sub>i</sub>)&nbsp;gives a ticket to the horse lottery&nbsp;k<sub>i</sub>. (Here S is well defined since the&nbsp;u(S<sub>i</sub>)'s sum to 1, from the definition of c.)&nbsp;This is the most subtle step, and the core of the theorem. Unpackaging it in words: a horse lottery where each outcome&nbsp;O<sub>i</sub>&nbsp;is associated with a utility u(S<sub>i</sub>) is equivalent to&nbsp;(for each i)&nbsp;there being a u(S<sub>i</sub>) chance of getting a horse lottery where outcome&nbsp;O<sub>i</sub>&nbsp;is associated with utility 1 .<sub>&nbsp;</sub></li>\n<li><strong>Claim 3:&nbsp;</strong>We can write&nbsp;u(S) = Sum of u(S<sub>i</sub>) x&nbsp;u<sup>*</sup>(k<sub>i</sub>)&nbsp;</li>\n</ul>\n<p>Combining these gives the desired theorem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 2, "ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zPCimXxSqzFPgezJJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 14, "extendedScore": null, "score": 1.5296496216846367e-06, "legacy": true, "legacyId": "25355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T20:36:07.078Z", "modifiedAt": null, "url": null, "title": "How Not to Make Money", "slug": "how-not-to-make-money", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N2BbX4aCTM7FTmPyT/how-not-to-make-money", "pageUrlRelative": "/posts/N2BbX4aCTM7FTmPyT/how-not-to-make-money", "linkUrl": "https://www.lesswrong.com/posts/N2BbX4aCTM7FTmPyT/how-not-to-make-money", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20Not%20to%20Make%20Money&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20Not%20to%20Make%20Money%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN2BbX4aCTM7FTmPyT%2Fhow-not-to-make-money%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20Not%20to%20Make%20Money%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN2BbX4aCTM7FTmPyT%2Fhow-not-to-make-money", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN2BbX4aCTM7FTmPyT%2Fhow-not-to-make-money", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1338, "htmlBody": "<p><em><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">Sarcastic Practical Advice Series</span></em><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">: 1 How Not to Make Money</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">I'm calling this a series because I would like it to be a series, feel free to write your own post on \"how not to do something many people want to do\", especially you,&nbsp;future me.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">I'm very good at not making money, and maybe this is a skill you have found yourself<span>&nbsp;</span></span><span style=\"font-family: Verdana, sans-serif;\"><a href=\"http://www.nytimes.com/2014/01/19/opinion/sunday/for-the-love-of-money.html?_r=0\"><span lang=\"EN-US\">needing to perfect</span></a></span><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">But worry not. Stop <em>rationalizing</em>!&nbsp;I'll teach you some of the craft before you can say all the palindromes in the Finnish language.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(1) Be one of those people who actually turn knowledge, general knowledge, into personally designed actions/policies. The kind of people who, upon learning that driving is more dangerous than being attacked by spiders, and experiencing the first person evolved fear of spiders, understands that he should be as afraid of driving badly as he is of spiders, or much more, and drives accordingly.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(2) Understand that there is no metaphysical&nbsp;Self, only a virtual center of narrative gravity (Read Dennett), whose manner of discounting time is hyperbolic (Read George Ainslie), weirdly self-representative (Read GEB), and basically<span>&nbsp;</span></span><span style=\"font-family: Verdana, sans-serif;\"><a href=\"http://wiki.lesswrong.com/wiki/Adaptation_executors\"><span lang=\"EN-US\">a mess</span></a></span><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(3) Read Reasons and Persons, by Parfit, and really give up on your Na&iuml;ve intuitions about personal identity over time. Using (1) act accordingly, i.e. screw<span>&nbsp;</span></span><span style=\"font-family: Verdana, sans-serif;\"><a href=\"/lw/hfw/why_is_it_rational_to_invest_in_retirement_i_dont/\"><span lang=\"EN-US\">future retired you</span></a></span><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(4) Go through a university program in the humanities, so no one tempts you by throwing money at you after you graduate - This has happened to an academically oriented friend of mine who graduated a Medical Doctor, but actually wanted to be in the lab playing with brains. - If you can make into Greek Mythology, or Iranian Literature, good for you, Philosophy is ok, as are social sciences, as long as you do theory and don't get into politics or institutional design later on. If you go to psychology, you are dangerously near Human Resources, so be sure to be doing it for the reasons Pinker would do it, because you want to understand our internal computer, not to treat people.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(5) Have some cash: This seems obvious, but it&rsquo;s worth reminding if you are a machine discounting hyperbolically, you'd better be safe for the next two months.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(6) Study research on happiness and money: Money doesn't buy happiness, and when it does, it's by<span>&nbsp;</span></span><span style=\"font-family: Verdana, sans-serif;\"><a href=\"http://www.youtube.com/watch?v=PsihkFWDt3Y\"><span lang=\"EN-US\">buying things to others</span></a></span><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">, <em>regardless of Price.<span>&nbsp;</span></em>Giving a bike, a Porsche, or a Starbucks coffee to your friends provides you the same amount of fuzzies. Use (1) act accordingly.&nbsp;</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(7) Be curious: If you are the kind of person who knows by heart that the Finish language is more propense to palindromization, you are in a great route not to make money. If you get<span>&nbsp;</span></span><span style=\"font-family: Verdana, sans-serif;\"><a href=\"http://www.explainxkcd.com/wiki/images/b/bf/theft.png\"><span lang=\"EN-US\">really excited about space</span></a></span><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">, good for you. If you are so moved by curiosity you can't sleep before you finally figure it out, worry not, money ain't coming your way.&nbsp;Don&rsquo;t forget <a href=\"/lw/h3f/drowning_in_an_information_ocean/\">all those really cool books</a> you want to read. </span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(8) Avoid being Anhedonic:<span>&nbsp;</span></span><span style=\"font-family: Verdana, sans-serif;\"><a href=\"http://en.wikipedia.org/wiki/Anhedonia\"><span lang=\"EN-US\">Anhedonia</span></a></span><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">&nbsp;is one of the great enemies of those who don't want to make money. If all feels more or less the same to you, there is great incentive to go after the gold, it won't harm you much, and it will afford you the number one value of the Anhedonic, a false sense of security, and the illusion that happiness lies somewhere ahead of you in the future. &nbsp;If you can be thrilled or excited by the latest Adam Sandler movie, if a double rainbow will make you cry like a baby even in a video, and if you watch <a href=\"http://www.youtube.com/watch?v=LgFJJ77w2nA\">this sax video</a> with a young, healthy, fertile female more than once because it&rsquo;s a good video, rest assured, you&rsquo;ll be fine. </span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\" lang=\"EN-US\">(9) What do you care what other people think?: <br /> Feynman nailed this aspect of the no-money making business. You may not have noticed but everyone, especially your family, thinks you should make money, Graham says </span></p>\n<blockquote><address><em><span style=\"font-size: 12.0pt; font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-ansi-language: EN-US; mso-fareast-language: PT-BR;\" lang=\"EN-US\">All parents tend to be more&nbsp;</span></em><em><span style=\"font-size: 12.0pt; font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-ansi-language: EN-US; mso-fareast-language: PT-BR;\" lang=\"EN-US\">conservative for their kids than they would for themselves, simply because, as&nbsp;</span></em><em><span style=\"font-size: 12.0pt; font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-ansi-language: EN-US; mso-fareast-language: PT-BR;\" lang=\"EN-US\">parents, they share risks more than rewards. If your eight year old son decides&nbsp;</span></em><em><span style=\"font-size: 12.0pt; font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-ansi-language: EN-US; mso-fareast-language: PT-BR;\" lang=\"EN-US\">to climb a tall tree, or your teenage daughter decides to date the local bad&nbsp;</span></em><em><span style=\"font-size: 12.0pt; font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-ansi-language: EN-US; mso-fareast-language: PT-BR;\" lang=\"EN-US\">boy, you won&rsquo;t get a share in the excitement, but if your son falls, or your&nbsp;</span></em><em><span style=\"font-size: 12.0pt; font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-ansi-language: EN-US; mso-fareast-language: PT-BR;\" lang=\"EN-US\">daughter gets pregnant, you&rsquo;ll have to deal with the consequences. -&nbsp;</span></em><span style=\"font-size: 12.0pt; font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-ansi-language: EN-US; mso-fareast-language: PT-BR;\" lang=\"EN-US\">How to do what you love. </span></address></blockquote>\n<p><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\" lang=\"EN-US\">It&rsquo;s not just parents; everyone gets more shares of your money than of your excitement. If this was not the case, Effective Altruists would be advocating roller coasters and volcano lairs with cat people, not high income careers. </span></p>\n<p><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\" lang=\"EN-US\">(10) Couchsurf and meet couchsurfers and world travelers: If you never did it, go around <a href=\"https://www.couchsurfing.org/\">couchsurfing</a> for a while. As it happens, due to many factors, travelling all the time, a dream of the majority, is cheaper than staying in one spot. Meeting world travelers like 1Mac Madison, 2Puneet Sahani, 3Frederico Balbiani, and 4Rand Hunt &nbsp;made me realize, respectively, that: 1 It&rsquo;s possible to travel 2/3<sup>rd</sup> of the time as a CS major; 2 Indian Citizenship and zero money won&rsquo;t stop you; 3 Not speaking English or wanting to work with what gave you degrees doesn&rsquo;t stop you; 4 Spending 90 dollars in 100 days is possible. You&rsquo;ll feel much less pressure to make money after meeting similar people and being one of them. </span></p>\n<p><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\" lang=\"EN-US\">(11) Don&rsquo;t experience Status Anxiety: The World suffers from an intense affliction. Alain de Botton named it <a href=\"http://www.youtube.com/watch?v=_acgqf27CIU\">Status Anxiety</a>. You are not just richer than most people nowadays. You are unimaginably, unbelievably wealthy (in term of resources you can use) in comparison to <a href=\"http://4sightsunglasses.com/wp-content/uploads/2011/02/World-GDP-per-Capita-1-2008-AD.jpg\">everyone that ever lived</a>. &nbsp;But the point is, the less time you spend comparing, regardless of who you are comparing with, the happier you feel.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\" lang=\"EN-US\">(12) Be persuadable by intellectuals outside traditional science, like De Botton and <a href=\"http://www.youtube.com/watch?v=45kNqUF6kC4\">Alan Watts</a>, but not by really terrible The Secret style self-help. </span></p>\n<p><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\" lang=\"EN-US\">(13) Consider money over-valued: In economics, the price of things is determined by the supply and demand of that particular thing. The interesting thing is that demand is not measured by how many people want something how badly, but this multiplied by each person&rsquo;s wealth&hellip; &nbsp;If so many (wealthy) people value Rolex watches, they will be overpriced for you, especially if they are paying in <em>luck</em>, inheritance, or interest, and you are paying in <em>work</em> (though both use money as a medium). <br /> <em>Money is a medium of trade, how could it be over-valued?</em> <br /> Simple, there are many other mediums of trade (being nice, becoming more attractive, being a good listener, going to the &ldquo;right place at the right time&rdquo;, knowledge, enviable skills, prestige, dominance, strength, signaling, risk &ndash; i.e. stealing, Vegas, or bitcoin - , sex, time, energy). If you think these items are cheaper than money, you go for them as your medium of trade. And indeed they are cheaper than money, because everyone knows that money is valuable, and nearly no one thought consciously of the trade value of those things.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\" lang=\"EN-US\">(14) Fake it till you don&rsquo;t make it: My final advice would be to try out not spending money. Do it for a month (I did it for two), set a personal unbearably low barrier according to your standards. Dine before going to dinner with friends, by bike, of course. Carry water instead of buying it. Deny any social activity that would be somewhat costly and substitute it for some personal project, internet download, or analogous near-free alternative. Exercise outside, not in the gym. Take notes on how good your days were, you may find out, as did Kingsley that: &ldquo;We act as though comfort and luxury were the chief requirements of life, when all that we need to make us happy is something to be enthusiastic about.&rdquo; &nbsp;Furthermore, <a href=\"http://www.ted.com/talks/barry_schwartz_on_the_paradox_of_choice.html\">with Barry Schwartz</a>, you may find out that less is more, and when you have fewer options of what to do, this gives you not only happiness, but extra capacity to use your <a href=\"http://en.wikipedia.org/wiki/Attention_economy\">psychological attention</a> to actually do what you want to do, do as<a href=\"http://thegrio.com/2012/09/13/why-president-obama-is-always-wearing-a-blue-or-gray-suit/\">&nbsp;Obama did</a>, save your precious share of mindspace. </span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Verdana, sans-serif; background-position: initial initial; background-repeat: initial initial;\" lang=\"EN-US\">There, I hope you feel more fully equipped not to make money, should you ever need this hard earned, practical life-skill. You&rsquo;re welcome.&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N2BbX4aCTM7FTmPyT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": -8, "extendedScore": null, "score": 1.5296559568755058e-06, "legacy": true, "legacyId": "25358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DBajPuTRK9x26jrvh", "fRr8625imjoP7FLFs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-25T00:50:24.991Z", "modifiedAt": null, "url": null, "title": "Can We Do Without Bridge Hypotheses?", "slug": "can-we-do-without-bridge-hypotheses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:28.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/heJZLrC6EeJaskLbu/can-we-do-without-bridge-hypotheses", "pageUrlRelative": "/posts/heJZLrC6EeJaskLbu/can-we-do-without-bridge-hypotheses", "linkUrl": "https://www.lesswrong.com/posts/heJZLrC6EeJaskLbu/can-we-do-without-bridge-hypotheses", "postedAtFormatted": "Saturday, January 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20We%20Do%20Without%20Bridge%20Hypotheses%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20We%20Do%20Without%20Bridge%20Hypotheses%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FheJZLrC6EeJaskLbu%2Fcan-we-do-without-bridge-hypotheses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20We%20Do%20Without%20Bridge%20Hypotheses%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FheJZLrC6EeJaskLbu%2Fcan-we-do-without-bridge-hypotheses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FheJZLrC6EeJaskLbu%2Fcan-we-do-without-bridge-hypotheses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1027, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/jd9/building_phenomenological_bridges/\">Building Phenomenological Bridges</a>, <a href=\"/lw/on/reductionism/\">Reductionism</a></p>\n<hr />\n<p>&nbsp;</p>\n<p>Bridge hypotheses are extremely awkward. It's risky to draw permanent artificial lines between categories of hypothesis ('physical' vs. 'bridge'). We might not give the right complexity penalties to one kind of hypothesis relative to the other. Or we might implement a sensible framework for bridge hypotheses in one kind of brain that fails to predict the radically new phenomenology that results from expanding one's visual cortex onto new hardware.</p>\n<p>We'd have to hope that it makes sense to talk about 'correct' bridging rules (correctly relating a hypothesis about external stimuli or about transistors composing yourself, to which settings are in fact the ones you call 'green'), even though they're quite different from ordinary physical descriptions of the world. And, since fully general and error-free knowledge of the phenomenologies of possible agents will probably not be available to a seed AGI or to its programmers, we'd have to hope that it's possible to build a self-modifying inductor robust enough that mistaken bridge predictions would just result in a quick Bayesian update towards better ideas. It's definitely a dangling thread.</p>\n<p><em>Why</em>, then, can't we do without them? Maybe they're a handy heuristic for agents with incomplete knowledge &mdash; but can they truly <em>never</em> be eliminated?</p>\n<p>The notion of an irreducible divide between an AI's subjective sensations and its models of the objective world may sound suspiciously dualistic. If we live in a purely physical world, then why shouldn't a purely physical agent, once it&rsquo;s come to a complete understanding of itself and the world, be able to dispense with explicit bridges? These are, after all, the agent's <em>beliefs</em>&nbsp;that we're talking about. In the limit, intuitively, accurate beliefs should just look like the world. So shouldn't the agent's phenomenological self-models eventually end up collapsing into its physical world-models &mdash; dispensing with a metaphysically basic self/world distinction?<sup>1</sup></p>\n<p>Yes and no. When humans first began hypothesizing about <a href=\"/lw/t5/when_anthropomorphism_became_stupid/\">the relationship between mind and matter</a>, the former domain did not appear to be reducible to the latter. A number of philosophers concluded from this that there was a deep metaphysical divide between the two. But as the sciences of mind began to erode that belief in mind-matter dualism, they didn't eliminate the conceptual, linguistic, or intuitive distinctness of our mental and physical models. It <a href=\"http://www.sfu.ca/~kathleea/docs/Eliminative%20materialism.pdf\">may well be</a> that we'll never abandon an <a href=\"http://mind.ucsd.edu/syllabi/06-07/Phil285/readings/true-believers.pdf\">intentional stance</a> toward many phenomena, even once we've fully reduced them to their physical, biological, or computational underpinnings. <a href=\"/lw/on/reductionism/\">Models of different levels</a> can remain useful even once we've recognized that they co-refer.</p>\n<p>In the case of an artificial scientist, beliefs in a fundamental sensation-v.-world dichotomy may dissolve even if the agent retains a useful conceptual distinction between its perceptual stream and the rest of the world. A lawful, unified physics need not be best modeled by agents with only a single world-modeling subprocess. 'There is one universe' doesn't imply 'one eye is optimal for viewing the universe'; 'there is one Earth' doesn't imply 'one leg is optimal for walking it'. The cases seem different chiefly because the leg/ground distinction is easier for humans to keep straight than the <a href=\"/lw/erp/\">map/territory</a> distinction.</p>\n<p>Empirical reasoning requires a representational process that produces updates, and another representational process that gets updated. Eliminate the latter, and gone is the AI&rsquo;s memory and expectation. (Imagine <a href=\"/lw/jd9/building_phenomenological_bridges/\">Cai</a> experiencing its sequence of colors forever without considering any states of affairs they predict.) Eliminate the former, and the AGI has nothing but its frozen memories. (Imagine Cai without any sensory input, just a floating array of static world-models.) Keep both and eliminate bridging, and Cai painstakingly collects its visual data only to throw it all away; it has beliefs, but it never updates them.</p>\n<p>Can we replace perceptions and expectations with a single kind-of-perceptiony kind-of-expectationish epistemic process, in a way that obviates any need for bridge hypotheses?</p>\n<p>Maybe, but I don't know what that would look like. An agent's perceptions and its hypotheses are of different types, just by virtue of having distinct functions; and its meta-representations must portray them as such, lest its metacognitive reasoning fall into systemic error. Striving mightily to conflate the two may not make any more sense than striving to get an agent to smell colors or taste sounds.<sup>2</sup></p>\n<p>The only candidate I know of for a framework that may sidestep this distinction without thereby catching fire is <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a>, which was brought up by&nbsp;<a href=\"https://www.facebook.com/groups/233397376818827/permalink/233401646818400/?comment_id=234423326716232&amp;offset=100&amp;total_comments=317\">Jim Babcock</a>, <a href=\"/lw/jd9/building_phenomenological_bridges/a8ai\">Vladimir Slepnev</a>, and <a href=\"/lw/jd9/building_phenomenological_bridges/a9ba\">Wei Dei</a>. UDT eliminates the need for bridge hypotheses in a particularly bold way, by doing away with <a href=\"/lw/1fu/why_and_why_not_bayesian_updating/\">updatable hypotheses</a> altogether.</p>\n<p>I don't understand UDT well enough to say how it bears on the problem of naturalizing induction, but I may return to this point when I have a better grasp on it. If UDT turns out to solve or dissolve the problem, it will be especially useful to have on hand a particular reductionism-related problem that afflicts other kinds of agents and is solved by UDT. This will be valuable even if UDT has other features that are undesirable enough to force us to come up with alternative solutions to naturalized induction.</p>\n<p>For now, I'll just make a general point: It's usually good policy for an AGI to <a href=\"/lw/hs/think_like_reality/\">think like reality</a>; but if an introspectible distinction between updatable information and update-causing information <em>is</em> useful for real-world inductors, then we shouldn't strip all traces of it from artificial reasoners, for much the same reason we shouldn't reduce our sensory apparatuses to a single modality in an attempt to ape the unity of our world's dynamics. Reductionism restricts what we can rationally believe about the territory, but it doesn't restrict the idiom of our maps.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p><sup>1</sup> This is close to the worry&nbsp;<a href=\"/lw/jd9/building_phenomenological_bridges/a8fg\">Alex Flint</a>&nbsp;raised, though our main concern is with the agent's ability to reduce its own mental types, since this is a less avoidable problem than a third party trying to do the same.</p>\n<p><sup>2</sup> The analogy to sensory modality is especially apt given that phenomenological bridge hypotheses can link sensory channels instead of linking a sensory channel to a hypothesized physical state. For instance, '<em>I see yellow whenever I taste isoamyl acetate</em>' can function as a bridge between sensations an agent types as 'vision' and sensations an agent types as 'taste'.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "heJZLrC6EeJaskLbu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "25302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ethRJh2E7mSSjzCay", "tPqQdLCuxanjhoaNs", "f4RJtHBPvDRJcCTva", "KJ9MFBPwXGwNpadf2", "W6nXfmKTrgaiaLSRg", "tWLFWAndSZSYN6rPB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-25T03:16:09.989Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Probability Discussion", "slug": "meetup-urbana-champaign-probability-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7uwK9wjcmDtA2MnWi/meetup-urbana-champaign-probability-discussion", "pageUrlRelative": "/posts/7uwK9wjcmDtA2MnWi/meetup-urbana-champaign-probability-discussion", "linkUrl": "https://www.lesswrong.com/posts/7uwK9wjcmDtA2MnWi/meetup-urbana-champaign-probability-discussion", "postedAtFormatted": "Saturday, January 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Probability%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Probability%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7uwK9wjcmDtA2MnWi%2Fmeetup-urbana-champaign-probability-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Probability%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7uwK9wjcmDtA2MnWi%2Fmeetup-urbana-champaign-probability-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7uwK9wjcmDtA2MnWi%2Fmeetup-urbana-champaign-probability-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w7'>Urbana-Champaign: Probability Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 January 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">412 W. Elm St, Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a meetup! I'm writing an article on the foundations of probability - why having probabilities is a good idea for agents. So that would be a good thing to talk about. And then, other things!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w7'>Urbana-Champaign: Probability Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7uwK9wjcmDtA2MnWi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5301018399958104e-06, "legacy": true, "legacyId": "25359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Probability_Discussion\">Discussion article for the meetup : <a href=\"/meetups/w7\">Urbana-Champaign: Probability Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 January 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">412 W. Elm St, Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a meetup! I'm writing an article on the foundations of probability - why having probabilities is a good idea for agents. So that would be a good thing to talk about. And then, other things!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Probability_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/w7\">Urbana-Champaign: Probability Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Probability Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Probability_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Probability Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Probability_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-25T06:49:27.698Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-5", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BGQ8x8dSc5XuNkdcg/meetup-washington-dc-fun-and-games-meetup-5", "pageUrlRelative": "/posts/BGQ8x8dSc5XuNkdcg/meetup-washington-dc-fun-and-games-meetup-5", "linkUrl": "https://www.lesswrong.com/posts/BGQ8x8dSc5XuNkdcg/meetup-washington-dc-fun-and-games-meetup-5", "postedAtFormatted": "Saturday, January 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGQ8x8dSc5XuNkdcg%2Fmeetup-washington-dc-fun-and-games-meetup-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGQ8x8dSc5XuNkdcg%2Fmeetup-washington-dc-fun-and-games-meetup-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBGQ8x8dSc5XuNkdcg%2Fmeetup-washington-dc-fun-and-games-meetup-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w8'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.\n(Sorry for the late notice, I forgot that the Learn-To-Code thing wasn't actually the meetup.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w8'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BGQ8x8dSc5XuNkdcg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5303396668983984e-06, "legacy": true, "legacyId": "25361", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/w8\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.\n(Sorry for the late notice, I forgot that the Learn-To-Code thing wasn't actually the meetup.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/w8\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-25T11:55:44.072Z", "modifiedAt": null, "url": null, "title": "Solomonoff induction without perfect memory", "slug": "solomonoff-induction-without-perfect-memory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Squark", "createdAt": "2013-02-04T19:29:04.489Z", "isAdmin": false, "displayName": "Squark"}, "userId": "k4QpNYXcigqfG85t6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TwYbEnt75qkcmvBhC/solomonoff-induction-without-perfect-memory", "pageUrlRelative": "/posts/TwYbEnt75qkcmvBhC/solomonoff-induction-without-perfect-memory", "linkUrl": "https://www.lesswrong.com/posts/TwYbEnt75qkcmvBhC/solomonoff-induction-without-perfect-memory", "postedAtFormatted": "Saturday, January 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solomonoff%20induction%20without%20perfect%20memory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolomonoff%20induction%20without%20perfect%20memory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwYbEnt75qkcmvBhC%2Fsolomonoff-induction-without-perfect-memory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solomonoff%20induction%20without%20perfect%20memory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwYbEnt75qkcmvBhC%2Fsolomonoff-induction-without-perfect-memory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwYbEnt75qkcmvBhC%2Fsolomonoff-induction-without-perfect-memory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 781, "htmlBody": "<p><em>In this post I construct a variant of <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff induction</a>&nbsp;which allows for agents with imperfect memory.</em></p>\n<p>Solomonoff induction is a mathematical formalization of <a href=\"http://wiki.lesswrong.com/wiki/Occam%27s_razor\">Occam's razor</a>, and is supposed to be a \"master equation\" of epistemic rationality. In order words, forming rational expectations about the future is supposed to be tantamount to computing Solomonoff induction (approximately, since it's incomputable).</p>\n<p>Solomonoff induction operates by assigning probabilities <img src=\"http://www.codecogs.com/png.latex?\\Pr(s^* \\mid s)\" alt=\"\" />&nbsp;to continuations <img src=\"http://www.codecogs.com/png.latex?s^*\" alt=\"\" />&nbsp;of a given finite sequence <img src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" width=\"7\" height=\"8\" />. In the simplest formalism, the sequence elements are bits.&nbsp;<img src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" width=\"7\" height=\"8\" />&nbsp;represents past observations and&nbsp;<img src=\"http://www.codecogs.com/png.latex?s^*\" alt=\"\" />&nbsp;represents a possible future.</p>\n<p>A rational agent&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;is supposed to use&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\Pr(s^* \\mid s)\" alt=\"\" />&nbsp;to evaluate the probability of&nbsp;<img src=\"http://www.codecogs.com/png.latex?s^*\" alt=\"\" />. There are two problems with this. One is that&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\Pr(s^* \\mid s)\" alt=\"\" />&nbsp;is uncomputable whereas&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;has a limited amount of computing resources in its disposal. For now, we are going to ignore this. Another problem is that&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;doesn't have direct access to&nbsp;<img src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" width=\"7\" height=\"8\" />.&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;can only estimate&nbsp;<img src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" width=\"7\" height=\"8\" />&nbsp;from&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />'s memory. Moreover, this estimation depends on knowledge&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;has about reality which is supposed to be generated by Solomonoff induction itself. In other words, inferring the future from the past only makes sense in the approximation of perfect memory, in general we need to be able to infer both the past and the future from the present.</p>\n<p>The bit-sequence formalism is not well-adapted to this, since the present state of&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;has to contain all of&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />'s memory about the past so it has to be more than a single bit. I suggest solving this by considering sequences <img src=\"http://www.codecogs.com/png.latex?\\upsilon\" alt=\"\" />&nbsp;of natural numbers instead of bit-sequences <img src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" width=\"7\" height=\"8\" />. Generalizing regular Solomonoff induction to natural numbers is straight-forward: the random program <img src=\"http://www.codecogs.com/png.latex?R\" alt=\"\" width=\"14\" height=\"14\" />&nbsp;computes convergent sequences of lower bounds for the transition probabilities</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\Pr_R(\\upsilon_i=n \\mid \\lbrace \\upsilon_j \\rbrace_{j&lt;i})\" alt=\"\" width=\"152\" height=\"27\" /></p>\n<p>Now we want a new induction procedure whose input is a single natural number&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"10\" height=\"8\" />&nbsp;representing the state of&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />'s consciousness which allows assigning probabilities to sequences&nbsp;of natural numbers containing&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"10\" height=\"8\" />. We achieve this by assigning the following probabilities to Solomonoff hypotheses&nbsp;<img src=\"http://www.codecogs.com/png.latex?R\" alt=\"\" width=\"14\" height=\"14\" />:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\Pr(R \\mid n) = Z^{-1} E(\\ln (t(n) + 2)^{-1} \\mid R)\" alt=\"\" /></p>\n<p>Here,&nbsp;<img src=\"http://www.codecogs.com/png.latex?Z^{-1}\" alt=\"\" />&nbsp;is a normalization factor,&nbsp;<img src=\"http://www.codecogs.com/png.latex?E\" alt=\"\" width=\"14\" height=\"13\" />&nbsp;stands for expectation value and&nbsp;<img src=\"http://www.codecogs.com/png.latex?t(n) \\in \\mathbb{N} \\cup \\lbrace \\infty \\rbrace\" alt=\"\" />&nbsp;is defined by</p>\n<p><img src=\"http://www.codecogs.com/png.latex?t(n) = \\min \\lbrace i \\mid \\upsilon_i=n \\rbrace\" alt=\"\" /></p>\n<p>where&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\lbrace \\upsilon_i \\rbrace\" alt=\"\" width=\"31\" height=\"19\" />&nbsp;is the&nbsp;<em>infinite </em>sequence of natural numbers \"generated\" by&nbsp;<img src=\"http://www.codecogs.com/png.latex?R\" alt=\"\" width=\"14\" height=\"14\" />.</p>\n<p>The factor <img src=\"http://www.codecogs.com/png.latex?\\ln (t(n) + 2)^{-1}\" alt=\"\" />&nbsp;penalizes hypotheses in which&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"10\" height=\"8\" />&nbsp;appears late in&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\lbrace \\upsilon_i \\rbrace\" alt=\"\" width=\"31\" height=\"19\" />. Its form was chosen to achieve equal suppression of two spurious hypotheses we want to avoid:</p>\n<ul>\n<li>The \"timeless hypothesis\" <img src=\"http://www.codecogs.com/png.latex?TL\" alt=\"\" width=\"25\" height=\"13\" />&nbsp;which computes \"true\"&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\lbrace \\upsilon_i \\rbrace\" alt=\"\" width=\"31\" height=\"19\" />&nbsp;silently (i.e. w/o outputting it), extracts&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"10\" height=\"8\" />&nbsp;by evaluating&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\upsilon_{t(n)}\" alt=\"\" />&nbsp;and generates the constant sequence&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\forall i : \\upsilon_i = n\" alt=\"\" />. It gains a factor of about&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\ln t(n)\" alt=\"\" />&nbsp;on the \"correct\" hypothesis by reducing age to 0 but loses a factor of about&nbsp;<img src=\"http://www.codecogs.com/png.latex?t(n)\" alt=\"\" width=\"29\" height=\"19\" />&nbsp;due to the increased complexity of specifying&nbsp;<img src=\"http://www.codecogs.com/png.latex?t(n)\" alt=\"\" width=\"29\" height=\"19\" />&nbsp;explicitly. Its overall suppression is thus about&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\frac{t(n)}{\\ln {t(n)}}\" alt=\"\" /></li>\n<li>The \"Boltzmann brain\" hypothesis <img src=\"http://www.codecogs.com/png.latex?BB\" alt=\"\" width=\"29\" height=\"13\" />&nbsp;which generates the sequence&nbsp;<img src=\"http://www.codecogs.com/png.latex?\\forall i : \\upsilon_i = i\" alt=\"\" />. It gains due to its reduced complexity but is suppressed by <img src=\"http://www.codecogs.com/png.latex?\\frac {\\ln{n}}{\\ln{t(n)}}\" alt=\"\" />&nbsp;due to increased age. Since an agent is supposed to record memories, we expect&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"10\" height=\"8\" />&nbsp;to be exponential in&nbsp;<img src=\"http://www.codecogs.com/png.latex?t(n)\" alt=\"\" width=\"29\" height=\"19\" />. We thus get the same order-of-magnitude suppression as before</li>\n</ul>\n<div>In this case equal suppression implies (roughly) maximal suppression since there is a trade-off between suppressing <img src=\"http://www.codecogs.com/png.latex?TL\" alt=\"\" width=\"25\" height=\"13\" />&nbsp;and suppressing&nbsp;<img src=\"http://www.codecogs.com/png.latex?BB\" alt=\"\" width=\"29\" height=\"13\" />.</div>\n<div><br /></div>\n<p>I think it should be possible to define a class of cellular automata&nbsp;<img src=\"http://www.codecogs.com/png.latex?CA\" alt=\"\" width=\"27\" height=\"14\" />&nbsp;and initial conditions <img src=\"http://www.codecogs.com/png.latex?I\" alt=\"\" width=\"9\" height=\"13\" />&nbsp;s.t. the state of the automaton evolves indefinitely rather than e.g. becoming periodic, for which my induction correctly infers the rules of&nbsp;<img src=\"http://www.codecogs.com/png.latex?CA\" alt=\"\" width=\"27\" height=\"14\" />&nbsp;from the state of the automaton at a sufficiently late time. Note that for many cellular automata, the state is exponential in the time since changes propagate at a fixed velocity.</p>\n<p>Another application is to the <a href=\"/lw/19d/the_anthropic_trilemma/\">anthropic trilemma</a>. This formalism suggests continuity of experience <em>is</em>&nbsp;fundamentally meaningful and that subjective probabilities behave just like objective probabilities. We thus eliminate the second and third horns. However, explicitly applying this formalism to solve solve the trilemma remains challenging. In a future post I am going to argue the first horn is actually correct but without giving a qualitative proof using this formalism.</p>\n<p>A somewhat speculative application is applying the formalism repeatedly w/o recording the past. Given consciousness state&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"10\" height=\"8\" />&nbsp;we are able to compute the probability distribution of next consciousness state&nbsp;<img src=\"http://www.codecogs.com/png.latex?m\" alt=\"\" width=\"15\" height=\"8\" />. If we wish to consistently continue one step further, we should use both&nbsp;<img src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" width=\"10\" height=\"8\" />&nbsp;and&nbsp;<img src=\"http://www.codecogs.com/png.latex?m\" alt=\"\" width=\"15\" height=\"8\" />. However,&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;can only use&nbsp;<img src=\"http://www.codecogs.com/png.latex?m\" alt=\"\" width=\"15\" height=\"8\" />&nbsp;to estimate probabilities. This way we are led to a <em>Markovian</em>&nbsp;stochastic process. The physical meaning of this construction is not clear but in some ways this Markovian process is more interesting than the non-Markovian process you get with regular Solomonoff induction or with recording the past in the current formalism. In regular Solomonoff induction, as time progresses&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;gains information but the physics is fixed. So to speak, the map improves but the territory stays the same. The Markovian version allows for actual <em>physics</em>&nbsp;to change but&nbsp;<img src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" width=\"13\" height=\"13\" />&nbsp;cannot notice it! The process follows a pattern but the pattern keeps randomly evolving. Maybe this construction is more than just a mathematical artifact?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TwYbEnt75qkcmvBhC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 1.5306812796591396e-06, "legacy": true, "legacyId": "25356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y7jZ9BLEeuNTzgAE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-25T14:52:41.851Z", "modifiedAt": null, "url": null, "title": "Open thread, January 25- February 1", "slug": "open-thread-january-25-february-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.874Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qrJERG64YPmMDeHEN/open-thread-january-25-february-1", "pageUrlRelative": "/posts/qrJERG64YPmMDeHEN/open-thread-january-25-february-1", "linkUrl": "https://www.lesswrong.com/posts/qrJERG64YPmMDeHEN/open-thread-january-25-february-1", "postedAtFormatted": "Saturday, January 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20January%2025-%20February%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20January%2025-%20February%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrJERG64YPmMDeHEN%2Fopen-thread-january-25-february-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20January%2025-%20February%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrJERG64YPmMDeHEN%2Fopen-thread-january-25-february-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqrJERG64YPmMDeHEN%2Fopen-thread-january-25-february-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qrJERG64YPmMDeHEN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 1.5308787233474352e-06, "legacy": true, "legacyId": "25362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 318, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-26T09:17:29.099Z", "modifiedAt": null, "url": null, "title": "Applying reinforcement learning theory to reduce felt temporal distance", "slug": "applying-reinforcement-learning-theory-to-reduce-felt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:07.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PBWqFL6P8XphH79Pm/applying-reinforcement-learning-theory-to-reduce-felt", "pageUrlRelative": "/posts/PBWqFL6P8XphH79Pm/applying-reinforcement-learning-theory-to-reduce-felt", "linkUrl": "https://www.lesswrong.com/posts/PBWqFL6P8XphH79Pm/applying-reinforcement-learning-theory-to-reduce-felt", "postedAtFormatted": "Sunday, January 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applying%20reinforcement%20learning%20theory%20to%20reduce%20felt%20temporal%20distance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplying%20reinforcement%20learning%20theory%20to%20reduce%20felt%20temporal%20distance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBWqFL6P8XphH79Pm%2Fapplying-reinforcement-learning-theory-to-reduce-felt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applying%20reinforcement%20learning%20theory%20to%20reduce%20felt%20temporal%20distance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBWqFL6P8XphH79Pm%2Fapplying-reinforcement-learning-theory-to-reduce-felt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBWqFL6P8XphH79Pm%2Fapplying-reinforcement-learning-theory-to-reduce-felt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 886, "htmlBody": "<p>(<a href=\"http://kajsotala.fi/2014/01/applying-reinforcement-learning-theory-to-reduce-felt-temporal-distance/\">cross-posted from my blog</a>)</p>\n<p>It is a <a href=\"http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node9.html\">basic principle of reinforcement learning</a> to distinguish between reward and value, where the reward of a state is the immediate, intrinsic desirability of the state, whereas the value of the state is proportional to the rewards of the other states that you can reach from that state.</p>\n<p>For example, suppose that I&rsquo;m playing a competitive game of chess, and in addition to winning I happen to like capturing my opponent&rsquo;s pieces, even when it doesn&rsquo;t contribute to winning. I assign a reward of 10 points to winning, -10 to losing, 0 to a stalemate, and 1 point to each piece that I capture in the game. Now my opponent offers me a chance to capture one of his pawns, an action that would give me one point worth of reward. But when I look at the situation more closely, I see that it&rsquo;s a trap: if I did capture the piece, I would be forced into a set of moves that would inevitably result in my defeat. So the value, or long-term reward, of that state is actually something close to -9.<br /><br />Once I realize this, I also realize that making that move is almost exactly equivalent to agreeing to resign in exchange for my opponent letting me capture one of his pieces. My defeat won&rsquo;t be instant, but by making that move, I would nonetheless be choosing to lose.<br /><br />Now consider a dilemma that I might be faced with when coming home late some evening. I have no food at home, but I&rsquo;m feeling exhausted and don&rsquo;t want to bother with going to the store, and I&rsquo;ve already eaten today anyway. But I also know that if I wake up with no food in the house, then I will quickly end up with low energy, which makes it harder to go to the store, which means my energy levels will drop further, and so on until I&rsquo;ll finally get something to eat much later, after wasting a long time in an uncomfortable state.<br /><br />Typically, temporal discounting means that I&rsquo;m aware of this in the evening, but nonetheless skip the visit to the store. The penalty from not going feels remote, whereas the discomfort of going feels close, and that ends up dominating my decision-making. Besides, I can always hope that the next morning will be an exception, and I&rsquo;ll actually get myself to go to the store right from the moment when I wake up!<br /><br />And I haven&rsquo;t tried this out for very long, but it feels like explicitly framing the different actions in terms of reward and value could be useful in reducing the impact of that experienced distance. I skip the visit to the store because being hungry in the morning is something that seems remote. But if I think that skipping the visit is exactly the same thing as choosing to be hungry in the morning, and that the value of skipping the visit is not the momentary relief of being home earlier but rather the inevitable consequence of the causal chain that it sets in motion &ndash; culminating in hours of hunger and low energy &ndash; then that feels a lot different.<br /><br />And of course, I can propagate the consequences earlier back in time as well: if I think that I simply won&rsquo;t have the energy to get food when I finally come home, then I should realize that I need to go buy the food before setting out on that trip. Otherwise I&rsquo;ll again set in motion a causal chain whose end result is being hungry. So then <em>not going shopping before I leave</em> becomes <em>exactly the same thing</em> as <em>being hungry next morning</em>.<br /><br />More examples of the same:</p>\n<ul>\n<li>Slightly earlier I considered taking a shower, and realized that if I'd take a shower in my current state of mind I'd inevitably make it into a bath as well. So I wasn't really just considering whether to take a shower, but whether to take a shower *and* a bath. That said, I wasn't in a hurry anywhere and there didn't seem to be a big harm in also taking the bath, so I decided to go ahead with it.</li>\n<li>While in the shower/bath, I started thinking about this post, and decided that I wanted to get it written. But I also wanted to enjoy my hot bath for a while longer. Considering it, I realized that staying in the bath for too long might cause me to lose my motivation for writing this, so there was a chance that staying in the bath would become the same thing as choosing not to get this written. I decided that the risk wasn't worth it, and got up.</li>\n<li>If I'm going somewhere and I choose a route that causes me to walk past a fast-food place selling something that I know I shouldn't eat, and I know that the sight of that fast-food place is very likely to tempt me to eat there anyway, then choosing that particular route is the same thing as choosing to go eat something that I know I shouldn't.</li>\n</ul>\n<p>Related post: <a href=\"http://kajsotala.fi/2010/04/applied-cognitive-science-learning-from-a-faux-pas/\">Applied cognitive science: learning from a faux pas</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2d4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PBWqFL6P8XphH79Pm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 19, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "25365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-26T13:16:15.873Z", "modifiedAt": null, "url": null, "title": "Productivity as a function of ability in theoretical fields", "slug": "productivity-as-a-function-of-ability-in-theoretical-fields", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:03.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e84qrSoooAHfHXhbi/productivity-as-a-function-of-ability-in-theoretical-fields", "pageUrlRelative": "/posts/e84qrSoooAHfHXhbi/productivity-as-a-function-of-ability-in-theoretical-fields", "linkUrl": "https://www.lesswrong.com/posts/e84qrSoooAHfHXhbi/productivity-as-a-function-of-ability-in-theoretical-fields", "postedAtFormatted": "Sunday, January 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Productivity%20as%20a%20function%20of%20ability%20in%20theoretical%20fields&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProductivity%20as%20a%20function%20of%20ability%20in%20theoretical%20fields%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe84qrSoooAHfHXhbi%2Fproductivity-as-a-function-of-ability-in-theoretical-fields%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Productivity%20as%20a%20function%20of%20ability%20in%20theoretical%20fields%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe84qrSoooAHfHXhbi%2Fproductivity-as-a-function-of-ability-in-theoretical-fields", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe84qrSoooAHfHXhbi%2Fproductivity-as-a-function-of-ability-in-theoretical-fields", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1315, "htmlBody": "<p>I argued in <a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/\">this post</a> that the differences in capability between different researchers are vast (Kaj Sotala provided me with&nbsp;<a href=\"/r/discussion/lw/jhy/division_of_cognitive_labour_in_accordance_with/ae8b\">some interesting empirical evidence</a>&nbsp;that backs up this claim). Einstein's contributions to physics or <a href=\"http://en.wikipedia.org/wiki/John_von_Neumann\">John von Neumann</a>'s contributions to mathematics (and a number of other disciplines) are arguably at least hundreds of times greater than that of an average physicist or mathematician.</p>\n<p>At the same time, <a href=\"/lw/ql/my_childhood_role_model/\">Yudkowsky argues that \"in the space of brain designs\"</a> the difference between the village idiot and Einstein is tiny. Their brains are extremely similar, with the exception of some \"minor genetic tweaks\". Hence we get the following picture:</p>\n<p><img src=\"/static/imported/2008/05/21/mindscalereal.png\" alt=\"\" width=\"400\" height=\"63\" /></p>\n<div><br /></div>\n<div>The picture I am painting is rather something like this:</div>\n<div><br /></div>\n<div><img src=\"http://images.lesswrong.com/t3_jk7_1.png\" alt=\"\" width=\"399\" height=\"151\" /></div>\n<div>It would seem that these pictures are incompatible - something that would be a problem for my picture, since I think that Yudkowsky's picture is right. So how can they both be true? The answer is, obviously, that they are measuring different things. The first is measuring something like <em>difference in brain design</em>&nbsp;<em>that is relevant for intelligence</em>. The second is rather measuring the difference in capability to come up with <em>physical theories that are of use</em> <em>for mankind</em>. Here the village idiot is on par with the chimp and the mouse - all of whom have no such capability whatsover. The average physicist has some such capability, but it's just a fraction of Einstein's.</div>\n<div><br /></div>\n<div>Why is this? Well it is not because the village idiot has no capability at all to come up with physical theories. In fact, a <a href=\"http://en.wikipedia.org/wiki/Na%C3%AFve_physics\">primitive physical theory that is quite useful</a> is hard-wired into our brains. Rather, the reason is that the village idiot has no capability to come up with a physical theory that is not already well-known.</div>\n<div><br /></div>\n<div>Problems in theoretical physics and mathematics are typically problems that are so complex that they are hard to solve for some of the world's smartest people. This means that unless you're quite smart, your chances of contributing anything at at all to these disciplines is very slim. But, if you are but a tiny bit smarter than everyone else, you'll be able to spot solutions to problem after problem that others have struggled with - these problems being problems precisely because they were hard to solve for people with a certain level of intelligence. Thus we get something like the following relationship between cognitive ability, in Yudkowsky's sense, and ability to come up with useful physical theories, i.e. productivity - what I'm talking about:</div>\n<div><br /></div>\n<div><img src=\"http://images.lesswrong.com/t3_jk7_6.png?v=86e80583ec355e282e3826726f3275a8\" alt=\"\" width=\"481\" height=\"289\" /><br /></div>\n<div><br /></div>\n<div>It is for this reason that people like von Neumann and Einstein are so vastly much more productive than the average mathematician/physicist. The difference in intelligence is tiny on Yudkowsky's scale - obviously much smaller than that between Einstein and the village idiot - but this tiny difference allowed von Neumann and Einstein to solve lots of problems that were just too hard for other mathematicians/physicists. (It follows that an artificial intelligence just a tiny bit smarter than Einstein and von Neumann would be as much more productive than them as they are in relation to other mathematician/physicists).</div>\n<div><br /></div>\n<div>\n<div>(Obviously other characteristics besides intelligence are very important in these fields - e.g. &nbsp;work ethic. I put that complication aside here, though.)</div>\n</div>\n<div><br /></div>\n<div>The same pattern holds in many other fields - e.g. sports. In a sense, the difference in ability between Rafael Nadal and no 300 on the ATP ranking is very small - e.g. they are hitting the ball roughly as hard, are roughly as good at, say, shooting the ball within half a metre of the base-line when not under pressure, etc - but this small difference in ability makes for a huge difference in productivity (in the sense that lots of people want to watch Nadal - which means that his games generate a lot of utility - but few people want to watch no 300).&nbsp;</div>\n<div><br /></div>\n<div>But there are also fields where you have an entirely different pattern. The difference in productivity between the world's best cleaner and the average cleaner is, I'd guess, tiny. Similarly, if Peter is twice as strong as Paul, he will be able to fetch as much as water as needed in half the time Paul needs - neither more, nor less. In other words, the relationship between ability and productivity in these fields is linear:</div>\n<div><img src=\"http://images.lesswrong.com/t3_jk7_8.png?v=4ca454d089cf399dbe21f592d504b8d7\" alt=\"\" width=\"540\" height=\"344\" /></div>\n<div>You get approximately this linear pattern in many physical jobs, but also in some intellectual jobs. Assume, for instance, that there is an intellectual field where the only thing that determines your productivity is your ability to acquire and memorize factual information. Say also that this field is neatly separated into small problems, so that your knowledge of one problem doesn't affect your ability to solve other problems. In this case, a twice as good capacity to acquire and memorize factual information will mean that you'll be able to solve twice as many of these problems - neither more nor less. Now there is obviously no intellectual field where you have exactly this pattern, but there are fields - the more \"descriptive\", as opposed to theoretical, social sciences come to mind - which at least approach it, and where the differences in productivity hence are much smaller than they are in theoretical physics or mathematics. (Of course, there are other patterns besides these; for instance, in some jobs, what's important is that you meet some minimum level of ability, beyond which more ability translates in very little additional productivity.)</div>\n<div><br /></div>\n<div>Due to the fact that different academic disciplines have more or less the same pay structure, and are governed by similar rules and social institutions, these large differences between them are, however, seldom noted. This contributes to our inability to see how huge the differences in productivity between different scientists are in some disciplines.</div>\n<div><br /></div>\n<div>The difference between these two patterns is due to the fact that the first kinds of jobs are more \"social\" than the latter kinds in a particular way. The usefulness of your work in theoretical physics is dependent on how good others are at theoretical physics in a way the usefulness of your water fetching isn't. Even if you're weak, you'll still contribute something by carrying a small amount of water to put out a fire, but if you're not above a certain level of cognitive ability, your work in theoretical physics will have no value whatsoever.</div>\n<div><br /></div>\n<div>I suppose that economists must have written on this phenomenon - what I term <em>other-dependent productivity</em>. If so I'd be interested in that and in adopting their terminology.</div>\n<div><br /></div>\n<div>I think one reason why people have trouble accepting Yudkowsky's picture is that they note how vastly much more productive Einstein was than an average physicist (let alone the village idiot...) and then infer that this difference must be due to a vast difference in intelligence. Hence pointing out that the difference in productivity could be vast even though the difference in intelligence is not, due to the fact that productivity in theoretical physics is strongly other-dependent, should make people more disposed to accept Yudkowsky's picture.</div>\n<div><br /></div>\n<div>It would be interesting to discuss what the relationship between ability and productivity is in different jobs and intellectual fields.&nbsp;I leave that for later, though. Obviously, the question of how ability is to be defined is relevant here. This question was extensively discussed in&nbsp;<a href=\"/lw/ql/my_childhood_role_model/\">the comments to Yudkowsky's post</a>&nbsp;but I have avoided to discuss it for two reasons: firstly, because I think it is possible to get an intuitive grasp of the phenomena I'm discussing without a precise definition of ability, and, secondly, because an extensive discussion of this notion would have made the post far too long and complicated.</div>\n<div><br /></div>\n<div><strong>Edit</strong>: <a href=\"http://marginalrevolution.com/marginalrevolution/2010/09/winner-take-all-economics.html\">Here is a relevant article I just found on Marginal Revolution</a> on \"winner-take-all economies\"&nbsp;where \"small differences in skills can mean large differences in returns\". It also has some useful tips for further reading.&nbsp;</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1, "x3zyEPFaJANB2BHmP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e84qrSoooAHfHXhbi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 37, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "25351", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9FWWDoMWA7nbFmh6g", "3Jpchgy53D2gB5qdk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-26T18:49:45.644Z", "modifiedAt": null, "url": null, "title": "How to become a PC?", "slug": "how-to-become-a-pc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.482Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7pqgoxjQLp6cenqXD/how-to-become-a-pc", "pageUrlRelative": "/posts/7pqgoxjQLp6cenqXD/how-to-become-a-pc", "linkUrl": "https://www.lesswrong.com/posts/7pqgoxjQLp6cenqXD/how-to-become-a-pc", "postedAtFormatted": "Sunday, January 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20become%20a%20PC%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20become%20a%20PC%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pqgoxjQLp6cenqXD%2Fhow-to-become-a-pc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20become%20a%20PC%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pqgoxjQLp6cenqXD%2Fhow-to-become-a-pc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7pqgoxjQLp6cenqXD%2Fhow-to-become-a-pc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>\"Cryonics has a 95% chance of failure, by my estimation; it would be downright /embarrassing/ to die on the day before real immortality is discovered. Thus, I want to improve my general health and longevity.\"</p>\n<p>That thought has gotten me through three weeks of gradually increasing exercise and diet improvement (I'm eating an apple right now) - but my enthusiasm is starting to flag. So I'm looking for new thoughts that will help me keep going, and keep improving. A few possibilities that I've thought of:</p>\n<p>Pride: \"If I'm so smart, then I should be able to do /better/ than those other people who don't even know about Bayesian updates, let alone the existence of akrasia...\"</p>\n<p>Sloth: \"If I stop now, it's going to be /so much/ harder and more painful to start up again, instead of just keeping on keeping on...\"</p>\n<p>Desire: \"I already like hiking and camping - if I keep this up, I'll be able to carry enough weight to finally take that long trip I've occasionally considered...\"</p>\n<p>Curiosity: \"I'm as geeky a nerd as you can find. I wonder how far I can hack my own body?\"</p>\n<p>Pride again: \"I already keep a hiker's first-aid kit in my pocket, and make other preparations for events that happen rarely. How stupid do I have to be not to put at least that much effort into making my everyday life easier?\"</p>\n<p>&nbsp;</p>\n<p>Does anyone have any experience in such self-motivation? Does this set of mental tricks seem like a sufficiently viable approach? Are there any other approaches that seem worth a shot?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7pqgoxjQLp6cenqXD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 22, "extendedScore": null, "score": 1.532752119689287e-06, "legacy": true, "legacyId": "25367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 138, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-26T19:29:42.378Z", "modifiedAt": null, "url": null, "title": "Foundations of Probability", "slug": "foundations-of-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:28.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EQ33emneF3Fh62Nn2/foundations-of-probability", "pageUrlRelative": "/posts/EQ33emneF3Fh62Nn2/foundations-of-probability", "linkUrl": "https://www.lesswrong.com/posts/EQ33emneF3Fh62Nn2/foundations-of-probability", "postedAtFormatted": "Sunday, January 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Foundations%20of%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFoundations%20of%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQ33emneF3Fh62Nn2%2Ffoundations-of-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Foundations%20of%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQ33emneF3Fh62Nn2%2Ffoundations-of-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQ33emneF3Fh62Nn2%2Ffoundations-of-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1358, "htmlBody": "<h3><strong style=\"font-size: small;\">Beginning of:</strong><span style=\"font-size: small;\">&nbsp;</span><span style=\"font-size: small; font-weight: normal;\">Logical Uncertainty sequence</span></h3>\n<p>Suppose that we are designing a robot. In order for this robot to reason about the outside world, it will need to use probabilities.</p>\n<p>Our robot can then use its knowledge to acquire cookies, which we have programmed it to value. For example, we might wager a cookie with the robot on the motion of a certain stock price.</p>\n<p>In the coming sequence, I'd like to add a new capability to our robot. It has to do with how the robot handles very hard math problems. If we ask \"what's the last digit of the <a href=\"http://en.wikipedia.org/wiki/Knuth's_up-arrow_notation\">3^^^3</a>'th prime number?\", our robot should at some point&nbsp;<em>give up</em>, before the sun explodes and the point becomes moot.</p>\n<p>If there are math problems our robot can't solve, what should it do if we offer it a bet about the last digit of the 3^^^3'th prime? It's going to have to approximate - robots need to make lots of approximations, even for simple tasks like finding the strategy that maximizes cookies.</p>\n<p>Intuitively, it seems like if we can't find the real answer, the&nbsp;last digit is equally likely to be 1, 3, 7 or 9; our robot should take bets as if it assigned those digits equal probability. But to assign some probability to the wrong answer is logically equivalent to assigning probability to 0=1. When we learn more, it will become clear that this is a problem - we aren't ready to upgrade our robot yet.</p>\n<p>Let's begin with a review of the foundations of probability.</p>\n<div>\n<p><a id=\"more\"></a></p>\n</div>\n<p>What I call foundations of probability are arguments for why our robot should ever want to use probabilities. I will cover four of them, ranging from the worldly (\"make bets in the following way or you lose money\") to the ethereal (\"here's a really elegant set of axioms\"). To use the word \"probability\" to describe the subject of such disparate arguments can seem odd, but keep in mind the naive definition of probability as that number that's 1/6 for a fair die rolling 6 and 30% for clear weather tomorrow.</p>\n<p><strong>Dutch Books</strong></p>\n<p>The concretest of concrete foundations is the Dutch book arguments. A Dutch book is a collection of bets that is certain to lose you money. If you violate the rules of probability, you'll agree to these certain-loss bets (or not take a certain-win bet).</p>\n<p>For example, if you think that each side of the coin has a 55% chance of showing up, then you'll pay $1 for a bet that pays out $0.98 if the coin lands heads and $0.98 if the coin lands tails. If taking bets where you're guaranteed to lose is bad, then you're not allowed to have probabilities for mutually exclusive things that sum to more than 1.</p>\n<p>Similar arguments hold for other properties of probability. If your probabilities for exhaustive events add up to less than 1, you'll pass up free money, which is bad. If you disobey the sum rule or the product rule, you'll agree to a guaranteed loss, which is bad, etcetera. Thus, say the Dutch book arguments, our probabilities have to behave the way they do because we don't want to take guaranteed losses or pass up free money.</p>\n<p>There are many assumptions underlying this whole scenario. Our agent in these arguments already tries to decide using probability-like numbers, all we show is that the numbers have to follow the same rules as probabilities. Why can't our agent follow a totally different method of decision making, like picking randomly or alphabetization?</p>\n<p>One can show that e.g. picking randomly will sometimes throw away money. But there is a deeper principle here: an agent that wants to avoid throwing away money or passing up free money has to act <em>as if</em>&nbsp;it had numbers that followed probability-rules, and that's a good enough reason for our agent to have probabilities.</p>\n<p>Still, some people dislike Dutch book arguments because they focus on an extreme scenario where a malicious bookie is trying to exploit our agent. To avoid this, we'll need a more abstract foundation.</p>\n<p>You can learn more about Dutch book arguments <a href=\"http://plato.stanford.edu/entries/epistemology-bayesian/supplement2.html\">here</a>&nbsp;and&nbsp;<a href=\"http://m-phi.blogspot.com/2013/09/the-mathematics-of-dutch-book-arguments.html\">here</a>.</p>\n<p><strong>Savage's Foundation</strong></p>\n<p>Leonard Savage formulated a basis for decision-making that is sort of a grown-up version of Dutch book arguments. From seven desiderata, none of which mention probability, he derived that an agent that wants to act consistently will act as if&nbsp;it had probabilistic beliefs.</p>\n<p>What are the desiderata about, if not probability? They define an agent that has preferences, and is able to take actions, which are defined as things that lead to outcomes, and can lead to different outcomes depending on external possibilities in event-space. They require that the agent's actions be consistent in commonsensical ways. These requirements are sufficient to show that assigning probabilities to the external events is the best way to do things.</p>\n<p>Savage's theorem provides one set of conditions for when we should use probabilities. But it doesn't help us choose which probabilities to assign - anything consistent works. The idea that probabilities are degrees of belief, and that they are derived from some starting information, is left to our next foundation.</p>\n<p>You can learn more about Savage's foundation <a href=\"http://www.econ2.jhu.edu/people/Karni/savageseu.pdf\">here</a>.</p>\n<p><strong>Cox's Theorem</strong></p>\n<p>Cox's theorem is a break from justifying probabilities with gambling. Rather than starting from an agent that wants to achieve good outcomes, and showing that having probabilities is a good idea, Richard Cox started with desired properties of a \"degree of plausibility,\" and showed that probabilities are what a good belief-number should be.</p>\n<p>One special facet of Cox's desiderata is that they refer to plausibility of an event, given your information - what will eventually become P(event | information).</p>\n<p>There are six or so desiderata, but I think there are three interesting ones: When you're completely certain, your plausibilities should satisfy the rules of classical logic. Every rational plausibility has at least one event with that plausibility. P(A and B|X) can be found as a function of P(A|X) and P(B|A and X).</p>\n<p>These desiderata are a motley assortment. The desideratum that there's an infinite variety of events is the most strange, but it is satisfied if our universe contains a continuous random process or if we can flip a coin as many times as we want. If the desiderata obtain, Cox's theorem shows that we can give pretty much any belief a probability. The perspective of Cox's theorem is useful because it lets us keep talking straightforwardly about probabilities even if betting or decision-making has become nontrivial.</p>\n<p>You can learn more about Cox's theorem in the first two chapters of Jaynes&nbsp;<a href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\">here</a>&nbsp;(in fact, the next few posts are parallel to the first two chapters of Jaynes), and also <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.4276&amp;rep=rep1&amp;type=pdf\">here</a>.&nbsp;Jaynes&nbsp;includes an additional desideratum in this foundation, which we will cover in the next post.</p>\n<p><strong>Kolmogorov Axioms</strong></p>\n<p>At the far extreme of abstraction, we have the Kolmogorov axioms for probability. Here they are:</p>\n<p>P(E) is a non-negative real number, E is an event that belongs to event-space F.</p>\n<p>P(some event occurs)=1.</p>\n<p>Any countable sequence of disjoint events (E1, E2...) satisfies P(E1 or E2 or...) = sum of all the P(E).</p>\n<p>Though it was not their intended purpose, these can be seen as a Cox-style list of desiderata for degrees of plausibility. Their main virtue is that they're simple and handy to mathematicians who like set theory.</p>\n<p>You can learn more about Kolmogorov's axioms <a href=\"http://en.wikipedia.org/wiki/Probability_axioms\">here</a>.</p>\n<p>&nbsp;</p>\n<p>Look back at our robot trying to bet on the 3^^^3'th prime number.&nbsp;Our robot has preferences, so it can be Dutch booked. Its reward depends on the math problem and we want it to act consistently, so Savage's theorem applies. Cox's theorem applies if we allow our robot to make combined bets on math and dice. It even seems like the Kolmogorov axioms should hold. Resting upon these foundations, our robot should assign numbers to mathematical statements, and they should behave like probabilities.</p>\n<p>But we can't get specific about that, because we have a problem - we don't know how to actually find the numbers yet. Our foundations tell us that the probabilities of the two sides of a coin will add to 1, but they don't care whether P(heads) is 0.5 or 0.99999. If Dutch book arguments can't tell us that a coin lands heads half the time, what can? Tune in next time to find out.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">First post in the sequence <em>Logical Uncertainty</em></p>\n<p style=\"text-align:right\">Next post: <a href=\"/lw/jfl/putting_in_the_numbers/\">Putting in the Numbers</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EQ33emneF3Fh62Nn2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 1.5327967910873385e-06, "legacy": true, "legacyId": "25197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h3 id=\"Beginning_of__Logical_Uncertainty_sequence\"><strong style=\"font-size: small;\">Beginning of:</strong><span style=\"font-size: small;\">&nbsp;</span><span style=\"font-size: small; font-weight: normal;\">Logical Uncertainty sequence</span></h3>\n<p>Suppose that we are designing a robot. In order for this robot to reason about the outside world, it will need to use probabilities.</p>\n<p>Our robot can then use its knowledge to acquire cookies, which we have programmed it to value. For example, we might wager a cookie with the robot on the motion of a certain stock price.</p>\n<p>In the coming sequence, I'd like to add a new capability to our robot. It has to do with how the robot handles very hard math problems. If we ask \"what's the last digit of the <a href=\"http://en.wikipedia.org/wiki/Knuth's_up-arrow_notation\">3^^^3</a>'th prime number?\", our robot should at some point&nbsp;<em>give up</em>, before the sun explodes and the point becomes moot.</p>\n<p>If there are math problems our robot can't solve, what should it do if we offer it a bet about the last digit of the 3^^^3'th prime? It's going to have to approximate - robots need to make lots of approximations, even for simple tasks like finding the strategy that maximizes cookies.</p>\n<p>Intuitively, it seems like if we can't find the real answer, the&nbsp;last digit is equally likely to be 1, 3, 7 or 9; our robot should take bets as if it assigned those digits equal probability. But to assign some probability to the wrong answer is logically equivalent to assigning probability to 0=1. When we learn more, it will become clear that this is a problem - we aren't ready to upgrade our robot yet.</p>\n<p>Let's begin with a review of the foundations of probability.</p>\n<div>\n<p><a id=\"more\"></a></p>\n</div>\n<p>What I call foundations of probability are arguments for why our robot should ever want to use probabilities. I will cover four of them, ranging from the worldly (\"make bets in the following way or you lose money\") to the ethereal (\"here's a really elegant set of axioms\"). To use the word \"probability\" to describe the subject of such disparate arguments can seem odd, but keep in mind the naive definition of probability as that number that's 1/6 for a fair die rolling 6 and 30% for clear weather tomorrow.</p>\n<p><strong id=\"Dutch_Books\">Dutch Books</strong></p>\n<p>The concretest of concrete foundations is the Dutch book arguments. A Dutch book is a collection of bets that is certain to lose you money. If you violate the rules of probability, you'll agree to these certain-loss bets (or not take a certain-win bet).</p>\n<p>For example, if you think that each side of the coin has a 55% chance of showing up, then you'll pay $1 for a bet that pays out $0.98 if the coin lands heads and $0.98 if the coin lands tails. If taking bets where you're guaranteed to lose is bad, then you're not allowed to have probabilities for mutually exclusive things that sum to more than 1.</p>\n<p>Similar arguments hold for other properties of probability. If your probabilities for exhaustive events add up to less than 1, you'll pass up free money, which is bad. If you disobey the sum rule or the product rule, you'll agree to a guaranteed loss, which is bad, etcetera. Thus, say the Dutch book arguments, our probabilities have to behave the way they do because we don't want to take guaranteed losses or pass up free money.</p>\n<p>There are many assumptions underlying this whole scenario. Our agent in these arguments already tries to decide using probability-like numbers, all we show is that the numbers have to follow the same rules as probabilities. Why can't our agent follow a totally different method of decision making, like picking randomly or alphabetization?</p>\n<p>One can show that e.g. picking randomly will sometimes throw away money. But there is a deeper principle here: an agent that wants to avoid throwing away money or passing up free money has to act <em>as if</em>&nbsp;it had numbers that followed probability-rules, and that's a good enough reason for our agent to have probabilities.</p>\n<p>Still, some people dislike Dutch book arguments because they focus on an extreme scenario where a malicious bookie is trying to exploit our agent. To avoid this, we'll need a more abstract foundation.</p>\n<p>You can learn more about Dutch book arguments <a href=\"http://plato.stanford.edu/entries/epistemology-bayesian/supplement2.html\">here</a>&nbsp;and&nbsp;<a href=\"http://m-phi.blogspot.com/2013/09/the-mathematics-of-dutch-book-arguments.html\">here</a>.</p>\n<p><strong id=\"Savage_s_Foundation\">Savage's Foundation</strong></p>\n<p>Leonard Savage formulated a basis for decision-making that is sort of a grown-up version of Dutch book arguments. From seven desiderata, none of which mention probability, he derived that an agent that wants to act consistently will act as if&nbsp;it had probabilistic beliefs.</p>\n<p>What are the desiderata about, if not probability? They define an agent that has preferences, and is able to take actions, which are defined as things that lead to outcomes, and can lead to different outcomes depending on external possibilities in event-space. They require that the agent's actions be consistent in commonsensical ways. These requirements are sufficient to show that assigning probabilities to the external events is the best way to do things.</p>\n<p>Savage's theorem provides one set of conditions for when we should use probabilities. But it doesn't help us choose which probabilities to assign - anything consistent works. The idea that probabilities are degrees of belief, and that they are derived from some starting information, is left to our next foundation.</p>\n<p>You can learn more about Savage's foundation <a href=\"http://www.econ2.jhu.edu/people/Karni/savageseu.pdf\">here</a>.</p>\n<p><strong id=\"Cox_s_Theorem\">Cox's Theorem</strong></p>\n<p>Cox's theorem is a break from justifying probabilities with gambling. Rather than starting from an agent that wants to achieve good outcomes, and showing that having probabilities is a good idea, Richard Cox started with desired properties of a \"degree of plausibility,\" and showed that probabilities are what a good belief-number should be.</p>\n<p>One special facet of Cox's desiderata is that they refer to plausibility of an event, given your information - what will eventually become P(event | information).</p>\n<p>There are six or so desiderata, but I think there are three interesting ones: When you're completely certain, your plausibilities should satisfy the rules of classical logic. Every rational plausibility has at least one event with that plausibility. P(A and B|X) can be found as a function of P(A|X) and P(B|A and X).</p>\n<p>These desiderata are a motley assortment. The desideratum that there's an infinite variety of events is the most strange, but it is satisfied if our universe contains a continuous random process or if we can flip a coin as many times as we want. If the desiderata obtain, Cox's theorem shows that we can give pretty much any belief a probability. The perspective of Cox's theorem is useful because it lets us keep talking straightforwardly about probabilities even if betting or decision-making has become nontrivial.</p>\n<p>You can learn more about Cox's theorem in the first two chapters of Jaynes&nbsp;<a href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\">here</a>&nbsp;(in fact, the next few posts are parallel to the first two chapters of Jaynes), and also <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.4276&amp;rep=rep1&amp;type=pdf\">here</a>.&nbsp;Jaynes&nbsp;includes an additional desideratum in this foundation, which we will cover in the next post.</p>\n<p><strong id=\"Kolmogorov_Axioms\">Kolmogorov Axioms</strong></p>\n<p>At the far extreme of abstraction, we have the Kolmogorov axioms for probability. Here they are:</p>\n<p>P(E) is a non-negative real number, E is an event that belongs to event-space F.</p>\n<p>P(some event occurs)=1.</p>\n<p>Any countable sequence of disjoint events (E1, E2...) satisfies P(E1 or E2 or...) = sum of all the P(E).</p>\n<p>Though it was not their intended purpose, these can be seen as a Cox-style list of desiderata for degrees of plausibility. Their main virtue is that they're simple and handy to mathematicians who like set theory.</p>\n<p>You can learn more about Kolmogorov's axioms <a href=\"http://en.wikipedia.org/wiki/Probability_axioms\">here</a>.</p>\n<p>&nbsp;</p>\n<p>Look back at our robot trying to bet on the 3^^^3'th prime number.&nbsp;Our robot has preferences, so it can be Dutch booked. Its reward depends on the math problem and we want it to act consistently, so Savage's theorem applies. Cox's theorem applies if we allow our robot to make combined bets on math and dice. It even seems like the Kolmogorov axioms should hold. Resting upon these foundations, our robot should assign numbers to mathematical statements, and they should behave like probabilities.</p>\n<p>But we can't get specific about that, because we have a problem - we don't know how to actually find the numbers yet. Our foundations tell us that the probabilities of the two sides of a coin will add to 1, but they don't care whether P(heads) is 0.5 or 0.99999. If Dutch book arguments can't tell us that a coin lands heads half the time, what can? Tune in next time to find out.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">First post in the sequence <em>Logical Uncertainty</em></p>\n<p style=\"text-align:right\">Next post: <a href=\"/lw/jfl/putting_in_the_numbers/\">Putting in the Numbers</a></p>", "sections": [{"title": "Beginning of:\u00a0Logical Uncertainty sequence", "anchor": "Beginning_of__Logical_Uncertainty_sequence", "level": 1}, {"title": "Dutch Books", "anchor": "Dutch_Books", "level": 2}, {"title": "Savage's Foundation", "anchor": "Savage_s_Foundation", "level": 2}, {"title": "Cox's Theorem", "anchor": "Cox_s_Theorem", "level": 2}, {"title": "Kolmogorov Axioms", "anchor": "Kolmogorov_Axioms", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bnFP4yWmsFxaKjg3E"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-26T22:47:01.960Z", "modifiedAt": null, "url": null, "title": "Volunteering programmer hours / discussing how to improve LessWrong's software ", "slug": "volunteering-programmer-hours-discussing-how-to-improve", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:36.823Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ocxcgyKq8vxtkXcPp/volunteering-programmer-hours-discussing-how-to-improve", "pageUrlRelative": "/posts/ocxcgyKq8vxtkXcPp/volunteering-programmer-hours-discussing-how-to-improve", "linkUrl": "https://www.lesswrong.com/posts/ocxcgyKq8vxtkXcPp/volunteering-programmer-hours-discussing-how-to-improve", "postedAtFormatted": "Sunday, January 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Volunteering%20programmer%20hours%20%2F%20discussing%20how%20to%20improve%20LessWrong's%20software%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVolunteering%20programmer%20hours%20%2F%20discussing%20how%20to%20improve%20LessWrong's%20software%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FocxcgyKq8vxtkXcPp%2Fvolunteering-programmer-hours-discussing-how-to-improve%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Volunteering%20programmer%20hours%20%2F%20discussing%20how%20to%20improve%20LessWrong's%20software%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FocxcgyKq8vxtkXcPp%2Fvolunteering-programmer-hours-discussing-how-to-improve", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FocxcgyKq8vxtkXcPp%2Fvolunteering-programmer-hours-discussing-how-to-improve", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>There's been some discussion of how to improve the structure of LessWrong at the site software level - for example <a href=\"https://code.google.com/p/lesswrong/issues/detail?id=389\">adding subreddits</a>&nbsp;or <a href=\"/lw/jer/what_is_the_maindiscussion_distinction_and_what/#comments\">modifying how main and discussion work</a>. One roadblock to this that's been mentioned is a shortage of programmer hours. I'd like to volunteer mine.</p>\n<p>I recently finished a course on web development in which, among other things, I build a Reddit clone using Ruby on Rails and Backbone.js. It's been several months since I've written any Python, and I'm somewhat wary of the time required to get familiar with the LessWrong codebase, but I think think the time would be worth it for me: it could potentially improve LessWrong a lot and would let me tick off my \"have contributed to an open source project\" box.</p>\n<p>Of course, before any of that happens, there needs to be some agreement on what changes we think would be a good idea. So... discuss.<br /><br />EDIT: For context, it's been suggested that part of the benefit of subforums is it could defuse debates over \"what topics are appropriate for LessWrong.\" We could even have an \"off-topic\" subforum, a common feature of online discussion forums - I think bringing the format of LessWrong more into line with what's standard on other websites could help newbies be less confused here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ocxcgyKq8vxtkXcPp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 25, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "25368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-26T23:57:56.943Z", "modifiedAt": null, "url": null, "title": "How to make AIXI-tl incapable of learning", "slug": "how-to-make-aixi-tl-incapable-of-learning-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "itaibn0", "createdAt": "2011-11-21T06:56:11.103Z", "isAdmin": false, "displayName": "itaibn0"}, "userId": "kebS6JpfAxXhnsqAR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q6E7DmeaxSDJEFmiN/how-to-make-aixi-tl-incapable-of-learning-0", "pageUrlRelative": "/posts/Q6E7DmeaxSDJEFmiN/how-to-make-aixi-tl-incapable-of-learning-0", "linkUrl": "https://www.lesswrong.com/posts/Q6E7DmeaxSDJEFmiN/how-to-make-aixi-tl-incapable-of-learning-0", "postedAtFormatted": "Sunday, January 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20make%20AIXI-tl%20incapable%20of%20learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20make%20AIXI-tl%20incapable%20of%20learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ6E7DmeaxSDJEFmiN%2Fhow-to-make-aixi-tl-incapable-of-learning-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20make%20AIXI-tl%20incapable%20of%20learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ6E7DmeaxSDJEFmiN%2Fhow-to-make-aixi-tl-incapable-of-learning-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ6E7DmeaxSDJEFmiN%2Fhow-to-make-aixi-tl-incapable-of-learning-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 645, "htmlBody": "<p>Consider a simple game: You are shown a random-looking 512-bit string h. You may then press one of two buttons, labeled '0' and '1'. No matter which button you press, you will then be shown a 256-bit string s such that SHA512(s) = h. In addition, if you pressed '1' you are given 1$.<br /><br />This game seems pretty simple, right? s and h are irrelevant, and you should simply press '1' all the time (I'm assuming you value recieving money). Well, let's see how AIXI and AIXI-tl fare at this game.<br /><br />Let's say the machine already played the game many times. Its memory is h0, <strong>b0</strong>, r0, s0, h1, ..., <strong>b_(n-1)</strong>, r_(n-1), s_(n-1), h_n, where the list is in chronological order, inputs are unbolded while decisions are bolded, and r_i is the reward signal. It is always the case that r_i=b_i and h_i=SHA512(s_i).<br /><br />First let's look at AIXI. It searches for models that compress and extrapolate this history up to the limit of its planning horizon. One class of such models is this: there is a list s0, ..., s_N of random 256-bits strings and a list b0, ..., b_N of (possibly compressible) bits. The history is SHA512(s0), <strong>b0</strong>, b0, s0, ..., <strong>b_(N-1)</strong>, s_(N-1), SHA512(s_N), <strong>b_N</strong>. Here s_i for i&lt;n must match the s_i in its memory, and s_n must be the almost certainly unique value with SHA512(s_n) = h_n. While I don't have a proof, it intuitively seems like this class of models will dominate the machine's probability mass, and repeated arg-max should lead to the action of outputting 1. It wouldn't always do this due to exploration/exploitation considerations and due to the incentive to minimize K (b0, ... b_N) built into its prior, but it should do it most of the time. So AIXI seems good.<br /><br />Now let's consider AIXI-tl. It picks outputs by having provably correct programs assign lower bounds to the expected utility of these outputs, where the expected utility is defined as it is in AIXI. This would include accepting the analysis I just made with AIXI if that analysis can be made provably accurate. Here lies a problem: the agent has seen h_n but hasn't seen s_n. Therefore, it can't be certain that there is an s_n with SHA512(s_n)=h_n. Therefore, it can't be certain that the models used for AIXI actually works (this isn't a problem for AIXI since it has infinite computational power and can always determine that there is such an s_n).<br /><br />There is an ad hoc fix for this for AIXI-tl: Take the same model as before, but have h_n specifically be a random string rather than being SHA512(s_n). This seems at first to work okay. However, it adds n, the current time, as an input to the model, which adds to its complexity. Now other models dependent on the current time also need to be considered. For instance, what if h_n was a random string, and in addition r_n=not(b_n)? This models seems more complicated, but maybe in the programming language used for the Solomonoff prior it is shorter. The key point is that the agent won't be able to update past that initial prior no matter how many times it plays the game.&nbsp; In that case AIXI-tl may consistently prefer 0, assuming there aren't any other models it considers that make its behavior even more complicated.<br /><br />The key problem is that AIXI-tl is handling logical uncertainty badly. The reasonable thing to do upon seeing h_n is assuming that it, like all h_i before it, is a hash of some 256-bit string. Instead, it finds itself unable to prove this fact and is forced into assuming the present h_n is special. This makes it assume the present time is special and makes it incapable of learning from experience.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q6E7DmeaxSDJEFmiN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "25369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T00:05:35.767Z", "modifiedAt": null, "url": null, "title": "How to make AIXI-tl incapable of learning", "slug": "how-to-make-aixi-tl-incapable-of-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.660Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "itaibn0", "createdAt": "2011-11-21T06:56:11.103Z", "isAdmin": false, "displayName": "itaibn0"}, "userId": "kebS6JpfAxXhnsqAR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mJS8YRuotsBzwDoEn/how-to-make-aixi-tl-incapable-of-learning", "pageUrlRelative": "/posts/mJS8YRuotsBzwDoEn/how-to-make-aixi-tl-incapable-of-learning", "linkUrl": "https://www.lesswrong.com/posts/mJS8YRuotsBzwDoEn/how-to-make-aixi-tl-incapable-of-learning", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20make%20AIXI-tl%20incapable%20of%20learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20make%20AIXI-tl%20incapable%20of%20learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJS8YRuotsBzwDoEn%2Fhow-to-make-aixi-tl-incapable-of-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20make%20AIXI-tl%20incapable%20of%20learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJS8YRuotsBzwDoEn%2Fhow-to-make-aixi-tl-incapable-of-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJS8YRuotsBzwDoEn%2Fhow-to-make-aixi-tl-incapable-of-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 646, "htmlBody": "<p>Consider a simple game: You are shown a random-looking 512-bit string h. You may then press one of two buttons, labeled '0' and '1'. No matter which button you press, you will then be shown a 256-bit string s such that SHA512(s) = h. In addition, if you pressed '1' you are given 1$.<br /><br />This game seems pretty simple, right? s and h are irrelevant, and you should simply press '1' all the time (I'm assuming you value recieving money). Well, let's see how AIXI and AIXI-tl fare at this game.<br /><br />Let's say the machine already played the game many times. Its memory is h0, <strong>b0</strong>, r0, s0, h1, ..., <strong>b_(n-1)</strong>, r_(n-1), s_(n-1), h_n, where the list is in chronological order, inputs are unbolded while decisions are bolded, and r_i is the reward signal. It is always the case that r_i=b_i and h_i=SHA512(s_i).<br /><br />First let's look at AIXI. It searches for models that compress and extrapolate this history up to the limit of its planning horizon. One class of such models is this: there is a list s0, ..., s_N of random 256-bits strings and a list b0, ..., b_N of (possibly compressible) bits. The history is SHA512(s0), <strong>b0</strong>, b0, s0, ..., b_(N-1), s_(N-1), SHA512(s_N), <strong>b_N</strong>. Here s_i for i&lt;n must match the s_i in its memory, and s_n must be the almost certainly unique value with SHA512(s_n) = h_n. While I don't have a proof, it intuitively seems like this class of models will dominate the machines probability mass, and repeated arg-max should lead to the action of outputting 1. It wouldn't always do this due to exploration/exploitation considerations and due to the incentive to minimize K (b0, ... b_N) built into its prior, but it should do it most of the time. So AIXI seems good.<br /><br />Now let's consider AIXI-tl. It picks outputs by having provably correct programs assign lower bounds to the expected utility, and picking the one with the best lower bound, where expected utility is as measured by AIXI. This would include accepting the analysis I just made with AIXI if that analysis can be made provably accurate. Here lies a problem: the agent has seen h_n but hasn't seen s_n. Therefore, it can't be certain that there is an s_n with SHA512(s_n)=h_n. Therefore, it can't be certain that the models used for AIXI actually works (this isn't a problem for AIXI since it has infinite computational power and can always determine that there is such an s_n).<br /><br />There is an ad hoc fix for this for AIXI-tl: Take the same model as before, but h_n is a random string rather than being SHA512(s_n). This seems at first to work okay. However, it adds n, the current time, as an input to the model, which adds to its complexity. Now other models dependent on the current time also need to be considered. For instance, what if h_n was a random string, and in addition r_n=not(b_n). This models seems more complicated, but maybe in the programming language used for the Solomonoff prior it is shorter. The key point is that the agent won't be able to update past that initial prior no matter how many times it plays the game.&nbsp; In that case AIXI-tl may consistently prefer 0, assuming there aren't any other models it considers that make its behavior even more complicated.<br /><br />The key problem is that AIXI-tl is handling logical uncertainty badly. The reasonable thing to do upon seeing h_n is assuming that it, like all h_i before it, is a hash of some 256-bit string. Instead, it finds itself unable to prove this fact and is forced into assuming the present h_n is special. This makes it assume the present time is special and makes it incapable of learning from experience.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "TiEFKWDvD3jsKumDx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mJS8YRuotsBzwDoEn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 1.5331053836076688e-06, "legacy": true, "legacyId": "25370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T09:33:57.943Z", "modifiedAt": null, "url": null, "title": "Google may be trying to take over the world", "slug": "google-may-be-trying-to-take-over-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:00.127Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "28iKD7fEnHvK8pNNm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YgHQ9Nez4S8277Wr2/google-may-be-trying-to-take-over-the-world", "pageUrlRelative": "/posts/YgHQ9Nez4S8277Wr2/google-may-be-trying-to-take-over-the-world", "linkUrl": "https://www.lesswrong.com/posts/YgHQ9Nez4S8277Wr2/google-may-be-trying-to-take-over-the-world", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Google%20may%20be%20trying%20to%20take%20over%20the%20world&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoogle%20may%20be%20trying%20to%20take%20over%20the%20world%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgHQ9Nez4S8277Wr2%2Fgoogle-may-be-trying-to-take-over-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Google%20may%20be%20trying%20to%20take%20over%20the%20world%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgHQ9Nez4S8277Wr2%2Fgoogle-may-be-trying-to-take-over-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgHQ9Nez4S8277Wr2%2Fgoogle-may-be-trying-to-take-over-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>So I know we've already seen them buying a bunch of ML and robotics companies, but now they're purchasing <a href=\"http://recode.net/2014/01/26/exclusive-google-to-buy-artificial-intelligence-startup-deepmind-for-400m/\">Shane Legg's AGI startup</a>.&nbsp; This is after they've acquired Boston Dynamics, several smaller robotics and ML firms, and started their own life-extension firm.</p>\n<p>&nbsp;</p>\n<p>Is it just me, or are they trying to make Accelerando or something closely related actually happen?&nbsp; Given that they're buying up real experts and not just \"AI is inevitable\" prediction geeks (who shall remain politely unnamed out of respect for their real, original expertise in machine learning), has someone had a polite word with them about not killing all humans by sheer accident?<em><br /></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qHDus5MuMNqQxJbjD": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YgHQ9Nez4S8277Wr2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 33, "extendedScore": null, "score": 0.000124, "legacy": true, "legacyId": "25372", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 133, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T09:49:08.814Z", "modifiedAt": null, "url": null, "title": "Skepticism about Probability", "slug": "skepticism-about-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:29.269Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZdSqQordj7YkCirsP/skepticism-about-probability", "pageUrlRelative": "/posts/ZdSqQordj7YkCirsP/skepticism-about-probability", "linkUrl": "https://www.lesswrong.com/posts/ZdSqQordj7YkCirsP/skepticism-about-probability", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Skepticism%20about%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASkepticism%20about%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZdSqQordj7YkCirsP%2Fskepticism-about-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Skepticism%20about%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZdSqQordj7YkCirsP%2Fskepticism-about-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZdSqQordj7YkCirsP%2Fskepticism-about-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>I've raised arguments for philosophical scepticism before, which have mostly been argued against in a Popper-esque manner of arguing that even if we don't know anything with certainty, we can have legitimate knowledge on probabilities.</p>\n<p>The problem with this, however, is how you answer a sceptic about the notion of probability having a correlation with reality. Probability depends upon axioms of probability- how are said axioms to be justified? It can't be by definition, or it has no correlation to reality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZdSqQordj7YkCirsP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": -11, "extendedScore": null, "score": -3e-05, "legacy": true, "legacyId": "25373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 129, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T13:51:02.219Z", "modifiedAt": null, "url": null, "title": "Meetup : Rationality Vienna", "slug": "meetup-rationality-vienna", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XnoitWvb6FTDbttHN/meetup-rationality-vienna", "pageUrlRelative": "/posts/XnoitWvb6FTDbttHN/meetup-rationality-vienna", "linkUrl": "https://www.lesswrong.com/posts/XnoitWvb6FTDbttHN/meetup-rationality-vienna", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rationality%20Vienna&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rationality%20Vienna%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXnoitWvb6FTDbttHN%2Fmeetup-rationality-vienna%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rationality%20Vienna%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXnoitWvb6FTDbttHN%2Fmeetup-rationality-vienna", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXnoitWvb6FTDbttHN%2Fmeetup-rationality-vienna", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w9'>Rationality Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 February 2014 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vienna, Reichsratstra\u00dfe 17</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Manuel will talk about Universal Artificial Intelligence, which is the theory of perfectly rational agents. It claims to once-and-for-all define what rationality, intelligence, success, understanding etc. really are. Enough flamebait for a lively discussion There will be some maths presented for those who care about such things, but the math is not necessary to understand the theory.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w9'>Rationality Vienna</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XnoitWvb6FTDbttHN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5340293253365931e-06, "legacy": true, "legacyId": "25374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rationality_Vienna\">Discussion article for the meetup : <a href=\"/meetups/w9\">Rationality Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 February 2014 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vienna, Reichsratstra\u00dfe 17</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Manuel will talk about Universal Artificial Intelligence, which is the theory of perfectly rational agents. It claims to once-and-for-all define what rationality, intelligence, success, understanding etc. really are. Enough flamebait for a lively discussion There will be some maths presented for those who care about such things, but the math is not necessary to understand the theory.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rationality_Vienna1\">Discussion article for the meetup : <a href=\"/meetups/w9\">Rationality Vienna</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rationality Vienna", "anchor": "Discussion_article_for_the_meetup___Rationality_Vienna", "level": 1}, {"title": "Discussion article for the meetup : Rationality Vienna", "anchor": "Discussion_article_for_the_meetup___Rationality_Vienna1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T14:30:26.937Z", "modifiedAt": null, "url": null, "title": "Flashes of Nondecisionmaking", "slug": "flashes-of-nondecisionmaking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:25.558Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iadfrScmtzmhK7ZzF/flashes-of-nondecisionmaking", "pageUrlRelative": "/posts/iadfrScmtzmhK7ZzF/flashes-of-nondecisionmaking", "linkUrl": "https://www.lesswrong.com/posts/iadfrScmtzmhK7ZzF/flashes-of-nondecisionmaking", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Flashes%20of%20Nondecisionmaking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFlashes%20of%20Nondecisionmaking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiadfrScmtzmhK7ZzF%2Fflashes-of-nondecisionmaking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Flashes%20of%20Nondecisionmaking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiadfrScmtzmhK7ZzF%2Fflashes-of-nondecisionmaking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiadfrScmtzmhK7ZzF%2Fflashes-of-nondecisionmaking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 855, "htmlBody": "<p>If you crash a bicycle and cut your knee, it bleeds. You can apply pressure to the wound or otherwise aid in clotting it, but you can't fully control the blood. You can't think, \"Body! I command you not to bleed!\" Nor can you directly say, \"I choose not to bleed\" through pure will alone.</p>\n<p>This is easy enough to understand. We don't have direct control over our blood. We can apply some measure of indirect to it -- taking aspirin might thin the blood, breathing deeply and relaxing might slow the pulse and the flow of blood slightly -- but we do not have direct and instant control over the flow of our blood.</p>\n<p>That's our&nbsp;blood. It's quite a personal thing, when you think about it.</p>\n<p>At the same time, there's a view that we have full control and choice over our actions in a given situation.</p>\n<p>I no longer believe this to be the case.</p>\n<p>We&nbsp;can&nbsp;staunch the flow of bleeding through applying pressure, a cloth, perhaps slowing down our pulse and bloodflow through lowering stress and deep breathing. But we can't, in the moment, command or control blood by force of will or mind alone.</p>\n<p>Likewise, I'm starting to believe we have lots of indirect control over our patterns of action in our lives, but perhaps less control and command in individual moments.</p>\n<p>When a person rolls out of bed, they usually do very similar things each morning. How much control or command do they have -- mentally or analytically or however you want to define it -- over these actions?</p>\n<p>Not much, I'd say.</p>\n<p>Yet, they have immense indirect control, similar to blood flow. If you normally lay out your clothes the night before, and you lay out running clothes instead of work clothes, and set your alarm for an hour earlier, your chances of running go up a lot. There still may be an element of choice or self-command when you decide to run or not, but it's very possible there&nbsp;wasn't&nbsp;choice or self-command available if you did not rearrange your environment with that sort of indirect pressure.</p>\n<p>I had an experience recently that was incredibly distressing. It was strange and very unpleasant at the time, but I'm now thankful for it.</p>\n<p>I was at a convenience store when I realized I was in the process of buying some junk food and energy drinks.</p>\n<p>My mind recognized this, but seemingly had not so much say on what's going on. My legs were just walking the familiar convenience store aisles near my home, picking up two of this energy drink, one of that pack of peanut M&amp;M's, and so on.</p>\n<p>I don't know if I could have stopped the pattern and put the items back in the moment. At the time, I was shocked to realize that I was watching myself act, but I hadn't stopped and started thinking or pondering. My legs and hands were working seemingly slightly independent of myself.</p>\n<p>At the time, it was like a bad dream, or some sort of miserable and crazy experience. I shrugged it off -- strange things happen, you know? -- but I kept thinking about it periodically.</p>\n<p>I'd been training in meditation and impulse control a lot over the last six months, and been studying and experimenting a bit about how our minds work and cognitive psychology.</p>\n<p>My realization now, quite a while later, is that the distressing experience at the convenience store -- \"what the hell is going on here, I am seemingly not controlling my actions!\"-- was actually the beginning of a flash of a greater awareness of my day-to-day life.</p>\n<p>I believe now that we're&nbsp;constantly&nbsp;in nondecisionmaking mode. We're&nbsp;constantly&nbsp;running patterns or taking actions without conscious command or choice, similar to blood running from a cut.</p>\n<p>This process can be managed indirectly and affected, including in the moment it's happening if we're aware of it. But oftentimes, we don't even know we're metaphorically bleeding. We're just&nbsp;doing things, some of them \"smart\", some of them stupid and harmful.</p>\n<p>I've had more flashes of awareness, seeing myself running mechanical patterns during times I normally wouldn't have noticed them. Briefly, here and there. I've been sometimes able to radically course correct and do something entirely different. Othertimes, I try and fail to do something different. I haven't had a moment as puzzling as that first convenience store one.</p>\n<p>There's perhaps two takeaways here. The first is that greater training in awareness and meditation can lead to \"waking up\" or noticing the situation you're in more often. You probably already knew that.</p>\n<p>But the second and more important one, I think, is the idea that things that seem like&nbsp;choices&nbsp;aren't always so. We don't choose to bleed if we cut our knee. Once we realize we're bleeding, we can apply indirect pressure, de-stress, use external things like cloth or bandages, and otherwise manage the situation. We can also buy more protective clothing or improve our technique for the future, so we bleed less. But we can't simply say \"Body, I command you not to bleed\" nor \"I choose not to bleed\" if we are, in fact, bleeding.</p>\n<p>Indirect influence and control, immense amounts. More than most people realize. Direct influence and control? Perhaps not as much as commonly believed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YrLoz567b553YouZ2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iadfrScmtzmhK7ZzF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 44, "extendedScore": null, "score": 1.5340734649894077e-06, "legacy": true, "legacyId": "25375", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T15:01:16.132Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup : London - Paranoid Debating 2nd Feb, plus social 9th Feb", "slug": "meetup-meetup-london-paranoid-debating-2nd-feb-plus-social", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:08.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tL4Y3vJSd5RBKy7xg/meetup-meetup-london-paranoid-debating-2nd-feb-plus-social", "pageUrlRelative": "/posts/tL4Y3vJSd5RBKy7xg/meetup-meetup-london-paranoid-debating-2nd-feb-plus-social", "linkUrl": "https://www.lesswrong.com/posts/tL4Y3vJSd5RBKy7xg/meetup-meetup-london-paranoid-debating-2nd-feb-plus-social", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%20%3A%20London%20-%20Paranoid%20Debating%202nd%20Feb%2C%20plus%20social%209th%20Feb&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%20%3A%20London%20-%20Paranoid%20Debating%202nd%20Feb%2C%20plus%20social%209th%20Feb%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtL4Y3vJSd5RBKy7xg%2Fmeetup-meetup-london-paranoid-debating-2nd-feb-plus-social%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%20%3A%20London%20-%20Paranoid%20Debating%202nd%20Feb%2C%20plus%20social%209th%20Feb%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtL4Y3vJSd5RBKy7xg%2Fmeetup-meetup-london-paranoid-debating-2nd-feb-plus-social", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtL4Y3vJSd5RBKy7xg%2Fmeetup-meetup-london-paranoid-debating-2nd-feb-plus-social", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wa'>Meetup : London - Paranoid Debating 2nd Feb, plus social 9th Feb</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 February 2014 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next London meetup will be a session of <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">paranoid debating</a>. Trust no one.</p>\n\n<p>The venue will be the usual Shakespeare's head, and the time will be the usual 2pm until we get bored, after which we will probably be talking.</p>\n\n<p>In addition! There will be a purely-social meetup on the 9th, at the same time and place. And the next not-purely-social meetup is likely to be the 16th, but if it's not, there will be a purely-social one then as well. We're doing weekly meetups, but to avoid spamming /r/discussion, we're currently posting only the not-purely-social meetups to LW.</p>\n\n<p>If you have difficulty finding us, you can reach me on 07792009646.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">Google group</a> which you might find worth joining.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wa'>Meetup : London - Paranoid Debating 2nd Feb, plus social 9th Feb</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tL4Y3vJSd5RBKy7xg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "25376", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup___London___Paranoid_Debating_2nd_Feb__plus_social_9th_Feb\">Discussion article for the meetup : <a href=\"/meetups/wa\">Meetup : London - Paranoid Debating 2nd Feb, plus social 9th Feb</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 February 2014 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next London meetup will be a session of <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">paranoid debating</a>. Trust no one.</p>\n\n<p>The venue will be the usual Shakespeare's head, and the time will be the usual 2pm until we get bored, after which we will probably be talking.</p>\n\n<p>In addition! There will be a purely-social meetup on the 9th, at the same time and place. And the next not-purely-social meetup is likely to be the 16th, but if it's not, there will be a purely-social one then as well. We're doing weekly meetups, but to avoid spamming /r/discussion, we're currently posting only the not-purely-social meetups to LW.</p>\n\n<p>If you have difficulty finding us, you can reach me on 07792009646.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">Google group</a> which you might find worth joining.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup___London___Paranoid_Debating_2nd_Feb__plus_social_9th_Feb1\">Discussion article for the meetup : <a href=\"/meetups/wa\">Meetup : London - Paranoid Debating 2nd Feb, plus social 9th Feb</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup : London - Paranoid Debating 2nd Feb, plus social 9th Feb", "anchor": "Discussion_article_for_the_meetup___Meetup___London___Paranoid_Debating_2nd_Feb__plus_social_9th_Feb", "level": 1}, {"title": "Discussion article for the meetup : Meetup : London - Paranoid Debating 2nd Feb, plus social 9th Feb", "anchor": "Discussion_article_for_the_meetup___Meetup___London___Paranoid_Debating_2nd_Feb__plus_social_9th_Feb1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T16:04:33.876Z", "modifiedAt": null, "url": null, "title": "[Link] Death, long lives, uploading - a conworlding perspective", "slug": "link-death-long-lives-uploading-a-conworlding-perspective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:58.599Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vulture", "createdAt": "2012-03-24T19:29:34.072Z", "isAdmin": false, "displayName": "Vulture"}, "userId": "v7KPsDBuK3pLtqTLo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pu3c48yddQrrRPghP/link-death-long-lives-uploading-a-conworlding-perspective", "pageUrlRelative": "/posts/Pu3c48yddQrrRPghP/link-death-long-lives-uploading-a-conworlding-perspective", "linkUrl": "https://www.lesswrong.com/posts/Pu3c48yddQrrRPghP/link-death-long-lives-uploading-a-conworlding-perspective", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Death%2C%20long%20lives%2C%20uploading%20-%20a%20conworlding%20perspective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Death%2C%20long%20lives%2C%20uploading%20-%20a%20conworlding%20perspective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPu3c48yddQrrRPghP%2Flink-death-long-lives-uploading-a-conworlding-perspective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Death%2C%20long%20lives%2C%20uploading%20-%20a%20conworlding%20perspective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPu3c48yddQrrRPghP%2Flink-death-long-lives-uploading-a-conworlding-perspective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPu3c48yddQrrRPghP%2Flink-death-long-lives-uploading-a-conworlding-perspective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 515, "htmlBody": "<p><a href=\"http://zompist.wordpress.com/2014/01/23/death-long-lives-uploading/\">Death, long lives, uploading</a></p>\n<p>Mark Rosenfelder (aka zompist, of <a href=\"http://zompist.com/kit.html\">language construction kit</a> fame) writes about the advantages and drawbacks of mortality and its alternatives, in fiction and real life. Rosenfelder, as an author, clearly takes <a href=\"/lw/y0/31_laws_of_fun/\">Fun Theory</a> very seriously. After discussing the mental and physical decline that age usually entail, he assumes that the most difficult to surmount of these problems will be the loss of mental flexibility and tolerance of novelty. He then uses this obstacle to offer interesting fun-theoretic arguments against uploading and cryonics:</p>\n<blockquote>\n<p>One futuristic approach to the problem: get yourself uploaded to a computer, so you can stay alive indefinitely.&nbsp; I think it&rsquo;d be horrible to give up food, sex, exercise, and the rest of our bodily experience, even if we posit that you can still somehow retain your visual qualia.&nbsp; But I can see the attraction of wanting to find out what&rsquo;s next.&nbsp; Perhaps you could hibernate for fifty years at a time, then wake up and avidly consume all the pop culture that&rsquo;s been created since last time.&nbsp; Avoid Sturgeon&rsquo;s Law and read just the best 10% of stuff, forever!</p>\n<p>However,&nbsp;I suspect the plan would fall apart in under 200 years.&nbsp;&nbsp;How much really grabs us from that long ago?&nbsp;&nbsp;We do read stuff that old, of course, but it&rsquo;s only a tiny fraction of our mental diet.&nbsp; The past is a strange world that takes some effort to immerse ourselves in&ndash; when it doesn&rsquo;t repel us with a mindset that&rsquo;s now confusing, boring, or vile.&nbsp; 400 years ago is even harder to grok, and&nbsp;1000 is an alien world.&nbsp; And looking <em>back</em>,&nbsp;I&rsquo;d maintain, is far easier than looking forward.&nbsp; We&rsquo;re exposed to the past as history and literature&ndash; we can read Jane Austen or Jonathan Swift or Moli&egrave;re far easier than they&rsquo;d be able to understand us.</p>\n<p>Imagine Jules Verne, for instance, trying to make sense of <a title=\"Doing the&nbsp;Laundry\" href=\"http://zompist.wordpress.com/2011/08/16/doing-the-laundry/\">a Laundry novel</a>.&nbsp; The prose itself might not be too difficult.&nbsp; The idea of monsters and government bureaucracies would be understood.&nbsp; But he&rsquo;d miss the allusions to Lovecraft and spy novels, and references to the Cold War and computers would require a whole education to follow.&nbsp; Something like an episode of <em>The Simpsons</em> would probably produce complete befuddlement.</p>\n<p>I&rsquo;m not saying it couldn&rsquo;t be done, just that it&rsquo;d require quite a bit more work than it sounds like.&nbsp; And just visiting the future in one-year reading binges, you&rsquo;d never really fit into the culture&ndash; you&rsquo;d be an increasingly alienated dinosaur.</p>\n</blockquote>\n<p>And how he addressed the issue in his own far-future conworld:</p>\n<blockquote>\n<p>In the Incatena, I posit that the problem is solved by people loosening up their brains once a century or two.&nbsp; Basically, you lose a bunch of memories, fade out some of the more habitual neural pathways, recover some of the intellectual flexibility (and ignorance) of adolescence.&nbsp; Maybe change your body type and/or sex while you&rsquo;re at it.&nbsp; You want to be <em>you</em> just enough to feel continuity, but not enough to become a curmudgeon.&nbsp; (And becoming an AI, though it&rsquo;s an option, is viewed as a form of death.)</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pu3c48yddQrrRPghP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.5341788794435835e-06, "legacy": true, "legacyId": "25378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qZJBighPrnv9bSqTZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-27T22:20:25.189Z", "modifiedAt": null, "url": null, "title": "Are you a virtue ethicist at heart?", "slug": "are-you-a-virtue-ethicist-at-heart", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mt77BTkdBMmDyq2Aq/are-you-a-virtue-ethicist-at-heart", "pageUrlRelative": "/posts/mt77BTkdBMmDyq2Aq/are-you-a-virtue-ethicist-at-heart", "linkUrl": "https://www.lesswrong.com/posts/mt77BTkdBMmDyq2Aq/are-you-a-virtue-ethicist-at-heart", "postedAtFormatted": "Monday, January 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20you%20a%20virtue%20ethicist%20at%20heart%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20you%20a%20virtue%20ethicist%20at%20heart%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmt77BTkdBMmDyq2Aq%2Fare-you-a-virtue-ethicist-at-heart%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20you%20a%20virtue%20ethicist%20at%20heart%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmt77BTkdBMmDyq2Aq%2Fare-you-a-virtue-ethicist-at-heart", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmt77BTkdBMmDyq2Aq%2Fare-you-a-virtue-ethicist-at-heart", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 349, "htmlBody": "<p>Disclaimer: I am not a philosopher, so this post will likely seem amateurish to the subject matter experts.&nbsp;</p>\n<p>LW is big on consequentialism, utilitarianism and other quantifiable ethics one can potentially program into a computer to make it provably friendly. However, I posit that most of us intuitively use virtue ethics, and not deontology or consequentialism. In other words, when judging one's actions we intuitively value the person's motivations over the rules they follow or the consequences of said actions. We may reevaluate our judgment later, based on laws and/or actual or expected usefulness, but the initial impulse still remains, even if overridden. To quote <a href=\"http://en.wikipedia.org/wiki/Casimir,_Comte_de_Montrond\">Casimir de Montrond</a>, \"Mistrust first impulses; they are nearly always good\" (the quote is usually misattributed to Talleyrand).</p>\n<p>Some examples:</p>\n<ul>\n<li>Eliezer in a facebook post linked the article&nbsp;<a href=\"http://science.time.com/2014/01/24/charity-doing-good-is-bad/\">When Doing Good Means You&rsquo;re Bad</a>, which points out that people taking commission to raise a lot of money for charity are commonly considered less moral than those who raise much less but are not paid to do so (\"tainted altruism\").&nbsp;</li>\n<li>This was brought up at a meetup: a pregnant woman in a dire financial situation who decides to have an abortion because she does not want a burden of raising a baby is judged harsher than a woman in a similar situation whose motivation is to avoid inflicting harsh life on the prospective child.</li>\n<li>In real-life trolley problems even the committed utilitarians (like commanders during war time) are likely to hesitate before sacrificing lives to save more.</li>\n</ul>\n<p>I am not sure how to classify religious fanaticism (or other bigotry), but it seems to require a heavy dose of virtue ethics (feeling righteous), in addition to following the (deontological) tenets of whichever belief, with some consequentialism (for the greater good) mixed in.</p>\n<p>When I try to introspect my own moral decisions (like whether to tell the truth, or to cheat on a test, or to drive over the speed limit), I can usually find a grain of virtue ethics inside. It might be followed or overridden, sometimes habitually, but it is always there. Can you?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mt77BTkdBMmDyq2Aq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "25380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-28T02:31:15.011Z", "modifiedAt": null, "url": null, "title": "What was that article named?", "slug": "what-was-that-article-named", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:08.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hqnxiT3v2S9ThEsd2/what-was-that-article-named", "pageUrlRelative": "/posts/hqnxiT3v2S9ThEsd2/what-was-that-article-named", "linkUrl": "https://www.lesswrong.com/posts/hqnxiT3v2S9ThEsd2/what-was-that-article-named", "postedAtFormatted": "Tuesday, January 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20was%20that%20article%20named%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20was%20that%20article%20named%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqnxiT3v2S9ThEsd2%2Fwhat-was-that-article-named%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20was%20that%20article%20named%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqnxiT3v2S9ThEsd2%2Fwhat-was-that-article-named", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqnxiT3v2S9ThEsd2%2Fwhat-was-that-article-named", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p>I'm recalling a Less Wrong post about how rationality only leads to winning if you \"have enough of it\". Like if you're \"90% rational\", you'll often \"lose\" to someone who's only \"10% rational\". I can't find it. Does anyone know what I'm talking about, and if so can you link to it?</p>\n<p>I'm not sure if this is appropriate to ask. If not, let me know.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hqnxiT3v2S9ThEsd2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "25382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-29T01:32:17.379Z", "modifiedAt": null, "url": null, "title": "Self-Study Questions Thread", "slug": "self-study-questions-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:43.838Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TylerJay", "createdAt": "2010-08-16T22:37:13.189Z", "isAdmin": false, "displayName": "TylerJay"}, "userId": "rR64xYGdnRFZ5MPQc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jTYWox8euWdN7DSGn/self-study-questions-thread", "pageUrlRelative": "/posts/jTYWox8euWdN7DSGn/self-study-questions-thread", "linkUrl": "https://www.lesswrong.com/posts/jTYWox8euWdN7DSGn/self-study-questions-thread", "postedAtFormatted": "Wednesday, January 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-Study%20Questions%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-Study%20Questions%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTYWox8euWdN7DSGn%2Fself-study-questions-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-Study%20Questions%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTYWox8euWdN7DSGn%2Fself-study-questions-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTYWox8euWdN7DSGn%2Fself-study-questions-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<div>There are a lot of autodidacts on LessWrong and many of us hold <a title=\"scholarship\" href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">scholarship</a> to be a virtue. &nbsp;Learning things on your own can be tricky for a number of reasons, but there is a lot of good material on LessWrong on how to do it <a title=\"efficiently\" href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">efficiently</a>. &nbsp;We know that it's usually best to <a title=\"build small skills in the right order\" href=\"/lw/58m/build_small_skills_in_the_right_order/\">build small skills in the right order</a>, but sometimes it's difficult to figure out just what that order is, especially across disciplines. &nbsp;University programs have strict prerequisites that are supposed to ensure that you can handle a course before you take it, but taking a college course catalog's prerequisites to be absolute is an easy way to get demotivated before you learn anything you're actually interested in.</div>\n<div><br /></div>\n<div>I've seen a lot of people run into trouble with self-study when they really <em>want</em>&nbsp;<em>to</em> study topic x, but know that they <em>have to</em> study topic y first as a prerequisite. &nbsp;(For example, I once <a href=\"/r/discussion/lw/isj/rewiring_my_brain_gentle_help_appreciated/\">saw someone on LW</a> who wanted to start learning programming, but thought he couldn't get started without learning a bunch of math first.) &nbsp;For progress to be <a href=\"/lw/1ul/for_progress_to_be_by_accumulation_and_not_by/\">by accumulation and not by random walk</a>, it is sometimes necessary to do this, but it can be hard to tell.</div>\n<div><br /></div>\n<div>I wanted to create a thread where would-be autodidacts with limited knowledge of their target domain can ask these kinds of questions so that they can better plan their courses of study. &nbsp;Whether it's questions about specific texts, recommended prerequisites, MOOCs, course overlap, whatever. &nbsp;</div>\n<div><br /></div>\n<div>Any general discussion about self-study is welcome too.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jTYWox8euWdN7DSGn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "25385", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["64FdKLwmea8MCLWkE", "37sHjeisS9uJufi4u", "qwdupkFd6kmeZHYXy", "ik4REw8i66KjEiWYj", "ufBYjpi9gK6uvtkh5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-29T06:02:38.134Z", "modifiedAt": null, "url": null, "title": "Personal Psychiatric Analysis", "slug": "personal-psychiatric-analysis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.950Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pscheyer", "createdAt": "2010-02-26T06:22:11.935Z", "isAdmin": false, "displayName": "pscheyer"}, "userId": "a9HpYyEsAMwMEj72e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EcBZfAhrjXZEXvLDQ/personal-psychiatric-analysis", "pageUrlRelative": "/posts/EcBZfAhrjXZEXvLDQ/personal-psychiatric-analysis", "linkUrl": "https://www.lesswrong.com/posts/EcBZfAhrjXZEXvLDQ/personal-psychiatric-analysis", "postedAtFormatted": "Wednesday, January 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Personal%20Psychiatric%20Analysis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APersonal%20Psychiatric%20Analysis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEcBZfAhrjXZEXvLDQ%2Fpersonal-psychiatric-analysis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Personal%20Psychiatric%20Analysis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEcBZfAhrjXZEXvLDQ%2Fpersonal-psychiatric-analysis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEcBZfAhrjXZEXvLDQ%2Fpersonal-psychiatric-analysis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 686, "htmlBody": "<p>Imagine reading about the following&nbsp;result buried in a prestigious journal:</p>\r\n<p>&nbsp;</p>\r\n<blockquote>\r\n<p>We administered [Drug X] to 10,000 patients 80+ years of age selected to be a statistical representation of the populace. None had exhibited any prior&nbsp;medical history to suggest unusual conditions, outside of the normal range of issues collected over a lifetime. 1/3 of the patients were selected as a control&nbsp;group, and the others were entered into a longitudinal study of&nbsp;[Drug X] in which they were given varying doses over a 30 year timespan. [Please read charitably and flesh this out to be a good, well run longitudinal study by your personal standards. The important thing is the number of patients involved.]&nbsp;</p>\r\n<p>Of the patients administered [drugx] 1x/month for 10 years, we found that there was an increase of average lifespan by 1 year compared to normal actuarial tables. We are unsure of the cause of this. We also had one patient who has yet to die after 30 years and shows no signs of aging. Our drug has effectively demonstrated its properties as a medication designed to reduce cholesterol and will proceed to be approved for normal prescription.</p>\r\n</blockquote>\r\n<p>Now, personally, reading this I would be completely uninterested in the normal result and fascinated by the one, crazy, outlier. Living to the age of 110 is abnormal enough that within&nbsp;6,666 people selected as a statistical representation of the population, it is extremely unlikely that anyone would live that long, much less continue performing at the apparent health of an 80 year old.</p>\r\n<p>How small would the sample size have to be before you would consider trying the drug yourself, just to see if you, too, lived forever as long as you took it? What adverse effects and hassles would you go through to try it? Would these factors interact to influence your decision (Mild headaches and a pill 4x/day in exchange for maybe apparent eternal life? Sign me up!)</p>\r\n<p>&nbsp;</p>\r\n<p>This example is an oversimplification to make a point- often in clinical trials there are odd outliers in the results. Patients who went into full remission, or had a full recovery, or were cured of schizophrenia completely.</p>\r\n<p>In the example above, if the sample size had been 10 people, 9 of whom had no adverse effects and one who lived forever, I would take it. I have been known to try nootropics with little or no proven effect, because there are outliers in their samples who have&nbsp;claimed tremendously helpful effects and few people with adverse effects, and i want to see if I get lucky. I think that if even the right placebo could cause changes which improve my effectiveness, it would be worth a shot.</p>\r\n<p>As far as I know, psychiatrists cannot reliably predict that a given drug will improve a patient's long-term diagnosis, and psychiatrists/psychologists cannot even reliably agree on what condition a patient is manifesting. Mental disorders appear to resist diagnosis and solution, unlike, say, a broken leg or a sucking chest wound. I have learned that Cognitive Behavioral Therapy (CBT)&nbsp;has consistent results against a number of disorders, so I have endeavored to learn and apply CBT to my own life without a psychologist or psychiatrist. It has proven extremely effective and worthwhile.</p>\r\n<p>Here is the topic for discussion:&nbsp; should we trust psychiatric analysis using frequentist statistics and ignore the outliers, or should we&nbsp;individually analyze psychiatric studies to see if they contain outliers who show symptoms which we personally desire? Should we act differently when seeking nootropics to improve performance than we do when seeking medication for crippling OCD? Should we trust our psychiatrists, who are probably not very statistically savvy and probably don't read the cases of the outliers?</p>\r\n<p>Where are the holes in my logic, which suggests that psychiatrists who think like medical doctors/general practitioners&nbsp;have a completely incorrect perspective (the law of averages) for finding and testing potential solutions for the extremely personalized medicinal field of psychotherapy/psychiatry (in which everyone is, actually, an extremely unique snowflake.).</p>\r\n<p>&nbsp;</p>\r\n<p>This is more of a thought-provoking prompt than a well-researched post, so please excuse any apparent assertions in the above, all of which is provided for the sake of argument and arises from anecdata.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EcBZfAhrjXZEXvLDQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 3, "extendedScore": null, "score": 1.5367343546347897e-06, "legacy": true, "legacyId": "25386", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-29T08:52:38.889Z", "modifiedAt": null, "url": null, "title": "Tricky Bets and Truth-Tracking Fields", "slug": "tricky-bets-and-truth-tracking-fields", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LzyN9wzEdfS3j5SmT/tricky-bets-and-truth-tracking-fields", "pageUrlRelative": "/posts/LzyN9wzEdfS3j5SmT/tricky-bets-and-truth-tracking-fields", "linkUrl": "https://www.lesswrong.com/posts/LzyN9wzEdfS3j5SmT/tricky-bets-and-truth-tracking-fields", "postedAtFormatted": "Wednesday, January 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tricky%20Bets%20and%20Truth-Tracking%20Fields&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATricky%20Bets%20and%20Truth-Tracking%20Fields%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzyN9wzEdfS3j5SmT%2Ftricky-bets-and-truth-tracking-fields%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tricky%20Bets%20and%20Truth-Tracking%20Fields%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzyN9wzEdfS3j5SmT%2Ftricky-bets-and-truth-tracking-fields", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzyN9wzEdfS3j5SmT%2Ftricky-bets-and-truth-tracking-fields", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 770, "htmlBody": "<p>While visiting Oxford for MIRI&rsquo;s <a href=\"http://intelligence.org/workshops/#november-2013\">November 2013 workshop</a>, I had the pleasure of visiting a meeting of &ldquo;Skeptics in the Pub&rdquo; in the delightfully British-sounding town of <em>High Wycombe</em> in <em>Buckinghamshire</em>. (Say that aloud in a British accent and try not to grin; I dare you!)</p>\n<p>I presented a mildly drunk intro to applied rationality, followed by a 2-hour Q&amp;A that, naturally, wandered into the subject of why AI will inevitably eat the Earth. I must have been fairly compelling despite the beer, because at one point I noticed the bartenders were leaning uncomfortably over one end of the bar in order to hear me, ignoring thirsty customers at the other end.</p>\n<p>Anyhoo, at one point I was talking about the role of formal knowledge in applied rationality, so I explained <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff&rsquo;s lightsaber</a> and why it made me think the wave function never collapses.</p>\n<p>Someone &mdash; I can&rsquo;t recall who; let&rsquo;s say &ldquo;Bob&rdquo; &mdash; wisely asked, &ldquo;But if quantum interpretations all predict the same observations, what does it mean for you to say the wave function never collapses? What do you <em>anticipate</em>?&rdquo; <a id=\"fnref:1\" class=\"footnote\" title=\"see footnote\" href=\"#fn:1\">[1]</a></p>\n<p>Now, I don&rsquo;t actually know whether the <a href=\"http://plato.stanford.edu/entries/qm-manyworlds/#5\">usual proposals</a> for experimental tests of collapse make sense, so instead I answered:</p>\n<blockquote>\n<p>Well, I think theoretical physics is truth-tracking enough that it <em>eventually</em> converges toward true theories, so one thing I anticipate as a result of favoring a no-collapse view is that a significantly greater fraction of physicists will reject collapse in 20 years, compared to today.</p>\n</blockquote>\n<p>Had Bob and I wanted to bet on whether the wave function collapses or not, that would have been an awfully tricky bet to settle. But if we roughly agree on the truth-trackingness of physics as a field, then we can use the consensus of physicists a decade or two from now as a proxy for physical truth, and bet on that instead.</p>\n<p>This won&rsquo;t work for some fields. For example, philosophy sometimes looks more like a random walk than a truth-tracking inquiry &mdash; or, more charitably, it tracks truth on the scale of <em>centuries</em> rather than <em>decades</em>. For example, did you know that one year after the cover of <em>TIME</em> asked &ldquo;Is God dead?&rdquo;, a philosopher named Alvin Plantinga launched a <a href=\"http://www.christianitytoday.com/ct/2008/july/13.22.html?paging=off\">renaissance in Christian philosophy</a>, such that theism and Christian particularism were <em>more</em> commonly defended by analytic philosophers in the 1970s than they were in the 1930s? I also have the impression that moral realism was a more popular view in the 1990s than it was in the 1970s, and that physicalism is less common today than it was in the 1960s, but I&rsquo;m less sure about those.</p>\n<p>You can also do this for bets that are hard to settle for a different kind of reason, e.g. an <a href=\"/lw/ie/the_apocalypse_bet/\">apocalypse bet</a>. <a id=\"fnref:2\" class=\"footnote\" title=\"see footnote\" href=\"#fn:2\">[2]</a> Suppose Bob and I want to bet on whether smarter-than-human AI is technologically feasible. Trouble is, if it&rsquo;s ever proven that superhuman AI is feasible, that event might overthrow the global economy, making it hard to collect the bet, or at least pointless.</p>\n<p>But suppose Bob and I agree that AI scientists, or computer scientists, or technology advisors to first-world governments, or some other set of experts, is likely to converge toward the true answer on the feasibility of superhuman AI as time passes, as humanity learns more, etc. Then we can instead make a bet on whether it will be the case, 20 years from now, that a significantly increased or decreased fraction of those experts will think superhuman AI is feasible.</p>\n<p>Often, there won&rsquo;t be acceptable polls of the experts at both times, for settling the bet. But domain experts typically have a general sense of whether some view has become more or less common in their field over time. So Bob and I could agree to poll a randomly chosen subset of our chosen expert community 20 years from now, asking them how common the view in question is at that time and how common it was 20 years earlier, and settle our bet that way.</p>\n<p>Getting the details right for this sort of long-term bet isn&rsquo;t trivial, but I don't see a fatal flaw. Is there a fatal flaw in the idea that I&rsquo;ve missed? <a id=\"fnref:3\" class=\"footnote\" title=\"see footnote\" href=\"#fn:3\">[3]</a></p>\n<p>&nbsp;</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn:1\">\n<p>I can&rsquo;t recall exactly how the conversation went, but it was <em>something</em> like this. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:1\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:2\">\n<p>See also Jones, <a href=\"http://econlog.econlib.org/archives/2012/09/how_to_bet_on_b.html\">How to bet on bad futures</a>. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:2\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:3\">\n<p>I also doubt I&rsquo;m the first person to describe this idea in writing: please link to other articles making this point if you know of any. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:3\">&nbsp;\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LzyN9wzEdfS3j5SmT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "25387", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Kyc5dFDzBg4WccrbK", "dLzZWNGD23zqNLvt3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-29T15:16:01.984Z", "modifiedAt": null, "url": null, "title": "Recommendation request: Budgeting/accounting software for a non-profit", "slug": "recommendation-request-budgeting-accounting-software-for-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:22.106Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rwtH9FfXcNtpKmjvP/recommendation-request-budgeting-accounting-software-for-a", "pageUrlRelative": "/posts/rwtH9FfXcNtpKmjvP/recommendation-request-budgeting-accounting-software-for-a", "linkUrl": "https://www.lesswrong.com/posts/rwtH9FfXcNtpKmjvP/recommendation-request-budgeting-accounting-software-for-a", "postedAtFormatted": "Wednesday, January 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recommendation%20request%3A%20Budgeting%2Faccounting%20software%20for%20a%20non-profit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecommendation%20request%3A%20Budgeting%2Faccounting%20software%20for%20a%20non-profit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwtH9FfXcNtpKmjvP%2Frecommendation-request-budgeting-accounting-software-for-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recommendation%20request%3A%20Budgeting%2Faccounting%20software%20for%20a%20non-profit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwtH9FfXcNtpKmjvP%2Frecommendation-request-budgeting-accounting-software-for-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrwtH9FfXcNtpKmjvP%2Frecommendation-request-budgeting-accounting-software-for-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>I'm currently working for a non-profit, and a colleague recently stated that there isn't a good software solution for non-profit accounting.</p>\n<p>This seems unlikely. Any recommendations?</p>\n<p>I know that's a pretty generic description, but it seems like requirements would be quite similar across US non-profits. I can seek out more info if needed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rwtH9FfXcNtpKmjvP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.537356277345928e-06, "legacy": true, "legacyId": "25388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-29T20:27:24.738Z", "modifiedAt": null, "url": null, "title": "Futurism's Track Record", "slug": "futurism-s-track-record", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:30.264Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ycPKhdmDgWsoyvKd/futurism-s-track-record", "pageUrlRelative": "/posts/6ycPKhdmDgWsoyvKd/futurism-s-track-record", "linkUrl": "https://www.lesswrong.com/posts/6ycPKhdmDgWsoyvKd/futurism-s-track-record", "postedAtFormatted": "Wednesday, January 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Futurism's%20Track%20Record&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFuturism's%20Track%20Record%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ycPKhdmDgWsoyvKd%2Ffuturism-s-track-record%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Futurism's%20Track%20Record%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ycPKhdmDgWsoyvKd%2Ffuturism-s-track-record", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ycPKhdmDgWsoyvKd%2Ffuturism-s-track-record", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 448, "htmlBody": "<p>It would be nice (and expensive) to get a systematic survey on this, but my impressions <a id=\"fnref:1\" class=\"footnote\" title=\"see footnote\" href=\"#fn:1\">[1]</a> after tracking down lots of past technology predictions, and reading histories of technological speculation and invention, and reading about &ldquo;elite common sense&rdquo; at various times in the past, are that:</p>\n<ul>\n<li>Elite common sense at a given time almost always <em>massively</em> underestimates what will be technologically feasible in the future.</li>\n<li>&ldquo;Futurists&rdquo; in history tend to be far more accurate about what will be technologically feasible (when they don&rsquo;t grossly violate known physics), but they are often too optimistic about timelines, and (like everyone else) show little ability to predict (1) the long-term social consequences of future technologies, or (2) the details of which (technologically feasible; successfully prototyped) things will make commercial sense, or be popular products.</li>\n</ul>\n<p>Naturally, as someone who thinks it&rsquo;s incredibly important to predict the long-term future as well as we can while also avoiding overconfidence, I try to put myself in a position to learn what past futurists were doing right, and what they were doing wrong. For example, I recommend: Be a fox not a hedgehog. Do calibration training. Know how your brain works. Build quantitative models even if you don&rsquo;t believe the outputs, <a href=\"/lw/jfm/another_critique_of_effective_altruism/aagj\">so that</a> specific pieces of the model are easier to attack and update. Have broad confidence intervals over the timing of innovations. Remember to forecast future developments by looking at trends in many inputs to innovation, not just the &ldquo;calendar years&rdquo; input. Use <a href=\"/lw/hzu/model_combination_and_adjustment/\">model combination</a>. Study history and learn from it. Etc.</p>\n<p>Anyway: do others who have studied the history of futurism, elite common sense, innovation, etc. have different impressions about futurism&rsquo;s track record? And, anybody want to do a PhD thesis examining futurism&rsquo;s track record? Or on some piece of it, <em>ala</em> <a href=\"http://intelligence.org/2013/09/04/how-effectively-can-we-plan-for-future-decades/\">this</a> or <a href=\"http://intelligence.org/files/PredictingAI.pdf\">this</a> or <a href=\"/lw/diz/kurzweils_predictions_good_accuracy_poor/\">this</a>? :)</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn:1\">\n<p>I should explain one additional piece of reasoning which contributes to my impressions on the matter. How do I think about futurist predictions of technologies that haven&rsquo;t yet been definitely demonstrated to be technologically feasible or infeasible? For these, I try to use something like the <a href=\"/lw/jl7/tricky_bets_and_truthtracking_fields/\">truth-tracking fields proxy</a>. E.g. very few intellectual elites (outside Turing, von Neumann, Good, etc.) in 1955 thought AGI would be technologically feasible. By 1980, we&rsquo;d made a bunch of progress in computing and AI and neuroscience, and a much greater proportion of intellectual elites came to think AGI would be technologically feasible. Today, I think the proportion is even greater. The issue hasn&rsquo;t been &ldquo;definitely decided&rdquo; yet (from a social point of view), but things are strongly trending in favor of Good and Turing, and against (e.g.) Dreyfus. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:1\">&nbsp;\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b0": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ycPKhdmDgWsoyvKd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 1.5377064093319875e-06, "legacy": true, "legacyId": "25390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iyRpsScBa6y4rduEt", "kK5rabDsKWMkup7gw", "LzyN9wzEdfS3j5SmT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-29T21:49:53.387Z", "modifiedAt": null, "url": null, "title": "Logical and Indexical Uncertainty", "slug": "logical-and-indexical-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:36.330Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SFLCB5BgjzruJv9sp/logical-and-indexical-uncertainty", "pageUrlRelative": "/posts/SFLCB5BgjzruJv9sp/logical-and-indexical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/SFLCB5BgjzruJv9sp/logical-and-indexical-uncertainty", "postedAtFormatted": "Wednesday, January 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20and%20Indexical%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20and%20Indexical%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFLCB5BgjzruJv9sp%2Flogical-and-indexical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20and%20Indexical%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFLCB5BgjzruJv9sp%2Flogical-and-indexical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFLCB5BgjzruJv9sp%2Flogical-and-indexical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1239, "htmlBody": "<p><a href=\"http://bywayofcontradiction.com/?p=484\">Cross-posted</a> on <a href=\"http://bywayofcontradiction.com/\">By Way of Contradiction</a></p>\n<p>Imagine I shot a photon at a half silvered mirror which reflects the photon with \"probability\" 1/2 and lets the photon pass through with \"probability\" 1/2.</p>\n<p>Now, Imagine I calculated the trillionth decimal digit of pi, and checked whether it was even or odd. As a Bayesian, you use the term \"probability\" in this situation too, and to you, the \"probability\" that the digit is odd is 1/2.</p>\n<p>What is the difference between these too situations? Assuming the many worlds interpretation of quantum mechanics, the first probability comes from indexical uncertainty, while the second comes from logical uncertainty. In indexical uncertainty, both possibilities are true in different parts of whatever your multiverse model is, but you are unsure which part of that multiverse you are in. In logical uncertainty, only one of the possibilities is true, but you do not have information about which one. It may seem at first like this should not change our decision theory, but I believe there are good reasons why we should care about what type of uncertainty we are talking about.</p>\n<p>I present here 6 reasons why we potentially care about the 2 different types of uncertainties. I do not agree with all of these ideas, but I present them anyway, because it seems reasonable that some people might argue for them. Is there anything I have missed?</p>\n<p>1) Anthropics</p>\n<p>Suppose Sleeping Beauty volunteers to undergo the following experiment, which is described to her before it begins. On Sunday she is given a drug that sends her to sleep, and a coin is tossed. If the coin lands heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug that makes her forget the events of Monday only, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again. Beauty wakes up in the experiment and is asked, \"With what subjective probability do you believe that the coin landed tails?\"</p>\n<p>People argue about whether the \"correct answer\" to this question should be 1/3 or 1/2. Some say that the question is malformed, and needs to be rewritten as a decision theory question. Another view is that the question actually depends on the coin flip:</p>\n<p>If the coin flip is a indexical coin flip, then there are effectively 3 copies of sleeping beauty, and in 1 on those copies, the coin came up tails, so you should say 1/3. On the other hand, if it is a logical coin flip, then you cannot compare the two copies of you waking up in one possible world with the one copy of you waking up in the other possible world. Only one of the worlds is logically consistent. The trillionth digit of pi is not changed by you waking up, and you will wake up regardless of the state of the trillionth digit of pi.</p>\n<p>2) Risk Aversion</p>\n<p>Imagine that I were to build a doomsday device. The device flips a coin, and if the coin comes up heads, it destroys the Earth, and everything on it. If the coin comes up tails, it does nothing. Would you prefer if the coin flip were a logical coin flip, or a indexical coin flip?</p>\n<p>You probably prefer the indexical coin flip. It feels more safe to have the world continue on in half of the universes, then to risk destroying the world in all universes. I do not think this feeling arises from biassed thinking, but instead from a true difference in preferences. To me, destroying the world in all of the universes is actually much more than twice as bad as destroying the world in half of the universes.</p>\n<p>3) Preferences vs Beliefs</p>\n<p>In updateless decision theory, you want to choose the output of your decision procedure. If there are multiple copies of yourself in the universe, you do not ask about which copy you are, but instead just choose the output which maximizes your utility of the universe in which all of your copies output that value. The \"expected\" utility comes from your logical uncertainty about what the universe is like. There is not much room in this theory for indexical uncertainty. Instead the indexical uncertainty is encoded into your utility function. The fact that you prefer to be given a reward with indexical probability 99% than given a reward with indexical probability 1% should instead be viewed as you preferring the universe in which 99% of the copies of you receive the reward to the universe in which 1% of the copies of you receive the reward.</p>\n<p>In this view, it seems that indexical uncertainty should be viewed as preferences, while logical uncertainty should be viewed as beliefs. It is important to note that this all adds up to normality. If we are trying to maximize our expected utility, the only thing we do with preferences and beliefs is multiply them together, so for the most part it doesn't change much to think of something as a preference as opposed to belief.</p>\n<p>4) Altruism</p>\n<p>In <a href=\"http://bywayofcontradiction.com/?p=29\">Subjective Altruism</a>, I asked a question about whether or not when being altruistic towards someone else, you should try to maximize their expected utility relative to you probability function or relative to their probability function. If your answer was to choose the option which maximizes your expectation of their utility, then it is actually very important whether indexical uncertainty is a belief or a preference.</p>\n<p>5) Sufficient Reflection</p>\n<p>In theory, given enough time, you can settle logical uncertainties just by thinking about them. However, given enough time, you can settle indexical uncertainties by making observations. It seems to me that there is not a meaningful difference between observations that take place entirely within your mind and observations about the outside world. I therefore do not think this difference means very much.</p>\n<p>6) Consistency</p>\n<p>Logical uncertainty seems like it is harder to model, since it means you are assigning probabilities to possibly inconsistent theories, and all inconsistent theories are logically equivalent. You might want some measure of equivalence of your various theories, and it would have to be different from logical equivalence. Indexical uncertainty does not appear to have the same issues, at least not in an obvious way. However, I think this issue only comes from looking at the problem in the wrong way. I believe that probabilities should only be assigned to logical statements, not to entire theories. Then, since everything is finite, you can treat sentences as equivalent only after you have proven them equivalent.</p>\n<p>7) Counterfactual Mugging</p>\n<p>Omega appears and says that it has just tossed a fair coin, and given that the coin came up tails, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But Omega also tells you that if the coin came up heads instead of tails, it'd give you $10000, but only if you'd agree to give it $100 if the coin came up tails.</p>\n<p>It seems reasonable to me that people might feel very different about this question based on whether or not the coin is logical or indexical. To me, it makes sense to give up the $100 either way, but it seems possible to change the question in such a way that the type of coin flip might matter.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SFLCB5BgjzruJv9sp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 32, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "25391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-29T22:26:20.792Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup-indexical and logical uncertainty", "slug": "meetup-west-la-meetup-indexical-and-logical-uncertainty", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qHjwRNFmAyTYzJRxt/meetup-west-la-meetup-indexical-and-logical-uncertainty", "pageUrlRelative": "/posts/qHjwRNFmAyTYzJRxt/meetup-west-la-meetup-indexical-and-logical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/qHjwRNFmAyTYzJRxt/meetup-west-la-meetup-indexical-and-logical-uncertainty", "postedAtFormatted": "Wednesday, January 29th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup-indexical%20and%20logical%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup-indexical%20and%20logical%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqHjwRNFmAyTYzJRxt%2Fmeetup-west-la-meetup-indexical-and-logical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup-indexical%20and%20logical%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqHjwRNFmAyTYzJRxt%2Fmeetup-west-la-meetup-indexical-and-logical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqHjwRNFmAyTYzJRxt%2Fmeetup-west-la-meetup-indexical-and-logical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wb'>West LA Meetup-indexical and logical uncertainty</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 January 2014 02:32:41PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to get in: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".\nParking is free for 3 hours.\nDiscussion: We will discuss the difference between logical and indexical uncertainty, mostly inspired by my recent blog post. I expect that this will be a short discussion, and most of the time will be casual conversation. We will also discuss a new meeting place, (or maybe even new time) so if you have opinions on this, and will not be there, leave a comment. This meetup will be in less than 5 hours at the time of this post. Sorry for the short notice. No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wb'>West LA Meetup-indexical and logical uncertainty</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qHjwRNFmAyTYzJRxt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5378401827472522e-06, "legacy": true, "legacyId": "25393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_indexical_and_logical_uncertainty\">Discussion article for the meetup : <a href=\"/meetups/wb\">West LA Meetup-indexical and logical uncertainty</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 January 2014 02:32:41PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to get in: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".\nParking is free for 3 hours.\nDiscussion: We will discuss the difference between logical and indexical uncertainty, mostly inspired by my recent blog post. I expect that this will be a short discussion, and most of the time will be casual conversation. We will also discuss a new meeting place, (or maybe even new time) so if you have opinions on this, and will not be there, leave a comment. This meetup will be in less than 5 hours at the time of this post. Sorry for the short notice. No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_indexical_and_logical_uncertainty1\">Discussion article for the meetup : <a href=\"/meetups/wb\">West LA Meetup-indexical and logical uncertainty</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup-indexical and logical uncertainty", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_indexical_and_logical_uncertainty", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup-indexical and logical uncertainty", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_indexical_and_logical_uncertainty1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-30T01:20:10.579Z", "modifiedAt": null, "url": null, "title": "Huffington Post article on DeepMind-requested AI ethics board, links back to LW [link]", "slug": "huffington-post-article-on-deepmind-requested-ai-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:19.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZrT9wAQZFATEE9CfH/huffington-post-article-on-deepmind-requested-ai-ethics", "pageUrlRelative": "/posts/ZrT9wAQZFATEE9CfH/huffington-post-article-on-deepmind-requested-ai-ethics", "linkUrl": "https://www.lesswrong.com/posts/ZrT9wAQZFATEE9CfH/huffington-post-article-on-deepmind-requested-ai-ethics", "postedAtFormatted": "Thursday, January 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Huffington%20Post%20article%20on%20DeepMind-requested%20AI%20ethics%20board%2C%20links%20back%20to%20LW%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHuffington%20Post%20article%20on%20DeepMind-requested%20AI%20ethics%20board%2C%20links%20back%20to%20LW%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrT9wAQZFATEE9CfH%2Fhuffington-post-article-on-deepmind-requested-ai-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Huffington%20Post%20article%20on%20DeepMind-requested%20AI%20ethics%20board%2C%20links%20back%20to%20LW%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrT9wAQZFATEE9CfH%2Fhuffington-post-article-on-deepmind-requested-ai-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrT9wAQZFATEE9CfH%2Fhuffington-post-article-on-deepmind-requested-ai-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>http://www.huffingtonpost.com/2014/01/29/google-ai_n_4683343.html</p>\n<p>Not going to summarize the article content, but I think this is the highest-level publication linking to LW so far.</p>\n<p>Also, it's appears that Shane Legg, Jaan Tallin and others at Deep Mind leveraged the acquisition and moved the friendly AI conversation to a higher level, quite possibly highest level at Google. Interesting times, these are.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZrT9wAQZFATEE9CfH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 19, "extendedScore": null, "score": 1.5380357380237425e-06, "legacy": true, "legacyId": "25395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-30T06:41:42.112Z", "modifiedAt": null, "url": null, "title": "Putting in the Numbers", "slug": "putting-in-the-numbers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:57.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bnFP4yWmsFxaKjg3E/putting-in-the-numbers", "pageUrlRelative": "/posts/bnFP4yWmsFxaKjg3E/putting-in-the-numbers", "linkUrl": "https://www.lesswrong.com/posts/bnFP4yWmsFxaKjg3E/putting-in-the-numbers", "postedAtFormatted": "Thursday, January 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Putting%20in%20the%20Numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APutting%20in%20the%20Numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnFP4yWmsFxaKjg3E%2Fputting-in-the-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Putting%20in%20the%20Numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnFP4yWmsFxaKjg3E%2Fputting-in-the-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnFP4yWmsFxaKjg3E%2Fputting-in-the-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 908, "htmlBody": "<p><strong>Followup To:</strong>&nbsp;<a href=\"/lw/jfx/foundations_of_probability/\">Foundations of Probability</a></p>\n<p>In the previous post, we reviewed reasons why having probabilities is a good idea. These foundations defined probabilities as numbers following certain rules, like the product rule and the rule that mutually exclusive probabilities sum to 1 at most. These probabilities have to hang together as a coherent whole. But just because probabilities hang together a certain way, doesn't actually tell us what numbers to assign.</p>\n<p>I can say a coin flip has P(heads)=0.5, or I can say it has P(heads)=0.999; both are perfectly valid probabilities, as long as P(tails) is consistent. This post will be about how to actually get to the numbers.</p>\n<p><a id=\"more\"></a></p>\n<p>If the probabilities aren't fully determined by our desiderata, what do we need to determine the probabilities? More desiderata!</p>\n<p>Our final desideratum is motivated by the perspective that our probability is based on some state of information. This is acknowledged explicitly in Cox's scheme, but is also just a physical necessity for any robot we build. Thus we add our new desideratum: Assign probabilities that are consistent with the information you have, but don't make up any extra information. It turns out this is enough to let us put numbers to the probabilities.</p>\n<p>In its simplest form, this desideratum is a symmetry principle. If you have the exact same information about two events, you should assign them the same probability - giving them different probabilities would be making up extra information. So if your background information is \"Flip a coin, the mutually exclusive and exhaustive probabilities are heads and tails,\" there is a symmetry between the labels \"heads\" and \"tails,\" which given our new desideratum lets us assign each P=0.5.</p>\n<p>Sometimes, though, we need to pull out the information theory. Using the fact that it doesn't produce information to split the probabilities up differently, we can specify something called \"information entropy\" (For more thoroughness, see chapter 11 of <a href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\">Jaynes</a>). The entropy of a probability distribution is a function that measures how uncertain you are. If I flip a coin and don't know about the outcome, I have one bit of entropy. If I flip two coins, I have two bits of entropy. In this way, the entropy is like the amount of information you're \"missing\" about the coin flips.<img style=\"margin: 10px;\" src=\"http://images.lesswrong.com/t3_jfl_0.png?v=db23affc813431eb24d7e9748fc7fa8f\" alt=\"Entropy of weighted coin\" width=\"318\" height=\"260\" align=\"right\" /></p>\n<p>The mathematical expression for information entropy is that it's the sum of each probability multiplied by its log. Entropy = -Sum( P(x)<span style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\"><strong>&middot;</strong></span>Log(P(x)) ), where the events x are mutually exclusive. Assigning probabilities is all about maximizing the entropy while obeying the constraints of our prior information.</p>\n<p>Suppose we roll a 4-sided die. Our starting information consists of our knowledge that there are sides numbered 1 to 4 (events 1, 2, 3, and 4 are exhaustive), and the die will land on just one of these sides (they're mutually exclusive). This let's us write our information entropy as -P(1)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(1)) - P(2)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(2)) - P(3)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(3)) - P(4)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(4)).</p>\n<p>Finding the probabilities is a maximization problem, subject to the constraints of our prior information. For the simple 4-sided die, our information just says that the probabilities have to add to 1. Simply knowing the fact that the entropy is concave down tells us that to maximize entropy we should split it up as evenly as possible - each side has a 1/4 chance of showing.</p>\n<p>That was pretty commonsensical. To showcase the power of maximizing information entropy, we can add an extra constraint.</p>\n<p>If we have additional knowledge that the average roll of our die is 3, then we want to maximize&nbsp;-P(1)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(1)) - P(2)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(2)) - P(3)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(3)) - P(4)<strong style=\"color: #444444; font-family: arial, sans-serif; line-height: 14.654545783996582px;\">&middot;</strong>Log(P(4)), given that the sum is 1 and the average is 3. We can either plug in the constraints and set partial derivatives to zero, or we can use a maximization technique like Lagrange multipliers.</p>\n<p>When we do this (again, more details in Jaynes ch. 11), it turns out the the probability distribution is shaped like an exponential curve. Which was unintuitive to me - my intuition likes straight lines. But it makes sense if you think about the partial derivative of the information entropy: 1+Log(P) = [some Lagrange multiplier constraints]. The steepness of the exponential controls how shifted the average roll is.</p>\n<p>&nbsp;</p>\n<p>The need for this extra desideratum has not always been obvious. People are able to intuitively figure out that a fair coin lands heads with probability 0.5. Seeing that their intuition is so useful, some people include that intuition as a fundamental part of their method of probability. The counter to this is to focus on constructing a robot, which only has those intuitions we can specify unambiguously.</p>\n<p>Another alternative to assigning probabilities based on maximum entropy is to pick a standard prior and use that. Sometimes this works wonderfully - it would be silly to rederive the binomial distribution every time you run into a coin-flipping problem. But sometimes people will use a well-known prior even if it doesn't match the information they have, just because their procedure is \"use a well-known prior.\" The only way to be safe from that mistake and from interminable disputes over \"which prior is right\" is to remember that a prior is only correct insofar as it captures some state of information.</p>\n<p>Next post, we will finally get to the problem of logical uncertainty, which will shake our foundations a bit. But I really like the principle of not making up information - even a robot that can't do hard math problems can aspire to not make up information.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence&nbsp;<em>Logical Uncertainty</em></p>\n<p style=\"text-align:right\">Previous Post: <a href=\"/lw/jfx/foundations_of_probability/\">Foundations of Probability</a></p>\n<p style=\"text-align:right\">Next post: <a href=\"/lw/jjk/logic_as_probability/\">Logic as Probability</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bnFP4yWmsFxaKjg3E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 17, "extendedScore": null, "score": 1.5383975642614848e-06, "legacy": true, "legacyId": "25185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EQ33emneF3Fh62Nn2", "WKkDD6u79pzmvyQ6a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-30T11:55:45.027Z", "modifiedAt": null, "url": null, "title": "Humans can drive cars", "slug": "humans-can-drive-cars", "viewCount": null, "lastCommentedAt": "2021-02-11T15:49:34.376Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Apprentice", "createdAt": "2009-05-20T13:06:35.976Z", "isAdmin": false, "displayName": "Apprentice"}, "userId": "X5f7X9PkRFwGyxWsb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XjNwWKdKpB28dv8QG/humans-can-drive-cars", "pageUrlRelative": "/posts/XjNwWKdKpB28dv8QG/humans-can-drive-cars", "linkUrl": "https://www.lesswrong.com/posts/XjNwWKdKpB28dv8QG/humans-can-drive-cars", "postedAtFormatted": "Thursday, January 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Humans%20can%20drive%20cars&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHumans%20can%20drive%20cars%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjNwWKdKpB28dv8QG%2Fhumans-can-drive-cars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Humans%20can%20drive%20cars%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjNwWKdKpB28dv8QG%2Fhumans-can-drive-cars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjNwWKdKpB28dv8QG%2Fhumans-can-drive-cars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 486, "htmlBody": "<p>There's been a lot of fuss lately about Google's gadgets. Computers can drive cars - pretty amazing, eh? I guess. But what amazed me as a child was that <em>people</em> can drive cars. I'd sit in the back seat while an adult controlled a machine taking us at insane speeds through a cluttered, seemingly quite unsafe environment. I distinctly remember thinking that something about this just doesn't add up.</p>\n<p>It looked to me like there was just no adequate mechanism to keep the car on the road. At the speeds cars travel, a tiny deviation from the correct course would take us flying off the road in just a couple of seconds. Yet the adults seemed pretty nonchalant about it - the adult in the driver's seat could have relaxed conversations with other people in the car. But I knew that people were pretty clumsy. I was an ungainly kid but I knew even the adults would bump into stuff, drop things and generally fumble from time to time. Why didn't that seem to happen in the car? I felt I was missing something. Maybe there were magnets in the road?</p>\n<p>Now that I am a driving adult I could more or less explain this to a 12-year-old me:</p>\n<p>1. Yes, the course needs to be controlled very exactly and you need to make constant tiny course corrections or you're off to a serious accident in no time.</p>\n<p>2. Fortunately, the steering wheel is a really good instrument for making small course corrections. The design is somewhat clumsiness-resistant.</p>\n<p>3. Nevertheless, you really are just one misstep away from death and you need to focus intently. You can't take your eyes off the road for even one second. Under good circumstances, you can have light conversations while driving but a big part of your mind is still tied up by the task.</p>\n<p>4. People can drive cars - but only just barely. You can't do it safely even while only mildly inebriated. That's not just an arbitrary law - the hit to your reflexes substantially increases the risks. You can do pretty much all other normal tasks after a couple of drinks, but not this.</p>\n<p>So my 12-year-old self was not completely mistaken but still ultimately wrong. There are no magnets in the road. The explanation for why driving works out is mostly that people are just somewhat more capable than I'd thought. In my more sunny moments I hope that I'm making similar errors when thinking about artificial intelligence. Maybe creating a safe AGI isn't as impossible as it looks to me. Maybe it isn't beyond human capabilities. Maybe.</p>\n<p>Edit: I intended no real analogy between AGI design and driving or car design - just the general observation that people are sometimes more competent than I expect. I find it interesting that multiple commenters note that they have also been puzzled by the relative safety of traffic. I'm not sure what lesson to draw.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "irYLXtT9hkPXoZqhH": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XjNwWKdKpB28dv8QG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 53, "extendedScore": null, "score": 0.00015, "legacy": true, "legacyId": "25398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-30T17:03:10.593Z", "modifiedAt": null, "url": null, "title": "Meetup : Inaugural Canberra meetup", "slug": "meetup-inaugural-canberra-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.059Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nbfAPcX8Tr3zNxPmQ/meetup-inaugural-canberra-meetup", "pageUrlRelative": "/posts/nbfAPcX8Tr3zNxPmQ/meetup-inaugural-canberra-meetup", "linkUrl": "https://www.lesswrong.com/posts/nbfAPcX8Tr3zNxPmQ/meetup-inaugural-canberra-meetup", "postedAtFormatted": "Thursday, January 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Inaugural%20Canberra%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Inaugural%20Canberra%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnbfAPcX8Tr3zNxPmQ%2Fmeetup-inaugural-canberra-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Inaugural%20Canberra%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnbfAPcX8Tr3zNxPmQ%2Fmeetup-inaugural-canberra-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnbfAPcX8Tr3zNxPmQ%2Fmeetup-inaugural-canberra-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wc'>Inaugural Canberra meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 February 2014 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">CSIT Building, Acton ACT 2601, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(I'm posting this on behalf of my lurker friend.)</p>\n\n<p>WHEN: 12 February 2014 07:30:00PM(+1100)\nWHERE: CSIT Building, Blg 108, North Road, ANU</p>\n\n<p>This will be the first meetup in Canberra! We will get to know each other and play board games, joining the weekly board games night held by the ANU Computer Science Students' Association. Vegan-friendly snacks and board games will be provided (although if anyone has board games, it would be great if you could bring your own). I will be waiting at the main door of the building from 7:20 to 7:40 to show people to the room where we will be meeting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wc'>Inaugural Canberra meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nbfAPcX8Tr3zNxPmQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.53909736290783e-06, "legacy": true, "legacyId": "25400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Inaugural_Canberra_meetup\">Discussion article for the meetup : <a href=\"/meetups/wc\">Inaugural Canberra meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 February 2014 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">CSIT Building, Acton ACT 2601, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(I'm posting this on behalf of my lurker friend.)</p>\n\n<p>WHEN: 12 February 2014 07:30:00PM(+1100)\nWHERE: CSIT Building, Blg 108, North Road, ANU</p>\n\n<p>This will be the first meetup in Canberra! We will get to know each other and play board games, joining the weekly board games night held by the ANU Computer Science Students' Association. Vegan-friendly snacks and board games will be provided (although if anyone has board games, it would be great if you could bring your own). I will be waiting at the main door of the building from 7:20 to 7:40 to show people to the room where we will be meeting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Inaugural_Canberra_meetup1\">Discussion article for the meetup : <a href=\"/meetups/wc\">Inaugural Canberra meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Inaugural Canberra meetup", "anchor": "Discussion_article_for_the_meetup___Inaugural_Canberra_meetup", "level": 1}, {"title": "Discussion article for the meetup : Inaugural Canberra meetup", "anchor": "Discussion_article_for_the_meetup___Inaugural_Canberra_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-30T18:08:55.878Z", "modifiedAt": null, "url": null, "title": "Multiverse-Wide Preference Utilitarianism", "slug": "multiverse-wide-preference-utilitarianism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Brian_Tomasik", "createdAt": "2013-06-08T20:34:53.353Z", "isAdmin": false, "displayName": "Brian_Tomasik"}, "userId": "6XzBgrHtBL4M7CpKL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pfZcECTv3toFt4YQ4/multiverse-wide-preference-utilitarianism", "pageUrlRelative": "/posts/pfZcECTv3toFt4YQ4/multiverse-wide-preference-utilitarianism", "linkUrl": "https://www.lesswrong.com/posts/pfZcECTv3toFt4YQ4/multiverse-wide-preference-utilitarianism", "postedAtFormatted": "Thursday, January 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Multiverse-Wide%20Preference%20Utilitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMultiverse-Wide%20Preference%20Utilitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfZcECTv3toFt4YQ4%2Fmultiverse-wide-preference-utilitarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Multiverse-Wide%20Preference%20Utilitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfZcECTv3toFt4YQ4%2Fmultiverse-wide-preference-utilitarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpfZcECTv3toFt4YQ4%2Fmultiverse-wide-preference-utilitarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3498, "htmlBody": "<h2>Summary</h2>\n<p>Some preference utilitarians care about satisfaction of preferences even when the organism with the preference doesn't know that it has been satisfied. These preference utilitarians should care to some degree about the preferences that people in other branches of our multiverse have regarding our own world, as well as the preferences of aliens regarding our world. In general, this suggests that we should give relatively more weight to tastes and values that we expect to be more universal among civilizations across the multiverse. This consideration is strongest in the case of aesthetic preferences about inanimate objects and is weaker for preferences about organisms that themselves have experiences.</p>\n<hr />\n<h2>Introduction</h2>\n<p>Classical utilitarianism aims to maximize the balance of happiness over suffering for all organisms. <a href=\"http://www.utilitarian-essays.com/hedonistic-vs-preference.html\">Preference utilitarianism</a> focuses on fulfillment vs. frustration of preferences, rather than just at hedonic experiences. So, for example, if someone has a preference for his house to go to his granddaughter after his death, then it would frustrate his preference if it instead went to his grandson, even though he wouldn't be around to experience negative emotions due to his preference being thwarted.</p>\n<h2>Non-hedonic preferences</h2>\n<p>In practice, most of people's preferences concern their own hedonic wellbeing. Some also concern the wellbeing of their children and friends, although often these preferences are manifested through direct happiness or suffering in oneself (e.g., being on the edge of your seat with anxiety when your 14-year-old daughter hasn't come home by midnight).</p>\n<p>However, some preferences are beyond hedonic experience by oneself. This is true of preferences about how the world will be after one dies, or whether the money you donated to that charity <em>actually</em> gets used well even if you wouldn't find out either way. It's true of many moral convictions. For instance, I want to actually reduce expected suffering rather than hook up to a machine that makes me think I reduced expected suffering and then blisses me out for the rest of my life. It's also true of some aesthetic preferences, such as the view that it would be good for art, music, and knowledge to exist even if no one was around to experience them.</p>\n<p>Certainly these non-hedonic preferences have hedonic effects. If I learned that I was going to be hooked up to a machine that would erase my moral convictions and bliss me out for the rest of my life, I would feel upset in the short run. However, almost certainly this aversive feeling would be outweighed by my pleasure and lack of suffering in the long run. So my preference conflicts with egoistic hedonism in this case. (My preference not to be blissed out <em>is</em> consistent with hedonistic <em>utilitarianism</em>, rather than hedonistic <em>egoism</em>, but hedonistic utilitarianism is a kind of moral system that exists outside the realm of hedonic preferences of an individual organism.)</p>\n<p>Because preference utilitarians believe that preference violations can be harmful even if they aren't accompanied by negative hedonic experience, there are some cases in which doing something that other people disapprove of is bad even if they never find out. For example, Muslims strongly oppose defacing the Quran. This means that, barring countervailing factors, it would be prima facie bad to deface a Quran in the privacy of your own home even if no one else knew about it.</p>\n<h2>Tyranny of the majority?</h2>\n<p>People sometimes object to utilitarianism on the grounds that it might allow for tyranny of the majority. This seems especially possible for preference utilitarianism, when considering preferences regarding the external world that don't directly affect a person's hedonic experience. For example, one might fear that if large numbers of people have a preference against gay sex, then even if these people are not emotionally affected by what goes on in the privacy of others' bedrooms, their preference against those private acts might still matter appreciably.</p>\n<p>As a preliminary comment, I should point out that preference utilitarianism typically optimizes <em>idealized</em> preferences rather than <em>actual</em> preferences. What's important is not what you think you want but what you would actually want if you were better informed, had greater philosophical reflectiveness, etc. While there are strong ostensible preferences against gay sex in the world, it's less clear that there are strong idealized preferences against it. It's plausible that many gay opponents would come to see that (safe) gay sex is actually a positive expression of pleasure and love rather than something vile.</p>\n<p>But let's ignore this for the moment and suppose that most people really did have idealized preferences against gay sex. In fact, let's suppose the world consists of N+2 people, two of whom are gay and would prefer to have sex with each other, and the other N of whom have idealized preferences opposing gay sex. If N is very large, do we have tyranny of the majority, according to which it's bad for the two gays to have sex?</p>\n<p>This is a complicated question that involves more subtlety than it may seem. Even if the direct preference summation came out against gay sex, it might still be better to allow it for other reasons. For instance, maybe at a meta level, a more libertarian stance on social issues tends to produce better outcomes in the long run. Maybe allowing gay sex increases people's tolerance, leading to a more positive society in the future. And so on. But for now let's consider just the direct preference summation: Does the balance of opposition to gay sex exceed the welfare of the gay individuals themselves?</p>\n<p>This answer isn't clear, and it depends how you weigh the different preferences. Intuitively it seems obvious that for large enough N, N people opposed to gay sex can trump two people who prefer it. On the other hand, that's less clear if we look at the matter from the perspective of scaled utility functions.</p>\n<ul>\n<li>Suppose unrealistically that the only thing the N anti-gay people care about is preventing gay sex. In particular, they're expected-gay-sex minimizers, who consider each act of gay sex as bad as another and aim to minimize the total amount that happens. The best possible world (normalized utility = 1) is one where no gay sex happens. The worst possible world (normalized utility = 0) is one where all N+2 people have gay sex. The world where just the two gay people have gay sex is almost as good as the best possible world. In particular, its normalized utility is N/(N+2). Thus, if gay sex happens, each anti-gay person only loses 2/(N+2) utility. Aggregated over all N anti-gay people, this is a loss of 2N/(N+2).</li>\n<li>Also unrealistically, suppose that the only thing the two gay people care about is having gay sex. Their normalized utility for having sex is 1 and for not having it is 0. Aggregated over the two of them, the total gain from having sex is 2.</li>\n<li>Because 2 &gt; 2N/(N+2), it's overall better in direct preference summation for the gay sex to happen as long as we weight each person's normalized utility equally. This is true regardless of N.</li>\n</ul>\n<p>That said, if the anti-gay people had diminishing marginal disutility for additional acts of gay sex, this conclusion would probably flip around.</p>\n<p>It feels intuitively suspicious to just sum normalized utility. As an example, consider a Beethoven utility monster -- a person whose only goal in life is to hear Beethoven's Ninth Symphony. This person has no other desires, and if he doesn't hear Beethoven's Ninth, it's as good as being dead. Meanwhile, other people also want to hear Beethoven's Ninth, but their desire for it is just a tiny fraction of what they care about. In particular, they value not dying and being able to live the rest of their lives 99,999 times as much as hearing Beethoven's Ninth.</p>\n<ul>\n<li>Each normal person's normalized utility without hearing the symphony is 0.99999. Hearing the symphony would make it 1.00000.</li>\n<li>The Beethoven utility monster would be at 0 without hearing the symphony and 1 hearing it.</li>\n<li>Thus, if we directly sum normalized utilities, it's better for the Beethoven utility monster to hear the symphony than for 99,999 regular people to do the same.</li>\n</ul>\n<p>This seems suspicious. Maybe it's because our intuitions are not well adapted to thinking about organisms with really different utility functions from ours, and if we interacted with them more -- seeing them struggle endlessly, risking life and limb for the symphony they so desire -- we would begin to feel differently. Another problem is that an organism's utility counts for less as soon as the range of its experience increases. If the Beethoven monster were transformed to want to hear Beethoven's Ninth <em>and</em> Eighth symphonies each with equal strength, suddenly the value of its hearing the Ninth alone is cut in half. Again, maybe this is plausible, but it's not clear. I think some people have the intuition that an organism with a broader range of possible joys counts more than one with fewer, though I'm not sure I agree with this.</p>\n<p>So the question of tyranny remains indeterminate. It depends on how you weigh different preferences. However, it remains the case that it may be instrumentally valuable to preserve norms of individual autonomy in order to produce better societies in the long run.</p>\n<h2>Preferences across worlds: A story of art maximizers</h2>\n<p>Consider the following (highly unrealistic) story. It's the year 2100. A group of three artist couples is traveling on the first manned voyage to Mars. These couples value art for art's sake, and in fact, their moral views consider art to be worthwhile even if no one experiences it. Their utility functions are linear in the amount of art that exists, and so they wish to maximize the expected amount of art in the galaxy -- converting planets and asteroids into van Gogh, Shakespeare, and Chopin.</p>\n<p>However, they don't quite agree on which art is best. One couple wants to maximize paintings, feeling that a galaxy filled with paintings would be worth +3. A galaxy filled with sculptures would be +2. And a galaxy filled with poetry or music would be worthless: 0. The second couple values poetry at +3, sculptures at +2, and the other art at 0. The third values music at +3, sculptures at +2, and everything else at 0. Despite their divergent views, they manage to get along in the joint Martian voyage.</p>\n<p>However, a few weeks into the trip, a terrestrial accident vaporizes Earth, leaving no one behind. The only humans are now the artists heading for Mars, where they land several months later.</p>\n<p>The original plan had been for Earth to send more supplies following this crew, but now that Earth is gone, the colonists have only the minimal resources that the Martian base currently has in stock. They plan to grow more food <a href=\"http://science1.nasa.gov/science-news/science-at-nasa/2004/25feb_greenhouses/\">in their greenhouse</a>, but this will take many months, and the artists will all starve in the meanwhile if they each stick around. They realize that it would be best if two of the couples sacrificed themselves so that the third would have enough supplies to continue to grow crops and eventually repopulate the planet.</p>\n<p>Rather than fighting for control of the Martian base, which could be costly and kill everyone, the three couples realize that everyone would be better off in expectation if they selected a winner by lottery. In particular, they use a quantum random number generator to apportion 1/3 probabilities for each couple to survive. The lottery takes place, and the winner is the first couple, which values paintings most highly. The other two couples wish the winning couple the best of luck and then head to the euthanasia pods.</p>\n<p>The pro-paintings couple makes it through the period of low food and manages to establish a successful farming operation. They then begin having children to populate the planet. After many generations, Mars is home to a thriving miniature city. All the inhabitants value paintings at +3, sculptures at +2, and everything else at 0, due to the influence of the civilization's founders.</p>\n<p>By the year 2700, the city's technology is sufficient to deploy von Neumann probes throughout the galaxy, converting planets into works of art. The city council convenes a meeting to decide exactly what kind of art should be deployed. Because everyone in the city prefers paintings, the council assumes the case will be open and shut. But as a formality, they invite their local philosopher, Dr. Muchos Mundos, to testify.</p>\n<blockquote>\n<p>Council president: Dr. Mundos, the council has proposed to deploy von Neumann probes that will fill the galaxy with paintings. Do you agree with this decision?</p>\n<p>Dr. Mundos: As I understand it, the council wishes to act in the optimal preference-utilitarian fashion on this question, right?</p>\n<p>Council president: Yes, of course. The greatest good for the greatest number. Given that everyone who has any preferences about art most prefers a galaxy of paintings, we feel it's clear that paintings are what we should deploy. It's true that when this colony was founded, there were two other couples who would have wanted poetry and music, but their former preferences are far outweighed by our vast population that now wants paintings.</p>\n<p>Dr. Mundos: I see. Are you familiar with the many-worlds interpretation (MWI) of quantum mechanics?</p>\n<p>Council president: I'm a politician and not a physicist, but maybe you can give me the run-down?</p>\n<p>Dr. Mundos: According to MWI, when quantum randomness occurs, it's not the case that just a single outcome is selected. Rather, all outcomes happen, and our experiences of the world split into different branches.</p>\n<p>Council president: Okay. What's the relevance to art policy?</p>\n<p>Dr. Mundos: Well, a quantum lottery was used to decide which colonizing couple would populate Mars. The painting lovers won in this branch of the multiverse, but the poetry lovers won in another branch with equal measure, and the music lovers won in a third branch, also with equal measure. Presumably the couples in those branches also populated Mars with a city about as populous as our own. And if they care about art for art's sake, regardless of whether they know about it or where it exists, then the populations of those cities in other Everett branches also care about what art <em>we</em> deploy.</p>\n<p>Council president: Oh dear, you're right. Our city contains M people, and suppose their cities have about the same populations. If we deploy paintings, our M citizens each get +3 of utility, and those in the other worlds get nothing. The aggregate is 3M. But if we deploy sculptures, which everyone values at +2, the total utility is 3 * 2M = 6M. This is much better than 3M for paintings.</p>\n<p>Dr. Mundos: Yes, exactly. Of course, we might have some uncertainty over whether the populations in the other branches survived. But even if the probability they survived was only, say, 1/3, then the expected utility of sculptures would still be 2M for us plus (1/3)(2M + 2M) = 4M/3 for them. The sum is more than 3M, so it would still be better to do sculptures.</p>\n</blockquote>\n<p>After further deliberation, the council agreed with this argument and deployed sculptures. The preference satisfaction of the poetry-loving and music-loving cities was improved.</p>\n<h2>Multiversal distribution of preferences</h2>\n<p>According to Max Tegmark's \"<a href=\"http://space.mit.edu/home/tegmark/PDF/multiverse_sciam.pdf\">Parallel Universes</a>,\" there's probably an exact copy of you reading this article within 10<sup>10<sup>28</sup></sup> meters away and in practice, probably much closer. As Tegmark explains, this claim assumes only basic physics that most cosmologists take for granted. Even nearer than this distance are many people very similar to you but with minor variations -- e.g., with brown eyes instead of blue, or who prefer virtue ethics over deontology.</p>\n<p>In fact, all possible people exist somewhere in the multiverse, if only due to random fluctuations of the type that produce <a href=\"https://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brains</a>. Nick Bostrom calls these \"<a href=\"http://www.anthropic-principle.com/?q=book/chapter_3#3c\">freak observers</a>.\" Just as there are art maximizers, there are also art minimizers who find art disgusting and want to eliminate as much of it as possible. For them, the thought of art triggers their brains' disgust centers instead of beauty centers.</p>\n<p>However, the <em>distribution</em> of organisms across the multiverse is not uniform. For instance, we should expect suffering reducers to be <a href=\"http://rationalaltruist.com/2013/06/13/against-moral-advocacy/comment-page-1/#comment-481\">much more common</a> than suffering increasers because organisms evolve to dislike suffering by themselves, their kin, and their reciprocal trading partners. Societies -- whether human <a href=\"http://www.scifiideas.com/related/writing-advice/aliens-and-morality/\">or alien</a> -- should often develop norms against cruelty for collective benefit.</p>\n<p>Human values give us some hints about what values across the multiverse look like, because human values are a kind of <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood\">maximum likelihood estimator</a> for the mode of the multiversal distribution. Of course, we should expect some variation about the mode. Even among humans, some cultural norms are distinct and others <a href=\"https://en.wikipedia.org/wiki/Human_universal\">are universal</a>. Probably values like not murdering, not causing unnecessary suffering, not stealing, etc. are more common among aliens than, say, the value of music or dance, which might be human-specific spandrels. Still, aliens may have their own spandrels that they call \"art,\" and they might value those things.</p>\n<p>Like human values, alien values might be mostly self-directed toward their own wellbeing, especially in their earlier Darwinian phases. Unless we meet the aliens face-to-face, we can't improve their welfare directly. However, the aliens may also have some outward-directed aesthetic and moral values that apply across space and time, like the value of art as seen by the art-maximizing cities on Mars in the previous section. If so, we can affect the satisfaction of these preferences by our actions, and presumably they should be included in preference-utilitarian calculations.</p>\n<p>As an example, suppose there were 10 civilizations. All 10 valued reducing suffering and social equality. 5 of the 10 also valued generating knowledge. Only 1 of the 10 valued creating paintings and poetry. Suppose our civilization values all of those things. Perhaps previously we were going to spend money on creating more poetry, because our citizens value that highly. However, upon considering that poetry would not satisfy the preferences of the other civilizations, we might switch more toward knowledge and especially toward suffering reduction and equality promotion.</p>\n<p>In general, considering the distribution of outward-directed preferences across the multiverse should lead us to favor more those preferences of ours that are more evolutionarily robust, i.e., that we predict more civilizations to have settled upon. One corollary is that we should care less about values that we have due to particular, idiosyncratic historical contingencies, such as who happened to win some very closely contested war, or what species were killed by a random asteroid strike. Values based on more inevitable historical trends should matter relatively more strongly.</p>\n<!-- This doesn't make sense because pref utils already favored compromise even in a single world: <a name=\"argument-for-democracy\"></a>\n<h3>A weak argument for democracy</h3>\n<p>There's a potential argument why multiversal preference utilitarians may prefer democratic compromise over authoritarian rule. As an example, suppose Earth had three factions all competing for dictatorial control of the planet and, after developing AI, the future of the galaxy. In a winner-takes-all contest for despotic rule, only one of those values wins, but it may be roughly random which that is; assume it's uniformly 1/3 for each side across the multiverse. Unless the winning faction cares about multiversal preference utilitarianism, its value system will favor what it cares about at the expense of everyone else.</p>\n<p>However, if instead, we had a democracy in which all three value systems were able to negotiate and reach a <a href=\"http://utilitarian-essays.com/compromise.html\" mce_href=\"http://utilitarian-essays.com/compromise.html\">compromise</a>, the result would be more than 1/3 as good as total control for each value system. Summed over all versions of this world in the multiverse, the total utility would be higher than if any individual faction won. Of course, this is only true if the winning probabilities are actually 1/3 for each group. If one side does systematically win a lot more, it will be more prevalent in the multiverse, and so it would be better from a multiversal perspective to favor what it wants.</p>\n-->\n<h2>Tyranny of the aliens?</h2>\n<p>Suppose, conservatively, that for every one human civilization, there are 1000 alien civilizations that have some outward-directed preferences (e.g., for more suffering reduction, justice, knowledge, etc.). Even if each alien civilization cares only a little bit about what we do, collectively do their preferences outweigh our preferences about our own destiny? Would we find ourselves beholden to the tyranny of the alien majority about our behavior?</p>\n<p>This question runs exactly parallel to the standard concern about tyranny of the majority for individuals within a society, so the same sorts of arguments will apply on each side. Just as in that case, it's possible aliens would place value on the ability of individual civilizations to make their own choices about how they're constituted without too much outside interference. Of course, this is just speculation.</p>\n<p>Even if tyranny of the alien majority was the result, we might choose to accept that conclusion. After all, it seems to yield more total preference satisfaction, which is what the preference utilitarians were aiming for.</p>\n<h2>Direct welfare may often dominate</h2>\n<p>In the preceding examples, I often focused on aesthetic values like art and knowledge for a specific reason: These are cases of preferences for something to exist or not where that thing does not itself have preferences. Art does not prefer for itself to keep existing or stop existing.</p>\n<p>However, many human preferences have implications for the preferences of others. For instance, a preference by humans for more wilderness may mean vast numbers of additional wild animals, many of whom strongly (implicitly) prefer not to have endured the <a href=\"http://www.utilitarian-essays.com/suffering-nature.html\">short lives and painful deaths</a> inherent to the bodies in which they found themselves born. A relatively weak aesthetic preference for nature by a relatively small number of people is compared against strong hedonic preferences by large numbers of animals not to have existed. In this case, the preferences of the animals clearly dominate. The same is true for preferences about creating space colonies and the like: The preferences of the people, animals, and other agents in those colonies will tend to far outweigh the preferences of their creators.</p>\n<p>Considering multiverse-wide aesthetic and moral preferences is thus cleanest in the case of preferences about inanimate things. Aliens' preferences about actions that affect the welfare of organisms in our civilization still matter, but relatively less than the contribution of their preferences about inanimate things.</p>\n<h2>Acknowledgments</h2>\n<p>This piece was inspired by Carl Shulman's \"<a href=\"http://reflectivedisequilibrium.blogspot.com/2012/07/rawls-original-position-potential.html\">Rawls' original position, potential people, and Pascal's Mugging</a>,\" as well as a conversation with Paul Christiano.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pfZcECTv3toFt4YQ4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 25, "extendedScore": null, "score": 1.5391714375944357e-06, "legacy": true, "legacyId": "25401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Summary\">Summary</h2>\n<p>Some preference utilitarians care about satisfaction of preferences even when the organism with the preference doesn't know that it has been satisfied. These preference utilitarians should care to some degree about the preferences that people in other branches of our multiverse have regarding our own world, as well as the preferences of aliens regarding our world. In general, this suggests that we should give relatively more weight to tastes and values that we expect to be more universal among civilizations across the multiverse. This consideration is strongest in the case of aesthetic preferences about inanimate objects and is weaker for preferences about organisms that themselves have experiences.</p>\n<hr>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>Classical utilitarianism aims to maximize the balance of happiness over suffering for all organisms. <a href=\"http://www.utilitarian-essays.com/hedonistic-vs-preference.html\">Preference utilitarianism</a> focuses on fulfillment vs. frustration of preferences, rather than just at hedonic experiences. So, for example, if someone has a preference for his house to go to his granddaughter after his death, then it would frustrate his preference if it instead went to his grandson, even though he wouldn't be around to experience negative emotions due to his preference being thwarted.</p>\n<h2 id=\"Non_hedonic_preferences\">Non-hedonic preferences</h2>\n<p>In practice, most of people's preferences concern their own hedonic wellbeing. Some also concern the wellbeing of their children and friends, although often these preferences are manifested through direct happiness or suffering in oneself (e.g., being on the edge of your seat with anxiety when your 14-year-old daughter hasn't come home by midnight).</p>\n<p>However, some preferences are beyond hedonic experience by oneself. This is true of preferences about how the world will be after one dies, or whether the money you donated to that charity <em>actually</em> gets used well even if you wouldn't find out either way. It's true of many moral convictions. For instance, I want to actually reduce expected suffering rather than hook up to a machine that makes me think I reduced expected suffering and then blisses me out for the rest of my life. It's also true of some aesthetic preferences, such as the view that it would be good for art, music, and knowledge to exist even if no one was around to experience them.</p>\n<p>Certainly these non-hedonic preferences have hedonic effects. If I learned that I was going to be hooked up to a machine that would erase my moral convictions and bliss me out for the rest of my life, I would feel upset in the short run. However, almost certainly this aversive feeling would be outweighed by my pleasure and lack of suffering in the long run. So my preference conflicts with egoistic hedonism in this case. (My preference not to be blissed out <em>is</em> consistent with hedonistic <em>utilitarianism</em>, rather than hedonistic <em>egoism</em>, but hedonistic utilitarianism is a kind of moral system that exists outside the realm of hedonic preferences of an individual organism.)</p>\n<p>Because preference utilitarians believe that preference violations can be harmful even if they aren't accompanied by negative hedonic experience, there are some cases in which doing something that other people disapprove of is bad even if they never find out. For example, Muslims strongly oppose defacing the Quran. This means that, barring countervailing factors, it would be prima facie bad to deface a Quran in the privacy of your own home even if no one else knew about it.</p>\n<h2 id=\"Tyranny_of_the_majority_\">Tyranny of the majority?</h2>\n<p>People sometimes object to utilitarianism on the grounds that it might allow for tyranny of the majority. This seems especially possible for preference utilitarianism, when considering preferences regarding the external world that don't directly affect a person's hedonic experience. For example, one might fear that if large numbers of people have a preference against gay sex, then even if these people are not emotionally affected by what goes on in the privacy of others' bedrooms, their preference against those private acts might still matter appreciably.</p>\n<p>As a preliminary comment, I should point out that preference utilitarianism typically optimizes <em>idealized</em> preferences rather than <em>actual</em> preferences. What's important is not what you think you want but what you would actually want if you were better informed, had greater philosophical reflectiveness, etc. While there are strong ostensible preferences against gay sex in the world, it's less clear that there are strong idealized preferences against it. It's plausible that many gay opponents would come to see that (safe) gay sex is actually a positive expression of pleasure and love rather than something vile.</p>\n<p>But let's ignore this for the moment and suppose that most people really did have idealized preferences against gay sex. In fact, let's suppose the world consists of N+2 people, two of whom are gay and would prefer to have sex with each other, and the other N of whom have idealized preferences opposing gay sex. If N is very large, do we have tyranny of the majority, according to which it's bad for the two gays to have sex?</p>\n<p>This is a complicated question that involves more subtlety than it may seem. Even if the direct preference summation came out against gay sex, it might still be better to allow it for other reasons. For instance, maybe at a meta level, a more libertarian stance on social issues tends to produce better outcomes in the long run. Maybe allowing gay sex increases people's tolerance, leading to a more positive society in the future. And so on. But for now let's consider just the direct preference summation: Does the balance of opposition to gay sex exceed the welfare of the gay individuals themselves?</p>\n<p>This answer isn't clear, and it depends how you weigh the different preferences. Intuitively it seems obvious that for large enough N, N people opposed to gay sex can trump two people who prefer it. On the other hand, that's less clear if we look at the matter from the perspective of scaled utility functions.</p>\n<ul>\n<li>Suppose unrealistically that the only thing the N anti-gay people care about is preventing gay sex. In particular, they're expected-gay-sex minimizers, who consider each act of gay sex as bad as another and aim to minimize the total amount that happens. The best possible world (normalized utility = 1) is one where no gay sex happens. The worst possible world (normalized utility = 0) is one where all N+2 people have gay sex. The world where just the two gay people have gay sex is almost as good as the best possible world. In particular, its normalized utility is N/(N+2). Thus, if gay sex happens, each anti-gay person only loses 2/(N+2) utility. Aggregated over all N anti-gay people, this is a loss of 2N/(N+2).</li>\n<li>Also unrealistically, suppose that the only thing the two gay people care about is having gay sex. Their normalized utility for having sex is 1 and for not having it is 0. Aggregated over the two of them, the total gain from having sex is 2.</li>\n<li>Because 2 &gt; 2N/(N+2), it's overall better in direct preference summation for the gay sex to happen as long as we weight each person's normalized utility equally. This is true regardless of N.</li>\n</ul>\n<p>That said, if the anti-gay people had diminishing marginal disutility for additional acts of gay sex, this conclusion would probably flip around.</p>\n<p>It feels intuitively suspicious to just sum normalized utility. As an example, consider a Beethoven utility monster -- a person whose only goal in life is to hear Beethoven's Ninth Symphony. This person has no other desires, and if he doesn't hear Beethoven's Ninth, it's as good as being dead. Meanwhile, other people also want to hear Beethoven's Ninth, but their desire for it is just a tiny fraction of what they care about. In particular, they value not dying and being able to live the rest of their lives 99,999 times as much as hearing Beethoven's Ninth.</p>\n<ul>\n<li>Each normal person's normalized utility without hearing the symphony is 0.99999. Hearing the symphony would make it 1.00000.</li>\n<li>The Beethoven utility monster would be at 0 without hearing the symphony and 1 hearing it.</li>\n<li>Thus, if we directly sum normalized utilities, it's better for the Beethoven utility monster to hear the symphony than for 99,999 regular people to do the same.</li>\n</ul>\n<p>This seems suspicious. Maybe it's because our intuitions are not well adapted to thinking about organisms with really different utility functions from ours, and if we interacted with them more -- seeing them struggle endlessly, risking life and limb for the symphony they so desire -- we would begin to feel differently. Another problem is that an organism's utility counts for less as soon as the range of its experience increases. If the Beethoven monster were transformed to want to hear Beethoven's Ninth <em>and</em> Eighth symphonies each with equal strength, suddenly the value of its hearing the Ninth alone is cut in half. Again, maybe this is plausible, but it's not clear. I think some people have the intuition that an organism with a broader range of possible joys counts more than one with fewer, though I'm not sure I agree with this.</p>\n<p>So the question of tyranny remains indeterminate. It depends on how you weigh different preferences. However, it remains the case that it may be instrumentally valuable to preserve norms of individual autonomy in order to produce better societies in the long run.</p>\n<h2 id=\"Preferences_across_worlds__A_story_of_art_maximizers\">Preferences across worlds: A story of art maximizers</h2>\n<p>Consider the following (highly unrealistic) story. It's the year 2100. A group of three artist couples is traveling on the first manned voyage to Mars. These couples value art for art's sake, and in fact, their moral views consider art to be worthwhile even if no one experiences it. Their utility functions are linear in the amount of art that exists, and so they wish to maximize the expected amount of art in the galaxy -- converting planets and asteroids into van Gogh, Shakespeare, and Chopin.</p>\n<p>However, they don't quite agree on which art is best. One couple wants to maximize paintings, feeling that a galaxy filled with paintings would be worth +3. A galaxy filled with sculptures would be +2. And a galaxy filled with poetry or music would be worthless: 0. The second couple values poetry at +3, sculptures at +2, and the other art at 0. The third values music at +3, sculptures at +2, and everything else at 0. Despite their divergent views, they manage to get along in the joint Martian voyage.</p>\n<p>However, a few weeks into the trip, a terrestrial accident vaporizes Earth, leaving no one behind. The only humans are now the artists heading for Mars, where they land several months later.</p>\n<p>The original plan had been for Earth to send more supplies following this crew, but now that Earth is gone, the colonists have only the minimal resources that the Martian base currently has in stock. They plan to grow more food <a href=\"http://science1.nasa.gov/science-news/science-at-nasa/2004/25feb_greenhouses/\">in their greenhouse</a>, but this will take many months, and the artists will all starve in the meanwhile if they each stick around. They realize that it would be best if two of the couples sacrificed themselves so that the third would have enough supplies to continue to grow crops and eventually repopulate the planet.</p>\n<p>Rather than fighting for control of the Martian base, which could be costly and kill everyone, the three couples realize that everyone would be better off in expectation if they selected a winner by lottery. In particular, they use a quantum random number generator to apportion 1/3 probabilities for each couple to survive. The lottery takes place, and the winner is the first couple, which values paintings most highly. The other two couples wish the winning couple the best of luck and then head to the euthanasia pods.</p>\n<p>The pro-paintings couple makes it through the period of low food and manages to establish a successful farming operation. They then begin having children to populate the planet. After many generations, Mars is home to a thriving miniature city. All the inhabitants value paintings at +3, sculptures at +2, and everything else at 0, due to the influence of the civilization's founders.</p>\n<p>By the year 2700, the city's technology is sufficient to deploy von Neumann probes throughout the galaxy, converting planets into works of art. The city council convenes a meeting to decide exactly what kind of art should be deployed. Because everyone in the city prefers paintings, the council assumes the case will be open and shut. But as a formality, they invite their local philosopher, Dr. Muchos Mundos, to testify.</p>\n<blockquote>\n<p>Council president: Dr. Mundos, the council has proposed to deploy von Neumann probes that will fill the galaxy with paintings. Do you agree with this decision?</p>\n<p>Dr. Mundos: As I understand it, the council wishes to act in the optimal preference-utilitarian fashion on this question, right?</p>\n<p>Council president: Yes, of course. The greatest good for the greatest number. Given that everyone who has any preferences about art most prefers a galaxy of paintings, we feel it's clear that paintings are what we should deploy. It's true that when this colony was founded, there were two other couples who would have wanted poetry and music, but their former preferences are far outweighed by our vast population that now wants paintings.</p>\n<p>Dr. Mundos: I see. Are you familiar with the many-worlds interpretation (MWI) of quantum mechanics?</p>\n<p>Council president: I'm a politician and not a physicist, but maybe you can give me the run-down?</p>\n<p>Dr. Mundos: According to MWI, when quantum randomness occurs, it's not the case that just a single outcome is selected. Rather, all outcomes happen, and our experiences of the world split into different branches.</p>\n<p>Council president: Okay. What's the relevance to art policy?</p>\n<p>Dr. Mundos: Well, a quantum lottery was used to decide which colonizing couple would populate Mars. The painting lovers won in this branch of the multiverse, but the poetry lovers won in another branch with equal measure, and the music lovers won in a third branch, also with equal measure. Presumably the couples in those branches also populated Mars with a city about as populous as our own. And if they care about art for art's sake, regardless of whether they know about it or where it exists, then the populations of those cities in other Everett branches also care about what art <em>we</em> deploy.</p>\n<p>Council president: Oh dear, you're right. Our city contains M people, and suppose their cities have about the same populations. If we deploy paintings, our M citizens each get +3 of utility, and those in the other worlds get nothing. The aggregate is 3M. But if we deploy sculptures, which everyone values at +2, the total utility is 3 * 2M = 6M. This is much better than 3M for paintings.</p>\n<p>Dr. Mundos: Yes, exactly. Of course, we might have some uncertainty over whether the populations in the other branches survived. But even if the probability they survived was only, say, 1/3, then the expected utility of sculptures would still be 2M for us plus (1/3)(2M + 2M) = 4M/3 for them. The sum is more than 3M, so it would still be better to do sculptures.</p>\n</blockquote>\n<p>After further deliberation, the council agreed with this argument and deployed sculptures. The preference satisfaction of the poetry-loving and music-loving cities was improved.</p>\n<h2 id=\"Multiversal_distribution_of_preferences\">Multiversal distribution of preferences</h2>\n<p>According to Max Tegmark's \"<a href=\"http://space.mit.edu/home/tegmark/PDF/multiverse_sciam.pdf\">Parallel Universes</a>,\" there's probably an exact copy of you reading this article within 10<sup>10<sup>28</sup></sup> meters away and in practice, probably much closer. As Tegmark explains, this claim assumes only basic physics that most cosmologists take for granted. Even nearer than this distance are many people very similar to you but with minor variations -- e.g., with brown eyes instead of blue, or who prefer virtue ethics over deontology.</p>\n<p>In fact, all possible people exist somewhere in the multiverse, if only due to random fluctuations of the type that produce <a href=\"https://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brains</a>. Nick Bostrom calls these \"<a href=\"http://www.anthropic-principle.com/?q=book/chapter_3#3c\">freak observers</a>.\" Just as there are art maximizers, there are also art minimizers who find art disgusting and want to eliminate as much of it as possible. For them, the thought of art triggers their brains' disgust centers instead of beauty centers.</p>\n<p>However, the <em>distribution</em> of organisms across the multiverse is not uniform. For instance, we should expect suffering reducers to be <a href=\"http://rationalaltruist.com/2013/06/13/against-moral-advocacy/comment-page-1/#comment-481\">much more common</a> than suffering increasers because organisms evolve to dislike suffering by themselves, their kin, and their reciprocal trading partners. Societies -- whether human <a href=\"http://www.scifiideas.com/related/writing-advice/aliens-and-morality/\">or alien</a> -- should often develop norms against cruelty for collective benefit.</p>\n<p>Human values give us some hints about what values across the multiverse look like, because human values are a kind of <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood\">maximum likelihood estimator</a> for the mode of the multiversal distribution. Of course, we should expect some variation about the mode. Even among humans, some cultural norms are distinct and others <a href=\"https://en.wikipedia.org/wiki/Human_universal\">are universal</a>. Probably values like not murdering, not causing unnecessary suffering, not stealing, etc. are more common among aliens than, say, the value of music or dance, which might be human-specific spandrels. Still, aliens may have their own spandrels that they call \"art,\" and they might value those things.</p>\n<p>Like human values, alien values might be mostly self-directed toward their own wellbeing, especially in their earlier Darwinian phases. Unless we meet the aliens face-to-face, we can't improve their welfare directly. However, the aliens may also have some outward-directed aesthetic and moral values that apply across space and time, like the value of art as seen by the art-maximizing cities on Mars in the previous section. If so, we can affect the satisfaction of these preferences by our actions, and presumably they should be included in preference-utilitarian calculations.</p>\n<p>As an example, suppose there were 10 civilizations. All 10 valued reducing suffering and social equality. 5 of the 10 also valued generating knowledge. Only 1 of the 10 valued creating paintings and poetry. Suppose our civilization values all of those things. Perhaps previously we were going to spend money on creating more poetry, because our citizens value that highly. However, upon considering that poetry would not satisfy the preferences of the other civilizations, we might switch more toward knowledge and especially toward suffering reduction and equality promotion.</p>\n<p>In general, considering the distribution of outward-directed preferences across the multiverse should lead us to favor more those preferences of ours that are more evolutionarily robust, i.e., that we predict more civilizations to have settled upon. One corollary is that we should care less about values that we have due to particular, idiosyncratic historical contingencies, such as who happened to win some very closely contested war, or what species were killed by a random asteroid strike. Values based on more inevitable historical trends should matter relatively more strongly.</p>\n<!-- This doesn't make sense because pref utils already favored compromise even in a single world: <a name=\"argument-for-democracy\"></a>\n<h3>A weak argument for democracy</h3>\n<p>There's a potential argument why multiversal preference utilitarians may prefer democratic compromise over authoritarian rule. As an example, suppose Earth had three factions all competing for dictatorial control of the planet and, after developing AI, the future of the galaxy. In a winner-takes-all contest for despotic rule, only one of those values wins, but it may be roughly random which that is; assume it's uniformly 1/3 for each side across the multiverse. Unless the winning faction cares about multiversal preference utilitarianism, its value system will favor what it cares about at the expense of everyone else.</p>\n<p>However, if instead, we had a democracy in which all three value systems were able to negotiate and reach a <a href=\"http://utilitarian-essays.com/compromise.html\" mce_href=\"http://utilitarian-essays.com/compromise.html\">compromise</a>, the result would be more than 1/3 as good as total control for each value system. Summed over all versions of this world in the multiverse, the total utility would be higher than if any individual faction won. Of course, this is only true if the winning probabilities are actually 1/3 for each group. If one side does systematically win a lot more, it will be more prevalent in the multiverse, and so it would be better from a multiversal perspective to favor what it wants.</p>\n-->\n<h2 id=\"Tyranny_of_the_aliens_\">Tyranny of the aliens?</h2>\n<p>Suppose, conservatively, that for every one human civilization, there are 1000 alien civilizations that have some outward-directed preferences (e.g., for more suffering reduction, justice, knowledge, etc.). Even if each alien civilization cares only a little bit about what we do, collectively do their preferences outweigh our preferences about our own destiny? Would we find ourselves beholden to the tyranny of the alien majority about our behavior?</p>\n<p>This question runs exactly parallel to the standard concern about tyranny of the majority for individuals within a society, so the same sorts of arguments will apply on each side. Just as in that case, it's possible aliens would place value on the ability of individual civilizations to make their own choices about how they're constituted without too much outside interference. Of course, this is just speculation.</p>\n<p>Even if tyranny of the alien majority was the result, we might choose to accept that conclusion. After all, it seems to yield more total preference satisfaction, which is what the preference utilitarians were aiming for.</p>\n<h2 id=\"Direct_welfare_may_often_dominate\">Direct welfare may often dominate</h2>\n<p>In the preceding examples, I often focused on aesthetic values like art and knowledge for a specific reason: These are cases of preferences for something to exist or not where that thing does not itself have preferences. Art does not prefer for itself to keep existing or stop existing.</p>\n<p>However, many human preferences have implications for the preferences of others. For instance, a preference by humans for more wilderness may mean vast numbers of additional wild animals, many of whom strongly (implicitly) prefer not to have endured the <a href=\"http://www.utilitarian-essays.com/suffering-nature.html\">short lives and painful deaths</a> inherent to the bodies in which they found themselves born. A relatively weak aesthetic preference for nature by a relatively small number of people is compared against strong hedonic preferences by large numbers of animals not to have existed. In this case, the preferences of the animals clearly dominate. The same is true for preferences about creating space colonies and the like: The preferences of the people, animals, and other agents in those colonies will tend to far outweigh the preferences of their creators.</p>\n<p>Considering multiverse-wide aesthetic and moral preferences is thus cleanest in the case of preferences about inanimate things. Aliens' preferences about actions that affect the welfare of organisms in our civilization still matter, but relatively less than the contribution of their preferences about inanimate things.</p>\n<h2 id=\"Acknowledgments\">Acknowledgments</h2>\n<p>This piece was inspired by Carl Shulman's \"<a href=\"http://reflectivedisequilibrium.blogspot.com/2012/07/rawls-original-position-potential.html\">Rawls' original position, potential people, and Pascal's Mugging</a>,\" as well as a conversation with Paul Christiano.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Non-hedonic preferences", "anchor": "Non_hedonic_preferences", "level": 1}, {"title": "Tyranny of the majority?", "anchor": "Tyranny_of_the_majority_", "level": 1}, {"title": "Preferences across worlds: A story of art maximizers", "anchor": "Preferences_across_worlds__A_story_of_art_maximizers", "level": 1}, {"title": "Multiversal distribution of preferences", "anchor": "Multiversal_distribution_of_preferences", "level": 1}, {"title": "Tyranny of the aliens?", "anchor": "Tyranny_of_the_aliens_", "level": 1}, {"title": "Direct welfare may often dominate", "anchor": "Direct_welfare_may_often_dominate", "level": 1}, {"title": "Acknowledgments", "anchor": "Acknowledgments", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-30T20:00:05.353Z", "modifiedAt": null, "url": null, "title": "On saving the world", "slug": "on-saving-the-world", "viewCount": null, "lastCommentedAt": "2022-03-29T05:28:09.531Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F2DZXsMdhGyX4FPAd/on-saving-the-world", "pageUrlRelative": "/posts/F2DZXsMdhGyX4FPAd/on-saving-the-world", "linkUrl": "https://www.lesswrong.com/posts/F2DZXsMdhGyX4FPAd/on-saving-the-world", "postedAtFormatted": "Thursday, January 30th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20saving%20the%20world&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20saving%20the%20world%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2DZXsMdhGyX4FPAd%2Fon-saving-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20saving%20the%20world%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2DZXsMdhGyX4FPAd%2Fon-saving-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2DZXsMdhGyX4FPAd%2Fon-saving-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4857, "htmlBody": "<p><em>This is the final post in my productivity sequence.</em></p>\n<p><em>The first post described&nbsp;<a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">what I achieved</a>. The <a href=\"/lw/jgh/habitual_productivity/\">next</a>&nbsp;<a href=\"/lw/jh0/deregulating_distraction_moving_towards_the_goal/\">three</a>&nbsp;<a href=\"/lw/jhs/dark_arts_of_rationality/\">posts</a>&nbsp;describe how. This post describes why, explaining the sources of my passion and the circumstances that convinced a young Nate to try and save the world.&nbsp;</em><em>Within, you will find no suggestions, no techniques to emulate, no new ideas to ponder. This is a rationalist coming-of-age story. With luck, you may find it inspiring. Regardless, I hope you can learn from my mistakes.</em></p>\n<p><em>Never fear, I'll be back to business soon &mdash; there's lots of studying to do. But before then, there's a story to tell, a memorial to what I left behind.</em></p>\n<hr />\n<p>I was raised Catholic. On my eighth birthday, having received my first communion about a year prior, I casually asked my priest how to reaffirm my faith and do something for the Lord. The memory is fuzzy, but I think I donated a chunk of allowance money and made a public confession at the following mass.</p>\n<p>A bunch of the grownups made a big deal out of it, as grownups are like to do. \"Faith of a child\", and all that. This confused me, especially when I realized that what I had done was rare. I wasn't trying to get pats on the head, I was appealing to the <em>Lord of the Heavens and the Earth</em>. Were we all on the same page, here? This was the <em>creator.</em>&nbsp;He was infinitely virtuous, and he had told us what to do.</p>\n<p>And yet, everyone was content to recite hymns once a week and donate for the reconstruction of the church. What about the rest of the world, the sick, the dying? Where were the proselytizers, the missionary opportunities? Why was everyone just sitting around?&nbsp;</p>\n<p>On that day, I became acquainted with civilizational inadequacy. I realized you could hand a room full of people the<em>&nbsp;literal word of God</em>, and they'd still struggle to pay attention for an hour every weekend.</p>\n<p>This didn't shake my faith, mind you. It didn't even occur to me that the grownups might not actually believe their tales. No, what I learned that day was that there are a lot of people who hold beliefs they aren't willing to act upon.</p>\n<p>Eventually, my faith faded. The distrust remained.</p>\n<p><a id=\"more\"></a></p>\n<h3>Gaining Confidence</h3>\n<p>I grew up in a small village, population ~1200. My early education took place in a one-room schoolhouse. The local towns eventually rolled all their school districts into one, but even then, my graduating class barely broke 50 people. It wasn't difficult to excel.</p>\n<p>Ages twelve and thirteen were rough &mdash; that was right after they merged school districts, and those were the years I was first put a few grades ahead in math classes. I was awkward and underconfident. I felt estranged and lonely, and it was easy to get shoehorned into the \"smart kid\" stereotype by all the new students.</p>\n<p>Eventually, though, I decided that the stereotype was bogus. Anyone&nbsp;intelligent should be able to escape such pigeonholing. In fact, I concluded that anyone with real smarts should be able to find their way out of <em>any</em> mess. I observed the confidence possessed by my peers, even those who seemed to have no reason for confidence. I noticed the ease with which they engaged in social interactions. I decided I could emulate these.</p>\n<p>I faked confidence, and it soon became real. I found that my social limitations had been largely psychological, and that the majority of my classmates were more than willing to be friends. I learned how to get good grades without alienating my peers. It helped that I tended to buck authority (I was no \"teacher's pet\") and that I enjoyed teaching others. I had a knack for pinpointing misunderstandings and was often able to teach better than the teachers could &mdash; as a peer, I could communicate on a different level.</p>\n<p>I started doing very well for myself. I got excellent grades with minimal effort. I overcame my social anxieties. I had a few close friends and was on good terms with most everyone else. I participated in a number of extra circulars where I held high status. As you may imagine, I grew quite arrogant.</p>\n<p>In retrospect, my accomplishments were hardly impressive. At the time, though, it felt like everyone else wasn't even <em>trying</em>. It became apparent that if I wanted something done right, I'd have to do it myself.</p>\n<h3>Shattered Illusions</h3>\n<p>Up until the age of fourteen I had this growing intuition that you can't trust others to actually get things done. This belief didn't become explicit until the end of ninth grade, when I learned how the government of the United States of America actually works.</p>\n<p>Allow me to provide a few pieces of context.</p>\n<p>For one thing, I was learning to program computers at the time. I had been programming for maybe a year and a half, and I was starting to form concepts of elegance and minimalism. I had a belief that the best design is a small design, a design forced by nature at every step along the way, a design that requires no arbitrary choices.</p>\n<p>For another thing, my religion had died not with a bang, but with a whimper. I'd compartmentalized it, and it had slowly withered away. I didn't Believe any more, but I didn't mind that others did. It was a happy fantasy, a social tool. Just as children are allowed to believe in Santa Claus, grownups were allowed to believe in Gods.</p>\n<p>The government, though, was a different matter all together. I assumed that a lot of very smart people had put a lot of effort into its design &mdash; that's what the \"Founding Fathers\" meme implied, anyway. But maybe it wasn't even that. Maybe I just possessed an unspoken, unchallenged belief that the grownups knew what they were doing, at least at the very highest levels. This was the very fabric of society itself: surely it was meticulously calibrated to maximize human virtue, to protect us from circumstance and evil.</p>\n<p>When I was finally told how the US government worked, I couldn't believe my ears. It was a <em>mess</em>. An arbitrary, clunky monstrosity full of loopholes a child could abuse. I could think of a dozen improvements off the top of my head.</p>\n<p>To give you an idea of how my teenaged mind worked, it was immediately clear to me that any first-order \"improvements\" suggested by na&iuml;ve ninth-graders would have unintended negative consequences. Therefore, improvement number one involved redesigning the system to make it easy to test many different improvements in parallel, adding machinery to adopt the improvements that were actually shown to work.</p>\n<p>Yet even these simple ideas were absent in the actual system. Corruption and inefficiency ran rampant. Worse, my peers didn't seem particularly perturbed: they took the system as a given, and merely memorized the machinery for long enough to pass a test. Even the grownups were apathetic: they dickered over who should have power <em>within</em> the system, never suggesting we should alter the system itself.</p>\n<p>My childhood illusions fell to pieces. I realized that nothing was meticulously managed, that the smartest people weren't in control, making sure that everything was optimal.&nbsp;All the world problems, the sicknesses and the injustices and the death: these weren't necessary evils, they were a product of neglect. The most important system of all was poorly coordinated, bloated, and outdated &mdash; and nobody seemed to care.</p>\n<h3>Deciding to Save the World</h3>\n<p>This is the context in which I decided to save the world. I wasn't as young and stupid as you might think &mdash; I didn't <em>believe</em> I was going to save the world. I just <em>decided</em> to. The world is big, and I was small. I knew that, in all likelihood, I'd struggle ineffectually for decades and achieve only a bitter, cynical adulthood.</p>\n<p>But the vast majority of my peers hadn't made it as far as I had. Even though a few were sympathetic, there was simply no way we could change things. It was outside of our control.</p>\n<p>The adults were worse. They smiled, they nodded, they commended my critical thinking skills. Then they went back to what they were doing. A few of them took the time to inform me that it's great to want to change the world and all, but eventually I'd realize that the best way to do that was to settle down and be a teacher, or run a church, or just be kind to others.</p>\n<p>I wasn't surprised. I already knew it was rare for people to actually try and fix things.</p>\n<p>I had youthful idealism, I had big ambitions, but I knew full well that I didn't actually have a chance. I <em>knew</em> that I wouldn't be able to single-handedly redesign the social contract, but I also knew that if everyone who made it as far as I did gave up just because changing the world is impossible, then the world would never change.</p>\n<p>If everybody was cowed by the simple fact that they can't succeed, then that one-in-a-million person who <em>can</em> succeed would never take their shot.</p>\n<p>So I was sure as hell going to take mine.</p>\n<h3>Broadening Scope</h3>\n<p>Mere impossibility was never a hurdle: <a href=\"http://www.goodreads.com/book/show/378.The_Phantom_Tollbooth\">The Phantom Tollbooth</a> saw to that at a young age. When grownups say you can't do something, what they mean is that <em>they</em> can't do it. I spent time devising strategies to get leverage and push governments out of their stagnant state and into something capable of growth.</p>\n<p>In 2005, a teacher to whom I'd ranted introduced me to another important book: <a href=\"http://en.wikipedia.org/wiki/Ishmael_(novel)\">Ishmael</a>. It wasn't the ideas that stuck with me &mdash; I disagreed with a few at the time, and I now disagree with most. No, what this book gave me was <em>scope</em>. This author, too, wished to save the world, and the breadth of his ideas exceeded my own. This book gave me no answers, but it gave me better questions</p>\n<p>Why merely hone the government, instead of redesigning it altogether?</p>\n<p>More importantly, <em>What sort of world are you aiming for?</em></p>\n<p>\"So you want to be an idealist?\", the book asked. \"Very well, but <em>what is your ideal?</em>\"</p>\n<p>I refocused, looking to fully define the ideals I strove for in a human social system. I knew I wouldn't be able to institute any solution directly, but I also knew that pushing governments would be much easier if I had something to push them <em>towards</em>.</p>\n<p>After all, the Communist Manifesto changed the world, once.</p>\n<p>This became my new goal: distill an ideal social structure for humans. The problem was insurmountable, of course, but this was hardly a deterrence. I was bright enough to understand truisms like \"no one system will work for everybody\" and \"you're not perfect enough to get this right\", but these were no trouble. I didn't need to directly specify an ideal social structure: a meta-structure, an imperfect system that ratchets towards perfection, a system that is optimal in the limit, would be fine by me.</p>\n<p>From my vantage point, old ideas like communism and democracy soon seemed laughable. Interesting ideas in their time, perhaps, but obviously doomed to failure. It's easy to build a utopia when you imagine that people will set aside their greed and overcome their apathy. But those aren't systems for <em>people</em>: People are greedy, and people are apathetic. I wanted something that worked &mdash; nay, thrived &mdash; when populated by actual humans, with all their flaws.</p>\n<p>I devoted time and effort to research and study. This was dangerous, as there was no feedback loop. As soon as I stepped beyond the achievements of history, there was no way to actually test anything I came up with. Many times, I settled on one idea for a few months, mulling it over, declaring it perfect. Time and again, I later found a fatal flaw, a piece of faulty reasoning, and the whole thing came tumbling down. After many cycles, I noticed that the flaws were usually visible in advance. I became cognizant of the fact that I'd been glossing over them, ignoring them, explaining them away.</p>\n<p>I learned not to trust my own decrees of perfection.&nbsp;I started monitoring my thought processes very closely. I learned to notice the little ghosts of doubt, to address them earlier and more thoroughly. (I became a staunch atheist, unsurprisingly.) This was, perhaps, the beginning of my rationalist training. Unfortunately, it was all self-directed. Somehow, it never occurred to me to read literature on how to think better. I didn't have much trust in psychological literature, anyway, and I was arrogant.</p>\n<h3>Communication Failures</h3>\n<p>It was during this period that I explicitly decided not to pursue math. I reasoned that in order to actually save the world, I'd need to focus on charisma, political connections, and a solid understanding of the machinery underlying the world's major governments. Upon graduating high school, I decided to go to a college in Washington D.C. and study political science. I double majored in Computer Science as a fallback plan, a way to actually make money as needed (and because I loved it).</p>\n<p>I went into my Poly Sci degree expecting to learn about the mechanics of society. Amusingly enough, I didn't know that \"Economics\" was a field. We didn't have any econ classes in my tiny high school, and nobody had seen fit to tell me about it. I expected \"Political Science\" to teach me the workings of nations <em>including</em> the world economy, but quickly realized that it's about the actual <em>politicians</em>, the social peacocking, the fa&ccedil;ades. Fortunately, a required Intro to Econ class soon remedied the situation, and I quickly changed my major to Economics.</p>\n<p>My ideas experienced significant refinement as I received formal training. Unfortunately, nobody would listen to them.</p>\n<p>It's not that they were dismissed as childish idealism: I had graduated to larger problems. I'd been thinking long and hard about the problem for a few years, and I'd had some interesting insights. But when I tried to explain them to people, almost everyone had immediate adverse reactions.</p>\n<p>I anticipated criticism, and relished the prospect. My ideas were in desperate need of an outside challenger. But the reactions of others were far worse than I anticipated.</p>\n<p>Nobody found flaws in my logic. Nobody challenged my bold claims. Instead, they simply failed to understand. They got stuck three or four points before the interesting points, and could go no further. I learned that most people don't understand basic economics or game theory. Many others were entrenched in <a href=\"http://wiki.lesswrong.com/wiki/Color_politics\">bluegreensmanship</a> and reflexively treated my suggestions as attacks. Aspiring politicians balked at the claim that Democracy, while perhaps an important step in our cultural evolution, can't possibly be the end of the line. Still others insisted that it's useless to discuss ideals, because they can never be achieved.</p>\n<p>In short, I found myself on the far side of a wide <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential gap</a>.</p>\n<p>I learned that many people, after falling into the gap, were incapable of climbing out, no matter how slowly I walked them through the intervening steps. They had already passed judgement on the conclusion, and rejected my attempts to root out their misconceptions, becoming impatient before actually listening. I grew very cautious with who I shared my ideas with, worrying that exposing them too quickly or in the wrong fashion would be a permanent setback.</p>\n<p>I had a small few friends who knew enough economics and other subjects to follow along and who wouldn't discard uncouth ideas outright. I began to value these people highly, as they were among the few who could actually put pressure on me, expose flaws in my reasoning, and help me come up with solutions.</p>\n<p>Eventually, I had a few insights that I've yet to find in the literature, a few ideas that I still actually believe are important. You'll excuse me if I don't mention them here: there is a lot of inferential distance. Perhaps one day I'll write a sequence.</p>\n<p>Even then, I could see no easy path to public support. Most people lacked the knowledge to understand my claims without effort, and lacked the incentive to put in the effort for some unproven boy.</p>\n<h3>Phase Two</h3>\n<p>Fortunately, I had other tricks up my sleeve.</p>\n<p>I attempted three different tech startups. Two of them failed. The last was healthier, but we shut it down because the expected gains were lower than an industry salary. In the interim, I honed my programming skills and secured an industry job (I'm a software engineer at Google).</p>\n<p>By the time I graduated, my ideas were largely refined and stable. I had settled upon a solid meta social system as an ideal to strive for, and I'm still fairly confident that it's a good one &mdash; one where the design is forced by nature at every step, one that requires no arbitrary choices, one that ratchets towards optimality. And even if the ideal was not perfect, the modern world is insane enough that even a small step towards a better-coordinated society would yield gigantic benefits.</p>\n<p>The problem changed from one of refining ideas to one of convincing others.</p>\n<p>It was clear that I couldn't spread my ideas by merely stating them, due to the inferential distance, so I started working on two indirect approaches in the hours after work.</p>\n<p>The first was a book, which went back to my roots: simple, low-cost ideas for how to change the <em>current</em> system of government in small ways that could have large payoffs. The goal of this project was to shake people from the blue-green mindset, to convince them that we should stop bickering within the framework and consider modifying the framework itself. This book was meant to the be first in a series, in which I'd slowly build towards more radical suggestions.</p>\n<p>The second project was designed to put people in a more rational frame of mind. I wanted people who could look past the labels and see the <em>things</em>, people who don't just memorize how the world works but see it as mutable, as something they can actually change. I wanted people that I could pull out of inferential gaps, in case they fell into mine.</p>\n<p>Upon introspection, I realized that much of my ability came from a specific outlook on the world that I had at a young age. I had a knack for understanding what the teachers were&nbsp;<em>trying</em> to teach me, for recognizing and discarding the cruft in their statements. I saw many fellow students putting stock in historical accidents of explanation where I found it easy to grasp the underlying concepts and drop the baggage. This ability to cull the cruft is important to understanding my grand designs.</p>\n<p>This reasoning (and a few other desires, including a perpetual fascination with math and physics) led me to create <a href=\"http://simplifience.com/\">simplifience</a>, a website that promotes such a mindset.</p>\n<p>It never made it to the point where I was comfortable publicizing it, but that hardly matters anymore. In retrospect, it's an unfinished jumble of rationality training, math explanations, and science enthusiasm. It's important in one key respect:</p>\n<p>As I was writing simplifience, I did a lot of research for it. During this research, I kept stumbling upon web articles on this one website that articulated what I was trying to express, only better. That website was LessWrong, and those articles were the Sequences.</p>\n<p>It took me an embarrassingly long time to actually pay attention. In fact, if you go to simplifience.com, you can watch as the articles grow more and more influenced by the sequences. My exposure to them was patchy, centered around ideas that I'd already had. It took me a while to realize that I should read the rest of them, that I might learn new things that extended the ideas I'd figured out on my own.</p>\n<p>It seemed like a good way to learn how to think better, to learn from someone who had had similar insights. I didn't even consider the possibility that this author, too, had some grand agenda. The idea that Eliezer's agenda could be more pressing than my own never even crossed my mind.</p>\n<p>At this point, you may be able to empathize with how I felt when I first realized the importance of an intelligence explosion.</p>\n<h3>Superseded</h3>\n<p>It was like getting ten years worth of wind knocked out of me.</p>\n<p>I saw something familiar in the sequences &mdash; the winding, meticulous explanations of someone struggling to bridge an inferential gap. I recognized the need to cover subjects that looked completely tangential to the actual point, just to get people to the level where they wouldn't reject the main ideas out-of-hand. I noticed the people falling to the side, debating issues two or three steps before the actual interesting problems. It was this familiar pattern, above all else, that made me <em>actually</em> pay attention.</p>\n<p>Everything clicked. I was already thoroughly convinced of civilizational inadequacy. I had long since concluded that there's not much that can hold a strong intelligence down. I had a sort of vague idea that an AI would seek out \"good\" values, but such illusions were easily dispelled &mdash; I was a moral relativist. And the stakes were as high as stakes go. Artificial intelligence was a problem more pressing than my own.</p>\n<p>The realization shook me to my core. It wasn't even the intelligence explosion idea that scared me, it was the revelation of a fatal flaw at the foundation of my beliefs. Poorly designed governments had awoken my fear that society can't handle coordination problems, but I never &mdash; not once in nearly a <em>decade</em> &mdash; stopped to consider whether designing better social systems was actually the best way to optimize the world.</p>\n<p>I professed a desire to save the world, but had misunderstood the playing field so badly that existential risk had never even crossed my mind.&nbsp;Somehow, I had missed the most important problems, and they should have been obvious. Something was <em>very</em>&nbsp;wrong.</p>\n<p>It was time to halt, melt, and catch fire.</p>\n<p>This was one of the most difficult things I've done.</p>\n<hr />\n<p>I was more careful, the second time around. The Sequences shook my foundations and brought the whole tower crashing down, but what I would build in its place was by no means a foregone conclusion.</p>\n<p>I had been blind to all existential risks, not just AI risk, and there was a possibility that I had missed other features of the problem space as well. I was well aware of the fact that, having been introduced to AI risk by Eliezer's writings, I was biased towards his viewpoint. I didn't want to make the same mistake twice, to jump for the <em>second</em>&nbsp;big problem that crossed my path just because it was larger than the first. I had to start from scratch, reasoning from the beginning. I knew I must watch out for conjunction fallacies caused by nice narratives, arguments made from high stakes (Pascal's mugging), putting too much stock on inside views, and so on. I had to figure out how to <em>actually</em>&nbsp;save the world.</p>\n<p>It took me a long time to deprogram, to get back to neutral. I considered carefully, accounting for my biases as best I could. I read a lot. I weighed the evidence. The process took many months.</p>\n<p>By July of 2013, I came to agree with MIRI's conclusions.</p>\n<h3>Disclaimer</h3>\n<p>Writing it all out like this, I realize that I've failed to convey the feeling of it all. Depending upon whether you believe that I was actually able to come up with better ways to structure people, you may feel that I'm either pretty accomplished or extremely deluded. Perhaps both.</p>\n<p>Really, though, it's neither. This raw story, which omits details from the rest of my life, paints a strange picture indeed. The intensity is distilled.</p>\n<p>I was not a zealot, in practice. My attempts to save the world didn't bleed much into the rest of my life. I learned early on that this wasn't the sort of thing that most people enjoyed discussing, and I was wary of inferential gaps. My work was done parallel to an otherwise normal life. Only a select few people were privy to my goals, my conclusions. The whole thing often felt disconnected from reality, just some unusual hobby. The majority of my friends, if they read this, will be surprised.</p>\n<p>There are many holes in this summary, too. It fails to capture the dark spots. It omits the feelings of uncertainty and helplessness, the cycles of guilt at being unproductive followed by lingering depression, the wavering between staunch idealism and a conviction that my goals were nothing but a comfortable fantasy. It skips over the year I burned out, writing the whole idea off, studying abroad and building myself a healthier mental state before returning and picking everything back up.</p>\n<p>Nothing in this summary describes the constant doubt about whether I was pursuing the best path or merely the easiest one. I've failed to mention my complete failure to network and my spectacular inability to find people who would actually take me seriously. It's hard to convey the fear that I was just <em>pretending</em> I wanted to save the world, just <em>acting</em> like I was trying, because that's the narrative that I wanted. How could someone 'smart' actually fail to find powerful friends if they were really trying for <em>nine years</em>?</p>\n<p>I claim no glory: the journey was messy, and it was poorly executed. I tell the story in part because people have asked me where my passion comes from and how I became aligned with MIRI's mission. Mostly, though, I tell the story because it feels like something I have to tell before moving on. It feels almost dishonest to try to save the world in this new way without at least acknowledging that I walked another path, once.</p>\n<h3>The source of my passion</h3>\n<p>So to those of you wondering where my passion comes from, I answer this: it has always been there. It was a small flame, when I was young, and it was fed by a deep mistrust in society's capabilities and a strong belief that if <em>anyone</em> can matter then I had better try.</p>\n<p>From my perspective, I've been dedicating my energy towards 'saving the world' since first I realized that the world was in need of saving. This passion was not recently kindled, it was merely redirected.</p>\n<p>There was a burst of productivity these past few months, after I refocused my efforts. I was given a new path, and on it the analogous obstacles have already been surmounted. MIRI has already spent years promoting that rational state of mind, bridging its inferential gap, finding people who can actually work on solving the problem instead of arguing about whether there is a problem to be solved. This was invigorating, like skipping ahead ten years in terms of where I wanted to be.</p>\n<p>Alongside that, I felt a burning need to catch up. I was late to the party, and I had been foolish for a very long time. I was terrified that I wouldn't actually be able to help &mdash; that, after all my work, the most I'd be able to do to solve the big problems was earn to give. I'd have done it, because the actual goal is to save the world, not to satisfy Nate. But the idea scared me, and the desire to keep actively working on the big problems drove me forward.</p>\n<p>In a way, too, everything got easier &mdash; I needed only to become good at logic and decision theory, to read a bunch of math textbooks, a task that was trivially measurable and joyfully easy compared to trying to convince the entire world to embrace strange, unpolished ideas.</p>\n<p>All these factors contributed to my recent productivity. But the passion, the fervor, the desire to optimize the future &mdash; that has been there for a long time. People sometimes ask where I get my passion from, and I find it hard to answer.</p>\n<p><em>We hold the entire future of the universe in our hands. Is that not justification enough?</em></p>\n<p>I learned a long time ago that most people are content to accept the way things are. Everyone wants the world to change, but most are cowed by the fact that they can't change it themselves.</p>\n<p>But if the chance that one person can save the world is one in a million, then there had better be a million people trying.</p>\n<p>It is this knowledge &mdash; that the world will only be saved by people who actually try to save it &mdash; that drives me.</p>\n<p>I still have these strange ideas, this pet inferential gap that I hope to bridge one day. It still hurts, that things important to me were superseded, but they <em>were</em> superseded, and it is better to know than to remain in the dark.</p>\n<p>When I was fourteen, I saw many horrors laid out before us: war, corruption, environmental destruction, and the silent tragedies of automobile accidents, courtroom injustices, and death by disease and aging. All around me, I saw a society that couldn't coordinate, full of people resigned to unnecessary fates.</p>\n<p>I was told to settle for making a small difference. I resolved to do the opposite.</p>\n<p>I made a promise to myself. I didn't promise to fix governments: that was a means to an end, a convenient solution for someone who didn't know how to look further out. I didn't promise to change the world, either: every little thing is a change, and not all changes are good. No, I promised to&nbsp;<em>save</em>&nbsp;the world.</p>\n<p>That promise still stands.</p>\n<p>The world sure as hell isn't going to save itself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NzSTgAtKwgivkfeYm": 4, "iP2X4jQNHMWHRNPne": 1, "xexCWMyds6QLWognu": 2, "JsJPrdgRGRqnci8cZ": 4, "irYLXtT9hkPXoZqhH": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F2DZXsMdhGyX4FPAd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 155, "baseScore": 207, "extendedScore": null, "score": 0.000531, "legacy": true, "legacyId": "25383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 209, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is the final post in my productivity sequence.</em></p>\n<p><em>The first post described&nbsp;<a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">what I achieved</a>. The <a href=\"/lw/jgh/habitual_productivity/\">next</a>&nbsp;<a href=\"/lw/jh0/deregulating_distraction_moving_towards_the_goal/\">three</a>&nbsp;<a href=\"/lw/jhs/dark_arts_of_rationality/\">posts</a>&nbsp;describe how. This post describes why, explaining the sources of my passion and the circumstances that convinced a young Nate to try and save the world.&nbsp;</em><em>Within, you will find no suggestions, no techniques to emulate, no new ideas to ponder. This is a rationalist coming-of-age story. With luck, you may find it inspiring. Regardless, I hope you can learn from my mistakes.</em></p>\n<p><em>Never fear, I'll be back to business soon \u2014 there's lots of studying to do. But before then, there's a story to tell, a memorial to what I left behind.</em></p>\n<hr>\n<p>I was raised Catholic. On my eighth birthday, having received my first communion about a year prior, I casually asked my priest how to reaffirm my faith and do something for the Lord. The memory is fuzzy, but I think I donated a chunk of allowance money and made a public confession at the following mass.</p>\n<p>A bunch of the grownups made a big deal out of it, as grownups are like to do. \"Faith of a child\", and all that. This confused me, especially when I realized that what I had done was rare. I wasn't trying to get pats on the head, I was appealing to the <em>Lord of the Heavens and the Earth</em>. Were we all on the same page, here? This was the <em>creator.</em>&nbsp;He was infinitely virtuous, and he had told us what to do.</p>\n<p>And yet, everyone was content to recite hymns once a week and donate for the reconstruction of the church. What about the rest of the world, the sick, the dying? Where were the proselytizers, the missionary opportunities? Why was everyone just sitting around?&nbsp;</p>\n<p>On that day, I became acquainted with civilizational inadequacy. I realized you could hand a room full of people the<em>&nbsp;literal word of God</em>, and they'd still struggle to pay attention for an hour every weekend.</p>\n<p>This didn't shake my faith, mind you. It didn't even occur to me that the grownups might not actually believe their tales. No, what I learned that day was that there are a lot of people who hold beliefs they aren't willing to act upon.</p>\n<p>Eventually, my faith faded. The distrust remained.</p>\n<p><a id=\"more\"></a></p>\n<h3 id=\"Gaining_Confidence\">Gaining Confidence</h3>\n<p>I grew up in a small village, population ~1200. My early education took place in a one-room schoolhouse. The local towns eventually rolled all their school districts into one, but even then, my graduating class barely broke 50 people. It wasn't difficult to excel.</p>\n<p>Ages twelve and thirteen were rough \u2014 that was right after they merged school districts, and those were the years I was first put a few grades ahead in math classes. I was awkward and underconfident. I felt estranged and lonely, and it was easy to get shoehorned into the \"smart kid\" stereotype by all the new students.</p>\n<p>Eventually, though, I decided that the stereotype was bogus. Anyone&nbsp;intelligent should be able to escape such pigeonholing. In fact, I concluded that anyone with real smarts should be able to find their way out of <em>any</em> mess. I observed the confidence possessed by my peers, even those who seemed to have no reason for confidence. I noticed the ease with which they engaged in social interactions. I decided I could emulate these.</p>\n<p>I faked confidence, and it soon became real. I found that my social limitations had been largely psychological, and that the majority of my classmates were more than willing to be friends. I learned how to get good grades without alienating my peers. It helped that I tended to buck authority (I was no \"teacher's pet\") and that I enjoyed teaching others. I had a knack for pinpointing misunderstandings and was often able to teach better than the teachers could \u2014 as a peer, I could communicate on a different level.</p>\n<p>I started doing very well for myself. I got excellent grades with minimal effort. I overcame my social anxieties. I had a few close friends and was on good terms with most everyone else. I participated in a number of extra circulars where I held high status. As you may imagine, I grew quite arrogant.</p>\n<p>In retrospect, my accomplishments were hardly impressive. At the time, though, it felt like everyone else wasn't even <em>trying</em>. It became apparent that if I wanted something done right, I'd have to do it myself.</p>\n<h3 id=\"Shattered_Illusions\">Shattered Illusions</h3>\n<p>Up until the age of fourteen I had this growing intuition that you can't trust others to actually get things done. This belief didn't become explicit until the end of ninth grade, when I learned how the government of the United States of America actually works.</p>\n<p>Allow me to provide a few pieces of context.</p>\n<p>For one thing, I was learning to program computers at the time. I had been programming for maybe a year and a half, and I was starting to form concepts of elegance and minimalism. I had a belief that the best design is a small design, a design forced by nature at every step along the way, a design that requires no arbitrary choices.</p>\n<p>For another thing, my religion had died not with a bang, but with a whimper. I'd compartmentalized it, and it had slowly withered away. I didn't Believe any more, but I didn't mind that others did. It was a happy fantasy, a social tool. Just as children are allowed to believe in Santa Claus, grownups were allowed to believe in Gods.</p>\n<p>The government, though, was a different matter all together. I assumed that a lot of very smart people had put a lot of effort into its design \u2014 that's what the \"Founding Fathers\" meme implied, anyway. But maybe it wasn't even that. Maybe I just possessed an unspoken, unchallenged belief that the grownups knew what they were doing, at least at the very highest levels. This was the very fabric of society itself: surely it was meticulously calibrated to maximize human virtue, to protect us from circumstance and evil.</p>\n<p>When I was finally told how the US government worked, I couldn't believe my ears. It was a <em>mess</em>. An arbitrary, clunky monstrosity full of loopholes a child could abuse. I could think of a dozen improvements off the top of my head.</p>\n<p>To give you an idea of how my teenaged mind worked, it was immediately clear to me that any first-order \"improvements\" suggested by na\u00efve ninth-graders would have unintended negative consequences. Therefore, improvement number one involved redesigning the system to make it easy to test many different improvements in parallel, adding machinery to adopt the improvements that were actually shown to work.</p>\n<p>Yet even these simple ideas were absent in the actual system. Corruption and inefficiency ran rampant. Worse, my peers didn't seem particularly perturbed: they took the system as a given, and merely memorized the machinery for long enough to pass a test. Even the grownups were apathetic: they dickered over who should have power <em>within</em> the system, never suggesting we should alter the system itself.</p>\n<p>My childhood illusions fell to pieces. I realized that nothing was meticulously managed, that the smartest people weren't in control, making sure that everything was optimal.&nbsp;All the world problems, the sicknesses and the injustices and the death: these weren't necessary evils, they were a product of neglect. The most important system of all was poorly coordinated, bloated, and outdated \u2014 and nobody seemed to care.</p>\n<h3 id=\"Deciding_to_Save_the_World\">Deciding to Save the World</h3>\n<p>This is the context in which I decided to save the world. I wasn't as young and stupid as you might think \u2014 I didn't <em>believe</em> I was going to save the world. I just <em>decided</em> to. The world is big, and I was small. I knew that, in all likelihood, I'd struggle ineffectually for decades and achieve only a bitter, cynical adulthood.</p>\n<p>But the vast majority of my peers hadn't made it as far as I had. Even though a few were sympathetic, there was simply no way we could change things. It was outside of our control.</p>\n<p>The adults were worse. They smiled, they nodded, they commended my critical thinking skills. Then they went back to what they were doing. A few of them took the time to inform me that it's great to want to change the world and all, but eventually I'd realize that the best way to do that was to settle down and be a teacher, or run a church, or just be kind to others.</p>\n<p>I wasn't surprised. I already knew it was rare for people to actually try and fix things.</p>\n<p>I had youthful idealism, I had big ambitions, but I knew full well that I didn't actually have a chance. I <em>knew</em> that I wouldn't be able to single-handedly redesign the social contract, but I also knew that if everyone who made it as far as I did gave up just because changing the world is impossible, then the world would never change.</p>\n<p>If everybody was cowed by the simple fact that they can't succeed, then that one-in-a-million person who <em>can</em> succeed would never take their shot.</p>\n<p>So I was sure as hell going to take mine.</p>\n<h3 id=\"Broadening_Scope\">Broadening Scope</h3>\n<p>Mere impossibility was never a hurdle: <a href=\"http://www.goodreads.com/book/show/378.The_Phantom_Tollbooth\">The Phantom Tollbooth</a> saw to that at a young age. When grownups say you can't do something, what they mean is that <em>they</em> can't do it. I spent time devising strategies to get leverage and push governments out of their stagnant state and into something capable of growth.</p>\n<p>In 2005, a teacher to whom I'd ranted introduced me to another important book: <a href=\"http://en.wikipedia.org/wiki/Ishmael_(novel)\">Ishmael</a>. It wasn't the ideas that stuck with me \u2014 I disagreed with a few at the time, and I now disagree with most. No, what this book gave me was <em>scope</em>. This author, too, wished to save the world, and the breadth of his ideas exceeded my own. This book gave me no answers, but it gave me better questions</p>\n<p>Why merely hone the government, instead of redesigning it altogether?</p>\n<p>More importantly, <em>What sort of world are you aiming for?</em></p>\n<p>\"So you want to be an idealist?\", the book asked. \"Very well, but <em>what is your ideal?</em>\"</p>\n<p>I refocused, looking to fully define the ideals I strove for in a human social system. I knew I wouldn't be able to institute any solution directly, but I also knew that pushing governments would be much easier if I had something to push them <em>towards</em>.</p>\n<p>After all, the Communist Manifesto changed the world, once.</p>\n<p>This became my new goal: distill an ideal social structure for humans. The problem was insurmountable, of course, but this was hardly a deterrence. I was bright enough to understand truisms like \"no one system will work for everybody\" and \"you're not perfect enough to get this right\", but these were no trouble. I didn't need to directly specify an ideal social structure: a meta-structure, an imperfect system that ratchets towards perfection, a system that is optimal in the limit, would be fine by me.</p>\n<p>From my vantage point, old ideas like communism and democracy soon seemed laughable. Interesting ideas in their time, perhaps, but obviously doomed to failure. It's easy to build a utopia when you imagine that people will set aside their greed and overcome their apathy. But those aren't systems for <em>people</em>: People are greedy, and people are apathetic. I wanted something that worked \u2014 nay, thrived \u2014 when populated by actual humans, with all their flaws.</p>\n<p>I devoted time and effort to research and study. This was dangerous, as there was no feedback loop. As soon as I stepped beyond the achievements of history, there was no way to actually test anything I came up with. Many times, I settled on one idea for a few months, mulling it over, declaring it perfect. Time and again, I later found a fatal flaw, a piece of faulty reasoning, and the whole thing came tumbling down. After many cycles, I noticed that the flaws were usually visible in advance. I became cognizant of the fact that I'd been glossing over them, ignoring them, explaining them away.</p>\n<p>I learned not to trust my own decrees of perfection.&nbsp;I started monitoring my thought processes very closely. I learned to notice the little ghosts of doubt, to address them earlier and more thoroughly. (I became a staunch atheist, unsurprisingly.) This was, perhaps, the beginning of my rationalist training. Unfortunately, it was all self-directed. Somehow, it never occurred to me to read literature on how to think better. I didn't have much trust in psychological literature, anyway, and I was arrogant.</p>\n<h3 id=\"Communication_Failures\">Communication Failures</h3>\n<p>It was during this period that I explicitly decided not to pursue math. I reasoned that in order to actually save the world, I'd need to focus on charisma, political connections, and a solid understanding of the machinery underlying the world's major governments. Upon graduating high school, I decided to go to a college in Washington D.C. and study political science. I double majored in Computer Science as a fallback plan, a way to actually make money as needed (and because I loved it).</p>\n<p>I went into my Poly Sci degree expecting to learn about the mechanics of society. Amusingly enough, I didn't know that \"Economics\" was a field. We didn't have any econ classes in my tiny high school, and nobody had seen fit to tell me about it. I expected \"Political Science\" to teach me the workings of nations <em>including</em> the world economy, but quickly realized that it's about the actual <em>politicians</em>, the social peacocking, the fa\u00e7ades. Fortunately, a required Intro to Econ class soon remedied the situation, and I quickly changed my major to Economics.</p>\n<p>My ideas experienced significant refinement as I received formal training. Unfortunately, nobody would listen to them.</p>\n<p>It's not that they were dismissed as childish idealism: I had graduated to larger problems. I'd been thinking long and hard about the problem for a few years, and I'd had some interesting insights. But when I tried to explain them to people, almost everyone had immediate adverse reactions.</p>\n<p>I anticipated criticism, and relished the prospect. My ideas were in desperate need of an outside challenger. But the reactions of others were far worse than I anticipated.</p>\n<p>Nobody found flaws in my logic. Nobody challenged my bold claims. Instead, they simply failed to understand. They got stuck three or four points before the interesting points, and could go no further. I learned that most people don't understand basic economics or game theory. Many others were entrenched in <a href=\"http://wiki.lesswrong.com/wiki/Color_politics\">bluegreensmanship</a> and reflexively treated my suggestions as attacks. Aspiring politicians balked at the claim that Democracy, while perhaps an important step in our cultural evolution, can't possibly be the end of the line. Still others insisted that it's useless to discuss ideals, because they can never be achieved.</p>\n<p>In short, I found myself on the far side of a wide <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential gap</a>.</p>\n<p>I learned that many people, after falling into the gap, were incapable of climbing out, no matter how slowly I walked them through the intervening steps. They had already passed judgement on the conclusion, and rejected my attempts to root out their misconceptions, becoming impatient before actually listening. I grew very cautious with who I shared my ideas with, worrying that exposing them too quickly or in the wrong fashion would be a permanent setback.</p>\n<p>I had a small few friends who knew enough economics and other subjects to follow along and who wouldn't discard uncouth ideas outright. I began to value these people highly, as they were among the few who could actually put pressure on me, expose flaws in my reasoning, and help me come up with solutions.</p>\n<p>Eventually, I had a few insights that I've yet to find in the literature, a few ideas that I still actually believe are important. You'll excuse me if I don't mention them here: there is a lot of inferential distance. Perhaps one day I'll write a sequence.</p>\n<p>Even then, I could see no easy path to public support. Most people lacked the knowledge to understand my claims without effort, and lacked the incentive to put in the effort for some unproven boy.</p>\n<h3 id=\"Phase_Two\">Phase Two</h3>\n<p>Fortunately, I had other tricks up my sleeve.</p>\n<p>I attempted three different tech startups. Two of them failed. The last was healthier, but we shut it down because the expected gains were lower than an industry salary. In the interim, I honed my programming skills and secured an industry job (I'm a software engineer at Google).</p>\n<p>By the time I graduated, my ideas were largely refined and stable. I had settled upon a solid meta social system as an ideal to strive for, and I'm still fairly confident that it's a good one \u2014 one where the design is forced by nature at every step, one that requires no arbitrary choices, one that ratchets towards optimality. And even if the ideal was not perfect, the modern world is insane enough that even a small step towards a better-coordinated society would yield gigantic benefits.</p>\n<p>The problem changed from one of refining ideas to one of convincing others.</p>\n<p>It was clear that I couldn't spread my ideas by merely stating them, due to the inferential distance, so I started working on two indirect approaches in the hours after work.</p>\n<p>The first was a book, which went back to my roots: simple, low-cost ideas for how to change the <em>current</em> system of government in small ways that could have large payoffs. The goal of this project was to shake people from the blue-green mindset, to convince them that we should stop bickering within the framework and consider modifying the framework itself. This book was meant to the be first in a series, in which I'd slowly build towards more radical suggestions.</p>\n<p>The second project was designed to put people in a more rational frame of mind. I wanted people who could look past the labels and see the <em>things</em>, people who don't just memorize how the world works but see it as mutable, as something they can actually change. I wanted people that I could pull out of inferential gaps, in case they fell into mine.</p>\n<p>Upon introspection, I realized that much of my ability came from a specific outlook on the world that I had at a young age. I had a knack for understanding what the teachers were&nbsp;<em>trying</em> to teach me, for recognizing and discarding the cruft in their statements. I saw many fellow students putting stock in historical accidents of explanation where I found it easy to grasp the underlying concepts and drop the baggage. This ability to cull the cruft is important to understanding my grand designs.</p>\n<p>This reasoning (and a few other desires, including a perpetual fascination with math and physics) led me to create <a href=\"http://simplifience.com/\">simplifience</a>, a website that promotes such a mindset.</p>\n<p>It never made it to the point where I was comfortable publicizing it, but that hardly matters anymore. In retrospect, it's an unfinished jumble of rationality training, math explanations, and science enthusiasm. It's important in one key respect:</p>\n<p>As I was writing simplifience, I did a lot of research for it. During this research, I kept stumbling upon web articles on this one website that articulated what I was trying to express, only better. That website was LessWrong, and those articles were the Sequences.</p>\n<p>It took me an embarrassingly long time to actually pay attention. In fact, if you go to simplifience.com, you can watch as the articles grow more and more influenced by the sequences. My exposure to them was patchy, centered around ideas that I'd already had. It took me a while to realize that I should read the rest of them, that I might learn new things that extended the ideas I'd figured out on my own.</p>\n<p>It seemed like a good way to learn how to think better, to learn from someone who had had similar insights. I didn't even consider the possibility that this author, too, had some grand agenda. The idea that Eliezer's agenda could be more pressing than my own never even crossed my mind.</p>\n<p>At this point, you may be able to empathize with how I felt when I first realized the importance of an intelligence explosion.</p>\n<h3 id=\"Superseded\">Superseded</h3>\n<p>It was like getting ten years worth of wind knocked out of me.</p>\n<p>I saw something familiar in the sequences \u2014 the winding, meticulous explanations of someone struggling to bridge an inferential gap. I recognized the need to cover subjects that looked completely tangential to the actual point, just to get people to the level where they wouldn't reject the main ideas out-of-hand. I noticed the people falling to the side, debating issues two or three steps before the actual interesting problems. It was this familiar pattern, above all else, that made me <em>actually</em> pay attention.</p>\n<p>Everything clicked. I was already thoroughly convinced of civilizational inadequacy. I had long since concluded that there's not much that can hold a strong intelligence down. I had a sort of vague idea that an AI would seek out \"good\" values, but such illusions were easily dispelled \u2014 I was a moral relativist. And the stakes were as high as stakes go. Artificial intelligence was a problem more pressing than my own.</p>\n<p>The realization shook me to my core. It wasn't even the intelligence explosion idea that scared me, it was the revelation of a fatal flaw at the foundation of my beliefs. Poorly designed governments had awoken my fear that society can't handle coordination problems, but I never \u2014 not once in nearly a <em>decade</em> \u2014 stopped to consider whether designing better social systems was actually the best way to optimize the world.</p>\n<p>I professed a desire to save the world, but had misunderstood the playing field so badly that existential risk had never even crossed my mind.&nbsp;Somehow, I had missed the most important problems, and they should have been obvious. Something was <em>very</em>&nbsp;wrong.</p>\n<p>It was time to halt, melt, and catch fire.</p>\n<p>This was one of the most difficult things I've done.</p>\n<hr>\n<p>I was more careful, the second time around. The Sequences shook my foundations and brought the whole tower crashing down, but what I would build in its place was by no means a foregone conclusion.</p>\n<p>I had been blind to all existential risks, not just AI risk, and there was a possibility that I had missed other features of the problem space as well. I was well aware of the fact that, having been introduced to AI risk by Eliezer's writings, I was biased towards his viewpoint. I didn't want to make the same mistake twice, to jump for the <em>second</em>&nbsp;big problem that crossed my path just because it was larger than the first. I had to start from scratch, reasoning from the beginning. I knew I must watch out for conjunction fallacies caused by nice narratives, arguments made from high stakes (Pascal's mugging), putting too much stock on inside views, and so on. I had to figure out how to <em>actually</em>&nbsp;save the world.</p>\n<p>It took me a long time to deprogram, to get back to neutral. I considered carefully, accounting for my biases as best I could. I read a lot. I weighed the evidence. The process took many months.</p>\n<p>By July of 2013, I came to agree with MIRI's conclusions.</p>\n<h3 id=\"Disclaimer\">Disclaimer</h3>\n<p>Writing it all out like this, I realize that I've failed to convey the feeling of it all. Depending upon whether you believe that I was actually able to come up with better ways to structure people, you may feel that I'm either pretty accomplished or extremely deluded. Perhaps both.</p>\n<p>Really, though, it's neither. This raw story, which omits details from the rest of my life, paints a strange picture indeed. The intensity is distilled.</p>\n<p>I was not a zealot, in practice. My attempts to save the world didn't bleed much into the rest of my life. I learned early on that this wasn't the sort of thing that most people enjoyed discussing, and I was wary of inferential gaps. My work was done parallel to an otherwise normal life. Only a select few people were privy to my goals, my conclusions. The whole thing often felt disconnected from reality, just some unusual hobby. The majority of my friends, if they read this, will be surprised.</p>\n<p>There are many holes in this summary, too. It fails to capture the dark spots. It omits the feelings of uncertainty and helplessness, the cycles of guilt at being unproductive followed by lingering depression, the wavering between staunch idealism and a conviction that my goals were nothing but a comfortable fantasy. It skips over the year I burned out, writing the whole idea off, studying abroad and building myself a healthier mental state before returning and picking everything back up.</p>\n<p>Nothing in this summary describes the constant doubt about whether I was pursuing the best path or merely the easiest one. I've failed to mention my complete failure to network and my spectacular inability to find people who would actually take me seriously. It's hard to convey the fear that I was just <em>pretending</em> I wanted to save the world, just <em>acting</em> like I was trying, because that's the narrative that I wanted. How could someone 'smart' actually fail to find powerful friends if they were really trying for <em>nine years</em>?</p>\n<p>I claim no glory: the journey was messy, and it was poorly executed. I tell the story in part because people have asked me where my passion comes from and how I became aligned with MIRI's mission. Mostly, though, I tell the story because it feels like something I have to tell before moving on. It feels almost dishonest to try to save the world in this new way without at least acknowledging that I walked another path, once.</p>\n<h3 id=\"The_source_of_my_passion\">The source of my passion</h3>\n<p>So to those of you wondering where my passion comes from, I answer this: it has always been there. It was a small flame, when I was young, and it was fed by a deep mistrust in society's capabilities and a strong belief that if <em>anyone</em> can matter then I had better try.</p>\n<p>From my perspective, I've been dedicating my energy towards 'saving the world' since first I realized that the world was in need of saving. This passion was not recently kindled, it was merely redirected.</p>\n<p>There was a burst of productivity these past few months, after I refocused my efforts. I was given a new path, and on it the analogous obstacles have already been surmounted. MIRI has already spent years promoting that rational state of mind, bridging its inferential gap, finding people who can actually work on solving the problem instead of arguing about whether there is a problem to be solved. This was invigorating, like skipping ahead ten years in terms of where I wanted to be.</p>\n<p>Alongside that, I felt a burning need to catch up. I was late to the party, and I had been foolish for a very long time. I was terrified that I wouldn't actually be able to help \u2014 that, after all my work, the most I'd be able to do to solve the big problems was earn to give. I'd have done it, because the actual goal is to save the world, not to satisfy Nate. But the idea scared me, and the desire to keep actively working on the big problems drove me forward.</p>\n<p>In a way, too, everything got easier \u2014 I needed only to become good at logic and decision theory, to read a bunch of math textbooks, a task that was trivially measurable and joyfully easy compared to trying to convince the entire world to embrace strange, unpolished ideas.</p>\n<p>All these factors contributed to my recent productivity. But the passion, the fervor, the desire to optimize the future \u2014 that has been there for a long time. People sometimes ask where I get my passion from, and I find it hard to answer.</p>\n<p><em>We hold the entire future of the universe in our hands. Is that not justification enough?</em></p>\n<p>I learned a long time ago that most people are content to accept the way things are. Everyone wants the world to change, but most are cowed by the fact that they can't change it themselves.</p>\n<p>But if the chance that one person can save the world is one in a million, then there had better be a million people trying.</p>\n<p>It is this knowledge \u2014 that the world will only be saved by people who actually try to save it \u2014 that drives me.</p>\n<p>I still have these strange ideas, this pet inferential gap that I hope to bridge one day. It still hurts, that things important to me were superseded, but they <em>were</em> superseded, and it is better to know than to remain in the dark.</p>\n<p>When I was fourteen, I saw many horrors laid out before us: war, corruption, environmental destruction, and the silent tragedies of automobile accidents, courtroom injustices, and death by disease and aging. All around me, I saw a society that couldn't coordinate, full of people resigned to unnecessary fates.</p>\n<p>I was told to settle for making a small difference. I resolved to do the opposite.</p>\n<p>I made a promise to myself. I didn't promise to fix governments: that was a means to an end, a convenient solution for someone who didn't know how to look further out. I didn't promise to change the world, either: every little thing is a change, and not all changes are good. No, I promised to&nbsp;<em>save</em>&nbsp;the world.</p>\n<p>That promise still stands.</p>\n<p>The world sure as hell isn't going to save itself.</p>", "sections": [{"title": "Gaining Confidence", "anchor": "Gaining_Confidence", "level": 1}, {"title": "Shattered Illusions", "anchor": "Shattered_Illusions", "level": 1}, {"title": "Deciding to Save the World", "anchor": "Deciding_to_Save_the_World", "level": 1}, {"title": "Broadening Scope", "anchor": "Broadening_Scope", "level": 1}, {"title": "Communication Failures", "anchor": "Communication_Failures", "level": 1}, {"title": "Phase Two", "anchor": "Phase_Two", "level": 1}, {"title": "Superseded", "anchor": "Superseded", "level": 1}, {"title": "Disclaimer", "anchor": "Disclaimer", "level": 1}, {"title": "The source of my passion", "anchor": "The_source_of_my_passion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "171 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 171, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uX3HjXo6BWos3Zgy5", "srwKRt9TsS5oxvJsh", "yFALNnscB2qgehnJv", "4DBBQkEQvNEWafkek"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 21, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-31T04:12:34.529Z", "modifiedAt": null, "url": null, "title": "Amanda Knox Guilty Again", "slug": "amanda-knox-guilty-again", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "christopherj", "createdAt": "2013-09-16T02:54:31.052Z", "isAdmin": false, "displayName": "christopherj"}, "userId": "pj8G6Wr3vaZz7defL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A4wM3y5ZZJtsZJXMY/amanda-knox-guilty-again", "pageUrlRelative": "/posts/A4wM3y5ZZJtsZJXMY/amanda-knox-guilty-again", "linkUrl": "https://www.lesswrong.com/posts/A4wM3y5ZZJtsZJXMY/amanda-knox-guilty-again", "postedAtFormatted": "Friday, January 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Amanda%20Knox%20Guilty%20Again&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmanda%20Knox%20Guilty%20Again%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA4wM3y5ZZJtsZJXMY%2Famanda-knox-guilty-again%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Amanda%20Knox%20Guilty%20Again%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA4wM3y5ZZJtsZJXMY%2Famanda-knox-guilty-again", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA4wM3y5ZZJtsZJXMY%2Famanda-knox-guilty-again", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today an Italian court has declared that <a href=\"http://www.cnn.com/2014/01/30/world/europe/italy-amanda-knox-retrial/\">Amanda Knox is, once again, guilty</a>. She did not attend that trial (is not required to in Italy), so her final verdict will be either by appeal to the Supreme Court of Italy or the US extradition court. Extradition requests might be impeded due to the fact US does not have double jeopardy.</p>\n<p>Previously on LessWrong, in <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">The Amanda Knox Test: How an Hour on the Internet Beats a Year in the Courtroom</a> there was some complaint that it actually took more than an hour on the internet to thoroughly research the case. Of course, the courts have been at this since 2007...</p>\n<p>Her co-defendant, Raffaele Sollecito, who did show up at the trial, got sentenced to 25 years, but I don't know for sure where he is now because apparently he's totally unimportant and who cares (the media's opinion, not mine). I'm fairly sure he's in Italy though. So far it seems the plan is to revoke his passport but not arrest him.</p>\n<p>Anyone want to take their hand at making predictions?</p>\n<ol>\n<li>Will the final appeal find Amanda Knox and/or Raffaele Sollecito innocent or guilty?</li>\n<li>When will the trial end? edit: I mean the inevitable appeal</li>\n<li>If convicted, will the US extradite Amanda Knox?</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A4wM3y5ZZJtsZJXMY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 11, "extendedScore": null, "score": 1.5398517559240023e-06, "legacy": true, "legacyId": "25402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G9dptrW9CJi7wNg3b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-31T05:20:33.359Z", "modifiedAt": null, "url": null, "title": "Meetup (West LA) : Virtual Evidence", "slug": "meetup-west-la-virtual-evidence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:13.257Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abramdemski", "createdAt": "2009-03-12T06:07:25.510Z", "isAdmin": false, "displayName": "abramdemski"}, "userId": "Q7NW4XaWQmfPfdcFj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uftz48etkKT6Xicbt/meetup-west-la-virtual-evidence", "pageUrlRelative": "/posts/uftz48etkKT6Xicbt/meetup-west-la-virtual-evidence", "linkUrl": "https://www.lesswrong.com/posts/uftz48etkKT6Xicbt/meetup-west-la-virtual-evidence", "postedAtFormatted": "Friday, January 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20(West%20LA)%20%3A%20Virtual%20Evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20(West%20LA)%20%3A%20Virtual%20Evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuftz48etkKT6Xicbt%2Fmeetup-west-la-virtual-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20(West%20LA)%20%3A%20Virtual%20Evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuftz48etkKT6Xicbt%2Fmeetup-west-la-virtual-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuftz48etkKT6Xicbt%2Fmeetup-west-la-virtual-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/wd\">Virtual Evidence</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">05 February 2014 07:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\"> 10850 West Pico Blvd, Los Angeles, CA 90064, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong>How to get in</strong>: Go to the <a rel=\"nofollow\" href=\"https://www.google.com/maps/preview/place/westside+tavern/@34.039811,-118.4288888,17z/data=!3m1!4b1!4m2!3m1!1s0x80c2bba2d2dbb76f:0x814aca2e2e7249c1\">Westside Tavern</a> in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\". Parking is free for 3 hours.</p>\n<p><strong>Discussion</strong>: If we ask an expert to judge, purely based on their observations and not on prior expectations, whether a rug is a genuine antique, and we get the answer \"Based only on what I could observe, I'd say 5 to 1 it's genuine\", how should we revise our beliefs? We could assume that 5/6 is the posterior probability which the expert arrives at, but this is in bad faith with respect to our request to ignore the prior and infer based on observation alone. In \"Probabilistic Reasoning in Intelligent Systems\", Judea Pearl discusses a solution to this problem called \"virtual evidence\". I'll approach this issue from a perspective of how we should treat opinions offered to us in everyday conversation, and relate it to the general idea of approximate Bayesian updates. The discussion may be more toward the mathematical side, or more toward the practical side, depending on the audience response.</p>\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</em></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/wd\">Virtual Evidence</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uftz48etkKT6Xicbt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 1.539928404521347e-06, "legacy": true, "legacyId": "25403", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Virtual_Evidence\">Discussion article for the meetup : <a href=\"/meetups/wd\">Virtual Evidence</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">05 February 2014 07:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\"> 10850 West Pico Blvd, Los Angeles, CA 90064, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong>How to get in</strong>: Go to the <a rel=\"nofollow\" href=\"https://www.google.com/maps/preview/place/westside+tavern/@34.039811,-118.4288888,17z/data=!3m1!4b1!4m2!3m1!1s0x80c2bba2d2dbb76f:0x814aca2e2e7249c1\">Westside Tavern</a> in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\". Parking is free for 3 hours.</p>\n<p><strong>Discussion</strong>: If we ask an expert to judge, purely based on their observations and not on prior expectations, whether a rug is a genuine antique, and we get the answer \"Based only on what I could observe, I'd say 5 to 1 it's genuine\", how should we revise our beliefs? We could assume that 5/6 is the posterior probability which the expert arrives at, but this is in bad faith with respect to our request to ignore the prior and infer based on observation alone. In \"Probabilistic Reasoning in Intelligent Systems\", Judea Pearl discusses a solution to this problem called \"virtual evidence\". I'll approach this issue from a perspective of how we should treat opinions offered to us in everyday conversation, and relate it to the general idea of approximate Bayesian updates. The discussion may be more toward the mathematical side, or more toward the practical side, depending on the audience response.</p>\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</em></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Virtual_Evidence1\">Discussion article for the meetup : <a href=\"/meetups/wd\">Virtual Evidence</a></h2>", "sections": [{"title": "Discussion article for the meetup : Virtual Evidence", "anchor": "Discussion_article_for_the_meetup___Virtual_Evidence", "level": 1}, {"title": "Discussion article for the meetup : Virtual Evidence", "anchor": "Discussion_article_for_the_meetup___Virtual_Evidence1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-31T17:02:38.521Z", "modifiedAt": null, "url": null, "title": "New LW Meetups: Hamburg, Ljubljana", "slug": "new-lw-meetups-hamburg-ljubljana", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mZRDDoygm35Nq3kmL/new-lw-meetups-hamburg-ljubljana", "pageUrlRelative": "/posts/mZRDDoygm35Nq3kmL/new-lw-meetups-hamburg-ljubljana", "linkUrl": "https://www.lesswrong.com/posts/mZRDDoygm35Nq3kmL/new-lw-meetups-hamburg-ljubljana", "postedAtFormatted": "Friday, January 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetups%3A%20Hamburg%2C%20Ljubljana&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetups%3A%20Hamburg%2C%20Ljubljana%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZRDDoygm35Nq3kmL%2Fnew-lw-meetups-hamburg-ljubljana%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetups%3A%20Hamburg%2C%20Ljubljana%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZRDDoygm35Nq3kmL%2Fnew-lw-meetups-hamburg-ljubljana", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZRDDoygm35Nq3kmL%2Fnew-lw-meetups-hamburg-ljubljana", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 550, "htmlBody": "<p><strong>This summary was posted to LW Main on January 24th. The following week's summary is <a href=\"/lw/jlp/new_lw_meetup_canberra/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/vz\">Meetup: First Meetup in Hamburg, Germany:&nbsp;<span class=\"date\">07 February 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/v5\"></a><a href=\"/meetups/vs\">Ljubljana:&nbsp;<span class=\"date\">25 January 2014 05:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/w0\">Atlanta Meetup, Topic: How to Become Immortal:&nbsp;<span class=\"date\">26 January 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/w2\">Frankfurt meetup:&nbsp;<span class=\"date\">09 February 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/w3\">Moscow, Another Meet up:&nbsp;<span class=\"date\">26 January 2014 04:00PM</span></a> <a href=\"/meetups/vk\"></a></li>\n<li><a href=\"/meetups/vt\">Pittsburgh Meetup 24 January 2014 06:00 PM:&nbsp;<span class=\"date\">24 January 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/w4\">Sydney Meetup: February:&nbsp;<span class=\"date\">26 February 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/vn\">Tel Aviv: Uses of Cryptography:&nbsp;<span class=\"date\">30 January 2014 08:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/w5\">Berkeley: Talk on communication:&nbsp;<span class=\"date\">29 January 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/w6\">Community Weekend in Berlin:&nbsp;<span class=\"date\">11 April 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/vo\">Brussels: Morality - also cake:&nbsp;<span class=\"date\">08 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/vv\">[Melbourne] February Rationality Dojo: Planning Fallacy:&nbsp;<span class=\"date\">02 February 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mZRDDoygm35Nq3kmL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.540720407942767e-06, "legacy": true, "legacyId": "25354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5kYBbziQHBqyeMSMW", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-31T17:24:38.810Z", "modifiedAt": null, "url": null, "title": "Practical Benefits of Rationality (LW Census Results)", "slug": "practical-benefits-of-rationality-lw-census-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:29.701Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GJFYAHDQ73ictpJME/practical-benefits-of-rationality-lw-census-results", "pageUrlRelative": "/posts/GJFYAHDQ73ictpJME/practical-benefits-of-rationality-lw-census-results", "linkUrl": "https://www.lesswrong.com/posts/GJFYAHDQ73ictpJME/practical-benefits-of-rationality-lw-census-results", "postedAtFormatted": "Friday, January 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Practical%20Benefits%20of%20Rationality%20(LW%20Census%20Results)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APractical%20Benefits%20of%20Rationality%20(LW%20Census%20Results)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGJFYAHDQ73ictpJME%2Fpractical-benefits-of-rationality-lw-census-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Practical%20Benefits%20of%20Rationality%20(LW%20Census%20Results)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGJFYAHDQ73ictpJME%2Fpractical-benefits-of-rationality-lw-census-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGJFYAHDQ73ictpJME%2Fpractical-benefits-of-rationality-lw-census-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5271, "htmlBody": "<p><em>by Dan from <a href=\"http://rationality.org/\">CFAR</a></em></p>\n<p>&nbsp;</p>\n<p><strong>Abstract</strong>: Two measures of the practical benefits of rationality, one a self-report of the benefits of being part of the rationality community and the other a measure of how often a person adds useful techniques to their repertoire, were included on the 2013 Less Wrong survey. In-person involvement with LW/CFAR predicted both measures of benefits, with friendships with LWers and attending a CFAR workshop showing the strongest and most consistent effects. Online Less Wrong participation and background had weaker and less consistent effects. Growth mindset also independently predicted both measures of practical benefits, and on the measure of technique acquisition there was an interaction effect suggesting that in-person LW/CFAR involvement may be especially beneficial for people high in growth mindset. However, some caution is warranted in interpreting these correlational, self-report results.</p>\n<p>&nbsp;</p>\n<h2>Introduction</h2>\n<p>Though I first found Less Wrong through my habit of reading interesting blogs, the main reason why I've gotten more and more involved in the rationality community is my suspicion that this rationality stuff might be pretty useful. Useful not only for thinking clearly about tricky intellectual topics, but also in ways that have more directly practical benefits.<br /><br />CFAR obviously has similar interests, as <a href=\"/lw/jej/why_cfar/\">it aims</a> to create a community of people who are effective at acting in the world.</p>\n<p>The <a href=\"/lw/jj0/2013_survey_results/\">2013 LW census/survey</a> provided an opportunity for us to probe how the rationality community is doing so far at finding these practical benefits, as it allowed us to survey a large cross section of the Less Wrong community. Unfortunately, there is not a standard, simple measure of practical benefits which we could just stick on the survey, and we were only able to use a correlational research design, but we sought to get some relevant information by coming up with two self-report questions to include on the survey.</p>\n<p>One question was somewhat broader than the set of practical benefits that we were interested in and the other was somewhat narrower. First, there was a broad self-report question asking people how much they had benefited from being involved in the rationality community. Second, we asked people more narrowly how often they successfully added a useful technique or approach to their repertoire. We were primarily interested in seeing whether involvement in the LW community would predict practical benefits on these two measures, and (if so) which forms of involvement would have the strongest relationship to these benefits.</p>\n<p>About 1400 people answered the relevant survey questions, including about 400 who have read the sequences, about 150 who regularly attend LW meetups, about 100 who have attended a full CFAR workshop, about 100 who interact with other LWers in person all the time, and about 50 who met a romantic partner through LW.&nbsp; The survey also included a brief scale measuring <a href=\"http://alumni.stanford.edu/get/page/magazine/article/?article_id=32124\">growth mindset</a>, and a question about age. <br /><br />Some methodological notes: In the body of this post I&rsquo;ve tried to put the results in a format that&rsquo;s relatively straightforward to interpret. More technical details and additional analyses are included in footnotes, and I can add more details in the comments. Note that the study design is entirely correlational, and the questions are all self-report (unlike <a href=\"/lw/fuv/participation_in_the_lw_community_associated_with/\">last year&rsquo;s questions</a>, which included tests of standard biases). This gives some reason for caution in interpreting the results, and I&rsquo;ll note some places where that is especially relevant.</p>\n<p>&nbsp;</p>\n<h2>Background &amp; Survey Design</h2>\n<p>The simple, obvious thing to do, in order to investigate how much people have benefited from their involvement in the rationality community, is to ask them that question. So we did: \"How much have you benefited from your exposure to and participation in the rationality community (Less Wrong, CFAR, in-person contact with LW/HPMOR readers, etc.)?\" There were 7 response options, which we can scale as -3 to +3, where +3 is &ldquo;My life is MUCH BETTER than it would have been without exposure to the rationality community&rdquo; (and -3 is &ldquo;... MUCH WORSE&hellip;&rdquo;).<br /><br />This straightforward question has a couple of straightforward limitations. For one, we might expect people who are involved in almost any activity to say that they benefit from it; self-reported benefit does not necessarily indicate actual benefit. Second, it could include a broad range of benefits, some of which might not have much to do with the usefulness of rationality (such as meeting your current romantic partner at a Less Wrong meetup). So we also included a narrower question related to competence which is less susceptible to these issues.<br /><br />A simple model of how people are able to become highly competent/productive/successful/impressive individuals is that they try lots of things and keep doing the ones that work. A person&rsquo;s work habits, the questions they ask during conversations, the methods that they use to make certain kinds of decisions, and many other things can all be developed through a similar iterative process. Over time, someone who has a good process in place of trying things &amp; sticking with the helpful ones will end up collecting a large set of habits/techniques/approaches/principles/etc. which work for them.<br /><br />The second set of questions which we included on the survey were based on this process, with the aim of measuring about how often people add a new useful technique to their repertoire. There were 3 survey questions based on a streamlined version of this process: first you hear about many different techniques, then you try some fraction of the techniques that you hear about, and then some fraction of the techniques that you try end up working for you and sticking as part of your repertoire. We first asked &ldquo;On average, about how often do you *read or hear about* another plausible-seeming technique or approach for being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?&rdquo;, then &ldquo;...how often do you *try out* another plausible-seeming technique..&rdquo;, and finally &ldquo;...how often do you find another technique or approach that *successfully helps you at*...&rdquo;&nbsp; This final question, about how frequently people acquire a new helpful technique, is our other main outcome measure of practical benefits.<br /><br />In reality, people often generate their own ideas of techniques to try, and try many variations rather than just a single thing (e.g., many people end up with their own <a href=\"/lw/jf1/one_year_of_pomodoros/\">personalized version of the pomodoro technique</a>). Focusing on the streamlined process of hear &rarr; try &rarr; acquire is a simplification which had two survey-specific benefits.&nbsp; First, having the context of &ldquo;hearing about a technique and then trying it&rdquo; was intended to make it clearer what to count as &ldquo;a technique,&rdquo; which is important since the outcome measure is a count of the number of techniques acquired. Second, including the &ldquo;hearing&rdquo; and &ldquo;trying&rdquo; questions allows us to probe this process in a bit more detail by (for example) breaking down the number of new techniques that a person acquired into two components: the number of new techniques that they tried and the hit rate (techniques acquired divided by techniques tried).<br /><br />One other predictor variable which we included on the survey was a 4-item measure of <a href=\"http://alumni.stanford.edu/get/page/magazine/article/?article_id=32124\">growth mindset</a>, which was taken from Carol Dweck&rsquo;s research (sample item: &ldquo;No matter what kind of person you are, you can always change substantially&rdquo;).<a name=\"footnote1back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote1\">[1]</a> A fixed mindset involves thinking that personal characteristics are fixed and unchangeable - you either have them or you don&rsquo;t - while a growth mindset involves thinking that personal characteristics can change as a person grows and develops. Dweck and her colleagues have found that growth mindset about a characteristic tends to be associated with more productive behaviors and more improvement over time. For example, children with a growth mindset about being good at thinking tend to seek out intellectual challenges which stretch their abilities, while children with a fixed mindset tend to avoid tasks that they might fail at and seek tasks which they know they can do well.<br /><br />A blog based on the idea of becoming less wrong sounds like it would reflect growth mindset more than fixed mindset, and many aspects of the local idea cluster seem to match that. Ideas like: there are systematic methods that you can learn which will allow you to form more accurate models of the world. Complex skills can be <a href=\"/lw/5p6/how_and_why_to_granularize/\">broken down</a> into simple trainable components. <a href=\"http://www.paulgraham.com/identity.html\">Don't get too attached</a> to a particular image of who you are and what you stand for.&nbsp; Mastering the right cognitive toolkit can make you more effective at accomplishing the things that you care about. <a href=\"http://wiki.lesswrong.com/wiki/Tsuyoku_Naritai\">Tsuyoku Naritai</a>! In addition to these connections to LW thinking, growth mindset also seems like it could facilitate the process of becoming more successful by trying out various changes to the way that you do things and sticking with the ones that work. Thus, we wanted to investigate whether people who were more involved in the rationality community (in various ways) had more of a growth mindset, and whether people with more of a growth mindset reported more practical benefits.<br /><br />The other main predictor variables were several different indicators of people's involvement in the rationality community:<br /><br /><em>LW background</em><br />A composite scale, which standardized and then averaged together four questions which all indicate a person&rsquo;s amount of background with the lesswrong.com website (and which, as I found on <a href=\"/lw/fuv/participation_in_the_lw_community_associated_with/\">previous</a> years' <a href=\"/lw/8p4/2011_survey_results/5dzg\">surveys</a>, all correlate with each other and show similar patterns of relationships with other variables). The four questions measured: having read the sequences (ranging from 1 &ldquo;Never even knew they existed until this moment&rdquo; to 7 &ldquo;[Read] All or nearly all of the Sequences&rdquo;), karma (log-transformed), LW Use (ranging from 1 &ldquo;I lurk, but never registered an account&rdquo; to 5 &ldquo;I've posted in Main&rdquo;), and length of time in the community (capped at 8 years).<br /><br /><em>Time per day on Less Wrong</em><br />&ldquo;How long, in approximate number of minutes, do you spend on Less Wrong in the average day?&rdquo; (log-transformed).<br /><br /><em>Meetup attendance</em><br />&ldquo;Do you attend Less Wrong meetup?&rdquo;<br />&ldquo;Yes, regularly,&rdquo; &ldquo;Yes, once or a few times,&rdquo; or &ldquo;No&rdquo; (categorical variable).<br /><br /><em>CFAR workshop attendance</em><br />Have you ever attended a CFAR workshop?<br />&ldquo;Yes, I have been to a full (3+ day) workshop,&rdquo; &ldquo;I have been to at least one CFAR class, but not a full (3+ day) workshop,&rdquo; or &ldquo;No&rdquo; (categorical variable).<br /><br /><em>LW friendships</em><br />&ldquo;Is physical interaction with the Less Wrong community otherwise a part of your everyday life, for example you live with other Less Wrongers, or you are close friends and frequently go out with them?&rdquo;<br />&ldquo;Yes, all the time,&rdquo; &ldquo;Yes, sometimes,&rdquo; or &ldquo;No&rdquo; (categorical variable).<br /><br /><em>LW romantic partner</em><br />Have you ever been in a romantic relationship with someone you met through the Less Wrong community?<br />&ldquo;Yes,&rdquo; &ldquo;I didn't meet them through the community, but they're part of the community now,&rdquo; or &ldquo;No&rdquo; (categorical variable).<br /><br />I considered combining these four measures of in-person involvement with the LW community (LW meetups, CFAR workshops, LW friendships, and LW romantic partners) into a single scale of in-person LW involvement, but there ended up being a large enough sample size within these groups and strong enough effects for me to analyze them separately.<br /><br />Respondents also reported their age (which was transformed by taking the square root).</p>\n<p>&nbsp;</p>\n<h2>Results</h2>\n<p><strong>I. Self-reported Benefit</strong><br /><em>\"How much have you benefited from your exposure to and participation in the rationality community (Less Wrong, CFAR, in-person contact with LW/HPMOR readers, etc.)?\"</em></p>\n<p><br />The average response to this question was a 1.4 on a -3 to +3 scale (SD = 1.08), and 15% of people selected the scale maximum &ldquo;My life is MUCH BETTER than it would have been without exposure to the rationality community.&rdquo;</p>\n<p>Which variables were associated with a larger self-reported benefit from the rationality community?</p>\n<p>In short, all of them.</p>\n<p>Each of the following variables was significantly related to this self-reported measure of benefit, and in a regression which controlled for the other variables all of them remained significant except for meetup attendance (which became p = 0.07).<a name=\"footnote2back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote2\">[2]</a> For ease of interpretation, I have reported the percent of people in each of the following groups who selected the scale maximum.&nbsp; I have sorted the variables in order of effect size, from largest to smallest, based on the results of the regression (see the footnote for more details).</p>\n<p>Percent of people in each subgroup answering &ldquo;My life is MUCH BETTER than it would have been without exposure to the rationality community&rdquo;<br /><br />61% LW romantic partner (n = 54)<br />44% attended a full CFAR workshop (n = 100)<br />19% age 25 or less (younger people reported more benefit) (n = 724)<br />50% LW friendships (n = 88)<br />28% above 3.0 on growth mindset scale (n = 277)<br />25% high LW background (n = 137)<br />35% regularly attend meetups (n = 156)<br />31% acquire a new technique every 3 weeks or more often (n = 213)<br />18% use LW for 30+ min per day (n = 218)<br />15% all respondents (n = 1451)<br /><br />Three noteworthy results:</p>\n<ul>\n<li>Each of the variables related to involvement in the rationality community was associated with reports of getting more benefit from the community. </li>\n<li>The strongest effects came from people who were involved in fairly intensive, in-person activities: finding a romantic partner through LW, attending a full CFAR workshop, and being around other LWers in person all the time.</li>\n<li>Three variables which were not directly related to community involvement &ndash; younger age, growth mindset, and acquiring new techniques &ndash; were all predictive of self-reported benefit from the rationality community.</li>\n</ul>\n<p>One interpretation of these results is that getting involved in the rationality community causes people to acquire useful rationality skills which improve their lives, with larger effects for people who get involved in more depth through close relationships, shared housing, CFAR workshops, etc. However, as noted above, these effects could also be due to non-rationality-related benefits (e.g., finding friends or a romantic partner), a tendency to say nice things about activities &amp; communities that you're a part of, or causal effects in the other direction (e.g., people who benefited the most from the Less Wrong website might be especially likely to attend a CFAR workshop or move into shared housing with other LWers).</p>\n<p><br />It is worth noting that growth mindset and acquiring new techniques were both predictive of larger benefit from the rationality community even though neither variable is directly related to involvement in the community.&nbsp; That makes these effects less open to some of the alternative explanations which could account for the community involvement effects and provides some validation of the self-report measure of benefits, although other causal paths are still a possibility (e.g., people who have changed more since they started reading LW may have come to have more of a growth mindset and also report more benefits).</p>\n<p><br /><strong>II. Acquiring New Techniques</strong></p>\n<p><em>\"On average, about how often do you find another technique or approach that successfully helps you at being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?\"</em></p>\n<p>The average response was a 2.23 (SD = 1.31) on a 1 to 8 scale where 2 is &ldquo;About once every six months&rdquo; and 3 is &ldquo;About once every 2 months.&rdquo;&nbsp; This can be interpreted more intuitively as acquiring one new technique every 146 days (as a geometric mean).<a name=\"footnote3back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote3\">[3]</a></p>\n<p>Which variables were associated with acquiring useful techniques more often?</p>\n<p>Only some of them.</p>\n<p>LW friendships and CFAR workshop attendance again had significant effects. The other two forms of in-person LW involvement, LW meetups and LW romantic partner, were also predictive of acquiring more techniques, but those effects did not remain significant in a regression controlling for the other variables. Time per day on Less Wrong had a weaker but reliable positive relationship with acquiring new techniques, while LW background had a significant relationship in the opposite direction: people with more LW background acquired fewer techniques. Younger age and growth mindset were again predictive of more benefit.</p>\n<p>Based on the results of a regression, here is the number of days per new technique acquired (sorted by effect size, smaller numbers indicate faster technique acquisition).<a name=\"footnote4back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote4\">[4]</a> In this list, both the number of days given and the order of the list reflect the results of the regression which controls statistically for the other predictor variables. (* = p &lt; .05, ** = p &lt; .01).</p>\n<p>85 days: LW friendships *<br />87 days: Age (younger) **<br />95 days: Attended a full CFAR workshop **<br />114 days: LW romantic partner (p = .21)<br />118 days: Growth mindset **<br />174 days: LW background (negative effect) **<br />131 days: Time per day on Less Wrong&nbsp; **<br />151 days: Regularly attend meetups (p = .63)<br />146 days: all respondents<br /><br />The pattern that was apparent on the self-report measure of benefit from the rationality community &ndash; that in-person interactions were more predictive of benefits than online participation &ndash; was even stronger on this measure. Attending a CFAR workshop and LW friendships had the largest effects, and these effects seem to be cumulative. People who both attended a full CFAR workshop and interacted with LW friends &ldquo;all the time&rdquo; (n = 39) acquired a new technique every 45 days on average, while people who had no in-person interaction with LWers by any of the 4 variables (n = 824) acquired a new technique every 165 days.</p>\n<p>Some of the alternative explanations for the effects on self-reported benefit seem less plausible here. For example, it seems less likely that people who have LW friendships would say that they try and acquire more new techniques out of a general tendency to say nice things about communities that you're a part of. Alternative causal paths are still a clear possibility, though. People who tend to try more things may be more likely to go to LW meetups, sign up for CFAR workshops, or move to a city where they can hang out in person with people from their favorite website.</p>\n<p><br /><strong>III. The Process of Trying &amp; Acquiring New Techniques</strong></p>\n<p><em>&ldquo;On average, about how often do you *read or hear about* another plausible-seeming technique or approach for being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?&rdquo;<br />&ldquo;On average, about how often do you *try out* another plausible-seeming technique or approach for being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?&rdquo;</em></p>\n<p>On average, people heard about a new technique every 12 days and tried a new technique about every 55 days. That means that (at least according to the streamlined model: hear &rarr; try &rarr; acquire) people tried about 22% of the techniques that they heard about, and added about 36% of the techniques that they tried to their repertoire.<a name=\"footnote5back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote5\">[5]</a></p>\n<p>Breaking down acquiring techniques into its two components, techniques tried and hit rate (techniques acquired divided by techniques tried) all of the effects discussed above involving acquiring techniques appear to be due to trying techniques, and not to the hit rate.&nbsp; None of the variables discussed here were predictive of hit rate, and the variables that predicted acquiring techniques were similarly predictive of trying techniques (though in most cases the effect was slightly weaker).&nbsp; In particular, trying techniques predicted self-reported benefit from the rationality community, and people with more LW background tried fewer techniques. People who both attended a full CFAR workshop and interacted with LW friends &ldquo;all the time&rdquo; (n = 39) tried a new technique every 13 days, while people who reported no in-person interaction with LWers (n = 849) tried a new technique every 65 days.</p>\n<p>These data provide some evidence that, if CFAR workshops, LW friendships, growth mindset, and time on Less Wrong cause people to acquire more techniques, a substantial portion of the effect comes from getting people to <a href=\"/lw/jgy/try_more_things/\">try more things</a> (and not just getting them to be more effective at trying the things that they already have been trying).</p>\n<p>However, these data do not clearly pin down is different about people's process of trying things. One might expect that hit rate reflects how good a person is at choosing what to try and actually trying it (in a way that makes useful techniques likely to stick), so the lack of effect on hit rate indicates that the difference is just in trying more things. But if someone improved at the process of trying things, becoming more efficient at getting useful-for-them techniques to stick and setting aside the not-useful-for-them techniques, then that might show up primarily as an increase in number of techniques tried (as they cycle through the try things process more rapidly &amp; more frequently). Or, a person who lowers their threshold for what techniques to try might start trying five times as many things and finding twice as many that work for them, which would show up as a drop in their hit rate (they'd also be adding useful techniques to their repertoire twice as fast).<a name=\"footnote6back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote6\">[6]</a></p>\n<p><br /><strong></strong></p>\n<p><strong>IV. Growth Mindset</strong><br /><br />Sample item:<em> &ldquo;You can do things differently, but the important parts of who you are can't really be changed&rdquo; </em>(reverse-scored).</p>\n<p>Growth mindset &ndash; seeing important parts of yourself as malleable, and focusing on what you can do to improve &ndash; seems like it could be related to the process of benefiting from the rationality community in multiple ways.&nbsp; Here are three:</p>\n<ol>\n<li>People with more of a growth mindset might tend try more things, acquire more useful rationality techniques, get more practical benefits out of the things they do.</li>\n<li>Being involved in the rationality community might cause people to shift towards a growth mindset from a fixed mindset.</li>\n<li>Relatively intensive involvement in the rationality community (such as living in a house with other LWers, or attending a CFAR workshop) might provide a bigger benefit to people with more of a growth mindset.</li>\n</ol>\n<p>Item 1 is what we've been looking at in the analysis of acquiring new techniques and self-reported benefit, with growth mindset as one of the predictor variables.&nbsp; The hypothesis is that people who score higher in growth mindset will report more benefit on those measures, and the data support that hypothesis (though these correlational results are also consistent with alternative causal hypotheses).</p>\n<p>Item 2 identifies a hypothesis which treats growth mindset as an outcome variable instead of a predictor variable: do people who regularly attend LW meetups have more of a growth mindset? Or those who have more LW background, or who have attended a CFAR workshop, or who have LW friends, etc.? This hypothesis is relatively straightforward to examine with this data set, although the correlational design leaves it an open question whether involvement in the LW community led to a growth mindset or whether having a growth mindset led to people getting more involved in the LW community.</p>\n<p>When looking at one variable at a time, each of the measures of in-person involvement in the LW community is significantly predictive of growth mindset. In order of effect size (given in Cohen's d, which counts standard deviations), growth mindset was predicted separately by LW romantic partner (d = 0.42), attending a CFAR workshop (d = 0.21), LW friendships (d = 0.20), and regularly attending meetups (d = 0.15). However, when controlling for the other predictor variables, only having a LW romantic partner remained statistically significant (d = 0.46, p = .03) and attending a CFAR workshop remained marginally significant (d = 0.18, p = .07); LW friendships and meetup attendance became nonsignificant (d &lt; 0.10, p &gt; 0.3).</p>\n<p>LW background showed the opposite pattern: it was not related to growth mindset on its own (r = -0.04, p = .13), but it became a highly significant predictor of lower growth mindset when controlling for the other variables related to LW involvement (r = -0.11, p &lt; .01). One plausible causal story that could explain this pattern of correlations is that people who are high in growth mindset who get involved in the website are more likely to also get involved in other in-person ways, while those lower in growth mindset are more likely to just stick with the website. This would lead to the negative relationship LW background and growth mindset when controlling for in-person LW involvement. According to this causal story, growth mindset is a cause of in-person LW involvement rather than a consequence.</p>\n<p>Younger age was the strongest predictor of growth mindset, whether controlling for other variables (r = -0.15, p &lt; .01) or not (r = -0.19, p &lt; 0.01), and time per day on Less Wrong was not a significant predictor.</p>\n<p>Item 3 from the list predicts an interaction effect between growth mindset and involvement in LW: the benefit of greater involvement in the LW community will be stronger among people high in growth mindset (or, equivalently, the benefit of growth mindset will be stronger among people who are more involved in the LW community). This hypothesis is particularly interesting because this interaction effect seems more plausible under the causal model where LW involvement and growth mindset both cause greater practical benefits than it does under the alternative causal theory that competence or a tendency to try things causes in-person LW involvement.</p>\n<p>When predicting self-reported benefit from the rationality there was no sign of these interaction effects, whether looking at the predictor variables one at a time or including them all in a multiple regression. Growth mindset was an equally strong predictor of self-reported benefit for people who are closely involved in the LW community (by each of the various measures) and for people who are less closely involved in the LW community.</p>\n<p>When predicting acquiring new techniques, these interaction effects were significant in several cases.<a name=\"footnote7back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote7\">[7]</a> A growth mindset was associated more strongly with acquiring among techniques among people who regularly attend LW meetups (p = .003), people who are younger (p = .005), people who have attended a CFAR workshop (p = .04), and (with marginal statistical significance) among people with LW friendships (p = .06).&nbsp; In a multiple regression that included each of these variables, none of these interaction effects was individually statistically significant except the age x growth mindset interaction (presumably because of the various forms of LW involvement were all associated with each other, making it difficult to tease apart their effects).<a name=\"footnote8back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote8\">[8]</a></p>\n<p>These results are consistent with the model that the various forms of in-person involvement with the rationality community are especially helpful at producing practical benefits for people who are high in growth mindset.</p>\n<p>&nbsp;</p>\n<h2>Conclusion</h2>\n<p>With this correlational research design there is a limit to how well we can distinguish the hypothesis that LW involvement leads to benefits from other causal stories, but each of the three main variables that we examined were related to in-person LW involvement in ways that were consistent with this hypothesis.</p>\n<p>People who have been involved with the in-person LW/CFAR community were especially likely to indicate that their life is better due to the LW community. They tended to report that they tried out and acquired new useful techniques more frequently, especially if they were also high in growth mindset. If spending time with LWers or attending a CFAR workshop leads people to try more rationality-related techniques, find more things that work well for them, and reap the benefits, then these are the results that we would expect to see.</p>\n<p>&nbsp;</p>\n<h2>Footnotes</h2>\n<p><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote1back\">[1]</a> <a name=\"footnote1\"></a> The 4 mindset questions on the survey were taken from Dweck's book <em>Mindset </em>(p. 13). These questions and others like them have been used to measure mindset in many published studies. Many of the questions that have been used focus more narrowly on mindset about intellectual ability, while these four questions deal more broadly with personal qualities.<br /><br /><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote2back\">[2]</a> <a name=\"footnote2\"></a> Unless otherwise noted, all reported effects are significant both in tests with only the single predictor variable and also in tests which controlled for the other predictor variables. A regression was run predicting benefit based on the LW involvement variables and age (growth mindset and acquiring new techniques were not controlled for, since they could be consequences of LW involvement which mediate the benefit). Though all three levels of the categorical variables were included in the regression, the effect size used to order the variables in the list was calculated as the standardized difference in least square means between the highest level of the group (e.g., regularly attend meetups) and the lowest level (e.g., never attend meetups), leaving out intermediate levels (e.g., occasionally attend meetups). To estimate the effect size of continuous variables, the correlation coefficient was translated into an equivalent standardized mean difference by the formula d = 2r/sqrt(1-r<sup>2</sup>).<br /><br /><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote3back\">[3]</a> <a name=\"footnote3\"></a> The 8 response options were coded as a 1-8 scale, which was used for all analyses. Each scale point indicates a 3-4x multiplier in how often a person acquires new techniques. This 8-point scale can be interpreted as a log scale for the variable \"days per technique acquired\" (they are associated approximately by the equation 7*3^(5-x)) so a mean on this scale is equivalent to the geometric mean of the number of days. For example, a 3.5 on the 8-point scale translates into 36 days, which is the geometric mean of 21 days (a 4 on the scale) and 63 days (approximately a 3 on the scale).<br /><br /><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote4back\">[4]</a> <a name=\"footnote4\"></a> For categorical variables, the number of days is based on the least squares mean for the highest level of the group (e.g., regularly attend meetups). For continuous variables, it is based on the regression equation predicting the values one standard deviation above the mean of the predictor variable.<br /><br /><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote5back\">[5]</a> <a name=\"footnote5\"></a> On the 8 point scale, &ldquo;heard about&rdquo; has mean = 4.48 (SD = 1.62) and &ldquo;tried&rdquo; has mean = 3.12 (SD = 1.56).&nbsp; Rate of trying is simply &ldquo;trying&rdquo; minus &ldquo;heard about,&rdquo; mean = -1.37 (SD = 1.42), and hit rate had scale mean = -0.94 (SD = 0.84). These numbers can also be interpreted as being on a log base 3 scale, so -1 on the hit rate scale corresponds to an actual hit rate of 1/3 (1 technique acquired for every 3 techniques tried).<br /><br /><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote6back\">[6]</a> <a name=\"footnote6\"></a> Trying techniques can be further broken down into two components, hearing about techniques and percentage tried (techniques tried divided by techniques heard about). The data suggest that both are relevant, but they are harder to tease apart with the limited statistical power of this data set. <br /><br /><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote7back\">[7]</a> <a name=\"footnote7\"></a> When looking at a single categorical variable, I only looked at the highest level of the group and the lowest level, leaving out the intermediate level. For example, I tested whether growth mindset was more strongly related to acquiring techniques among people who regularly attend meetups than among people who never attend meetups (leaving out the group that occasionally attends meetups). In the regression including all predictor variables, I included the intermediate level groups (since otherwise it would have been necessary to exclude the data of anyone who was in an intermediate level group on any of the variables). <br /><br /><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote8back\">[8]</a> <a name=\"footnote8\"></a> When I combined the four variables related to in-person involvement into a single composite scale (scoring the highest level of involvement on each variable as a 2 and the lowest level as a 0), the interaction between growth mindset and this in-person involvement scale was statistically significant in a multiple regression predicting techniques acquired (p &lt; .01).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GJFYAHDQ73ictpJME", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 31, "extendedScore": null, "score": 1.5407452425651634e-06, "legacy": true, "legacyId": "25404", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>by Dan from <a href=\"http://rationality.org/\">CFAR</a></em></p>\n<p>&nbsp;</p>\n<p><strong>Abstract</strong>: Two measures of the practical benefits of rationality, one a self-report of the benefits of being part of the rationality community and the other a measure of how often a person adds useful techniques to their repertoire, were included on the 2013 Less Wrong survey. In-person involvement with LW/CFAR predicted both measures of benefits, with friendships with LWers and attending a CFAR workshop showing the strongest and most consistent effects. Online Less Wrong participation and background had weaker and less consistent effects. Growth mindset also independently predicted both measures of practical benefits, and on the measure of technique acquisition there was an interaction effect suggesting that in-person LW/CFAR involvement may be especially beneficial for people high in growth mindset. However, some caution is warranted in interpreting these correlational, self-report results.</p>\n<p>&nbsp;</p>\n<h2 id=\"Introduction\">Introduction</h2>\n<p>Though I first found Less Wrong through my habit of reading interesting blogs, the main reason why I've gotten more and more involved in the rationality community is my suspicion that this rationality stuff might be pretty useful. Useful not only for thinking clearly about tricky intellectual topics, but also in ways that have more directly practical benefits.<br><br>CFAR obviously has similar interests, as <a href=\"/lw/jej/why_cfar/\">it aims</a> to create a community of people who are effective at acting in the world.</p>\n<p>The <a href=\"/lw/jj0/2013_survey_results/\">2013 LW census/survey</a> provided an opportunity for us to probe how the rationality community is doing so far at finding these practical benefits, as it allowed us to survey a large cross section of the Less Wrong community. Unfortunately, there is not a standard, simple measure of practical benefits which we could just stick on the survey, and we were only able to use a correlational research design, but we sought to get some relevant information by coming up with two self-report questions to include on the survey.</p>\n<p>One question was somewhat broader than the set of practical benefits that we were interested in and the other was somewhat narrower. First, there was a broad self-report question asking people how much they had benefited from being involved in the rationality community. Second, we asked people more narrowly how often they successfully added a useful technique or approach to their repertoire. We were primarily interested in seeing whether involvement in the LW community would predict practical benefits on these two measures, and (if so) which forms of involvement would have the strongest relationship to these benefits.</p>\n<p>About 1400 people answered the relevant survey questions, including about 400 who have read the sequences, about 150 who regularly attend LW meetups, about 100 who have attended a full CFAR workshop, about 100 who interact with other LWers in person all the time, and about 50 who met a romantic partner through LW.&nbsp; The survey also included a brief scale measuring <a href=\"http://alumni.stanford.edu/get/page/magazine/article/?article_id=32124\">growth mindset</a>, and a question about age. <br><br>Some methodological notes: In the body of this post I\u2019ve tried to put the results in a format that\u2019s relatively straightforward to interpret. More technical details and additional analyses are included in footnotes, and I can add more details in the comments. Note that the study design is entirely correlational, and the questions are all self-report (unlike <a href=\"/lw/fuv/participation_in_the_lw_community_associated_with/\">last year\u2019s questions</a>, which included tests of standard biases). This gives some reason for caution in interpreting the results, and I\u2019ll note some places where that is especially relevant.</p>\n<p>&nbsp;</p>\n<h2 id=\"Background___Survey_Design\">Background &amp; Survey Design</h2>\n<p>The simple, obvious thing to do, in order to investigate how much people have benefited from their involvement in the rationality community, is to ask them that question. So we did: \"How much have you benefited from your exposure to and participation in the rationality community (Less Wrong, CFAR, in-person contact with LW/HPMOR readers, etc.)?\" There were 7 response options, which we can scale as -3 to +3, where +3 is \u201cMy life is MUCH BETTER than it would have been without exposure to the rationality community\u201d (and -3 is \u201c... MUCH WORSE\u2026\u201d).<br><br>This straightforward question has a couple of straightforward limitations. For one, we might expect people who are involved in almost any activity to say that they benefit from it; self-reported benefit does not necessarily indicate actual benefit. Second, it could include a broad range of benefits, some of which might not have much to do with the usefulness of rationality (such as meeting your current romantic partner at a Less Wrong meetup). So we also included a narrower question related to competence which is less susceptible to these issues.<br><br>A simple model of how people are able to become highly competent/productive/successful/impressive individuals is that they try lots of things and keep doing the ones that work. A person\u2019s work habits, the questions they ask during conversations, the methods that they use to make certain kinds of decisions, and many other things can all be developed through a similar iterative process. Over time, someone who has a good process in place of trying things &amp; sticking with the helpful ones will end up collecting a large set of habits/techniques/approaches/principles/etc. which work for them.<br><br>The second set of questions which we included on the survey were based on this process, with the aim of measuring about how often people add a new useful technique to their repertoire. There were 3 survey questions based on a streamlined version of this process: first you hear about many different techniques, then you try some fraction of the techniques that you hear about, and then some fraction of the techniques that you try end up working for you and sticking as part of your repertoire. We first asked \u201cOn average, about how often do you *read or hear about* another plausible-seeming technique or approach for being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?\u201d, then \u201c...how often do you *try out* another plausible-seeming technique..\u201d, and finally \u201c...how often do you find another technique or approach that *successfully helps you at*...\u201d&nbsp; This final question, about how frequently people acquire a new helpful technique, is our other main outcome measure of practical benefits.<br><br>In reality, people often generate their own ideas of techniques to try, and try many variations rather than just a single thing (e.g., many people end up with their own <a href=\"/lw/jf1/one_year_of_pomodoros/\">personalized version of the pomodoro technique</a>). Focusing on the streamlined process of hear \u2192 try \u2192 acquire is a simplification which had two survey-specific benefits.&nbsp; First, having the context of \u201chearing about a technique and then trying it\u201d was intended to make it clearer what to count as \u201ca technique,\u201d which is important since the outcome measure is a count of the number of techniques acquired. Second, including the \u201chearing\u201d and \u201ctrying\u201d questions allows us to probe this process in a bit more detail by (for example) breaking down the number of new techniques that a person acquired into two components: the number of new techniques that they tried and the hit rate (techniques acquired divided by techniques tried).<br><br>One other predictor variable which we included on the survey was a 4-item measure of <a href=\"http://alumni.stanford.edu/get/page/magazine/article/?article_id=32124\">growth mindset</a>, which was taken from Carol Dweck\u2019s research (sample item: \u201cNo matter what kind of person you are, you can always change substantially\u201d).<a name=\"footnote1back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote1\">[1]</a> A fixed mindset involves thinking that personal characteristics are fixed and unchangeable - you either have them or you don\u2019t - while a growth mindset involves thinking that personal characteristics can change as a person grows and develops. Dweck and her colleagues have found that growth mindset about a characteristic tends to be associated with more productive behaviors and more improvement over time. For example, children with a growth mindset about being good at thinking tend to seek out intellectual challenges which stretch their abilities, while children with a fixed mindset tend to avoid tasks that they might fail at and seek tasks which they know they can do well.<br><br>A blog based on the idea of becoming less wrong sounds like it would reflect growth mindset more than fixed mindset, and many aspects of the local idea cluster seem to match that. Ideas like: there are systematic methods that you can learn which will allow you to form more accurate models of the world. Complex skills can be <a href=\"/lw/5p6/how_and_why_to_granularize/\">broken down</a> into simple trainable components. <a href=\"http://www.paulgraham.com/identity.html\">Don't get too attached</a> to a particular image of who you are and what you stand for.&nbsp; Mastering the right cognitive toolkit can make you more effective at accomplishing the things that you care about. <a href=\"http://wiki.lesswrong.com/wiki/Tsuyoku_Naritai\">Tsuyoku Naritai</a>! In addition to these connections to LW thinking, growth mindset also seems like it could facilitate the process of becoming more successful by trying out various changes to the way that you do things and sticking with the ones that work. Thus, we wanted to investigate whether people who were more involved in the rationality community (in various ways) had more of a growth mindset, and whether people with more of a growth mindset reported more practical benefits.<br><br>The other main predictor variables were several different indicators of people's involvement in the rationality community:<br><br><em>LW background</em><br>A composite scale, which standardized and then averaged together four questions which all indicate a person\u2019s amount of background with the lesswrong.com website (and which, as I found on <a href=\"/lw/fuv/participation_in_the_lw_community_associated_with/\">previous</a> years' <a href=\"/lw/8p4/2011_survey_results/5dzg\">surveys</a>, all correlate with each other and show similar patterns of relationships with other variables). The four questions measured: having read the sequences (ranging from 1 \u201cNever even knew they existed until this moment\u201d to 7 \u201c[Read] All or nearly all of the Sequences\u201d), karma (log-transformed), LW Use (ranging from 1 \u201cI lurk, but never registered an account\u201d to 5 \u201cI've posted in Main\u201d), and length of time in the community (capped at 8 years).<br><br><em>Time per day on Less Wrong</em><br>\u201cHow long, in approximate number of minutes, do you spend on Less Wrong in the average day?\u201d (log-transformed).<br><br><em>Meetup attendance</em><br>\u201cDo you attend Less Wrong meetup?\u201d<br>\u201cYes, regularly,\u201d \u201cYes, once or a few times,\u201d or \u201cNo\u201d (categorical variable).<br><br><em>CFAR workshop attendance</em><br>Have you ever attended a CFAR workshop?<br>\u201cYes, I have been to a full (3+ day) workshop,\u201d \u201cI have been to at least one CFAR class, but not a full (3+ day) workshop,\u201d or \u201cNo\u201d (categorical variable).<br><br><em>LW friendships</em><br>\u201cIs physical interaction with the Less Wrong community otherwise a part of your everyday life, for example you live with other Less Wrongers, or you are close friends and frequently go out with them?\u201d<br>\u201cYes, all the time,\u201d \u201cYes, sometimes,\u201d or \u201cNo\u201d (categorical variable).<br><br><em>LW romantic partner</em><br>Have you ever been in a romantic relationship with someone you met through the Less Wrong community?<br>\u201cYes,\u201d \u201cI didn't meet them through the community, but they're part of the community now,\u201d or \u201cNo\u201d (categorical variable).<br><br>I considered combining these four measures of in-person involvement with the LW community (LW meetups, CFAR workshops, LW friendships, and LW romantic partners) into a single scale of in-person LW involvement, but there ended up being a large enough sample size within these groups and strong enough effects for me to analyze them separately.<br><br>Respondents also reported their age (which was transformed by taking the square root).</p>\n<p>&nbsp;</p>\n<h2 id=\"Results\">Results</h2>\n<p><strong>I. Self-reported Benefit</strong><br><em>\"How much have you benefited from your exposure to and participation in the rationality community (Less Wrong, CFAR, in-person contact with LW/HPMOR readers, etc.)?\"</em></p>\n<p><br>The average response to this question was a 1.4 on a -3 to +3 scale (SD = 1.08), and 15% of people selected the scale maximum \u201cMy life is MUCH BETTER than it would have been without exposure to the rationality community.\u201d</p>\n<p>Which variables were associated with a larger self-reported benefit from the rationality community?</p>\n<p>In short, all of them.</p>\n<p>Each of the following variables was significantly related to this self-reported measure of benefit, and in a regression which controlled for the other variables all of them remained significant except for meetup attendance (which became p = 0.07).<a name=\"footnote2back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote2\">[2]</a> For ease of interpretation, I have reported the percent of people in each of the following groups who selected the scale maximum.&nbsp; I have sorted the variables in order of effect size, from largest to smallest, based on the results of the regression (see the footnote for more details).</p>\n<p>Percent of people in each subgroup answering \u201cMy life is MUCH BETTER than it would have been without exposure to the rationality community\u201d<br><br>61% LW romantic partner (n = 54)<br>44% attended a full CFAR workshop (n = 100)<br>19% age 25 or less (younger people reported more benefit) (n = 724)<br>50% LW friendships (n = 88)<br>28% above 3.0 on growth mindset scale (n = 277)<br>25% high LW background (n = 137)<br>35% regularly attend meetups (n = 156)<br>31% acquire a new technique every 3 weeks or more often (n = 213)<br>18% use LW for 30+ min per day (n = 218)<br>15% all respondents (n = 1451)<br><br>Three noteworthy results:</p>\n<ul>\n<li>Each of the variables related to involvement in the rationality community was associated with reports of getting more benefit from the community. </li>\n<li>The strongest effects came from people who were involved in fairly intensive, in-person activities: finding a romantic partner through LW, attending a full CFAR workshop, and being around other LWers in person all the time.</li>\n<li>Three variables which were not directly related to community involvement \u2013 younger age, growth mindset, and acquiring new techniques \u2013 were all predictive of self-reported benefit from the rationality community.</li>\n</ul>\n<p>One interpretation of these results is that getting involved in the rationality community causes people to acquire useful rationality skills which improve their lives, with larger effects for people who get involved in more depth through close relationships, shared housing, CFAR workshops, etc. However, as noted above, these effects could also be due to non-rationality-related benefits (e.g., finding friends or a romantic partner), a tendency to say nice things about activities &amp; communities that you're a part of, or causal effects in the other direction (e.g., people who benefited the most from the Less Wrong website might be especially likely to attend a CFAR workshop or move into shared housing with other LWers).</p>\n<p><br>It is worth noting that growth mindset and acquiring new techniques were both predictive of larger benefit from the rationality community even though neither variable is directly related to involvement in the community.&nbsp; That makes these effects less open to some of the alternative explanations which could account for the community involvement effects and provides some validation of the self-report measure of benefits, although other causal paths are still a possibility (e.g., people who have changed more since they started reading LW may have come to have more of a growth mindset and also report more benefits).</p>\n<p><br><strong>II. Acquiring New Techniques</strong></p>\n<p><em>\"On average, about how often do you find another technique or approach that successfully helps you at being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?\"</em></p>\n<p>The average response was a 2.23 (SD = 1.31) on a 1 to 8 scale where 2 is \u201cAbout once every six months\u201d and 3 is \u201cAbout once every 2 months.\u201d&nbsp; This can be interpreted more intuitively as acquiring one new technique every 146 days (as a geometric mean).<a name=\"footnote3back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote3\">[3]</a></p>\n<p>Which variables were associated with acquiring useful techniques more often?</p>\n<p>Only some of them.</p>\n<p>LW friendships and CFAR workshop attendance again had significant effects. The other two forms of in-person LW involvement, LW meetups and LW romantic partner, were also predictive of acquiring more techniques, but those effects did not remain significant in a regression controlling for the other variables. Time per day on Less Wrong had a weaker but reliable positive relationship with acquiring new techniques, while LW background had a significant relationship in the opposite direction: people with more LW background acquired fewer techniques. Younger age and growth mindset were again predictive of more benefit.</p>\n<p>Based on the results of a regression, here is the number of days per new technique acquired (sorted by effect size, smaller numbers indicate faster technique acquisition).<a name=\"footnote4back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote4\">[4]</a> In this list, both the number of days given and the order of the list reflect the results of the regression which controls statistically for the other predictor variables. (* = p &lt; .05, ** = p &lt; .01).</p>\n<p>85 days: LW friendships *<br>87 days: Age (younger) **<br>95 days: Attended a full CFAR workshop **<br>114 days: LW romantic partner (p = .21)<br>118 days: Growth mindset **<br>174 days: LW background (negative effect) **<br>131 days: Time per day on Less Wrong&nbsp; **<br>151 days: Regularly attend meetups (p = .63)<br>146 days: all respondents<br><br>The pattern that was apparent on the self-report measure of benefit from the rationality community \u2013 that in-person interactions were more predictive of benefits than online participation \u2013 was even stronger on this measure. Attending a CFAR workshop and LW friendships had the largest effects, and these effects seem to be cumulative. People who both attended a full CFAR workshop and interacted with LW friends \u201call the time\u201d (n = 39) acquired a new technique every 45 days on average, while people who had no in-person interaction with LWers by any of the 4 variables (n = 824) acquired a new technique every 165 days.</p>\n<p>Some of the alternative explanations for the effects on self-reported benefit seem less plausible here. For example, it seems less likely that people who have LW friendships would say that they try and acquire more new techniques out of a general tendency to say nice things about communities that you're a part of. Alternative causal paths are still a clear possibility, though. People who tend to try more things may be more likely to go to LW meetups, sign up for CFAR workshops, or move to a city where they can hang out in person with people from their favorite website.</p>\n<p><br><strong>III. The Process of Trying &amp; Acquiring New Techniques</strong></p>\n<p><em>\u201cOn average, about how often do you *read or hear about* another plausible-seeming technique or approach for being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?\u201d<br>\u201cOn average, about how often do you *try out* another plausible-seeming technique or approach for being more rational / more productive / happier / having better social relationships / having more accurate beliefs / etc.?\u201d</em></p>\n<p>On average, people heard about a new technique every 12 days and tried a new technique about every 55 days. That means that (at least according to the streamlined model: hear \u2192 try \u2192 acquire) people tried about 22% of the techniques that they heard about, and added about 36% of the techniques that they tried to their repertoire.<a name=\"footnote5back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote5\">[5]</a></p>\n<p>Breaking down acquiring techniques into its two components, techniques tried and hit rate (techniques acquired divided by techniques tried) all of the effects discussed above involving acquiring techniques appear to be due to trying techniques, and not to the hit rate.&nbsp; None of the variables discussed here were predictive of hit rate, and the variables that predicted acquiring techniques were similarly predictive of trying techniques (though in most cases the effect was slightly weaker).&nbsp; In particular, trying techniques predicted self-reported benefit from the rationality community, and people with more LW background tried fewer techniques. People who both attended a full CFAR workshop and interacted with LW friends \u201call the time\u201d (n = 39) tried a new technique every 13 days, while people who reported no in-person interaction with LWers (n = 849) tried a new technique every 65 days.</p>\n<p>These data provide some evidence that, if CFAR workshops, LW friendships, growth mindset, and time on Less Wrong cause people to acquire more techniques, a substantial portion of the effect comes from getting people to <a href=\"/lw/jgy/try_more_things/\">try more things</a> (and not just getting them to be more effective at trying the things that they already have been trying).</p>\n<p>However, these data do not clearly pin down is different about people's process of trying things. One might expect that hit rate reflects how good a person is at choosing what to try and actually trying it (in a way that makes useful techniques likely to stick), so the lack of effect on hit rate indicates that the difference is just in trying more things. But if someone improved at the process of trying things, becoming more efficient at getting useful-for-them techniques to stick and setting aside the not-useful-for-them techniques, then that might show up primarily as an increase in number of techniques tried (as they cycle through the try things process more rapidly &amp; more frequently). Or, a person who lowers their threshold for what techniques to try might start trying five times as many things and finding twice as many that work for them, which would show up as a drop in their hit rate (they'd also be adding useful techniques to their repertoire twice as fast).<a name=\"footnote6back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote6\">[6]</a></p>\n<p><br><strong></strong></p>\n<p><strong>IV. Growth Mindset</strong><br><br>Sample item:<em> \u201cYou can do things differently, but the important parts of who you are can't really be changed\u201d </em>(reverse-scored).</p>\n<p>Growth mindset \u2013 seeing important parts of yourself as malleable, and focusing on what you can do to improve \u2013 seems like it could be related to the process of benefiting from the rationality community in multiple ways.&nbsp; Here are three:</p>\n<ol>\n<li>People with more of a growth mindset might tend try more things, acquire more useful rationality techniques, get more practical benefits out of the things they do.</li>\n<li>Being involved in the rationality community might cause people to shift towards a growth mindset from a fixed mindset.</li>\n<li>Relatively intensive involvement in the rationality community (such as living in a house with other LWers, or attending a CFAR workshop) might provide a bigger benefit to people with more of a growth mindset.</li>\n</ol>\n<p>Item 1 is what we've been looking at in the analysis of acquiring new techniques and self-reported benefit, with growth mindset as one of the predictor variables.&nbsp; The hypothesis is that people who score higher in growth mindset will report more benefit on those measures, and the data support that hypothesis (though these correlational results are also consistent with alternative causal hypotheses).</p>\n<p>Item 2 identifies a hypothesis which treats growth mindset as an outcome variable instead of a predictor variable: do people who regularly attend LW meetups have more of a growth mindset? Or those who have more LW background, or who have attended a CFAR workshop, or who have LW friends, etc.? This hypothesis is relatively straightforward to examine with this data set, although the correlational design leaves it an open question whether involvement in the LW community led to a growth mindset or whether having a growth mindset led to people getting more involved in the LW community.</p>\n<p>When looking at one variable at a time, each of the measures of in-person involvement in the LW community is significantly predictive of growth mindset. In order of effect size (given in Cohen's d, which counts standard deviations), growth mindset was predicted separately by LW romantic partner (d = 0.42), attending a CFAR workshop (d = 0.21), LW friendships (d = 0.20), and regularly attending meetups (d = 0.15). However, when controlling for the other predictor variables, only having a LW romantic partner remained statistically significant (d = 0.46, p = .03) and attending a CFAR workshop remained marginally significant (d = 0.18, p = .07); LW friendships and meetup attendance became nonsignificant (d &lt; 0.10, p &gt; 0.3).</p>\n<p>LW background showed the opposite pattern: it was not related to growth mindset on its own (r = -0.04, p = .13), but it became a highly significant predictor of lower growth mindset when controlling for the other variables related to LW involvement (r = -0.11, p &lt; .01). One plausible causal story that could explain this pattern of correlations is that people who are high in growth mindset who get involved in the website are more likely to also get involved in other in-person ways, while those lower in growth mindset are more likely to just stick with the website. This would lead to the negative relationship LW background and growth mindset when controlling for in-person LW involvement. According to this causal story, growth mindset is a cause of in-person LW involvement rather than a consequence.</p>\n<p>Younger age was the strongest predictor of growth mindset, whether controlling for other variables (r = -0.15, p &lt; .01) or not (r = -0.19, p &lt; 0.01), and time per day on Less Wrong was not a significant predictor.</p>\n<p>Item 3 from the list predicts an interaction effect between growth mindset and involvement in LW: the benefit of greater involvement in the LW community will be stronger among people high in growth mindset (or, equivalently, the benefit of growth mindset will be stronger among people who are more involved in the LW community). This hypothesis is particularly interesting because this interaction effect seems more plausible under the causal model where LW involvement and growth mindset both cause greater practical benefits than it does under the alternative causal theory that competence or a tendency to try things causes in-person LW involvement.</p>\n<p>When predicting self-reported benefit from the rationality there was no sign of these interaction effects, whether looking at the predictor variables one at a time or including them all in a multiple regression. Growth mindset was an equally strong predictor of self-reported benefit for people who are closely involved in the LW community (by each of the various measures) and for people who are less closely involved in the LW community.</p>\n<p>When predicting acquiring new techniques, these interaction effects were significant in several cases.<a name=\"footnote7back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote7\">[7]</a> A growth mindset was associated more strongly with acquiring among techniques among people who regularly attend LW meetups (p = .003), people who are younger (p = .005), people who have attended a CFAR workshop (p = .04), and (with marginal statistical significance) among people with LW friendships (p = .06).&nbsp; In a multiple regression that included each of these variables, none of these interaction effects was individually statistically significant except the age x growth mindset interaction (presumably because of the various forms of LW involvement were all associated with each other, making it difficult to tease apart their effects).<a name=\"footnote8back\"></a><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote8\">[8]</a></p>\n<p>These results are consistent with the model that the various forms of in-person involvement with the rationality community are especially helpful at producing practical benefits for people who are high in growth mindset.</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>With this correlational research design there is a limit to how well we can distinguish the hypothesis that LW involvement leads to benefits from other causal stories, but each of the three main variables that we examined were related to in-person LW involvement in ways that were consistent with this hypothesis.</p>\n<p>People who have been involved with the in-person LW/CFAR community were especially likely to indicate that their life is better due to the LW community. They tended to report that they tried out and acquired new useful techniques more frequently, especially if they were also high in growth mindset. If spending time with LWers or attending a CFAR workshop leads people to try more rationality-related techniques, find more things that work well for them, and reap the benefits, then these are the results that we would expect to see.</p>\n<p>&nbsp;</p>\n<h2 id=\"Footnotes\">Footnotes</h2>\n<p><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote1back\">[1]</a> <a name=\"footnote1\"></a> The 4 mindset questions on the survey were taken from Dweck's book <em>Mindset </em>(p. 13). These questions and others like them have been used to measure mindset in many published studies. Many of the questions that have been used focus more narrowly on mindset about intellectual ability, while these four questions deal more broadly with personal qualities.<br><br><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote2back\">[2]</a> <a name=\"footnote2\"></a> Unless otherwise noted, all reported effects are significant both in tests with only the single predictor variable and also in tests which controlled for the other predictor variables. A regression was run predicting benefit based on the LW involvement variables and age (growth mindset and acquiring new techniques were not controlled for, since they could be consequences of LW involvement which mediate the benefit). Though all three levels of the categorical variables were included in the regression, the effect size used to order the variables in the list was calculated as the standardized difference in least square means between the highest level of the group (e.g., regularly attend meetups) and the lowest level (e.g., never attend meetups), leaving out intermediate levels (e.g., occasionally attend meetups). To estimate the effect size of continuous variables, the correlation coefficient was translated into an equivalent standardized mean difference by the formula d = 2r/sqrt(1-r<sup>2</sup>).<br><br><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote3back\">[3]</a> <a name=\"footnote3\"></a> The 8 response options were coded as a 1-8 scale, which was used for all analyses. Each scale point indicates a 3-4x multiplier in how often a person acquires new techniques. This 8-point scale can be interpreted as a log scale for the variable \"days per technique acquired\" (they are associated approximately by the equation 7*3^(5-x)) so a mean on this scale is equivalent to the geometric mean of the number of days. For example, a 3.5 on the 8-point scale translates into 36 days, which is the geometric mean of 21 days (a 4 on the scale) and 63 days (approximately a 3 on the scale).<br><br><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote4back\">[4]</a> <a name=\"footnote4\"></a> For categorical variables, the number of days is based on the least squares mean for the highest level of the group (e.g., regularly attend meetups). For continuous variables, it is based on the regression equation predicting the values one standard deviation above the mean of the predictor variable.<br><br><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote5back\">[5]</a> <a name=\"footnote5\"></a> On the 8 point scale, \u201cheard about\u201d has mean = 4.48 (SD = 1.62) and \u201ctried\u201d has mean = 3.12 (SD = 1.56).&nbsp; Rate of trying is simply \u201ctrying\u201d minus \u201cheard about,\u201d mean = -1.37 (SD = 1.42), and hit rate had scale mean = -0.94 (SD = 0.84). These numbers can also be interpreted as being on a log base 3 scale, so -1 on the hit rate scale corresponds to an actual hit rate of 1/3 (1 technique acquired for every 3 techniques tried).<br><br><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote6back\">[6]</a> <a name=\"footnote6\"></a> Trying techniques can be further broken down into two components, hearing about techniques and percentage tried (techniques tried divided by techniques heard about). The data suggest that both are relevant, but they are harder to tease apart with the limited statistical power of this data set. <br><br><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote7back\">[7]</a> <a name=\"footnote7\"></a> When looking at a single categorical variable, I only looked at the highest level of the group and the lowest level, leaving out the intermediate level. For example, I tested whether growth mindset was more strongly related to acquiring techniques among people who regularly attend meetups than among people who never attend meetups (leaving out the group that occasionally attends meetups). In the regression including all predictor variables, I included the intermediate level groups (since otherwise it would have been necessary to exclude the data of anyone who was in an intermediate level group on any of the variables). <br><br><a href=\"/lw/jlo/practical_benefits_of_rationality_lw_census/#footnote8back\">[8]</a> <a name=\"footnote8\"></a> When I combined the four variables related to in-person involvement into a single composite scale (scoring the highest level of involvement on each variable as a 2 and the lowest level as a 0), the interaction between growth mindset and this in-person involvement scale was statistically significant in a multiple regression predicting techniques acquired (p &lt; .01).</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Background & Survey Design", "anchor": "Background___Survey_Design", "level": 1}, {"title": "Results", "anchor": "Results", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JjGs6mDZxeCWkg3ii", "pJJdcZgB6mPNWoSWr", "2odrQi8cxKvTu6v7s", "vXwjg5PpwSTfuHJ2k", "jTk9m75y2bpujwRfb", "ZzCxs2AFThcTfFeKr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-31T21:35:37.593Z", "modifiedAt": null, "url": null, "title": "Best of Rationality Quotes, 2013 Edition", "slug": "best-of-rationality-quotes-2013-edition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:59.411Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KC2CjmmKM4jLZGA4L/best-of-rationality-quotes-2013-edition", "pageUrlRelative": "/posts/KC2CjmmKM4jLZGA4L/best-of-rationality-quotes-2013-edition", "linkUrl": "https://www.lesswrong.com/posts/KC2CjmmKM4jLZGA4L/best-of-rationality-quotes-2013-edition", "postedAtFormatted": "Friday, January 31st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20of%20Rationality%20Quotes%2C%202013%20Edition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20of%20Rationality%20Quotes%2C%202013%20Edition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKC2CjmmKM4jLZGA4L%2Fbest-of-rationality-quotes-2013-edition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20of%20Rationality%20Quotes%2C%202013%20Edition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKC2CjmmKM4jLZGA4L%2Fbest-of-rationality-quotes-2013-edition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKC2CjmmKM4jLZGA4L%2Fbest-of-rationality-quotes-2013-edition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Here is the 2013 edition of the Best of Rationality Quotes collection. (<a href=\"/lw/ggp/best_of_rationality_quotes_2012_edition/\">Here is last year's</a>.)</p>\n<p><strong><a href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes_2013/rq_only2013.html\">Best of Rationality Quotes 2013</a>&nbsp;</strong>(400kB page, 350 quotes)<br />and&nbsp;<strong><a href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes_2013/rq.html\">Best of Rationality Quotes 2009-2013</a></strong>&nbsp;(1600kB page, 1490 quotes)</p>\n<p>The page was built by a short script (<a href=\"http://people.mokk.bme.hu/~daniel/rationality_quotes_2013/\">source code here</a>) from all the LW Rationality Quotes threads so far. (We had such a thread each month since April 2009.) The script collects all comments with karma score 10 or more, and sorts them by score. Replies are not collected, only top-level comments.</p>\n<p>As is now usual, I provide various statistics and top-lists based on the data. (Source code for these is also at the above link, see the README.) I added these as comments to the post:</p>\n<ul>\n<li><a href=\"/r/lesswrong/lw/jlq/best_of_rationality_quotes_2013_edition/ah7b\">Top quote contributors by total karma score collected</a></li>\n<li><a href=\"/r/lesswrong/lw/jlq/best_of_rationality_quotes_2013_edition/ah7d\">Top quote contributors by karma score collected in 2013</a></li>\n<li><a href=\"/r/lesswrong/lw/jlq/best_of_rationality_quotes_2013_edition/ah7e\">Top quote contributors of 2013 by statistical significance level</a>&nbsp;(See&nbsp;<a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37ej\">this comment</a>&nbsp;for a description of this metric.)</li>\n<li><a href=\"/r/lesswrong/lw/jlq/best_of_rationality_quotes_2013_edition/ah7f\">Top original authors by number of quotes</a></li>\n<li><a href=\"/r/lesswrong/lw/jlq/best_of_rationality_quotes_2013_edition/ah7j\">Top original authors by total karma score collected</a></li>\n<li><a href=\"/r/lesswrong/lw/jlq/best_of_rationality_quotes_2013_edition/ah7m\">Best short quotes 2009-2013</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KC2CjmmKM4jLZGA4L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 43, "extendedScore": null, "score": 0.000132, "legacy": true, "legacyId": "25406", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zaWnu3PxP4YTYiuCm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-01T00:22:55.741Z", "modifiedAt": null, "url": null, "title": "Open Problems in FAI", "slug": "open-problems-in-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.490Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abramdemski", "createdAt": "2009-03-12T06:07:25.510Z", "isAdmin": false, "displayName": "abramdemski"}, "userId": "Q7NW4XaWQmfPfdcFj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ru8kpjSzzCvE3KH5Z/open-problems-in-fai", "pageUrlRelative": "/posts/ru8kpjSzzCvE3KH5Z/open-problems-in-fai", "linkUrl": "https://www.lesswrong.com/posts/ru8kpjSzzCvE3KH5Z/open-problems-in-fai", "postedAtFormatted": "Saturday, February 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Problems%20in%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Problems%20in%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fru8kpjSzzCvE3KH5Z%2Fopen-problems-in-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Problems%20in%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fru8kpjSzzCvE3KH5Z%2Fopen-problems-in-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fru8kpjSzzCvE3KH5Z%2Fopen-problems-in-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 845, "htmlBody": "<p>Edit: Please note that this write-up is poor quality, having the style of a hastily written personal note.</p>\n<p>It has been mentioned that there should be a better write-up of open problems in FAI, and as I understand it there is an ongoing effort to explain such open problems. My feeling has been that the recent effort has tended too hold off proposing solutions for too long. I prefer the approach in the Tiling Agents paper, which explained problems through example systems which fail in various respects. What follows is an outline of what I'd write if I spent significant time; I think it is enough to be of some use. This list very much reflects my personal interests &amp; beliefs.</p>\n<p>&nbsp;</p>\n<p>- Tarski's Undefinability Theorem</p>\n<p>&nbsp; - We would like a system to be able to reason about itself (in a few critical ways), and Tarski's Theorem is one of the important obstacles. Kripke provided the first hope of progress in this area by showing that we can embed a partial truth predicate in a language if we accept a \"gap\" (statements which cannot be assessed as true or false within the system's self-theory). Work over the decades has \"reduced the gap\" (capturing an increasing number of the self-judgements we want, while always leaving a gap). There are also \"glut\" theories (which must assess some things as both true and false), which typically mirror gap theories. Paul Christiano provided a theory of probabilistic self-reference which intuitively reduces the \"gap\" to infinitesimal size: the system's knowledge about its own probabilities can be wrong, but only by an infinitesimal. (For example, if it believes X, then it may fail to believe P(X)=1, but it will still believe P(X)&gt;c for all c&lt;1.) (Note, this feels a bit like a \"glut\" theory since the system solves the problem by saying too much rather than remaining silent.)</p>\n<p>- First Incompleteness Theorem</p>\n<p>&nbsp; - Logical Uncertainty:</p>\n<p>&nbsp; &nbsp; - First-Order (\"easy\"): assigning probabilities to eventual results of (halting) computations we don't have time to make. I claim this is mostly solved by the FOL prior: we can prefer simpler hypotheses about the behavior of systems, treating computations as black boxes which we use universal induction to predict, while allowing us to incorporate logical reasoning about the function's behavior via bayesian updates. It also solves somewhat more; I claim it will have better properties than Solomonoff if the environment contains objects like halting oracles. (Some deficiencies with respect to reasoning about halting will be mentioned in the next section, however.)</p>\n<p>&nbsp; &nbsp; - Second-Order (\"impossible\"): If we want to assign probabilities to programs halting, facts of number theory, or facts of set theory, we're in serious trouble. Using the FOL prior admits nonstandard models. It's not yet clear what qualities such a probability distribution should have. It seems reasonable to want universal statements to approach probability 1 as the set of positive examples approaches all the examples; this turns out to be as difficult to compute as all the bits in the arithmetic hierarchy. I thought it would be reasonable to restrict this to just the universal statements about computable predicates; ie, halting facts &amp; equivalent. Will Sawin proved that it is not possible for our beliefs about halting to approach arbitrarily close to the correct values without some false Sigma_2 statements approaching arbitrarily close to 1. It remains an open problem to construct such a prior. My proposal is to (focusing on sentences in the regular forms Pi_n or Sigma_n) require that we only introduce a quantified statement into a theory if a statement of the same form already exists; so sentences at a given level in the arithmetic hierarchy must wait for a sentence at the next lowest level to be introduced. This does not block any true sentences from being produced, but causes halting facts to converge as we eliminate possible halting times. It is an open problem whether this proposed distribution converges. If this distribution exists, call it the bad arithmetic prior (BAP).</p>\n<p>- Second Incompleteness Theorem</p>\n<p>&nbsp; - Lobian Obstacle:</p>\n<p>&nbsp; &nbsp; For a machine to plan its actions in the future, it needs to trust itself. The second incompleteness theorem (and, more generally, Lob's theorem) makes this difficult. (All this is insufficiently formal, but the tiling agents paper gives a good explanation.) Several partial solutions have been proposed in a deterministic setting. It is an open problem whether one of Dan Willard's several self-verifying systems solves this problem. (He had multiple proposals...) In case there is no purely logical solution, it seems intuitively promising to look for probabilistic self-trust. Difficulties are already presented by the previous section. If the BAP converges, then we can show that it has self-knowledge of that convergence of the form Paul Christiano described! This makes the false Sigma_2 beliefs feel more acceptable, because it is a necessary feature of the system's self-reference. However, I think it's the case that BAP ends up converging to 1 for *all* sigma_2 statements, which is really terrible.</p>\n<p>&nbsp; - Anti-Lobian Obstacle:</p>\n<p>&nbsp; &nbsp; In case the Lobian obstacle is solved, the anti-lobian obstacle may be a concern.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ru8kpjSzzCvE3KH5Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 1.541217450973437e-06, "legacy": true, "legacyId": "25407", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-01T18:39:25.445Z", "modifiedAt": null, "url": null, "title": "Learn (and Maybe Get a Credential in) Data Science", "slug": "learn-and-maybe-get-a-credential-in-data-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PJNz7xDqWNTpWTGtB/learn-and-maybe-get-a-credential-in-data-science", "pageUrlRelative": "/posts/PJNz7xDqWNTpWTGtB/learn-and-maybe-get-a-credential-in-data-science", "linkUrl": "https://www.lesswrong.com/posts/PJNz7xDqWNTpWTGtB/learn-and-maybe-get-a-credential-in-data-science", "postedAtFormatted": "Saturday, February 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learn%20(and%20Maybe%20Get%20a%20Credential%20in)%20Data%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearn%20(and%20Maybe%20Get%20a%20Credential%20in)%20Data%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJNz7xDqWNTpWTGtB%2Flearn-and-maybe-get-a-credential-in-data-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learn%20(and%20Maybe%20Get%20a%20Credential%20in)%20Data%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJNz7xDqWNTpWTGtB%2Flearn-and-maybe-get-a-credential-in-data-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJNz7xDqWNTpWTGtB%2Flearn-and-maybe-get-a-credential-in-data-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 582, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Coursera\">Coursera</a> is now offering a <a href=\"https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage\">sequence</a> of online courses on data science. They include:</p>\n<p>1. <a href=\"https://www.coursera.org/course/datascitoolbox\">The Data Scientist's Toolbox</a></p>\n<blockquote>\n<div>Upon completion of this course you  will be able to identify and classify data science problems. You will  also have created your Github account, created your first repository,  and pushed your first markdown file to your account.</div>\n</blockquote>\n<div><br /></div>\n<div>2. <a href=\"https://www.coursera.org/course/rprog\">R Programming</a></div>\n<div><br /></div>\n<div>\n<blockquote>\n<div>In this course you will learn how to program in R and how to use R  for effective data analysis. You will learn how to install and configure  software necessary for a statistical programming environment, discuss  generic programming language concepts as they are implemented in a  high-level statistical language. The course covers practical issues in  statistical computing which includes programming in R, reading data into  R, accessing R packages, writing R functions, debugging, and organizing  and commenting R code. Topics in statistical data analysis and  optimization will provide working examples.</div>\n</blockquote>\n</div>\n<div><br /></div>\n<div>3. <a href=\"https://www.coursera.org/course/getdata\">Getting and Cleaning Data</a></div>\n<div><br /></div>\n<blockquote>\n<div>Upon completion of this course you will be able to obtain data from a  variety of sources. You will know the principles of tidy data and data  sharing. Finally, you will understand and be able to apply the basic  tools for data cleaning and manipulation.</div>\n</blockquote>\n<div><br /></div>\n<div>4. <a href=\"https://www.coursera.org/course/exdata\">Exploratory Data Analysis</a></div>\n<div><br /></div>\n<blockquote>\n<div>After successfully completing this course you will be able to make  visual representations of data using the base, lattice, and ggplot2  plotting systems in R, apply basic principles of data graphics to create  rich analytic graphics from different types of datasets, construct  exploratory summaries of data in support of a specific question, and  create visualizations of multidimensional data using exploratory  multivariate statistical techniques.</div>\n</blockquote>\n<div><br /></div>\n<div>5. <a href=\"https://www.coursera.org/course/repdata\">Reproducible Research</a></div>\n<div><br /></div>\n<blockquote>\n<div>In this course you will learn to write a document using R markdown,  integrate live R code into a literate statistical program, compile R  markdown documents using knitr and related tools, and organize a data  analysis so that it is reproducible and accessible to others.</div>\n</blockquote>\n<div><br /></div>\n<div>6. <a href=\"https://www.coursera.org/course/statinference\">Statistical Inference</a></div>\n<blockquote>\n<div><br /></div>\n<div>In this class students will learn the fundamentals of statistical  inference. Students will receive a broad overview of the goals,  assumptions and modes of performing statistical inference. Students will  be able to perform inferential tasks in highly targeted settings and  will be able to use &nbsp;the skills developed as a roadmap for more complex  inferential challenges.</div>\n</blockquote>\n<div><br /></div>\n<div>7. <a href=\"https://www.coursera.org/course/regmods\">Regression Models</a></div>\n<div><br /></div>\n<blockquote>\n<div>In this course students will learn how to fit regression models, how to  interpret coefficients, how to investigate residuals and variability.  &nbsp;Students will further learn special cases of regression models  including use of dummy variables and multivariable adjustment.  Extensions to generalized linear models, especially considering Poisson  and logistic regression will be reviewed.</div>\n</blockquote>\n<div><br /></div>\n<div>8. <a href=\"https://www.coursera.org/course/predmachlearn\">Practical Machine Learning</a></div>\n<div><br /></div>\n<blockquote>\n<div>Upon completion of this course you will understand the components of a  machine learning algorithm. You will also know how to apply multiple  basic machine learning tools. You will also learn to apply these tools  to build and evaluate predictors on real data.</div>\n<div><br /></div>\n</blockquote>\n<div>9. <a href=\"https://www.coursera.org/course/devdataprod\">Developing Data Products</a></div>\n<div><br /></div>\n<blockquote>\n<div>Students will learn how communicate using statistics and statistical  products. Emphasis will be paid to communicating uncertainty in  statistical results. Students will learn how to create simple Shiny web  applications and R packages for their data products.</div>\n</blockquote>\n<div><br /></div>\n<div>You can take the entire sequence for free or pay $49 for each course in order to (upon completion) receive a <a href=\"https://s3.amazonaws.com/coursera/specializations/jhudatascience/cert_icon.png\">Specialization Certificate</a> from <a href=\"http://en.wikipedia.org/wiki/Johns_Hopkins_University\">Johns Hopkins University</a>.</div>\n<div><br /></div>\n<div>The very popular blog <a href=\"http://simplystatistics.org\">Simply Statistics</a> discusses the program <a href=\"http://simplystatistics.org/2014/01/21/the-johns-hopkins-data-science-specialization-on-coursera/\">here</a>.<br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PJNz7xDqWNTpWTGtB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 18, "extendedScore": null, "score": 1.5424565257821238e-06, "legacy": true, "legacyId": "25408", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-01T20:14:52.154Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Fallacy Categorization", "slug": "meetup-urbana-champaign-fallacy-categorization", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RQzB998pM6DPM9rGo/meetup-urbana-champaign-fallacy-categorization", "pageUrlRelative": "/posts/RQzB998pM6DPM9rGo/meetup-urbana-champaign-fallacy-categorization", "linkUrl": "https://www.lesswrong.com/posts/RQzB998pM6DPM9rGo/meetup-urbana-champaign-fallacy-categorization", "postedAtFormatted": "Saturday, February 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Fallacy%20Categorization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Fallacy%20Categorization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQzB998pM6DPM9rGo%2Fmeetup-urbana-champaign-fallacy-categorization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Fallacy%20Categorization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQzB998pM6DPM9rGo%2Fmeetup-urbana-champaign-fallacy-categorization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQzB998pM6DPM9rGo%2Fmeetup-urbana-champaign-fallacy-categorization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/we'>Urbana-Champaign: Fallacy Categorization</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 February 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For types of arguments commonly called fallacies,\nWhy can using this kind of argument mislead you?\nWhen can a similar argument be a good one?\nIs this argument flat-out nonsense, or is it weak Bayesian evidence? (<a href=\"http://lesswrong.com/lw/aq2/fallacies_as_weak_bayesian_evidence/\" rel=\"nofollow\">http://lesswrong.com/lw/aq2/fallacies_as_weak_bayesian_evidence/</a>)\nWhy do people make this argument? Does it have roots in cognitive biases? In useful heuristics?\nIf this is a fallacy in moral arguments, can it be a good argument to humans with not-too-uncommon moral preferences?</p>\n\n<p>We'll be running through this list, or a similar one:\n<a href=\"http://rationalwiki.org/wiki/Logical_fallacy\" rel=\"nofollow\">http://rationalwiki.org/wiki/Logical_fallacy</a></p>\n\n<p>Coordinates are: 40.109545,-88.227318\nMeetup will be held in the the Courtyard Cafe in the Illini Union, on the ground floor.</p>\n\n<p>Cross posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/Gh6TVPAfUaw\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/we'>Urbana-Champaign: Fallacy Categorization</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RQzB998pM6DPM9rGo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5425644652305473e-06, "legacy": true, "legacyId": "25409", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Fallacy_Categorization\">Discussion article for the meetup : <a href=\"/meetups/we\">Urbana-Champaign: Fallacy Categorization</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 February 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For types of arguments commonly called fallacies,\nWhy can using this kind of argument mislead you?\nWhen can a similar argument be a good one?\nIs this argument flat-out nonsense, or is it weak Bayesian evidence? (<a href=\"http://lesswrong.com/lw/aq2/fallacies_as_weak_bayesian_evidence/\" rel=\"nofollow\">http://lesswrong.com/lw/aq2/fallacies_as_weak_bayesian_evidence/</a>)\nWhy do people make this argument? Does it have roots in cognitive biases? In useful heuristics?\nIf this is a fallacy in moral arguments, can it be a good argument to humans with not-too-uncommon moral preferences?</p>\n\n<p>We'll be running through this list, or a similar one:\n<a href=\"http://rationalwiki.org/wiki/Logical_fallacy\" rel=\"nofollow\">http://rationalwiki.org/wiki/Logical_fallacy</a></p>\n\n<p>Coordinates are: 40.109545,-88.227318\nMeetup will be held in the the Courtyard Cafe in the Illini Union, on the ground floor.</p>\n\n<p>Cross posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/Gh6TVPAfUaw\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Fallacy_Categorization1\">Discussion article for the meetup : <a href=\"/meetups/we\">Urbana-Champaign: Fallacy Categorization</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Fallacy Categorization", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Fallacy_Categorization", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Fallacy Categorization", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Fallacy_Categorization1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YgNLfytckSyKTnDXN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-01T20:54:43.924Z", "modifiedAt": null, "url": null, "title": "February 2014 Media Thread", "slug": "february-2014-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:59.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fJTHc6samuJwHSzXe/february-2014-media-thread", "pageUrlRelative": "/posts/fJTHc6samuJwHSzXe/february-2014-media-thread", "linkUrl": "https://www.lesswrong.com/posts/fJTHc6samuJwHSzXe/february-2014-media-thread", "postedAtFormatted": "Saturday, February 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20February%202014%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFebruary%202014%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJTHc6samuJwHSzXe%2Ffebruary-2014-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=February%202014%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJTHc6samuJwHSzXe%2Ffebruary-2014-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJTHc6samuJwHSzXe%2Ffebruary-2014-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fJTHc6samuJwHSzXe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.5426095502826456e-06, "legacy": true, "legacyId": "25410", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-02T00:28:33.438Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: HPMOR Discussion (maybe Worm too?)", "slug": "meetup-washington-dc-hpmor-discussion-maybe-worm-too", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mC5DhHikBuKZ7pgSX/meetup-washington-dc-hpmor-discussion-maybe-worm-too", "pageUrlRelative": "/posts/mC5DhHikBuKZ7pgSX/meetup-washington-dc-hpmor-discussion-maybe-worm-too", "linkUrl": "https://www.lesswrong.com/posts/mC5DhHikBuKZ7pgSX/meetup-washington-dc-hpmor-discussion-maybe-worm-too", "postedAtFormatted": "Sunday, February 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20HPMOR%20Discussion%20(maybe%20Worm%20too%3F)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20HPMOR%20Discussion%20(maybe%20Worm%20too%3F)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmC5DhHikBuKZ7pgSX%2Fmeetup-washington-dc-hpmor-discussion-maybe-worm-too%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20HPMOR%20Discussion%20(maybe%20Worm%20too%3F)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmC5DhHikBuKZ7pgSX%2Fmeetup-washington-dc-hpmor-discussion-maybe-worm-too", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmC5DhHikBuKZ7pgSX%2Fmeetup-washington-dc-hpmor-discussion-maybe-worm-too", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wf'>Washington DC: HPMOR Discussion (maybe Worm too?)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 February 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about HPMOR and also maybe worm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wf'>Washington DC: HPMOR Discussion (maybe Worm too?)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mC5DhHikBuKZ7pgSX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.542851427469887e-06, "legacy": true, "legacyId": "25412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__HPMOR_Discussion__maybe_Worm_too__\">Discussion article for the meetup : <a href=\"/meetups/wf\">Washington DC: HPMOR Discussion (maybe Worm too?)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 February 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about HPMOR and also maybe worm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__HPMOR_Discussion__maybe_Worm_too__1\">Discussion article for the meetup : <a href=\"/meetups/wf\">Washington DC: HPMOR Discussion (maybe Worm too?)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: HPMOR Discussion (maybe Worm too?)", "anchor": "Discussion_article_for_the_meetup___Washington_DC__HPMOR_Discussion__maybe_Worm_too__", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: HPMOR Discussion (maybe Worm too?)", "anchor": "Discussion_article_for_the_meetup___Washington_DC__HPMOR_Discussion__maybe_Worm_too__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-02T08:02:13.976Z", "modifiedAt": null, "url": null, "title": "Arthur Chu: Jeopardy! champion through exemplary rationality", "slug": "arthur-chu-jeopardy-champion-through-exemplary-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:30.895Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "syllogism", "createdAt": "2010-12-09T02:25:23.672Z", "isAdmin": false, "displayName": "syllogism"}, "userId": "aHznJxGf4ZbruWkNa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TpyX8jpikt4ma3f9Z/arthur-chu-jeopardy-champion-through-exemplary-rationality", "pageUrlRelative": "/posts/TpyX8jpikt4ma3f9Z/arthur-chu-jeopardy-champion-through-exemplary-rationality", "linkUrl": "https://www.lesswrong.com/posts/TpyX8jpikt4ma3f9Z/arthur-chu-jeopardy-champion-through-exemplary-rationality", "postedAtFormatted": "Sunday, February 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arthur%20Chu%3A%20Jeopardy!%20champion%20through%20exemplary%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArthur%20Chu%3A%20Jeopardy!%20champion%20through%20exemplary%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpyX8jpikt4ma3f9Z%2Farthur-chu-jeopardy-champion-through-exemplary-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arthur%20Chu%3A%20Jeopardy!%20champion%20through%20exemplary%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpyX8jpikt4ma3f9Z%2Farthur-chu-jeopardy-champion-through-exemplary-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpyX8jpikt4ma3f9Z%2Farthur-chu-jeopardy-champion-through-exemplary-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<p>http://mentalfloss.com/article/54853/our-interview-jeopardy-champion-arthur-chu</p>\n<p>I'm not sure I've ever seen such a compelling \"rationality success story\". There's so much that's right here.<br /><br />The part that really grabs me about this is that there's no indication that his success has depended on \"natural\" skill or talent. And none of the strategies he's using are from novel research. He just studied the \"literature\" and took the results seriously. He didn't arbitrarily deviate from the known best practice based on aesthetics or intuition. And he kept a simple, single-minded focus on his goal. No lost purposes here --- just win as much money as possible, bank the winnings, and use it to self-insure. It's rationality-as-winning, plain and simple.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TpyX8jpikt4ma3f9Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 33, "extendedScore": null, "score": 1.543364843652811e-06, "legacy": true, "legacyId": "25415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-02T08:11:54.705Z", "modifiedAt": null, "url": null, "title": "On Straw Vulcan Rationality", "slug": "on-straw-vulcan-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:29.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8P4kvTCbynT6ibr4y/on-straw-vulcan-rationality", "pageUrlRelative": "/posts/8P4kvTCbynT6ibr4y/on-straw-vulcan-rationality", "linkUrl": "https://www.lesswrong.com/posts/8P4kvTCbynT6ibr4y/on-straw-vulcan-rationality", "postedAtFormatted": "Sunday, February 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Straw%20Vulcan%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Straw%20Vulcan%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8P4kvTCbynT6ibr4y%2Fon-straw-vulcan-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Straw%20Vulcan%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8P4kvTCbynT6ibr4y%2Fon-straw-vulcan-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8P4kvTCbynT6ibr4y%2Fon-straw-vulcan-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 588, "htmlBody": "<p>There's a core meme of rationalism that I think is fundamentally off-base. It's been bothering me for a long time &mdash; over a year now. It hasn't been easy for me, living this double life, pretending to be OK with propagating an instrumentally expedient idea that I know has no epistemic grounding. So I need to get this off my chest now: Our established terminology is not consistent with an evidence-based view of the Star Trek canon.</p>\n<p><a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan\">According to TVtropes</a>, a straw Vulcan is a character used to show that emotion is better than logic. I think a lot of people take \"<a href=\"http://www.youtube.com/watch?v=tLgNZ9aTEwc\">straw Vulcan rationality</a>\" it to mean something like, \"Being rational does not mean being like Vulcans from Star Trek.\"</p>\n<p>This is not fair to Vulcans from Star Trek.</p>\n<p>Central to the character of Spock &mdash; and something that it's easy to miss if you haven't seen every single episode and/or read a fair amount of fan fiction &mdash; is that he's being a Vulcan all wrong. He's half human, you see, and he's really insecure about that, because all the other kids made fun of him for it when he was growing up on Vulcan. He's spent most of his life resenting his human half, trying to prove to everyone (especially his father) that he's Vulcaner Than Thou. When the Vulcan Science Academy worried that his human mother might be an obstacle, it was the last straw for Spock. He jumped ship and joined Starfleet. Against his father's wishes.</p>\n<p>Spock is a mess of poorly handled emotional turmoil. It makes him cold and volatile.</p>\n<p>Real Vulcans aren't like that. They have stronger and more violent emotions than humans, so they've learned to master them out of necessity. Before the Vulcan Reformation, they were a collection of warring tribes who nearly tore their planet apart. Now, Vulcans understand emotions and are no longer at their mercy. Not when they apply their craft successfully, anyway. In the words of the prophet Surak, who created these cognitive disciplines with the purpose of saving Vulcan from certain doom, \"To gain mastery over the emotions, one must first embrace the many Guises of the Mind.\"</p>\n<p>Successful application of Vulcan philosophy looks positively CFARian.</p>\n<p>There <em>is</em> a ritual called \"kolinahr\" whose purpose is to completely rid oneself of emotion, but it was not developed by Surak, nor, to my knowledge, was it endorsed by him. It's an extreme religious practice, and I think the wisest Vulcans would consider it misguided<sup>1</sup>. Spock attempted kolinahr when he believed Kirk had died, which I take to be a great departure from cthia (the Vulcan Way) &mdash; not because he ultimately failed to complete the ritual<sup>2</sup>, but because he tried to smash his problems with a hammer rather than applying his training to sort things out skillfully. If there ever were such a thing as a right time for kolinahr, that would not have been it.</p>\n<p>So Spock is both a straw Vulcan <em>and</em> a straw man of Vulcans. Steel Vulcans are extremely powerful rationalists. Basically, Surak is what happens when science fiction authors try to invent Eliezer Yudkowsky without having met him.</p>\n<p>&nbsp;</p>\n<hr />\n<p>1) I admit that I notice I'm a little confused about this. Sarek, Spock's father and a highly influential diplomat, studied for a time with the Acolytes of Gol, who are the masters of kolinahr. If I've ever known what came of that, I've forgotten. I'm not sure whether that's canon, though.</p>\n<p>2) \"Sorry to meditate and run, but I've gotta go mind-meld with this giant space crystal thing. ...It's complicated.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8P4kvTCbynT6ibr4y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 23, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "25414", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-02T10:16:52.975Z", "modifiedAt": null, "url": null, "title": "How can I spend money to improve my life?", "slug": "how-can-i-spend-money-to-improve-my-life", "viewCount": null, "lastCommentedAt": "2020-12-24T00:14:29.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jpaulson", "createdAt": "2013-01-17T05:05:45.722Z", "isAdmin": false, "displayName": "Jonathan Paulson"}, "userId": "4NQjysoJPjEwcJ2Sv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wnnsqR784yx7KNtmk/how-can-i-spend-money-to-improve-my-life", "pageUrlRelative": "/posts/wnnsqR784yx7KNtmk/how-can-i-spend-money-to-improve-my-life", "linkUrl": "https://www.lesswrong.com/posts/wnnsqR784yx7KNtmk/how-can-i-spend-money-to-improve-my-life", "postedAtFormatted": "Sunday, February 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20can%20I%20spend%20money%20to%20improve%20my%20life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20can%20I%20spend%20money%20to%20improve%20my%20life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnnsqR784yx7KNtmk%2Fhow-can-i-spend-money-to-improve-my-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20can%20I%20spend%20money%20to%20improve%20my%20life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnnsqR784yx7KNtmk%2Fhow-can-i-spend-money-to-improve-my-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnnsqR784yx7KNtmk%2Fhow-can-i-spend-money-to-improve-my-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>On ChrisHallquist's <a href=\"/lw/jfq/things_i_wish_theyd_taught_me_when_i_was_younger/\">post extolling the virtues of money</a>, the top comment is Eliezer pointing out the lack of concrete examples. Can anyone think of any? This is not just hypothetical: if I think your suggestion is good, I will try it (and report back on how it went)<br /><br />I care about health, improving personal skills (particularly: programming, writing, people skills), gaining respect (particularly at work), and entertainment (these days: primarily books and computer games). If you think I should care about something else, feel free to suggest it.<br /><br />I am early-twenties programmer living in San Francisco. In the interest of getting advice useful to more than one person, I'll omit further personal details.<br /><br />Budget: $50/day<br /><br />If your idea requires significant ongoing time commitment, that is a major negative.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wnnsqR784yx7KNtmk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 6.7e-05, "legacy": true, "legacyId": "25416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 232, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QyZvL6hrmS6AEXJLF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-02T13:35:38.227Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes February 2014", "slug": "rationality-quotes-february-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.054Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "hRzKAvARt4HspRBWA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6zHPmoFsyYuYMfWuC/rationality-quotes-february-2014", "pageUrlRelative": "/posts/6zHPmoFsyYuYMfWuC/rationality-quotes-february-2014", "linkUrl": "https://www.lesswrong.com/posts/6zHPmoFsyYuYMfWuC/rationality-quotes-february-2014", "postedAtFormatted": "Sunday, February 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20February%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20February%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zHPmoFsyYuYMfWuC%2Frationality-quotes-february-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20February%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zHPmoFsyYuYMfWuC%2Frationality-quotes-february-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zHPmoFsyYuYMfWuC%2Frationality-quotes-february-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be upvoted or  downvoted separately. (If they are strongly related, reply to your own  comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or  Robin Hanson. If you'd like to revive an old quote from one of those  sources, please do so&nbsp;<a href=\"/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6zHPmoFsyYuYMfWuC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 1.543742343281746e-06, "legacy": true, "legacyId": "25417", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 486, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-02T15:11:59.505Z", "modifiedAt": "2019-11-13T05:53:04.612Z", "url": null, "title": "Rationality & Low-IQ People", "slug": "rationality-and-low-iq-people", "viewCount": null, "lastCommentedAt": "2014-03-30T07:21:55.967Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kokotajlod", "createdAt": "2013-02-25T13:45:56.499Z", "isAdmin": false, "displayName": "kokotajlod"}, "userId": "Gf6b4eSgp4kCKj8WY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D2dA8g6Goc3f8bkxL/rationality-and-low-iq-people", "pageUrlRelative": "/posts/D2dA8g6Goc3f8bkxL/rationality-and-low-iq-people", "linkUrl": "https://www.lesswrong.com/posts/D2dA8g6Goc3f8bkxL/rationality-and-low-iq-people", "postedAtFormatted": "Sunday, February 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20%26%20Low-IQ%20People&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20%26%20Low-IQ%20People%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2dA8g6Goc3f8bkxL%2Frationality-and-low-iq-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20%26%20Low-IQ%20People%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2dA8g6Goc3f8bkxL%2Frationality-and-low-iq-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2dA8g6Goc3f8bkxL%2Frationality-and-low-iq-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>This post is to raise a question about the demographics of rationality: Is rationality something that can appeal to low-IQ people as well?<br /><br />I don't mean in theory, I mean in practice. From what I've seen, people who are concerned about rationality (in the sense that it has on LW, OvercomingBias, etc.) are overwhelmingly high-IQ.<br /><br />Meanwhile, HPMOR and other stories in the \"rationality genre\" appeal to me, and to other people I know. However I wonder: Perhaps part of the reason they appeal to me is that I think of myself as a smart person, and this allows me to identify with the main characters, cheer when they think their way to victory, etc. If I thought of myself as a stupid person, then perhaps I would feel uncomfortable, insecure, and alienated while reading the same stories.</p>\n<p>So, I have four questions:<br /><br />1.) Do we have reason to believe that the kind of rationality promoted on LW, OvercomingBias, CFAR, etc. appeals to a fairly normal distribution of people around the IQ mean? Or should we think, as I suggested, that people with lower IQ's are disposed to find the idea of being rational less attractive?<br /><br />2.) Ditto, except replace \"being rational\" with \"celebrating rationality through stories like HPMOR.\" Perhaps people think that rationality is a good thing in much the same way that being wealthy is a good thing, but they don't think that it should be celebrated, or at least they don't find such celebrations appealing.<br /><br />3.) Supposing #1 and #2 have the answers I am suggesting, why?&nbsp;<br /><br />4.) Making the same supposition, what are the implications for the movement in general?&nbsp;</p>\n<p><br />Note: I chose to use IQ in this post instead of a more vague term like \"intelligence,\" but I could easily have done the opposite. I'm happy to do whichever version is less problematic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D2dA8g6Goc3f8bkxL", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 23, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "25413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-02-02T15:11:59.505Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-02T22:32:10.247Z", "modifiedAt": null, "url": null, "title": "Meetup : Princeton NJ Meetup", "slug": "meetup-princeton-nj-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:58.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MFbTM2hDxm9YQceu7/meetup-princeton-nj-meetup-1", "pageUrlRelative": "/posts/MFbTM2hDxm9YQceu7/meetup-princeton-nj-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/MFbTM2hDxm9YQceu7/meetup-princeton-nj-meetup-1", "postedAtFormatted": "Sunday, February 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Princeton%20NJ%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Princeton%20NJ%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMFbTM2hDxm9YQceu7%2Fmeetup-princeton-nj-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Princeton%20NJ%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMFbTM2hDxm9YQceu7%2Fmeetup-princeton-nj-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMFbTM2hDxm9YQceu7%2Fmeetup-princeton-nj-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wg'>Princeton NJ Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 February 2014 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Small World Coffee, 14 Witherspoon St.  Princeton, NJ 08540</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have the second ever Princeton NJ meetup! Even better than the first! Feel free to come whether or not you came to the first.</p>\n\n<p>A bunch of people game me their emails after the first one, but we made the mistake of going handwritten, so I suspect many people never got my email. In any case, there is to be a second meetup!</p>\n\n<p>Planned events:</p>\n\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">Paranoid debating</a>. If you'd like, feel free to pick a numerical quantity for the rest of us to try and estimate.</li>\n<li>Discussion of <a href=\"http://lesswrong.com/lw/24o/eight_short_studies_on_excuses/\">this arbitrarily chosen high-scoring post that I like.</a></li>\n<li>Misc. delights.</li>\n<li>Planning for future meetups, including a proper email list.</li>\n</ul>\n\n<p>As such, we hereby precommit to being at Small World Coffee on Witherspoon from 1pm to 3pm. I realize that people suggested better venues last time but I forgot, and the first duty of dictators is to do something when anything is better than nothing.</p>\n\n<p>By the sign of the paperclip so shall ye know us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wg'>Princeton NJ Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MFbTM2hDxm9YQceu7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5443501811645762e-06, "legacy": true, "legacyId": "25420", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Princeton_NJ_Meetup\">Discussion article for the meetup : <a href=\"/meetups/wg\">Princeton NJ Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 February 2014 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Small World Coffee, 14 Witherspoon St.  Princeton, NJ 08540</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have the second ever Princeton NJ meetup! Even better than the first! Feel free to come whether or not you came to the first.</p>\n\n<p>A bunch of people game me their emails after the first one, but we made the mistake of going handwritten, so I suspect many people never got my email. In any case, there is to be a second meetup!</p>\n\n<p>Planned events:</p>\n\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">Paranoid debating</a>. If you'd like, feel free to pick a numerical quantity for the rest of us to try and estimate.</li>\n<li>Discussion of <a href=\"http://lesswrong.com/lw/24o/eight_short_studies_on_excuses/\">this arbitrarily chosen high-scoring post that I like.</a></li>\n<li>Misc. delights.</li>\n<li>Planning for future meetups, including a proper email list.</li>\n</ul>\n\n<p>As such, we hereby precommit to being at Small World Coffee on Witherspoon from 1pm to 3pm. I realize that people suggested better venues last time but I forgot, and the first duty of dictators is to do something when anything is better than nothing.</p>\n\n<p>By the sign of the paperclip so shall ye know us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Princeton_NJ_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/wg\">Princeton NJ Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Princeton NJ Meetup", "anchor": "Discussion_article_for_the_meetup___Princeton_NJ_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Princeton NJ Meetup", "anchor": "Discussion_article_for_the_meetup___Princeton_NJ_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gFMH3Cqw4XxwL69iy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-03T09:48:42.949Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Scholarship and Happiness", "slug": "meetup-moscow-scholarship-and-happiness", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RrKvde5ihNo4JCboW/meetup-moscow-scholarship-and-happiness", "pageUrlRelative": "/posts/RrKvde5ihNo4JCboW/meetup-moscow-scholarship-and-happiness", "linkUrl": "https://www.lesswrong.com/posts/RrKvde5ihNo4JCboW/meetup-moscow-scholarship-and-happiness", "postedAtFormatted": "Monday, February 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Scholarship%20and%20Happiness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Scholarship%20and%20Happiness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrKvde5ihNo4JCboW%2Fmeetup-moscow-scholarship-and-happiness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Scholarship%20and%20Happiness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrKvde5ihNo4JCboW%2Fmeetup-moscow-scholarship-and-happiness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrKvde5ihNo4JCboW%2Fmeetup-moscow-scholarship-and-happiness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wh'>Moscow, Scholarship and Happiness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 February 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to a room inside the building at 16:00. So please do not be late.</p>\n\n<ul>\n<li><p>Workshop about acquiring scientific knowledge.</p></li>\n<li><p>Short presentation about one of cognitive biases.</p></li>\n<li><p>Stumbling on happiness for rationalists presentation.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wh'>Moscow, Scholarship and Happiness</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RrKvde5ihNo4JCboW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5451172419781805e-06, "legacy": true, "legacyId": "25422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Scholarship_and_Happiness\">Discussion article for the meetup : <a href=\"/meetups/wh\">Moscow, Scholarship and Happiness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 February 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to a room inside the building at 16:00. So please do not be late.</p>\n\n<ul>\n<li><p>Workshop about acquiring scientific knowledge.</p></li>\n<li><p>Short presentation about one of cognitive biases.</p></li>\n<li><p>Stumbling on happiness for rationalists presentation.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Scholarship_and_Happiness1\">Discussion article for the meetup : <a href=\"/meetups/wh\">Moscow, Scholarship and Happiness</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Scholarship and Happiness", "anchor": "Discussion_article_for_the_meetup___Moscow__Scholarship_and_Happiness", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Scholarship and Happiness", "anchor": "Discussion_article_for_the_meetup___Moscow__Scholarship_and_Happiness1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-03T15:30:19.834Z", "modifiedAt": null, "url": null, "title": "Open Thread for February 3 - 10", "slug": "open-thread-for-february-3-10", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:34.366Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BAj5rWzshcwpYX9Bz/open-thread-for-february-3-10", "pageUrlRelative": "/posts/BAj5rWzshcwpYX9Bz/open-thread-for-february-3-10", "linkUrl": "https://www.lesswrong.com/posts/BAj5rWzshcwpYX9Bz/open-thread-for-february-3-10", "postedAtFormatted": "Monday, February 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20for%20February%203%20-%2010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20for%20February%203%20-%2010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAj5rWzshcwpYX9Bz%2Fopen-thread-for-february-3-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20for%20February%203%20-%2010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAj5rWzshcwpYX9Bz%2Fopen-thread-for-february-3-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAj5rWzshcwpYX9Bz%2Fopen-thread-for-february-3-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BAj5rWzshcwpYX9Bz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.5455048179161403e-06, "legacy": true, "legacyId": "25423", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 335, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-03T19:32:22.512Z", "modifiedAt": null, "url": null, "title": "Mind Hacks", "slug": "mind-hacks", "viewCount": null, "lastCommentedAt": "2020-05-22T14:24:58.216Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RyanCarey", "createdAt": "2011-04-27T00:19:14.586Z", "isAdmin": false, "displayName": "RyanCarey"}, "userId": "CBkbKSCEzEK2kLQww", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hYmqKm9CgAExnhaAW/mind-hacks", "pageUrlRelative": "/posts/hYmqKm9CgAExnhaAW/mind-hacks", "linkUrl": "https://www.lesswrong.com/posts/hYmqKm9CgAExnhaAW/mind-hacks", "postedAtFormatted": "Monday, February 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mind%20Hacks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMind%20Hacks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYmqKm9CgAExnhaAW%2Fmind-hacks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mind%20Hacks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYmqKm9CgAExnhaAW%2Fmind-hacks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYmqKm9CgAExnhaAW%2Fmind-hacks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 301, "htmlBody": "<p>I've been in the bay area for a week, and already I've heard so many tips and tricks for becoming smarter that I can barely keep track. I think this is a <em>very good thing</em>. If effective altruists and rationalists can become smarter, then it should improve the probability of favourable far-future outcomes. Note that:</p>\n<p>1) cognitive enhancement fits with the ideas that you will achieve most of your impact <a href=\"http://www.overcomingbias.com/2012/10/impatient-idealism.html\">in your middle age</a>, and that increasing your <a href=\"http://80000hours.org/pages/career-capital\">career capital</a> is integral to achieving impact.</p>\n<p>2) it might be possible to reinvest returns from cognitive enhancement by doing further research into cognitive enhancement. This is not to say that an intelligence explosion could occur within a human substrate - our capacity to alter our neural structure and thinking speed is likely to run up against hard evolutionary constraints in a way that machine intelligence will not. Nonetheless, cognitively enhanced humans could have an advantage in the creation of a <a href=\"/lw/cv9/building_toward_a_friendly_ai_team/\">friendly AI team</a>.</p>\n<p>I suggest that we collate our mind hacks <a href=\"http://www.reddit.com/r/rationality\">here</a>. Then we can vote them all up and down. This will generate a list of 'top rated posts of all time', which could give hints for curriculum design to organisations like CFAR. Here are a few suggestions:</p>\n<ul>\n<li>The reversal test for status quo bias</li>\n<li>Thinking for five minutes of plans that can be executed in five minutes</li>\n<li>Mind maps</li>\n<li>Asana</li>\n<li>Anki</li>\n<li>Speed reading</li>\n<li>Goal factoring</li>\n<li>Caffeine</li>\n<li>Modafinil</li>\n<li>Nicotine</li>\n<li>Creatine</li>\n<li>Transcranial magnetic stimulation</li>\n<li>and so on</li>\n</ul>\n<p>Of course, if something is less plausibly obtainable than transcranial magnetic stimulation, then it won't get any votes at all and doesn't meaningfully belong on this list (e.g. deep brain stimulation, brain-computer interfaces)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hYmqKm9CgAExnhaAW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "25426", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["p4Gd8pRcbnKo46hus"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-04T04:16:31.805Z", "modifiedAt": null, "url": null, "title": "Knightian Uncertainty from a Bayesian perspective", "slug": "knightian-uncertainty-from-a-bayesian-perspective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:36.998Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dEAok3uTZdhpHyoP9/knightian-uncertainty-from-a-bayesian-perspective", "pageUrlRelative": "/posts/dEAok3uTZdhpHyoP9/knightian-uncertainty-from-a-bayesian-perspective", "linkUrl": "https://www.lesswrong.com/posts/dEAok3uTZdhpHyoP9/knightian-uncertainty-from-a-bayesian-perspective", "postedAtFormatted": "Tuesday, February 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Knightian%20Uncertainty%20from%20a%20Bayesian%20perspective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKnightian%20Uncertainty%20from%20a%20Bayesian%20perspective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEAok3uTZdhpHyoP9%2Fknightian-uncertainty-from-a-bayesian-perspective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Knightian%20Uncertainty%20from%20a%20Bayesian%20perspective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEAok3uTZdhpHyoP9%2Fknightian-uncertainty-from-a-bayesian-perspective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEAok3uTZdhpHyoP9%2Fknightian-uncertainty-from-a-bayesian-perspective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2158, "htmlBody": "<p>Some people have maintained that there are events to which there's no rational basis for assigning probabilities. For example, John Maynard Keynes <a href=\"http://www.kaleidic.org/storage/Kaleidic%20Uncertainty.pdf\">wrote</a> of \"uncertainty\" in the following sense:</p>\n<blockquote>\n<p>\"By `uncertain' knowledge, let me explain, I do not mean merely to distinguish what is known for certain from what is only probable. The game of roulette is not subject, in this sense, to uncertainty...The sense in which I am using the term is that in which the prospect of a European war is uncertain, or the price of copper and the rate of interest twenty years hence...About these matters there is no scientific basis on which to form any calculable probability whatever. We simply do not know.\" (J.M. Keynes, 1937)</p>\n</blockquote>\n<p>This sort of uncertainty is sometimes referred to as <a href=\"http://en.wikipedia.org/wiki/Knightian_uncertainty\">Knightian uncertainty</a>.</p>\n<p>MIRI is interested in making probabilistic predictions about events such as the creation of general artificial intelligence, which are without precedent, and which therefore cannot be assigned probabilities via <a href=\"http://en.wikipedia.org/wiki/Frequentist_inference\">frequentist</a> means. Some of these events are presumably of the type that Keynes had in mind. At MIRI's request, I did a literature review looking for arguments against there being a rational basis for assigning probabilities to such events.</p>\n<p><a id=\"more\"></a></p>\n<h2><span style=\"font-weight: normal;\">Definitions of subjective probability</span></h2>\n<p>One can attempt to define the subjective probability that an agent assigns to an event to be, intuitively, the number that it would assign if it were to make a very large number of predictions with a view toward, for each x, assigning probability x% to a collection of events of which x% actually occur. Eliezer discusses the mathematical formalism behind this in&nbsp;<a href=\"http://yudkowsky.net/rational/technical/\">A Technical Explanation of a Technical Explanation</a>.&nbsp;</p>\n<p>Other definitions of subjective probabilities have been given by <a href=\"http://fitelson.org/probability/ramsey.pdf\">Ramsey (1931)</a>, <a href=\"http://archive.numdam.org/ARCHIVE/AIHP/AIHP_1937__7_1/AIHP_1937__7_1_1_0/AIHP_1937__7_1_1_0.pdf\">de Finetti (1937)</a>, <a href=\"http://fitelson.org/coherence/koopman.pdf\">Koopman (1940)</a>, <a href=\"http://www.amazon.com/Probability-Weighing-Evidence-I-Good/dp/B0000CHL1R/ref=sr_sp-atf_title_1_1?ie=UTF8&amp;qid=1391298205&amp;sr=8-1&amp;keywords=Probability+and+the+Weighing+of+Evidence\">Good (1950)</a>, <a href=\"http://www.amazon.com/The-Foundations-Statistics-Leonard-Savage/dp/0486623491\">Savage (1954)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Davidson-and-Suppes-A-finitistic-axiomatization-of-subjective-probability-and-utility.pdf\">Davidson and Suppes (1956)</a>, <a href=\"http://projecteuclid.org/euclid.aoms/1177706260\">Kraft, Pratt and Seidenberg (1959)</a>, <a href=\"http://www.econ.ucsb.edu/~tedb/Courses/GraduateTheoryUCSB/anscombeaumann.pdf\">Anscombe and Aumann (1963)</a> and&nbsp;<a href=\"http://ideas.repec.org/a/eee/matsoc/v14y1987i3p289-298.html\">Wakker (1989)</a>. (<a href=\"http://fitelson.org/coherence/fishburn.pdf\">Fishburn (1986)</a> gives a survey of the literature.) I have not studied the mathematical formalisms of most of these papers, but here's a definition inspired by them (one which is immune to some of the criticisms that have been raised against some of the definitions).</p>\n<p><em>Assume that for each number p between 0 and 1, there is a random process R that yields an outcome O' with \"objective\" probability p. Here \"objective\" probability refers to a probability that can be determined via physics or frequentist means. Your subjective probability of an event E is defined as follows. Suppose that you have an event F, that you strongly desire to happen, and a choice between the following options:</em></p>\n<ol>\n<li><em>F occurs if and only if E occurs.</em></li>\n<li><em>F occurs if and only if the outcome of R is O'</em></li>\n</ol>\n<p><em>Consider the set S of values of p such that you'd prefer #2 over #1. Then your subjective probability q of E is defined to be the greatest lower bound of S.</em>&nbsp;</p>\n<p>(F is usually taken to be a monetary reward arising from a bet.)</p>\n<p>For example, suppose that E and F are the both the event \"humanity survives for millions of years\" and you have the opportunity to push a button that will guarantee this with probability <em>p</em> and otherwise guarantee that this does not happen. If you're willing to push it when <em>p</em> = 99.999%, that means that you assign a probability less than 99.999% to humanity surviving for millions of years. If you're not willing to push it when <em>p</em> = 0.001%, that means that you assign a probability greater than 0.001% to humanity surviving for millions of years.</p>\n<p>Some objections to the definition are:</p>\n<ul>\n<li>Your value of <em>q</em> is sensitive to factors such as framing effects, mood and what evidence you happen to have considered most recently considered. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Kyburg-Bets-and-beliefs.pdf\">Kyburg (1968)</a>&nbsp;discusses this on&nbsp;pages 57-58, and <a href=\"http://homepage.sns.it/hosni/lori/readings/Shafer%20-%201986%20-%20Savage%20Revisited.pdf\">Shafer (1986)</a> discusses this on page 465. So q may not be well defined as a number.&nbsp;</li>\n<li>It assumes that you've considered the question in the definition. If an agent has never considered an event E, it doesn't have a probability attached to it stored in its memory, even implicitly. And even if one has considered the event E, one may not have had occasion to make an assessment, because of the absence of an event F for which #1 as in the definition could plausibly hold.</li>\n</ul>\n<p>These two objections also apply to the definition that Eliezer discusses in&nbsp;<a href=\"http://yudkowsky.net/rational/technical/\">A Technical Explanation of a Technical Explanation</a>.</p>\n<p>Addressing these points in turn:</p>\n<ul>\n<li><em>q</em> may still be well defined as an interval: in the example above involving humanity surviving for millions of years, it could be that the value of q that you assign fluctuates wildly between 10% and 90% depending on when you're asked and how you're asked, but that it always remains between 0.001% and 99.999%. Keynes <a href=\"http://www.howtothinkaboutthefuture.com/?p=99\">discussed</a> this in <a href=\"http://www.gutenberg.org/files/32625/32625-pdf.pdf\">A Treatise on Probability</a>, Kyburg suggests this in his 1968 paper, and Niklas Moller cites&nbsp;<a href=\"http://www.nssl.noaa.gov/users/brooks/public_html/feda/papers/eb1961ambiguity.pdf\">Ellsberg (1961)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Kaplan-Decision-theory-as-philosophy.pdf\">Kaplan (1983)</a> and<a href=\"http://www.amazon.com/Hard-Choices-Decision-Unresolved-Conflict/dp/0521386306\"> Levi (1986)</a> on page 66 of&nbsp;<a href=\"http://www.amazon.com/Handbook-Risk-Theory-Epistemology-Implications/dp/9400714327\">Handbook of Risk Theory</a>.</li>\n<li>One can make the agent aware of the possibility of the event E, and try to create such a suitable event F. This may not be feasible, for example, because one lacks the resources to create such an event F, or because E is in the far future. But if one wishes to assign a probability to an event E, one can imagine an associated event F, and imagine that one was making the choice between #1 and #2.</li>\n</ul>\n<p><span style=\"font-size: 16px;\"><strong>Pragmatic objections to assigning subjective probabilities</strong></span></p>\n<p>Even if subjective probabilities are well-defined (up to the two issues mentioned above), assigning a subjective probability in a given instance could be bad for one's epistemology. Some proponents of the idea of Knightian uncertainty may implicitly adhere to this position. Some ways in which assigning a subjective probability can lead one astray are given below.</p>\n<p><strong>Overconfidence in models</strong></p>\n<p>Suppose that one has a model of the world that one thinks is probably right and according to which the probability of an event E is extremely small. If one forgets that the model might be wrong, one might erroneously conclude that the probability of E occurring is extremely small. (Yvain discussed this in&nbsp;<a href=\"/lw/3be/confidence_levels_inside_and_outside_an_argument/\">Confidence levels inside and outside an argument</a>.)</p>\n<p>This appears to be close to Keynes' objection to assigning subjective probabilities. I have not studied Keynes' original work, but several people who have written about him seem to implicitly ascribe this position to him. For example, in a book review discussing Keynes, John Gray <a href=\"http://www.lrb.co.uk/v31/n22/john-gray/we-simply-do-not-know\">wrote</a>:</p>\n<blockquote>\n<p>Even our list of possible outcomes may turn out to have omitted the ones that are most important in shaping events. Such an omission was one of the factors that led Long-Term Capital Management, a highly leveraged hedge fund set up by two Nobel Prize winning economists, to fail in 1998-2000. The information used in applying the formula did not include the possibility of such events as the Asian financial crisis and Russia&rsquo;s default on its sovereign debt, which destabilised global financial markets and helped destroy the fund. The orthodoxy that came unstuck with the collapse of LTCM was not faulty because it neglected the vagaries of human moods; its mistake was to think that the unknown future could be turned into a set of calculable risks and, in effect, conjured out of existence, which was impossible. Several centuries earlier, Pascal &ndash; one of the founders of probability theory &ndash; had come to the same conclusion, when in the Pens&eacute;es he asks ironically: &lsquo;Is it probable that probability brings certainty?&rsquo; The central flaw of the economic orthodoxy against which Keynes fought in the 1930s was to imagine that an insoluble problem &ndash; human ignorance of the future &ndash; had been solved. The error was repeated in the 1990s, when economists came to believe that complex mathematical formulae could tame uncertainty in the murky world of derivatives.</p>\n</blockquote>\n<p>One can assign a probability to one's model of the world being accurate, to account for model uncertainty. Keynes' position is perhaps best interpreted as a statement about effect size: a claim that the probability that one should assign to one's model being inaccurate is large.&nbsp;</p>\n<p><strong>Insensitivity to robustness of evidence</strong></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Kyburg-Bets-and-beliefs.pdf\">Kyburg (1968)</a> argues that probabilities don't adequately pick up on robustness of evidence. He gives the example of drawing balls from an urn with black and white balls of unknown relative frequencies. He says that there's a big difference between</p>\n<ol>\n<li>An initial guess that the relative frequencies are 50%-50%</li>\n<li>A guess that the relative frequencies are 50%-50% after having drawn 1,000 balls and finding that the relative frequencies of the colors of balls drawn are about 50%-50%</li>\n</ol>\n<p>saying</p>\n<blockquote>\n<p>The person who offers odds of two to one on the first ball is not at all out of his mind in the same sense as the person who offers two to one odds on the 1001st ball.</p>\n</blockquote>\n<p>A single probability estimate does not pick up on how much one should update in response to incoming evidence.&nbsp;If one assigns a probability p to an event, one might mentally categorize the event in the reference class \"events with probability p\" and update too little or too much in response to incoming evidence on account of anchoring on other events of probability p (for which the probability is more robustly established or less robustly established than for the event in question).</p>\n<p>This may be addressed by replacing a subjective probability of an event with a&nbsp;<em>probability distribution</em>&nbsp;for an event: for each number p between 0 and 1, associating a probability q<sub>p</sub>&nbsp;that the event occurs with probability p. Quoting page 67 of <a href=\"http://www.amazon.com/Handbook-Risk-Theory-Epistemology-Implications/dp/9400714327\">Handbook of Risk Theory</a></p>\n<blockquote>\n<p>Multivalued measures generally take the form of a function that assigns a numerical value to each probability value between 0 and 1. This value represents the degree of reliability or plausibility of each particular probability value. Several interpretations of the measure have been used in the literature, for example, second-order probability (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Baron-Second-order-probabilities-and-belief-functions.pdf\">Baron 1987</a>; <a href=\"http://fitelson.org/seminar/skyrms.pdf\">Skyrms 1980</a>), fuzzy set membership (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Unwin-A-Fuzzy-Set-Theoretic-Foundation-for-Vagueness-in-Uncertainty-Analysis.pdf\">Unwin 1986</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Dubois-Prade-Decision-Evaluation-Methods-Under-Uncertainty-and-Imprecision.pdf\">Dubois and Prade 1988</a>), and epistemic reliability (<a href=\"http://www.nilsericsahlin.se/pdf/sahlin_unreliable_probabilities.pdf\">Gardenfors and Sahlin 1982</a>). See <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Moller-Safety-is-more-than-the-antonym-of-risk.pdf\">Moller et al. (2006)</a> for an overview.</p>\n</blockquote>\n<p><a href=\"/lw/igv/probability_knowledge_and_metaprobability/\">Probability, knowledge, and meta-probability</a>&nbsp;discusses E.T. Jaynes' approach to this.</p>\n<p><strong>Suppression of dependency of events</strong></p>\n<p>Given two events A and B to which one assigns probabilities p and q, the numbers p and q do not suffice to determine the probability that events A and B both occur. If one assigns probabilities to events, and forgets where the probabilities came from, there's a risk of tacitly assuming that the events are independent, and assigning probability pq to the conjunction of p and q, when the probability of the conjunction could be much higher or much lower. According to chapter 1 of Nate Silver's book&nbsp;<em>The Signal and the Noise,&nbsp;</em>similar mistakes contributed to the 2008 financial crisis: people in finance assigned a much smaller probability of a very large number of houses' prices dropping than they did to a smaller number of houses' prices dropping, even though the prices of different houses were correlated.</p>\n<h2>Conclusion</h2>\n<p><span style=\"font-size: small;\">While some people have said that subjective probabilities of arbitrary events are not meaningful, there are definitions that make the notion of subjective probability meaningful, though arguably only as an&nbsp;<em>intervals</em>&nbsp;rather than as&nbsp;<em>numbers</em>.&nbsp;Using intervals rather than numbers addresses some of the objections that have been raised.&nbsp;</span></p>\n<p><span style=\"font-size: small;\">A large part of the debate about whether one should assign subjective probabilities to arbitrary events is perhaps best conceptualized as a debate about <em>how large the probability intervals that one assigns should be</em>. In <em>Worst Case Scenarios </em>(pg 160) S</span>unstein wrote</p>\n<blockquote>\n<p>Suppose that the question is the likelihood that at least 100 million human beings will be alive in 10,000 years. For most people equipped with the knowledge they have, no probability can sensibly be assigned. Perhaps uncertainty is not unlimited; the likelihood can reasonably be described as above 0 percent and below 100 percent. But beyond that point, little can be said.</p>\n</blockquote>\n<p>In any given instance, one has the question of <em>how much</em>&nbsp;can be said. If you have a model of the world M that's accurate with probability at least p and M predicts an event E with probability at least q, then the probability of E is at least pq. If p is low, then this doesn't give a good lower bound on the probability of E. But suppose you have 2 independent models M<sub>1</sub>, and M<sub>2</sub>, where M<sub>i</sub>&nbsp;is accurate with probability at least p<sub>i</sub>&nbsp;and where M<sub>i</sub>&nbsp;predicts E with probability at least q<sub>i</sub>. Then the probability of E is bounded below by p<sub>1</sub>q<sub>1</sub>&nbsp;+ p<sub>2</sub>q<sub>2</sub>&nbsp;- p<sub>1</sub>q<sub>1</sub>p<sub>2</sub>q<sub>2</sub>. So by using&nbsp;<a href=\"/lw/hzu/model_combination_and_adjustment/\">model combination</a>&nbsp;you can get a better lower bound on the probability of E (although in practice the models used may not be fully independent, and if they're positively correlated then the lower bound will be worse).</p>\n<p>The ways in which assigning subjective probabilities can be bad for one's epistemology seem to fall under the broad heading \"failing to incorporate all of one's knowledge when assigning a probability and then using it uncritically, or forgetting that the probability that you assign to an event does not fully capture your knowledge pertaining to the event.\" These issues can be at least partially mitigated by keeping them in mind.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dEAok3uTZdhpHyoP9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 1.5463747283801915e-06, "legacy": true, "legacyId": "25397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Some people have maintained that there are events to which there's no rational basis for assigning probabilities. For example, John Maynard Keynes <a href=\"http://www.kaleidic.org/storage/Kaleidic%20Uncertainty.pdf\">wrote</a> of \"uncertainty\" in the following sense:</p>\n<blockquote>\n<p>\"By `uncertain' knowledge, let me explain, I do not mean merely to distinguish what is known for certain from what is only probable. The game of roulette is not subject, in this sense, to uncertainty...The sense in which I am using the term is that in which the prospect of a European war is uncertain, or the price of copper and the rate of interest twenty years hence...About these matters there is no scientific basis on which to form any calculable probability whatever. We simply do not know.\" (J.M. Keynes, 1937)</p>\n</blockquote>\n<p>This sort of uncertainty is sometimes referred to as <a href=\"http://en.wikipedia.org/wiki/Knightian_uncertainty\">Knightian uncertainty</a>.</p>\n<p>MIRI is interested in making probabilistic predictions about events such as the creation of general artificial intelligence, which are without precedent, and which therefore cannot be assigned probabilities via <a href=\"http://en.wikipedia.org/wiki/Frequentist_inference\">frequentist</a> means. Some of these events are presumably of the type that Keynes had in mind. At MIRI's request, I did a literature review looking for arguments against there being a rational basis for assigning probabilities to such events.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Definitions_of_subjective_probability\"><span style=\"font-weight: normal;\">Definitions of subjective probability</span></h2>\n<p>One can attempt to define the subjective probability that an agent assigns to an event to be, intuitively, the number that it would assign if it were to make a very large number of predictions with a view toward, for each x, assigning probability x% to a collection of events of which x% actually occur. Eliezer discusses the mathematical formalism behind this in&nbsp;<a href=\"http://yudkowsky.net/rational/technical/\">A Technical Explanation of a Technical Explanation</a>.&nbsp;</p>\n<p>Other definitions of subjective probabilities have been given by <a href=\"http://fitelson.org/probability/ramsey.pdf\">Ramsey (1931)</a>, <a href=\"http://archive.numdam.org/ARCHIVE/AIHP/AIHP_1937__7_1/AIHP_1937__7_1_1_0/AIHP_1937__7_1_1_0.pdf\">de Finetti (1937)</a>, <a href=\"http://fitelson.org/coherence/koopman.pdf\">Koopman (1940)</a>, <a href=\"http://www.amazon.com/Probability-Weighing-Evidence-I-Good/dp/B0000CHL1R/ref=sr_sp-atf_title_1_1?ie=UTF8&amp;qid=1391298205&amp;sr=8-1&amp;keywords=Probability+and+the+Weighing+of+Evidence\">Good (1950)</a>, <a href=\"http://www.amazon.com/The-Foundations-Statistics-Leonard-Savage/dp/0486623491\">Savage (1954)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Davidson-and-Suppes-A-finitistic-axiomatization-of-subjective-probability-and-utility.pdf\">Davidson and Suppes (1956)</a>, <a href=\"http://projecteuclid.org/euclid.aoms/1177706260\">Kraft, Pratt and Seidenberg (1959)</a>, <a href=\"http://www.econ.ucsb.edu/~tedb/Courses/GraduateTheoryUCSB/anscombeaumann.pdf\">Anscombe and Aumann (1963)</a> and&nbsp;<a href=\"http://ideas.repec.org/a/eee/matsoc/v14y1987i3p289-298.html\">Wakker (1989)</a>. (<a href=\"http://fitelson.org/coherence/fishburn.pdf\">Fishburn (1986)</a> gives a survey of the literature.) I have not studied the mathematical formalisms of most of these papers, but here's a definition inspired by them (one which is immune to some of the criticisms that have been raised against some of the definitions).</p>\n<p><em>Assume that for each number p between 0 and 1, there is a random process R that yields an outcome O' with \"objective\" probability p. Here \"objective\" probability refers to a probability that can be determined via physics or frequentist means. Your subjective probability of an event E is defined as follows. Suppose that you have an event F, that you strongly desire to happen, and a choice between the following options:</em></p>\n<ol>\n<li><em>F occurs if and only if E occurs.</em></li>\n<li><em>F occurs if and only if the outcome of R is O'</em></li>\n</ol>\n<p><em>Consider the set S of values of p such that you'd prefer #2 over #1. Then your subjective probability q of E is defined to be the greatest lower bound of S.</em>&nbsp;</p>\n<p>(F is usually taken to be a monetary reward arising from a bet.)</p>\n<p>For example, suppose that E and F are the both the event \"humanity survives for millions of years\" and you have the opportunity to push a button that will guarantee this with probability <em>p</em> and otherwise guarantee that this does not happen. If you're willing to push it when <em>p</em> = 99.999%, that means that you assign a probability less than 99.999% to humanity surviving for millions of years. If you're not willing to push it when <em>p</em> = 0.001%, that means that you assign a probability greater than 0.001% to humanity surviving for millions of years.</p>\n<p>Some objections to the definition are:</p>\n<ul>\n<li>Your value of <em>q</em> is sensitive to factors such as framing effects, mood and what evidence you happen to have considered most recently considered. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Kyburg-Bets-and-beliefs.pdf\">Kyburg (1968)</a>&nbsp;discusses this on&nbsp;pages 57-58, and <a href=\"http://homepage.sns.it/hosni/lori/readings/Shafer%20-%201986%20-%20Savage%20Revisited.pdf\">Shafer (1986)</a> discusses this on page 465. So q may not be well defined as a number.&nbsp;</li>\n<li>It assumes that you've considered the question in the definition. If an agent has never considered an event E, it doesn't have a probability attached to it stored in its memory, even implicitly. And even if one has considered the event E, one may not have had occasion to make an assessment, because of the absence of an event F for which #1 as in the definition could plausibly hold.</li>\n</ul>\n<p>These two objections also apply to the definition that Eliezer discusses in&nbsp;<a href=\"http://yudkowsky.net/rational/technical/\">A Technical Explanation of a Technical Explanation</a>.</p>\n<p>Addressing these points in turn:</p>\n<ul>\n<li><em>q</em> may still be well defined as an interval: in the example above involving humanity surviving for millions of years, it could be that the value of q that you assign fluctuates wildly between 10% and 90% depending on when you're asked and how you're asked, but that it always remains between 0.001% and 99.999%. Keynes <a href=\"http://www.howtothinkaboutthefuture.com/?p=99\">discussed</a> this in <a href=\"http://www.gutenberg.org/files/32625/32625-pdf.pdf\">A Treatise on Probability</a>, Kyburg suggests this in his 1968 paper, and Niklas Moller cites&nbsp;<a href=\"http://www.nssl.noaa.gov/users/brooks/public_html/feda/papers/eb1961ambiguity.pdf\">Ellsberg (1961)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Kaplan-Decision-theory-as-philosophy.pdf\">Kaplan (1983)</a> and<a href=\"http://www.amazon.com/Hard-Choices-Decision-Unresolved-Conflict/dp/0521386306\"> Levi (1986)</a> on page 66 of&nbsp;<a href=\"http://www.amazon.com/Handbook-Risk-Theory-Epistemology-Implications/dp/9400714327\">Handbook of Risk Theory</a>.</li>\n<li>One can make the agent aware of the possibility of the event E, and try to create such a suitable event F. This may not be feasible, for example, because one lacks the resources to create such an event F, or because E is in the far future. But if one wishes to assign a probability to an event E, one can imagine an associated event F, and imagine that one was making the choice between #1 and #2.</li>\n</ul>\n<p><span style=\"font-size: 16px;\"><strong>Pragmatic objections to assigning subjective probabilities</strong></span></p>\n<p>Even if subjective probabilities are well-defined (up to the two issues mentioned above), assigning a subjective probability in a given instance could be bad for one's epistemology. Some proponents of the idea of Knightian uncertainty may implicitly adhere to this position. Some ways in which assigning a subjective probability can lead one astray are given below.</p>\n<p><strong id=\"Overconfidence_in_models\">Overconfidence in models</strong></p>\n<p>Suppose that one has a model of the world that one thinks is probably right and according to which the probability of an event E is extremely small. If one forgets that the model might be wrong, one might erroneously conclude that the probability of E occurring is extremely small. (Yvain discussed this in&nbsp;<a href=\"/lw/3be/confidence_levels_inside_and_outside_an_argument/\">Confidence levels inside and outside an argument</a>.)</p>\n<p>This appears to be close to Keynes' objection to assigning subjective probabilities. I have not studied Keynes' original work, but several people who have written about him seem to implicitly ascribe this position to him. For example, in a book review discussing Keynes, John Gray <a href=\"http://www.lrb.co.uk/v31/n22/john-gray/we-simply-do-not-know\">wrote</a>:</p>\n<blockquote>\n<p>Even our list of possible outcomes may turn out to have omitted the ones that are most important in shaping events. Such an omission was one of the factors that led Long-Term Capital Management, a highly leveraged hedge fund set up by two Nobel Prize winning economists, to fail in 1998-2000. The information used in applying the formula did not include the possibility of such events as the Asian financial crisis and Russia\u2019s default on its sovereign debt, which destabilised global financial markets and helped destroy the fund. The orthodoxy that came unstuck with the collapse of LTCM was not faulty because it neglected the vagaries of human moods; its mistake was to think that the unknown future could be turned into a set of calculable risks and, in effect, conjured out of existence, which was impossible. Several centuries earlier, Pascal \u2013 one of the founders of probability theory \u2013 had come to the same conclusion, when in the Pens\u00e9es he asks ironically: \u2018Is it probable that probability brings certainty?\u2019 The central flaw of the economic orthodoxy against which Keynes fought in the 1930s was to imagine that an insoluble problem \u2013 human ignorance of the future \u2013 had been solved. The error was repeated in the 1990s, when economists came to believe that complex mathematical formulae could tame uncertainty in the murky world of derivatives.</p>\n</blockquote>\n<p>One can assign a probability to one's model of the world being accurate, to account for model uncertainty. Keynes' position is perhaps best interpreted as a statement about effect size: a claim that the probability that one should assign to one's model being inaccurate is large.&nbsp;</p>\n<p><strong id=\"Insensitivity_to_robustness_of_evidence\">Insensitivity to robustness of evidence</strong></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Kyburg-Bets-and-beliefs.pdf\">Kyburg (1968)</a> argues that probabilities don't adequately pick up on robustness of evidence. He gives the example of drawing balls from an urn with black and white balls of unknown relative frequencies. He says that there's a big difference between</p>\n<ol>\n<li>An initial guess that the relative frequencies are 50%-50%</li>\n<li>A guess that the relative frequencies are 50%-50% after having drawn 1,000 balls and finding that the relative frequencies of the colors of balls drawn are about 50%-50%</li>\n</ol>\n<p>saying</p>\n<blockquote>\n<p>The person who offers odds of two to one on the first ball is not at all out of his mind in the same sense as the person who offers two to one odds on the 1001st ball.</p>\n</blockquote>\n<p>A single probability estimate does not pick up on how much one should update in response to incoming evidence.&nbsp;If one assigns a probability p to an event, one might mentally categorize the event in the reference class \"events with probability p\" and update too little or too much in response to incoming evidence on account of anchoring on other events of probability p (for which the probability is more robustly established or less robustly established than for the event in question).</p>\n<p>This may be addressed by replacing a subjective probability of an event with a&nbsp;<em>probability distribution</em>&nbsp;for an event: for each number p between 0 and 1, associating a probability q<sub>p</sub>&nbsp;that the event occurs with probability p. Quoting page 67 of <a href=\"http://www.amazon.com/Handbook-Risk-Theory-Epistemology-Implications/dp/9400714327\">Handbook of Risk Theory</a></p>\n<blockquote>\n<p>Multivalued measures generally take the form of a function that assigns a numerical value to each probability value between 0 and 1. This value represents the degree of reliability or plausibility of each particular probability value. Several interpretations of the measure have been used in the literature, for example, second-order probability (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Baron-Second-order-probabilities-and-belief-functions.pdf\">Baron 1987</a>; <a href=\"http://fitelson.org/seminar/skyrms.pdf\">Skyrms 1980</a>), fuzzy set membership (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Unwin-A-Fuzzy-Set-Theoretic-Foundation-for-Vagueness-in-Uncertainty-Analysis.pdf\">Unwin 1986</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Dubois-Prade-Decision-Evaluation-Methods-Under-Uncertainty-and-Imprecision.pdf\">Dubois and Prade 1988</a>), and epistemic reliability (<a href=\"http://www.nilsericsahlin.se/pdf/sahlin_unreliable_probabilities.pdf\">Gardenfors and Sahlin 1982</a>). See <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Moller-Safety-is-more-than-the-antonym-of-risk.pdf\">Moller et al. (2006)</a> for an overview.</p>\n</blockquote>\n<p><a href=\"/lw/igv/probability_knowledge_and_metaprobability/\">Probability, knowledge, and meta-probability</a>&nbsp;discusses E.T. Jaynes' approach to this.</p>\n<p><strong id=\"Suppression_of_dependency_of_events\">Suppression of dependency of events</strong></p>\n<p>Given two events A and B to which one assigns probabilities p and q, the numbers p and q do not suffice to determine the probability that events A and B both occur. If one assigns probabilities to events, and forgets where the probabilities came from, there's a risk of tacitly assuming that the events are independent, and assigning probability pq to the conjunction of p and q, when the probability of the conjunction could be much higher or much lower. According to chapter 1 of Nate Silver's book&nbsp;<em>The Signal and the Noise,&nbsp;</em>similar mistakes contributed to the 2008 financial crisis: people in finance assigned a much smaller probability of a very large number of houses' prices dropping than they did to a smaller number of houses' prices dropping, even though the prices of different houses were correlated.</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p><span style=\"font-size: small;\">While some people have said that subjective probabilities of arbitrary events are not meaningful, there are definitions that make the notion of subjective probability meaningful, though arguably only as an&nbsp;<em>intervals</em>&nbsp;rather than as&nbsp;<em>numbers</em>.&nbsp;Using intervals rather than numbers addresses some of the objections that have been raised.&nbsp;</span></p>\n<p><span style=\"font-size: small;\">A large part of the debate about whether one should assign subjective probabilities to arbitrary events is perhaps best conceptualized as a debate about <em>how large the probability intervals that one assigns should be</em>. In <em>Worst Case Scenarios </em>(pg 160) S</span>unstein wrote</p>\n<blockquote>\n<p>Suppose that the question is the likelihood that at least 100 million human beings will be alive in 10,000 years. For most people equipped with the knowledge they have, no probability can sensibly be assigned. Perhaps uncertainty is not unlimited; the likelihood can reasonably be described as above 0 percent and below 100 percent. But beyond that point, little can be said.</p>\n</blockquote>\n<p>In any given instance, one has the question of <em>how much</em>&nbsp;can be said. If you have a model of the world M that's accurate with probability at least p and M predicts an event E with probability at least q, then the probability of E is at least pq. If p is low, then this doesn't give a good lower bound on the probability of E. But suppose you have 2 independent models M<sub>1</sub>, and M<sub>2</sub>, where M<sub>i</sub>&nbsp;is accurate with probability at least p<sub>i</sub>&nbsp;and where M<sub>i</sub>&nbsp;predicts E with probability at least q<sub>i</sub>. Then the probability of E is bounded below by p<sub>1</sub>q<sub>1</sub>&nbsp;+ p<sub>2</sub>q<sub>2</sub>&nbsp;- p<sub>1</sub>q<sub>1</sub>p<sub>2</sub>q<sub>2</sub>. So by using&nbsp;<a href=\"/lw/hzu/model_combination_and_adjustment/\">model combination</a>&nbsp;you can get a better lower bound on the probability of E (although in practice the models used may not be fully independent, and if they're positively correlated then the lower bound will be worse).</p>\n<p>The ways in which assigning subjective probabilities can be bad for one's epistemology seem to fall under the broad heading \"failing to incorporate all of one's knowledge when assigning a probability and then using it uncritically, or forgetting that the probability that you assign to an event does not fully capture your knowledge pertaining to the event.\" These issues can be at least partially mitigated by keeping them in mind.</p>", "sections": [{"title": "Definitions of subjective probability", "anchor": "Definitions_of_subjective_probability", "level": 1}, {"title": "Overconfidence in models", "anchor": "Overconfidence_in_models", "level": 2}, {"title": "Insensitivity to robustness of evidence", "anchor": "Insensitivity_to_robustness_of_evidence", "level": 2}, {"title": "Suppression of dependency of events", "anchor": "Suppression_of_dependency_of_events", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "35 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GrtbTAPfkJa4D6jjH", "2xmKZu73gZLDEQw7c", "iyRpsScBa6y4rduEt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-04T05:40:25.958Z", "modifiedAt": null, "url": null, "title": "Beware Trivial Fears", "slug": "beware-trivial-fears", "viewCount": null, "lastCommentedAt": "2022-02-17T20:26:09.727Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2kDnKq2QyGfEJMzEo/beware-trivial-fears", "pageUrlRelative": "/posts/2kDnKq2QyGfEJMzEo/beware-trivial-fears", "linkUrl": "https://www.lesswrong.com/posts/2kDnKq2QyGfEJMzEo/beware-trivial-fears", "postedAtFormatted": "Tuesday, February 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beware%20Trivial%20Fears&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeware%20Trivial%20Fears%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kDnKq2QyGfEJMzEo%2Fbeware-trivial-fears%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beware%20Trivial%20Fears%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kDnKq2QyGfEJMzEo%2Fbeware-trivial-fears", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kDnKq2QyGfEJMzEo%2Fbeware-trivial-fears", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 518, "htmlBody": "<p>Does the surveillance state affect us? It has affected me, and I didn't realize that it was affecting me until recently. I give a few examples of how it has affected me:</p>\n<ol>\n<li>I was once engaged in a discussion on Facebook about Obama's foreign policy. Around that time, I was going to apply for a US visa. I stopped the discussion early. Semi-consciously, I was worried that what I was writing would be checked by US visa officials and would lead to my visa being denied.</li>\n<li>I was once really interested in reading up on the Unabomber and his manifesto, because somebody mentioned that he had some interesting ideas, and though fundamentally misguided, he might have been onto something. I didn't explore much because I was worried---again semi-consciously---that my traffic history would be logged on some NSA computer somewhere, and that I'd pattern match to the Unabomber (I'm a physics grad student, the Unabomber was a mathematician).</li>\n<li>I didn't visit <a href=\"https://en.wikipedia.org/wiki/Silk_Road_(marketplace)\">Silk Road</a> as I was worried that my visits would be traced, even though I had no plans of buying anything.</li>\n<li>Just generally, I try to not search for some really weird stuff that I want to search for (I'm a curious guy!).&nbsp;</li>\n<li>I was almost not going to write this post.&nbsp;</li>\n</ol>\n<div>And these are just the ones that I became conscious of. I wonder how many more have slipped under the radar.</div>\n<div><br /></div>\n<div>Yes, I know these fears are silly. In fact, writing them out makes them feel even more silly. But they still affected my behavior. Now, I may be atypical. But I'm sure I'm not <em>that</em> atypical. I'm sure many, many people refrain from visiting and exploring parts of the Internet and writing things on different forums and blogs because of the fear of being recorded and the data being used against them. Especially susceptible to this fear are immigrants.</div>\n<div><br /></div>\n<div>In <a href=\"/lw/f1/beware_trivial_inconveniences/\">Beware Trivial Inconveniences</a>, Yvain points out that the <a href=\"https://en.wikipedia.org/wiki/Great_Firewall_of_China\">Great Firewall of China</a> is very easy to bypass but the vast majority of Chinese people don't bypass it because it's a trivial inconvenience.</div>\n<div><br /></div>\n<div>I would like to introduce the analogous and very related concept of a&nbsp;<em><strong>trivial fear</strong></em>: fear of low probability events that affects behavior in a major way, especially over a large population. Much more insidiously, the people experiencing these fears don't even realize they're experiencing it: because the fear is of small magnitude, it can be rationalized away easily.</div>\n<div><br /></div>\n<div>In this particular case, the fear acts in a way so as to restrict the desire for information and free speech.</div>\n<div><br /></div>\n<div>In a recent conversation, a friend mentioned that calling the modern surveillance state 'Orwellian' is hyperbole. Maybe so. I don't know if the surveillance state is a Good Thing or a Bad Thing. I'm not an economist or a political scientist or a moral philosopher. I simply want to point out that the main lesson from <em>1984</em> is not the exact details of the dystopia, but the fact that the people living in the dystopia weren't even remotely aware that they were living in one.</div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"CYMR6p5iZG75QAT8a": 1, "FkzScn5byCs9PxGsA": 1, "3ee9k6NJfcGzL6kMS": 1, "5f5c37ee1b5cdee568cfb23e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2kDnKq2QyGfEJMzEo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 55, "extendedScore": null, "score": 0.000152, "legacy": true, "legacyId": "25427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-04T19:17:36.358Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Book Swap meetup", "slug": "meetup-washington-dc-book-swap-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oJdm8cACAnAPBTi3E/meetup-washington-dc-book-swap-meetup-0", "pageUrlRelative": "/posts/oJdm8cACAnAPBTi3E/meetup-washington-dc-book-swap-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/oJdm8cACAnAPBTi3E/meetup-washington-dc-book-swap-meetup-0", "postedAtFormatted": "Tuesday, February 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Book%20Swap%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Book%20Swap%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoJdm8cACAnAPBTi3E%2Fmeetup-washington-dc-book-swap-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Book%20Swap%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoJdm8cACAnAPBTi3E%2Fmeetup-washington-dc-book-swap-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoJdm8cACAnAPBTi3E%2Fmeetup-washington-dc-book-swap-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wi'>Washington DC Book Swap meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 February 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">national portrait gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to exchange books!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wi'>Washington DC Book Swap meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oJdm8cACAnAPBTi3E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5473988808554194e-06, "legacy": true, "legacyId": "25431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup\">Discussion article for the meetup : <a href=\"/meetups/wi\">Washington DC Book Swap meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 February 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">national portrait gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to exchange books!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup1\">Discussion article for the meetup : <a href=\"/meetups/wi\">Washington DC Book Swap meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Book Swap meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Book Swap meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-05T01:00:44.457Z", "modifiedAt": null, "url": null, "title": "[LINK] Reinventing Explanation: Data Presentation as Intuition Pump", "slug": "link-reinventing-explanation-data-presentation-as-intuition", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VAuroch", "createdAt": "2013-11-07T11:01:09.015Z", "isAdmin": false, "displayName": "VAuroch"}, "userId": "idJgwEhhiRhTzHpst", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r54yhGGpcMrSXye8G/link-reinventing-explanation-data-presentation-as-intuition", "pageUrlRelative": "/posts/r54yhGGpcMrSXye8G/link-reinventing-explanation-data-presentation-as-intuition", "linkUrl": "https://www.lesswrong.com/posts/r54yhGGpcMrSXye8G/link-reinventing-explanation-data-presentation-as-intuition", "postedAtFormatted": "Wednesday, February 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Reinventing%20Explanation%3A%20Data%20Presentation%20as%20Intuition%20Pump&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Reinventing%20Explanation%3A%20Data%20Presentation%20as%20Intuition%20Pump%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr54yhGGpcMrSXye8G%2Flink-reinventing-explanation-data-presentation-as-intuition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Reinventing%20Explanation%3A%20Data%20Presentation%20as%20Intuition%20Pump%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr54yhGGpcMrSXye8G%2Flink-reinventing-explanation-data-presentation-as-intuition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr54yhGGpcMrSXye8G%2Flink-reinventing-explanation-data-presentation-as-intuition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p><a href=\"http://michaelnielsen.org/reinventing_explanation/index.html\">A great article</a> by Michael Nielsen on failures of intuition and ways to present data more effectively so that we don't get caught by those failures. It reminded me of concepts like <a href=\"http://wiki.lesswrong.com/wiki/Log_odds\">log odds</a> in common use around here, and also to the recent discussion of <a href=\"/r/discussion/lw/jlx/rationality_lowiq_people/\">teaching rationality techniques to average people.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r54yhGGpcMrSXye8G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 1.5477891997024604e-06, "legacy": true, "legacyId": "25433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["D2dA8g6Goc3f8bkxL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-05T04:12:05.662Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Logical uncertainty", "slug": "meetup-urbana-champaign-logical-uncertainty", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fFZBmH9Ry2dCoW8te/meetup-urbana-champaign-logical-uncertainty", "pageUrlRelative": "/posts/fFZBmH9Ry2dCoW8te/meetup-urbana-champaign-logical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/fFZBmH9Ry2dCoW8te/meetup-urbana-champaign-logical-uncertainty", "postedAtFormatted": "Wednesday, February 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Logical%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Logical%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfFZBmH9Ry2dCoW8te%2Fmeetup-urbana-champaign-logical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Logical%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfFZBmH9Ry2dCoW8te%2Fmeetup-urbana-champaign-logical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfFZBmH9Ry2dCoW8te%2Fmeetup-urbana-champaign-logical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wj'>Urbana-Champaign: Logical uncertainty</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 February 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">412 W. Elm St, Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'm writing a sequence on logical uncertainty, so come ask me anything about it. Digressions encouraged. There may also be cookies.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wj'>Urbana-Champaign: Logical uncertainty</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fFZBmH9Ry2dCoW8te", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.5480069414037888e-06, "legacy": true, "legacyId": "25434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Logical_uncertainty\">Discussion article for the meetup : <a href=\"/meetups/wj\">Urbana-Champaign: Logical uncertainty</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 February 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">412 W. Elm St, Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'm writing a sequence on logical uncertainty, so come ask me anything about it. Digressions encouraged. There may also be cookies.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Logical_uncertainty1\">Discussion article for the meetup : <a href=\"/meetups/wj\">Urbana-Champaign: Logical uncertainty</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Logical uncertainty", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Logical_uncertainty", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Logical uncertainty", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Logical_uncertainty1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-05T06:18:51.306Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Melbourne - Last Practical Rationality Meetup", "slug": "meetup-lw-melbourne-last-practical-rationality-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "CeTijoorgtNBeSrZG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GpPNparcuTuxzPpks/meetup-lw-melbourne-last-practical-rationality-meetup", "pageUrlRelative": "/posts/GpPNparcuTuxzPpks/meetup-lw-melbourne-last-practical-rationality-meetup", "linkUrl": "https://www.lesswrong.com/posts/GpPNparcuTuxzPpks/meetup-lw-melbourne-last-practical-rationality-meetup", "postedAtFormatted": "Wednesday, February 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Melbourne%20-%20Last%20Practical%20Rationality%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Melbourne%20-%20Last%20Practical%20Rationality%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpPNparcuTuxzPpks%2Fmeetup-lw-melbourne-last-practical-rationality-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Melbourne%20-%20Last%20Practical%20Rationality%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpPNparcuTuxzPpks%2Fmeetup-lw-melbourne-last-practical-rationality-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpPNparcuTuxzPpks%2Fmeetup-lw-melbourne-last-practical-rationality-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wk'>LW Melbourne - Last Practical Rationality Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 February 2014 07:15:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 Kings St, West Melbourne., Melbourne </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be having our last Friday night Practical Rationality Meetup this week. It will be the last meetup advertised on Meetup.com.\nThe session will be on the How To Change Your Mind sequence, and is designed for those with no prior knowledge of the sequences.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wk'>LW Melbourne - Last Practical Rationality Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GpPNparcuTuxzPpks", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 0, "baseScore": 0, "extendedScore": null, "score": 1.548151212696202e-06, "legacy": true, "legacyId": "25435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Melbourne___Last_Practical_Rationality_Meetup\">Discussion article for the meetup : <a href=\"/meetups/wk\">LW Melbourne - Last Practical Rationality Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 February 2014 07:15:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 Kings St, West Melbourne., Melbourne </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be having our last Friday night Practical Rationality Meetup this week. It will be the last meetup advertised on Meetup.com.\nThe session will be on the How To Change Your Mind sequence, and is designed for those with no prior knowledge of the sequences.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Melbourne___Last_Practical_Rationality_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/wk\">LW Melbourne - Last Practical Rationality Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Melbourne - Last Practical Rationality Meetup", "anchor": "Discussion_article_for_the_meetup___LW_Melbourne___Last_Practical_Rationality_Meetup", "level": 1}, {"title": "Discussion article for the meetup : LW Melbourne - Last Practical Rationality Meetup", "anchor": "Discussion_article_for_the_meetup___LW_Melbourne___Last_Practical_Rationality_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-05T17:47:30.952Z", "modifiedAt": null, "url": null, "title": "I love zebras", "slug": "i-love-zebras", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:32.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Apprentice", "createdAt": "2009-05-20T13:06:35.976Z", "isAdmin": false, "displayName": "Apprentice"}, "userId": "X5f7X9PkRFwGyxWsb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b82AXhoTdxbvWTq3A/i-love-zebras", "pageUrlRelative": "/posts/b82AXhoTdxbvWTq3A/i-love-zebras", "linkUrl": "https://www.lesswrong.com/posts/b82AXhoTdxbvWTq3A/i-love-zebras", "postedAtFormatted": "Wednesday, February 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20love%20zebras&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20love%20zebras%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb82AXhoTdxbvWTq3A%2Fi-love-zebras%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20love%20zebras%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb82AXhoTdxbvWTq3A%2Fi-love-zebras", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb82AXhoTdxbvWTq3A%2Fi-love-zebras", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10, "htmlBody": "<p>Go zebras!&nbsp;Bevtvany grkg erzbirq nf vg unf freirq vgf checbfr.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b82AXhoTdxbvWTq3A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "25437", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-05T22:54:26.519Z", "modifiedAt": null, "url": null, "title": "Salary or startup? How do-gooders can gain more from risky careers", "slug": "salary-or-startup-how-do-gooders-can-gain-more-from-risky", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nEHwygPMHH4dHNexw/salary-or-startup-how-do-gooders-can-gain-more-from-risky", "pageUrlRelative": "/posts/nEHwygPMHH4dHNexw/salary-or-startup-how-do-gooders-can-gain-more-from-risky", "linkUrl": "https://www.lesswrong.com/posts/nEHwygPMHH4dHNexw/salary-or-startup-how-do-gooders-can-gain-more-from-risky", "postedAtFormatted": "Wednesday, February 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Salary%20or%20startup%3F%20How%20do-gooders%20can%20gain%20more%20from%20risky%20careers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASalary%20or%20startup%3F%20How%20do-gooders%20can%20gain%20more%20from%20risky%20careers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEHwygPMHH4dHNexw%2Fsalary-or-startup-how-do-gooders-can-gain-more-from-risky%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Salary%20or%20startup%3F%20How%20do-gooders%20can%20gain%20more%20from%20risky%20careers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEHwygPMHH4dHNexw%2Fsalary-or-startup-how-do-gooders-can-gain-more-from-risky", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEHwygPMHH4dHNexw%2Fsalary-or-startup-how-do-gooders-can-gain-more-from-risky", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>&nbsp;</p>\n<p>See&nbsp;http://80000hours.org/blog/12-salary-or-startup-how-do-gooders-can-gain-more-from-risky-careers.</p>\n<p>The expected value of risky careers like startups if often much higher than less risky careers. However, this is more than offset by peoples' risk-aversiveness due to diminishing marginal utility. But... if you're an effective altruist, the money you make doesn't have diminishing marginal value. So... it seems that risky careers like startups are a good choice, if you're trying to maximize your positive impact.</p>\n<p>What do you guys think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8sh6iLwYWDJ7z3fPo": 2, "qAvbtzdG2A2RBn7in": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nEHwygPMHH4dHNexw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 1.5492851566444856e-06, "legacy": true, "legacyId": "25439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-05T23:03:10.533Z", "modifiedAt": null, "url": null, "title": "[LINK] Cliffs Notes: \"Probability Theory: The Logic of Science\", part 1", "slug": "link-cliffs-notes-probability-theory-the-logic-of-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fvg8ee6JEHwPWpWka/link-cliffs-notes-probability-theory-the-logic-of-science", "pageUrlRelative": "/posts/fvg8ee6JEHwPWpWka/link-cliffs-notes-probability-theory-the-logic-of-science", "linkUrl": "https://www.lesswrong.com/posts/fvg8ee6JEHwPWpWka/link-cliffs-notes-probability-theory-the-logic-of-science", "postedAtFormatted": "Wednesday, February 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Cliffs%20Notes%3A%20%22Probability%20Theory%3A%20The%20Logic%20of%20Science%22%2C%20part%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Cliffs%20Notes%3A%20%22Probability%20Theory%3A%20The%20Logic%20of%20Science%22%2C%20part%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffvg8ee6JEHwPWpWka%2Flink-cliffs-notes-probability-theory-the-logic-of-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Cliffs%20Notes%3A%20%22Probability%20Theory%3A%20The%20Logic%20of%20Science%22%2C%20part%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffvg8ee6JEHwPWpWka%2Flink-cliffs-notes-probability-theory-the-logic-of-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffvg8ee6JEHwPWpWka%2Flink-cliffs-notes-probability-theory-the-logic-of-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>http://reasonableapproximation.net/2014/02/02/cliffs-notes-pttlos-part-1.html</p>\n<blockquote>\n<p>A book sometimes cited on LessWrong as recommended reading is E.T. Jaynes' <a href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\"><em>Probability Theory: The Logic of Science</em></a>. I intend to write a series of posts reading this book, summarizing the key points, and solving the exercises. (There are no solutions in the book.)</p>\n<p>The book has over 750 pages. This will take me a long time, if I finish at all. I'm not committing to finishing. For example, if this turns out not to be a thing worth doing, I hope that I will notice that and stop. I'm also not committing to any particular posting rate while I continue.</p>\n</blockquote>\n<p>(I'm not cross-posting this because I expect to have to make edits and don't want to have to make them in two places.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fvg8ee6JEHwPWpWka", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "25440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-06T00:24:41.862Z", "modifiedAt": null, "url": null, "title": "How big of an impact would cleaner political debates have on society?", "slug": "how-big-of-an-impact-would-cleaner-political-debates-have-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:58.752Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5BYPdPqsRcQ2u27S9/how-big-of-an-impact-would-cleaner-political-debates-have-on", "pageUrlRelative": "/posts/5BYPdPqsRcQ2u27S9/how-big-of-an-impact-would-cleaner-political-debates-have-on", "linkUrl": "https://www.lesswrong.com/posts/5BYPdPqsRcQ2u27S9/how-big-of-an-impact-would-cleaner-political-debates-have-on", "postedAtFormatted": "Thursday, February 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20big%20of%20an%20impact%20would%20cleaner%20political%20debates%20have%20on%20society%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20big%20of%20an%20impact%20would%20cleaner%20political%20debates%20have%20on%20society%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BYPdPqsRcQ2u27S9%2Fhow-big-of-an-impact-would-cleaner-political-debates-have-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20big%20of%20an%20impact%20would%20cleaner%20political%20debates%20have%20on%20society%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BYPdPqsRcQ2u27S9%2Fhow-big-of-an-impact-would-cleaner-political-debates-have-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BYPdPqsRcQ2u27S9%2Fhow-big-of-an-impact-would-cleaner-political-debates-have-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>See <a href=\"http://www.youtube.com/watch?v=AF-BZsrtoPs\">this Newsroom clip</a>.</p>\n<p>Basically, their news network is trying to change the way political debates work by having the moderator force the candidates to answer the questions that are asked of them, not interrupt each other, justify arguments that are based on obvious falsehoods etc.</p>\n<p>How big of a positive impact do you guys think that this would have on society?</p>\n<p>My initial thoughts are that it would be <em>huge</em>. It would lead to better politicians, which would be a <a href=\"/lw/58g/levels_of_action/\">high level of action</a>. The positive effects would trickle down into many aspects of our society.</p>\n<p>The question then becomes, \"can we make this happen?\". I don't see a way right now, but the idea has enough upside to me that I keep it in the back of my mind in case I come up with a plausible way of implementing the change.</p>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5BYPdPqsRcQ2u27S9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 2, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "25441", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["guDcrPqLsnhEjrPZj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-06T08:50:24.171Z", "modifiedAt": null, "url": null, "title": "Meetup : [ALERT] West LA [ALERT] Location Change!!", "slug": "meetup-alert-west-la-alert-location-change", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P5pWcncEWXRof2nx3/meetup-alert-west-la-alert-location-change", "pageUrlRelative": "/posts/P5pWcncEWXRof2nx3/meetup-alert-west-la-alert-location-change", "linkUrl": "https://www.lesswrong.com/posts/P5pWcncEWXRof2nx3/meetup-alert-west-la-alert-location-change", "postedAtFormatted": "Thursday, February 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5BALERT%5D%20West%20LA%20%5BALERT%5D%20Location%20Change!!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5BALERT%5D%20West%20LA%20%5BALERT%5D%20Location%20Change!!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP5pWcncEWXRof2nx3%2Fmeetup-alert-west-la-alert-location-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5BALERT%5D%20West%20LA%20%5BALERT%5D%20Location%20Change!!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP5pWcncEWXRof2nx3%2Fmeetup-alert-west-la-alert-location-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP5pWcncEWXRof2nx3%2Fmeetup-alert-west-la-alert-location-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wl'>[ALERT] West LA [ALERT] Location Change!!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>WARNING</strong></p>\n\n<p><strong>THE BAD THING WILL HAPPEN IF YOU GO TO THE OLD LOCATION</strong></p>\n\n<p><strong>YOU WILL BE ALONE, AND SCARED</strong></p>\n\n<p><strong><em>WE HAVE MOVED</em></strong></p>\n\n<p>We are now meeting in <a href=\"https://maps.google.com/maps?q=Del+Taco,+11066+Santa+Monica+Boulevard,+Los+Angeles,+CA+90025&amp;hl=en&amp;sll=34.047335,-118.443432&amp;sspn=0.006418,0.012392&amp;t=h&amp;hq=Del+Taco,&amp;hnear=11066+Santa+Monica+Blvd,+Los+Angeles,+California+90025&amp;z=17\" rel=\"nofollow\">this</a> Del Taco.</p>\n\n<p><strong>Parking</strong> is completely free, no chicanery required. This is because we have <em>relocated</em> to a Del Taco where parking is free.</p>\n\n<p><strong>Discussion</strong>:  <em>We have relocated</em>. Do <em>not</em> go to the old location. We are now meeting in a Del Taco near Santa Monica Blvd. and Sepulveda Blvd. It is not a very classy place, but there is cheap food and plenty of room.</p>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li>Please, do not go to the old location. Go to the new location. Seriously.</li>\n</ul>\n\n<p><em>No prior knowledge or exposure to Less Wrong or Del Taco is necessary; this will be generally accessible</em>. Only suffering and death awaits those who go to the wrong place.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wl'>[ALERT] West LA [ALERT] Location Change!!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P5pWcncEWXRof2nx3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.5499646416172773e-06, "legacy": true, "legacyId": "25442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____ALERT__West_LA__ALERT__Location_Change__\">Discussion article for the meetup : <a href=\"/meetups/wl\">[ALERT] West LA [ALERT] Location Change!!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong id=\"WARNING\">WARNING</strong></p>\n\n<p><strong id=\"THE_BAD_THING_WILL_HAPPEN_IF_YOU_GO_TO_THE_OLD_LOCATION\">THE BAD THING WILL HAPPEN IF YOU GO TO THE OLD LOCATION</strong></p>\n\n<p><strong id=\"YOU_WILL_BE_ALONE__AND_SCARED\">YOU WILL BE ALONE, AND SCARED</strong></p>\n\n<p><strong id=\"WE_HAVE_MOVED\"><em>WE HAVE MOVED</em></strong></p>\n\n<p>We are now meeting in <a href=\"https://maps.google.com/maps?q=Del+Taco,+11066+Santa+Monica+Boulevard,+Los+Angeles,+CA+90025&amp;hl=en&amp;sll=34.047335,-118.443432&amp;sspn=0.006418,0.012392&amp;t=h&amp;hq=Del+Taco,&amp;hnear=11066+Santa+Monica+Blvd,+Los+Angeles,+California+90025&amp;z=17\" rel=\"nofollow\">this</a> Del Taco.</p>\n\n<p><strong>Parking</strong> is completely free, no chicanery required. This is because we have <em>relocated</em> to a Del Taco where parking is free.</p>\n\n<p><strong>Discussion</strong>:  <em>We have relocated</em>. Do <em>not</em> go to the old location. We are now meeting in a Del Taco near Santa Monica Blvd. and Sepulveda Blvd. It is not a very classy place, but there is cheap food and plenty of room.</p>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li>Please, do not go to the old location. Go to the new location. Seriously.</li>\n</ul>\n\n<p><em>No prior knowledge or exposure to Less Wrong or Del Taco is necessary; this will be generally accessible</em>. Only suffering and death awaits those who go to the wrong place.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____ALERT__West_LA__ALERT__Location_Change__1\">Discussion article for the meetup : <a href=\"/meetups/wl\">[ALERT] West LA [ALERT] Location Change!!</a></h2>", "sections": [{"title": "Discussion article for the meetup : [ALERT] West LA [ALERT] Location Change!!", "anchor": "Discussion_article_for_the_meetup____ALERT__West_LA__ALERT__Location_Change__", "level": 1}, {"title": "WARNING", "anchor": "WARNING", "level": 2}, {"title": "THE BAD THING WILL HAPPEN IF YOU GO TO THE OLD LOCATION", "anchor": "THE_BAD_THING_WILL_HAPPEN_IF_YOU_GO_TO_THE_OLD_LOCATION", "level": 2}, {"title": "YOU WILL BE ALONE, AND SCARED", "anchor": "YOU_WILL_BE_ALONE__AND_SCARED", "level": 2}, {"title": "WE HAVE MOVED", "anchor": "WE_HAVE_MOVED", "level": 2}, {"title": "Discussion article for the meetup : [ALERT] West LA [ALERT] Location Change!!", "anchor": "Discussion_article_for_the_meetup____ALERT__West_LA__ALERT__Location_Change__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-06T12:29:08.136Z", "modifiedAt": null, "url": null, "title": "True numbers and fake numbers", "slug": "true-numbers-and-fake-numbers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:36.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/87AAbbSw4xWBaGgCf/true-numbers-and-fake-numbers", "pageUrlRelative": "/posts/87AAbbSw4xWBaGgCf/true-numbers-and-fake-numbers", "linkUrl": "https://www.lesswrong.com/posts/87AAbbSw4xWBaGgCf/true-numbers-and-fake-numbers", "postedAtFormatted": "Thursday, February 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20True%20numbers%20and%20fake%20numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrue%20numbers%20and%20fake%20numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87AAbbSw4xWBaGgCf%2Ftrue-numbers-and-fake-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=True%20numbers%20and%20fake%20numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87AAbbSw4xWBaGgCf%2Ftrue-numbers-and-fake-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F87AAbbSw4xWBaGgCf%2Ftrue-numbers-and-fake-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<blockquote>\n<p>In physical science the first essential step in the direction of learning any subject is to find principles of numerical reckoning and practicable methods for measuring some quality connected with it. I often say that when you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meagre and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely in your thoughts advanced to the state of&nbsp;<em>Science</em>, whatever the matter may be.</p>\n<p>-- Lord Kelvin</p>\n</blockquote>\n<p>If you believe that science is about describing things mathematically, you can fall into a strange sort of trap where you come up with some numerical quantity, discover interesting facts about it, use it to analyze real-world situations - but never actually get around to measuring it. I call such things \"theoretical quantities\" or \"fake numbers\", as opposed to \"measurable quantities\" or \"true numbers\".</p>\n<p>An example of a \"true number\" is mass. We can measure the mass of a person or a car, and we use these values in engineering all the time. An example of a \"fake number\" is utility. I've never seen a concrete utility value used anywhere, though I always hear about nice mathematical laws that it must obey.</p>\n<p>The difference is not just about units of measurement. In economics you can see fake numbers happily coexisting with true numbers using the same units. Price is a true number measured in dollars, and you see concrete values and graphs everywhere. \"Consumer surplus\" is also measured in dollars, but good luck calculating the consumer surplus of a single cheeseburger, never mind drawing a graph of aggregate consumer surplus for the US! If you ask five economists to calculate it, you'll get five different indirect estimates, and it's not obvious that there's a true number to be measured in the first place.</p>\n<p>Another example of a fake number is \"complexity\" or \"maintainability\" in software engineering. Sure, people have proposed different methods of measuring it. But if they were measuring a true number, I'd expect them to agree to the 3rd decimal place, which they don't :-) The existence of multiple measuring methods that give the same result is one of the differences between a true number and a fake one. Another sign is what happens when two of these methods disagree: do people say that they're both equally valid, or do they insist that one must be wrong and try to find the error?</p>\n<p>It's certainly possible to improve something without measuring it. You can learn to play the piano pretty well without quantifying your progress. But we should probably try harder to find measurable components of \"intelligence\", \"rationality\", \"productivity\" and other such things, because we'd be better at improving them if we had true numbers in our hands.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "87AAbbSw4xWBaGgCf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 29, "extendedScore": null, "score": 9.9e-05, "legacy": true, "legacyId": "25438", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 131, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-06T13:13:09.505Z", "modifiedAt": null, "url": null, "title": "Expression of Interest: Brisbane LessWrong Meetup", "slug": "expression-of-interest-brisbane-lesswrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:32.122Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TRManderson", "createdAt": "2013-07-28T05:37:44.823Z", "isAdmin": false, "displayName": "TRManderson"}, "userId": "ZK65M6cmP2PGbdTgG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JM6aJbmD6jhBzF8wc/expression-of-interest-brisbane-lesswrong-meetup", "pageUrlRelative": "/posts/JM6aJbmD6jhBzF8wc/expression-of-interest-brisbane-lesswrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/JM6aJbmD6jhBzF8wc/expression-of-interest-brisbane-lesswrong-meetup", "postedAtFormatted": "Thursday, February 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Expression%20of%20Interest%3A%20Brisbane%20LessWrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExpression%20of%20Interest%3A%20Brisbane%20LessWrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJM6aJbmD6jhBzF8wc%2Fexpression-of-interest-brisbane-lesswrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Expression%20of%20Interest%3A%20Brisbane%20LessWrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJM6aJbmD6jhBzF8wc%2Fexpression-of-interest-brisbane-lesswrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJM6aJbmD6jhBzF8wc%2Fexpression-of-interest-brisbane-lesswrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>The new Canberra meetup has spurred me on to think about creating a Brisbane, Australia meetup for LessWrong. I would have posted this in the open thread, but this is significantly more likely to be seen by any potential attendees.</p>\n<p>This post is obviously to see who, if anyone, would be wanting to attend a LessWrong meetup if one was to be started in Brisbane. I'm also curious to see how many LessWrongers also attend things like Brisbane Skeptics in the Pub (at the Plough Inn) or are members of UQ Skeptics etc (and if those people would be interested in a dedicated LW meetup nonetheless).</p>\n<p>Comment away, Brisbane-ites!</p>\n<p>EDIT: People of now, I'm not making any assumptions about upvotes so please comment if you're interested. Those of you who are upvoting anyway, thanks.</p>\n<p>EDIT 2: People of the distant future, feel free to comment however old this post is when you stumble upon it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JM6aJbmD6jhBzF8wc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.5502643899396938e-06, "legacy": true, "legacyId": "25443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T06:23:58.436Z", "modifiedAt": null, "url": null, "title": "[Link] Cause Prioritization - Paul Christiano 1:15h", "slug": "link-cause-prioritization-paul-christiano-1-15h", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.305Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BLDkTGvKy44Sbc8K4/link-cause-prioritization-paul-christiano-1-15h", "pageUrlRelative": "/posts/BLDkTGvKy44Sbc8K4/link-cause-prioritization-paul-christiano-1-15h", "linkUrl": "https://www.lesswrong.com/posts/BLDkTGvKy44Sbc8K4/link-cause-prioritization-paul-christiano-1-15h", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Cause%20Prioritization%20-%20Paul%20Christiano%201%3A15h&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Cause%20Prioritization%20-%20Paul%20Christiano%201%3A15h%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBLDkTGvKy44Sbc8K4%2Flink-cause-prioritization-paul-christiano-1-15h%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Cause%20Prioritization%20-%20Paul%20Christiano%201%3A15h%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBLDkTGvKy44Sbc8K4%2Flink-cause-prioritization-paul-christiano-1-15h", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBLDkTGvKy44Sbc8K4%2Flink-cause-prioritization-paul-christiano-1-15h", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>Paul Christiano speaks about cause priorization within Effective Altruism. Melbourne.<br /><br /><span style=\"font-family: arial, sans-serif; color: #333333;\"><span style=\"line-height: 17px;\">Cause Prioritization: Many effective altruists place a high degree of importance on working out what the most important cause to support is. This is one way that effective altruism is distinguishable from other traditional altruism or charity.</span></span><br /><br /><span style=\"font-family: arial, sans-serif; color: #333333;\"><span style=\"line-height: 17px;\"><a href=\"http://www.youtube.com/watch?v=uAloUCRVa5I\">http://www.youtube.com/watch?v=uAloUCRVa5I</a></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BLDkTGvKy44Sbc8K4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 1.551441322830023e-06, "legacy": true, "legacyId": "25447", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T12:16:19.212Z", "modifiedAt": null, "url": null, "title": "Jobs and internships available at the Centre for Effective Altruism", "slug": "jobs-and-internships-available-at-the-centre-for-effective", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tog", "createdAt": "2011-10-04T12:54:07.164Z", "isAdmin": false, "displayName": "tog"}, "userId": "b4f6teTtsKfegjTaH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pJK8ptvcCzc83jsE3/jobs-and-internships-available-at-the-centre-for-effective", "pageUrlRelative": "/posts/pJK8ptvcCzc83jsE3/jobs-and-internships-available-at-the-centre-for-effective", "linkUrl": "https://www.lesswrong.com/posts/pJK8ptvcCzc83jsE3/jobs-and-internships-available-at-the-centre-for-effective", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Jobs%20and%20internships%20available%20at%20the%20Centre%20for%20Effective%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJobs%20and%20internships%20available%20at%20the%20Centre%20for%20Effective%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpJK8ptvcCzc83jsE3%2Fjobs-and-internships-available-at-the-centre-for-effective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Jobs%20and%20internships%20available%20at%20the%20Centre%20for%20Effective%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpJK8ptvcCzc83jsE3%2Fjobs-and-internships-available-at-the-centre-for-effective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpJK8ptvcCzc83jsE3%2Fjobs-and-internships-available-at-the-centre-for-effective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1078, "htmlBody": "<p dir=\"ltr\"><span>I work at the&nbsp;</span><span><a href=\"http://home.centreforeffectivealtruism.org/\">Centre for Effective Altruism</a></span><span>, and </span><span>thought some LessWrongers might be interested in applying for the jobs and internships that we're </span><span>currently offering - or else know others who might be! CEA&rsquo;s been discussed on LessWrong <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">several</a> <a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">times</a> <a href=\"/lw/ia0/should_you_work_at_80000_hours/\">before</a></span><span>, and is the umbrella organisation to which <a href=\"http://80000hours.org/\">80,000 Hours</a></span><span> and <a href=\"http://givingwhatwecan.org/\">Giving What We Can</a></span><span> belong. We&rsquo;re also starting to branch out into global prioritisation research (e.g. comparing the value of work on AI risk and on helping the global poor) and pure promotion of effective altruist ideas.</span></p>\n<p dir=\"ltr\">CEA is an exciting and inspiring place to work. You&rsquo;re part of a focused team of intelligent, enthusiastic, driven colleagues; everyone involved is passionate about their work, making the office an inspiring place to be. &nbsp;We&rsquo;re based in the centre of Oxford, where we share offices with the Future of Humanity Institute, and which is a global hub for the effective altruist community. For more, see this explanation of <a href=\"http://home.centreforeffectivealtruism.org/careers/why-work-with-us?utm_campaign=lesswrong-jobs-post-2014\">why working at CEA is an excellent opportunity</a>.</p>\n<p dir=\"ltr\">The deadline to apply is 5pm GMT on February the 21st. To apply, or see more information about these roles, visit our <a href=\"http://home.centreforeffectivealtruism.org/careers?utm_campaign=lesswrong-jobs-post-2014\">careers page</a>. If you know anyone who might be interested in these opportunities, do pass them on. Here is the full list of jobs and internships that are available:</p>\n<p dir=\"ltr\"><span><br /></span></p>\n<h1><span>Director of Development</span></h1>\n<p dir=\"ltr\"><span>We&rsquo;re looking for a new staff member to lead our fundraising efforts. CEA is a young and rapidly growing charity, which has grown to a &pound;300,000 annual budget over 18 months. In this role you&rsquo;ll be fundraising for two exciting organisations that are part of the Centre for Effective Altruism - Giving What We Can and 80,000 Hours. CEA has a track record of incubating new charitable projects, and will continue to expand into new areas.</span></p>\n<p dir=\"ltr\">In order to meet its growth targets CEA has set the ambitious goal of raising an extra &pound;150,000 over the next year. Currently, CEA is funded primarily by &lsquo;high net worth&rsquo; donors, but also a number of medium-sized donors focussed on the effectiveness of their donations, and some grants. For this role we are interested in people with previous experience in fundraising. We would be willing to pay up to &pound;40,000, dependent on experience.</p>\n<p dir=\"ltr\"><span><br /></span></p>\n<h1><span>Director of Community at Giving What We Can</span></h1>\n<p dir=\"ltr\"><span>Giving What We Can is looking for an exceptional candidate to lead its outreach to existing and potential members. GWWC asks people to pledge to donate 10% of their income to the best charities that they are aware of; to date, it has found 430 members who have collectively pledged over &pound;100 million. The Director of Community is an essential part of our team, and the process by which we have impact in helping those in poverty. We have found that in order to be willing to pledge to give 10% of their income for the rest of their lives, people need personal contact with a friendly member of staff who can answer their questions and introduce them to other people doing the same.</span></p>\n<p dir=\"ltr\">GWWC is looking for someone willing to stay in this role for many years. If successful, this person could eventually oversee a team of people attracting and engaging members. They would be working closely with a Director of Communications who would help to generate prospective members and an Executive Director who would set the strategic direction of the organisation.</p>\n<p dir=\"ltr\"><span><br /></span></p>\n<h1><span>Lead Developer at 80,000 Hours</span></h1>\n<p dir=\"ltr\"><span>We&rsquo;ve got an exciting opportunity to become a core member of the 80,000 Hours team. We&rsquo;re looking for someone to join us in a full-time paid position as a lead developer, helping 80,000 Hours drive forward its mission to help thousands of people identify and take high leverage ways to have a social impact.</span></p>\n<p dir=\"ltr\">We are looking for a developer to work closely with our founding team, in order to:</p>\n<ul>\n<li><span>Develop an online platform for people to learn about our research</span></li>\n<li><span>Create interactive ways to deliver our career coaching online</span></li>\n<li><span>Help start other tech projects focused on evidence-based approaches to improving the world.</span></li>\n</ul>\n<p><span>There may also be the opportunity to work more broadly with the CEA, helping to foster an incubator for high impact projects. The role is also highly flexible, and we&rsquo;ll tailor it to your skills, interests and development aims.</span></p>\n<p><span><br /></span></p>\n<h1><span>Careers Analyst at 80,000 Hours</span></h1>\n<p dir=\"ltr\"><span>Another full-time paid role at 80,000 Hours that we&rsquo;re advertising for is that of a Careers Analyst, helping us in our mission through providing high quality high impact careers guidance. In this role you would:</span></p>\n<ul>\n<li><span>Give one-on-one coaching to amazing people who want to change the world, as part of our case studies</span></li>\n<li><span>Do research into \ufb01nding the most promising career opportunities in the world</span></li>\n<li><span>Promote our research in the international media, online and through other outreach</span></li>\n<li><span>Monitor our impact.</span></li>\n</ul>\n<p><span>The role is also highly flexible, and we&rsquo;ll tailor it to your skills, interests and development aims. We are particularly interested in people with knowledge of economics.</span></p>\n<p><span><br /></span></p>\n<h1><span>Internships on our Graduate Volunteer Scheme</span></h1>\n<p dir=\"ltr\"><span>Both Giving What We Can and 80,000 Hours are looking for talented and enthusiastic people to volunteer with us full-time in Oxford. We run a Graduate Volunteer Scheme on which you can intern with us for several months. We can often provide accommodation and expenses to ensure that you&rsquo;re not left out of pocket. As a Graduate Volunteer or intern, there are lots of different areas to work on, and there&rsquo;s plenty of flexibility to adjust the role so it plays to your strengths and development aims. We make it our responsibility to ensure your time here allows you to grow as much as possible, as well as just being lots of fun! We&rsquo;re looking for hardworking individuals with a strong desire for personal development who are deeply interested in making the world a better place in an effective way.</span></p>\n<p dir=\"ltr\">See <a href=\"http://home.centreforeffectivealtruism.org/careers/graduate-volunteer-scheme?utm_campaign=lesswrong-jobs-post-2014\">here</a> for more details about the scheme. If you would be interested in interning with us, please <a href=\"http://bit.ly/1bn0RRX\">let us know via this form</a>.</p>\n<p dir=\"ltr\"><span><br /></span></p>\n<h1><span>Two week internships on Giving What We Can's September development programme</span></h1>\n<p dir=\"ltr\"><span>Do you want to join the fight against global poverty and gain experience of research or communications within the voluntary sector? Giving What We Can is running a development programme for students interested in promoting effective charitable giving. In this two week period (15th-26th September 2014) interns will gain training and experience in the area of their choice; either cost-effectiveness Research or Communications. For more details and to apply, see <a href=\"http://givingwhatwecan.org/getting-involved/september-internship-2014?utm_campaign=lesswrong-jobs-post-2014\">this page</a></span><span>.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pJK8ptvcCzc83jsE3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 26, "extendedScore": null, "score": 1.5518439765120602e-06, "legacy": true, "legacyId": "25449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p dir=\"ltr\"><span>I work at the&nbsp;</span><span><a href=\"http://home.centreforeffectivealtruism.org/\">Centre for Effective Altruism</a></span><span>, and </span><span>thought some LessWrongers might be interested in applying for the jobs and internships that we're </span><span>currently offering - or else know others who might be! CEA\u2019s been discussed on LessWrong <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">several</a> <a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">times</a> <a href=\"/lw/ia0/should_you_work_at_80000_hours/\">before</a></span><span>, and is the umbrella organisation to which <a href=\"http://80000hours.org/\">80,000 Hours</a></span><span> and <a href=\"http://givingwhatwecan.org/\">Giving What We Can</a></span><span> belong. We\u2019re also starting to branch out into global prioritisation research (e.g. comparing the value of work on AI risk and on helping the global poor) and pure promotion of effective altruist ideas.</span></p>\n<p dir=\"ltr\">CEA is an exciting and inspiring place to work. You\u2019re part of a focused team of intelligent, enthusiastic, driven colleagues; everyone involved is passionate about their work, making the office an inspiring place to be. &nbsp;We\u2019re based in the centre of Oxford, where we share offices with the Future of Humanity Institute, and which is a global hub for the effective altruist community. For more, see this explanation of <a href=\"http://home.centreforeffectivealtruism.org/careers/why-work-with-us?utm_campaign=lesswrong-jobs-post-2014\">why working at CEA is an excellent opportunity</a>.</p>\n<p dir=\"ltr\">The deadline to apply is 5pm GMT on February the 21st. To apply, or see more information about these roles, visit our <a href=\"http://home.centreforeffectivealtruism.org/careers?utm_campaign=lesswrong-jobs-post-2014\">careers page</a>. If you know anyone who might be interested in these opportunities, do pass them on. Here is the full list of jobs and internships that are available:</p>\n<p dir=\"ltr\"><span><br></span></p>\n<h1 id=\"Director_of_Development\"><span>Director of Development</span></h1>\n<p dir=\"ltr\"><span>We\u2019re looking for a new staff member to lead our fundraising efforts. CEA is a young and rapidly growing charity, which has grown to a \u00a3300,000 annual budget over 18 months. In this role you\u2019ll be fundraising for two exciting organisations that are part of the Centre for Effective Altruism - Giving What We Can and 80,000 Hours. CEA has a track record of incubating new charitable projects, and will continue to expand into new areas.</span></p>\n<p dir=\"ltr\">In order to meet its growth targets CEA has set the ambitious goal of raising an extra \u00a3150,000 over the next year. Currently, CEA is funded primarily by \u2018high net worth\u2019 donors, but also a number of medium-sized donors focussed on the effectiveness of their donations, and some grants. For this role we are interested in people with previous experience in fundraising. We would be willing to pay up to \u00a340,000, dependent on experience.</p>\n<p dir=\"ltr\"><span><br></span></p>\n<h1 id=\"Director_of_Community_at_Giving_What_We_Can\"><span>Director of Community at Giving What We Can</span></h1>\n<p dir=\"ltr\"><span>Giving What We Can is looking for an exceptional candidate to lead its outreach to existing and potential members. GWWC asks people to pledge to donate 10% of their income to the best charities that they are aware of; to date, it has found 430 members who have collectively pledged over \u00a3100 million. The Director of Community is an essential part of our team, and the process by which we have impact in helping those in poverty. We have found that in order to be willing to pledge to give 10% of their income for the rest of their lives, people need personal contact with a friendly member of staff who can answer their questions and introduce them to other people doing the same.</span></p>\n<p dir=\"ltr\">GWWC is looking for someone willing to stay in this role for many years. If successful, this person could eventually oversee a team of people attracting and engaging members. They would be working closely with a Director of Communications who would help to generate prospective members and an Executive Director who would set the strategic direction of the organisation.</p>\n<p dir=\"ltr\"><span><br></span></p>\n<h1 id=\"Lead_Developer_at_80_000_Hours\"><span>Lead Developer at 80,000 Hours</span></h1>\n<p dir=\"ltr\"><span>We\u2019ve got an exciting opportunity to become a core member of the 80,000 Hours team. We\u2019re looking for someone to join us in a full-time paid position as a lead developer, helping 80,000 Hours drive forward its mission to help thousands of people identify and take high leverage ways to have a social impact.</span></p>\n<p dir=\"ltr\">We are looking for a developer to work closely with our founding team, in order to:</p>\n<ul>\n<li><span>Develop an online platform for people to learn about our research</span></li>\n<li><span>Create interactive ways to deliver our career coaching online</span></li>\n<li><span>Help start other tech projects focused on evidence-based approaches to improving the world.</span></li>\n</ul>\n<p><span>There may also be the opportunity to work more broadly with the CEA, helping to foster an incubator for high impact projects. The role is also highly flexible, and we\u2019ll tailor it to your skills, interests and development aims.</span></p>\n<p><span><br></span></p>\n<h1 id=\"Careers_Analyst_at_80_000_Hours\"><span>Careers Analyst at 80,000 Hours</span></h1>\n<p dir=\"ltr\"><span>Another full-time paid role at 80,000 Hours that we\u2019re advertising for is that of a Careers Analyst, helping us in our mission through providing high quality high impact careers guidance. In this role you would:</span></p>\n<ul>\n<li><span>Give one-on-one coaching to amazing people who want to change the world, as part of our case studies</span></li>\n<li><span>Do research into \ufb01nding the most promising career opportunities in the world</span></li>\n<li><span>Promote our research in the international media, online and through other outreach</span></li>\n<li><span>Monitor our impact.</span></li>\n</ul>\n<p><span>The role is also highly flexible, and we\u2019ll tailor it to your skills, interests and development aims. We are particularly interested in people with knowledge of economics.</span></p>\n<p><span><br></span></p>\n<h1 id=\"Internships_on_our_Graduate_Volunteer_Scheme\"><span>Internships on our Graduate Volunteer Scheme</span></h1>\n<p dir=\"ltr\"><span>Both Giving What We Can and 80,000 Hours are looking for talented and enthusiastic people to volunteer with us full-time in Oxford. We run a Graduate Volunteer Scheme on which you can intern with us for several months. We can often provide accommodation and expenses to ensure that you\u2019re not left out of pocket. As a Graduate Volunteer or intern, there are lots of different areas to work on, and there\u2019s plenty of flexibility to adjust the role so it plays to your strengths and development aims. We make it our responsibility to ensure your time here allows you to grow as much as possible, as well as just being lots of fun! We\u2019re looking for hardworking individuals with a strong desire for personal development who are deeply interested in making the world a better place in an effective way.</span></p>\n<p dir=\"ltr\">See <a href=\"http://home.centreforeffectivealtruism.org/careers/graduate-volunteer-scheme?utm_campaign=lesswrong-jobs-post-2014\">here</a> for more details about the scheme. If you would be interested in interning with us, please <a href=\"http://bit.ly/1bn0RRX\">let us know via this form</a>.</p>\n<p dir=\"ltr\"><span><br></span></p>\n<h1 id=\"Two_week_internships_on_Giving_What_We_Can_s_September_development_programme\"><span>Two week internships on Giving What We Can's September development programme</span></h1>\n<p dir=\"ltr\"><span>Do you want to join the fight against global poverty and gain experience of research or communications within the voluntary sector? Giving What We Can is running a development programme for students interested in promoting effective charitable giving. In this two week period (15th-26th September 2014) interns will gain training and experience in the area of their choice; either cost-effectiveness Research or Communications. For more details and to apply, see <a href=\"http://givingwhatwecan.org/getting-involved/september-internship-2014?utm_campaign=lesswrong-jobs-post-2014\">this page</a></span><span>.</span></p>", "sections": [{"title": "Director of Development", "anchor": "Director_of_Development", "level": 1}, {"title": "Director of Community at Giving What We Can", "anchor": "Director_of_Community_at_Giving_What_We_Can", "level": 1}, {"title": "Lead Developer at 80,000 Hours", "anchor": "Lead_Developer_at_80_000_Hours", "level": 1}, {"title": "Careers Analyst at 80,000 Hours", "anchor": "Careers_Analyst_at_80_000_Hours", "level": 1}, {"title": "Internships on our Graduate Volunteer Scheme", "anchor": "Internships_on_our_Graduate_Volunteer_Scheme", "level": 1}, {"title": "Two week internships on Giving What We Can's September development programme", "anchor": "Two_week_internships_on_Giving_What_We_Can_s_September_development_programme", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JmmA2Mf5GrY9D6nQD", "FCiMtrsM8mcmBtfTR", "B4vxxgSqnGTWR43C3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T12:32:23.843Z", "modifiedAt": null, "url": null, "title": "Maximizing life universally", "slug": "maximizing-life-universally", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:34.392Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "4X7y8WCQagvEfGABK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZaKBavunynX6JZ9gw/maximizing-life-universally", "pageUrlRelative": "/posts/ZaKBavunynX6JZ9gw/maximizing-life-universally", "linkUrl": "https://www.lesswrong.com/posts/ZaKBavunynX6JZ9gw/maximizing-life-universally", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Maximizing%20life%20universally&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaximizing%20life%20universally%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZaKBavunynX6JZ9gw%2Fmaximizing-life-universally%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Maximizing%20life%20universally%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZaKBavunynX6JZ9gw%2Fmaximizing-life-universally", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZaKBavunynX6JZ9gw%2Fmaximizing-life-universally", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 390, "htmlBody": "<p>Pain and pleasure as moral standards do not appeal to me. They are easily manipulated by drugs, and can lead to results such as killing sick people against their will.</p>\n<p>To me, life and death is much more interesting. There are issues in defining which lives are to be saved, what it means for a life to be \"maximized\", what life actually is, and so on. I propose trying to remain as objective as possible, and defining life through physics and information theory (think negentropy, Schr&ouml;dinger's \"What is Life\" and related works). I am not skilled in any of these sciences, so my chances of being less wrong in details of this are slim. But what I envision is something like \"Maximize (universally) the amount of computation energy causes before dissolving into high entropy\", or \"Maximize (universally) the number of orderly/non-chaotic events\". Probably severely wrong, but I hope you get the idea.</p>\n<p>I suppose that some rules/actions that may contribute to this goal (not considering all consequences), are:</p>\n<p>- Minimizing killing.</p>\n<p>- Having lots of children.</p>\n<p>- Sustainable agriculture.</p>\n<p>- Utilizing solar energy in deserts.</p>\n<p>- Building computers.</p>\n<p>- Production rather than consumption.</p>\n<p>- Colonizing space.</p>\n<p>and, ultimately, creating superintelligence, even if it means the end to humanity.</p>\n<p>This, to me, is the ultimate altruistic utilitarianism. I don't think I'm an utilitarian, though... But I wonder if some of you clever people have any insights to contribute with, helping me in getting to be less wrong?</p>\n<p>(Placing the following in parentheses is an invitation to discuss this part enclosed in parentheses and doing the main discussion in the wild.</p>\n<p>There are other ideas that appeal more to me personally:</p>\n<p>- Some kind of justice utilitarianism. That is, justice is defined out of people's (or other decent entities') self interest (survival, pleasure and pain, health, wealth, etc.) and the action relations between people (such as \"He hit me first\"). Then the universal goal is maximizing justice (reward and punish) and minimizing injustice (protect innocents).</p>\n<p>- Rational egoism based on maximizing learning.</p>\n<p>- Less attention paid to particular principles and more to everyday responsibilities, subjective conscience, and natural and social conformity.</p>\n<p>Last, but not least, focus on staying human and protecting humanity. Maybe extending it both upwards (think AGI) and downwards (to some other species and to human embryos), but protecting the weakness necessary for securing truthful social relations. Protecting weakness means suppressing potential power, most importantly unfriendly AI.</p>\n<p>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZaKBavunynX6JZ9gw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -17, "extendedScore": null, "score": -1.6e-05, "legacy": true, "legacyId": "25448", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T17:01:52.119Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Canberra", "slug": "new-lw-meetup-canberra", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5kYBbziQHBqyeMSMW/new-lw-meetup-canberra", "pageUrlRelative": "/posts/5kYBbziQHBqyeMSMW/new-lw-meetup-canberra", "linkUrl": "https://www.lesswrong.com/posts/5kYBbziQHBqyeMSMW/new-lw-meetup-canberra", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Canberra&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Canberra%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5kYBbziQHBqyeMSMW%2Fnew-lw-meetup-canberra%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Canberra%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5kYBbziQHBqyeMSMW%2Fnew-lw-meetup-canberra", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5kYBbziQHBqyeMSMW%2Fnew-lw-meetup-canberra", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 538, "htmlBody": "<p><strong>This summary was posted to LW Main on January 31st. The following week's summary is <a href=\"/lw/jmy/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/wc\">Inaugural Canberra meetup:&nbsp;<span class=\"date\">12 February 2014 07:30PM</span></a></li>\n<li><a href=\"/meetups/vz\">Meetup: First Meetup in Hamburg, Germany:&nbsp;<span class=\"date\">07 February 2014 07:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/w2\">Frankfurt meetup:&nbsp;<span class=\"date\">09 February 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/w4\">Sydney Meetup: February:&nbsp;<span class=\"date\">26 February 2014 06:30PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">01 February 2020 01:30PM</span></a></li>\n<li><a href=\"/meetups/w6\">[Berlin] Community Weekend in Berlin:&nbsp;<span class=\"date\">11 April 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/vo\">Brussels: Morality - also cake:&nbsp;<span class=\"date\">08 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/wa\">London - Paranoid Debating 2nd Feb, plus social 9th Feb:&nbsp;<span class=\"date\">02 February 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/wd\">[Los Angeles] Virtual Evidence:&nbsp;<span class=\"date\">05 February 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/vv\">[Melbourne] February Rationality Dojo: Planning Fallacy:&nbsp;<span class=\"date\">02 February 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/w9\">Vienna:&nbsp;<span class=\"date\">15 February 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5kYBbziQHBqyeMSMW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5521704307452556e-06, "legacy": true, "legacyId": "25405", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bux5GvEQXoQL3dfnY", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T17:09:59.692Z", "modifiedAt": null, "url": null, "title": "Publication: the \"anti-science\" trope is culturally polarizing and makes people distrust scientists", "slug": "publication-the-anti-science-trope-is-culturally-polarizing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jm7HeFrmLZBNoDbkS/publication-the-anti-science-trope-is-culturally-polarizing", "pageUrlRelative": "/posts/jm7HeFrmLZBNoDbkS/publication-the-anti-science-trope-is-culturally-polarizing", "linkUrl": "https://www.lesswrong.com/posts/jm7HeFrmLZBNoDbkS/publication-the-anti-science-trope-is-culturally-polarizing", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Publication%3A%20the%20%22anti-science%22%20trope%20is%20culturally%20polarizing%20and%20makes%20people%20distrust%20scientists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APublication%3A%20the%20%22anti-science%22%20trope%20is%20culturally%20polarizing%20and%20makes%20people%20distrust%20scientists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjm7HeFrmLZBNoDbkS%2Fpublication-the-anti-science-trope-is-culturally-polarizing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Publication%3A%20the%20%22anti-science%22%20trope%20is%20culturally%20polarizing%20and%20makes%20people%20distrust%20scientists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjm7HeFrmLZBNoDbkS%2Fpublication-the-anti-science-trope-is-culturally-polarizing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjm7HeFrmLZBNoDbkS%2Fpublication-the-anti-science-trope-is-culturally-polarizing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 294, "htmlBody": "<p>Paper by the Cultural Cognition Project:&nbsp;<a href=\"http://www.culturalcognition.net/blog/2014/1/31/the-culturally-polarizing-effect-of-the-anti-science-trope-o.html\">The culturally polarizing effect of the \"anti-science trope\" on vaccine risk perceptions</a></p>\n<p>This is a great paper (indeed, I think many at LW would find the whole site enjoyable). I'll try to summarize it here.</p>\n<p><span style=\"text-decoration: underline;\">Background:</span> The pro/anti vaccine debate has been hot recently. Many pro-vaccine people often say, \"The science is strong, the benefits are obvious, the risks are negligible; if you're anti-vaccine then you're anti-science\".</p>\n<p><span style=\"text-decoration: underline;\">Methods:</span> They showed experimental subjects an article basically saying the above.</p>\n<p><span style=\"text-decoration: underline;\">Results:</span> When reading such an article, a large number of people did not trust vaccines more, but rather, trusted the American Academy of Pediatrics <span style=\"font-weight: bold;\">less.</span></p>\n<p>&nbsp;</p>\n<p>My thoughts: I will strive to avoid labeling anybody as being \"anti-science\" or \"simply or willfully ignorant of current research\", etc., even when speaking of hypothetical 3rd parties on my facebook wall. This holds for evolution, global warming, vaccines, etc.</p>\n<p>///</p>\n<p>Also included in the article: references to other research that shows that evolution and global warming debates have already polarized people into distrusting scientists, and evidence that people are not yet polarized over the vaccine issue.</p>\n<p>If you intend to read the article yourself: I found it difficult to understand how the authors divided participants into the 4 quadrants (&alpha;, &szlig;, etc.) I will quote my friend, who explained it for me:</p>\n<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 10.909090995788574px; line-height: 12.799999237060547px; background-color: #edeff4;\">I was helped by following the <a href=\"http://www.culturalcognition.net/blog/2013/12/2/why-cultural-predispositions-matter-how-to-measure-them-a-fr.html\">link to where they first introduce that model</a>.</span></p>\n<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 10.909090995788574px; line-height: 12.799999237060547px; background-color: #edeff4;\">The people in the top left (&alpha;) worry about risks to public safety, such as global warming. The people in the bottom right (&delta;) worry about socially deviant behaviors, such as could be caused by the legalization of marijuana.</span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 10.909090995788574px; line-height: 12.799999237060547px; background-color: #edeff4;\" /><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 10.909090995788574px; line-height: 12.799999237060547px; background-color: #edeff4;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 10.909090995788574px; line-height: 12.799999237060547px; background-color: #edeff4;\">People in the top right (&beta;) worry about both public safety risks and deviant behaviors, and people in the bottom left (&gamma;) don't really worry about either.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jm7HeFrmLZBNoDbkS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "25451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T18:30:55.707Z", "modifiedAt": null, "url": null, "title": "L-zombies! (L-zombies?)", "slug": "l-zombies-l-zombies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:01.309Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7nAxgQYGYrEY5ZCAD/l-zombies-l-zombies", "pageUrlRelative": "/posts/7nAxgQYGYrEY5ZCAD/l-zombies-l-zombies", "linkUrl": "https://www.lesswrong.com/posts/7nAxgQYGYrEY5ZCAD/l-zombies-l-zombies", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20L-zombies!%20(L-zombies%3F)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AL-zombies!%20(L-zombies%3F)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nAxgQYGYrEY5ZCAD%2Fl-zombies-l-zombies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=L-zombies!%20(L-zombies%3F)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nAxgQYGYrEY5ZCAD%2Fl-zombies-l-zombies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nAxgQYGYrEY5ZCAD%2Fl-zombies-l-zombies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2174, "htmlBody": "<p><strong>Reply to:</strong> Benja<sub>2010</sub>'s <a href=\"/lw/22m/selfmodification_is_the_correct_justification_for/\">Self-modification is the correct justification for updateless decision theory</a>; Wei Dai's <a href=\"/lw/214/late_great_filter_is_not_bad_news/\">Late great filter is not bad news</a></p>\n<p><em>\"<a href=\"http://wiki.lesswrong.com/wiki/Philosophical_zombie\">P-zombie</a>\" is short for \"<span style=\"color: #ff6600;\"><strong>p</strong></span>hilosophical zombie\", but here I'm going to re-interpret it as standing for \"<span style=\"color: #ff6600;\"><strong>p</strong></span>hysical philosophical zombie\", and contrast it to what I call an \"l-zombie\", for \"<span style=\"color: #ff6600;\"><strong>l</strong></span>ogical philosophical zombie\".</em></p>\n<p>A p-zombie is an ordinary human body with an ordinary human brain that does all the usual things that human brains do, such as the things that cause us to move our mouths and say \"I think, therefore I am\", but that <em>isn't conscious</em>. (The usual consensus on LW is that p-zombies can't exist, but some philosophers disagree.) The notion of p-zombie accepts that human <em>behavior</em> is produced by physical, computable processes, but imagines that these physical processes don't produce <em>conscious experience</em> without some additional epiphenomenal factor.</p>\n<p>An l-zombie is a human being that could have existed, but doesn't: a Turing machine which, if anybody ever ran it, would compute that human's thought processes (and its interactions with a simulated environment); that would, if anybody ever ran it, compute the human saying \"I think, therefore I am\"; <em>but that never gets run</em>, and therefore isn't conscious. (If it's conscious anyway, it's not an l-zombie by this definition.) The notion of l-zombie accepts that human behavior is produced by computable processes, but supposes that these computational processes don't produce conscious experience without being physically instantiated.</p>\n<p>Actually, there probably aren't any l-zombies: <a href=\"http://en.wikipedia.org/wiki/Multiverse#Max_Tegmark.27s_four_levels\">The way the evidence is pointing</a>, it seems like we probably live in a spatially infinite universe where every physically possible human brain is instantiated <em>somewhere</em>, although some are instantiated less frequently than others; and if that's not true, there are the \"bubble universes\" arising from cosmological inflation, the branches of many-worlds quantum mechanics, and Tegmark's \"level IV\" multiverse of all mathematical structures, all suggesting again that all possible human brains are in fact instantiated. But (a) I don't think that even with all that evidence, we can be <em>overwhelmingly</em> certain that all brains are instantiated; and, more importantly actually, (b) I think that thinking about l-zombies can yield some useful insights into how to think about worlds where all humans exist, but some of them have more measure (\"magical reality fluid\") than others.</p>\n<p>So I ask: Suppose that we do indeed live in a world with l-zombies, where only some of all mathematically possible humans exist physically, and only those that do have conscious experiences. How should someone living in such a world reason about their experiences, and how should they make decisions &mdash; keeping in mind that if they were an l-zombie, they would still say \"<em>I</em> have conscious experiences, so clearly <em>I</em> can't be an l-zombie\"?</p>\n<p>If we <em>can't</em> update on our experiences to conclude that someone having these experiences must exist in the physical world, then we must of course conclude that we are almost certainly l-zombies: After all, if the physical universe isn't combinatorially large, the vast majority of mathematically possible conscious human experiences are <em>not</em> instantiated. You might argue that the universe you live in seems to run on relatively simple physical rules, so it should have high prior probability; but we haven't really figured out the exact rules of our universe, and although what we understand seems compatible with the hypothesis that there are simple underlying rules, that's not really proof that there <em>are</em> such underlying rules, if \"the <em>real</em> universe has simple rules, but <em>we</em> are l-zombies living in some random simulation with a hodgepodge of rules (that isn't actually ran)\" has the same prior probability; and worse, if you don't have all we <em>do</em> know about these rules loaded into your brain right now, you can't really verify that they make sense, since there is some mathematically possible simulation whose initial state has you <em>remember</em> seeing evidence that such simple rules exist, even if they don't; and much worse still, even if there <em>are</em> such simple rules, what evidence do you have that if these rules were actually executed, they would produce <em>you</em>? Only the fact that you, like, exist, but we're asking what happens if we don't let you update on that.</p>\n<p>I find myself quite unwilling to accept this conclusion that I shouldn't update, in the world we're talking about. I mean, I <em>actually</em> have conscious experiences. I, like, feel them and stuff! Yes, true, my slightly altered alter ego would reason the same way, and it would be wrong; but <em>I'm</em> right...</p>\n<p>...and <em>that</em> actually seems to offer a way out of the conundrum: Suppose that I decide to update on my experience. Then so will my alter ego, the l-zombie. This leads to a lot of l-zombies concluding \"I think, therefore I am\", and being <em>wrong</em>, and a lot of actual people concluding \"I think, therefore I am\", and being <em>right</em>. All the thoughts that are actually <em>consciously experienced</em> are, in fact, correct. This doesn't seem like such a terrible outcome. Therefore, I'm willing to provisionally endorse the reasoning \"I think, therefore I am\", and to endorse updating on the fact that I have conscious experiences to draw inferences about physical reality &mdash; taking into account the simulation argument, of course, and conditioning on living in a small universe, which is all I'm discussing in this post.</p>\n<p style=\"padding-left: 30px;\"><em><strong>NB. </strong></em>There's still something quite uncomfortable about the idea that all of my behavior, including the fact that I say \"I think therefore I am\", is explained by the mathematical process, but actually <em>being</em> conscious requires some extra magical reality fluid. So I still feel confused, and using the word l-zombie in analogy to p-zombie is a way of highlighting that. But this line of reasoning still feels like progress. FWIW.</p>\n<p>But if <em>that's</em> how we justify <em>believing</em> that we physically exist, that has some implications for how we should decide what to <em>do</em>. The argument is that nothing very bad happens if the l-zombies wrongly conclude that they actually exist. Mostly, that also seems to be true if they <em>act</em> on that belief: mostly, what l-zombies do doesn't seem to influence what happens in the real world, so if only things that actually happen are morally important, it doesn't seem to matter what the l-zombies decide to do. But there are exceptions.</p>\n<p>Consider the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a>: Accurate and trustworthy <a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> appears to you and explains that it just has thrown a very biased coin that had only a 1/1000 chance of landing heads. As it turns out, this coin <em>has</em> in fact landed heads, and now Omega is offering you a choice: It can either (A) create a Friendly AI or (B) destroy humanity. Which would you like? There is a catch, though: Before it threw the coin, Omega made a prediction about what you would do if the coin fell heads (and it was able to make a confident prediction about what you would choose). If the coin had fallen tails, it would have created an FAI if it has predicted that you'd choose (B), and it would have destroyed humanity if it has predicted that you would choose (A). (If it hadn't been able to make a confident prediction about what you would choose, it would just have destroyed humanity outright.)</p>\n<p>There is a clear argument that, if you expect to find yourself in a situation like this in the future, you would want to self-modify into somebody who would choose (B), since this gives humanity a much larger chance of survival. Thus, a decision theory stable under self-modification would answer (B). But if you update on the fact that you consciously experience Omega telling you that the coin landed heads, (A) would seem to be the better choice!</p>\n<p>One way of looking at this is that if the coin falls tails, the l-zombie that is told the coin landed heads still exists mathematically, and this l-zombie now has the power to influence what happens in the real world. If the argument for updating was that nothing bad happens even though the l-zombies get it wrong, well, that argument breaks here. The mathematical process that is your mind doesn't have any evidence about whether the coin landed heads or tails, because as a mathematical object it exists in both possible worlds, and it has to make a decision in both worlds, and that decision affects humanity's future in both worlds.</p>\n<p>Back in 2010, I wrote <a href=\"/lw/22m/selfmodification_is_the_correct_justification_for/\">a post</a> arguing that yes, you would want to self-modify into something that would choose (B), but that that was the <em>only</em> reason why you'd want to choose (B). Here's a variation on the above scenario that illustrates the point I was trying to make back then: Suppose that Omega tells you that it actually threw its coin a million years ago, and if it had fallen tails, it would have turned Alpha Centauri purple. Now throughout your history, the argument goes, you would never have had any motive to self-modify into something that chooses (B) in this particular scenario, because you've always known that Alpha Centauri isn't, in fact, purple.</p>\n<p>But this argument assumes that you know you're not a l-zombie; if the coin had in fact fallen tails, you wouldn't exist as a conscious being, but you'd still exist as a mathematical decision-making process, and that process would be able to influence the real world, so you-the-decision-process can't reason that \"I think, therefore I am, therefore the coin must have fallen heads, therefore I should choose (A).\" Partly because of this, I now accept choosing (B) as the (most likely to be) correct choice even in that case. (The rest of my change in opinion has to do with all ways of making my earlier intuition formal getting into trouble in decision problems where you can influence whether you're brought into existence, but that's a topic for another post.)</p>\n<p>However, should you <em>feel cheerful</em> while you're announcing your choice of (B), since with high (prior) probability, you've just saved humanity? That would lead to an actual conscious being feeling cheerful if the coin has landed heads and humanity is going to be destroyed, and an l-zombie computing, but <em>not</em> actually experiencing, cheerfulness if the coin has landed heads and humanity is going to be saved. Nothing good comes out of feeling cheerful, not even alignment of a conscious' being's map with the physical territory. So I think the correct thing is to choose (B), and to be deeply sad about it.</p>\n<p>You may be asking why I should care what the right probabilities to assign or the right feelings to have are, since these don't seem to play any role in making decisions; sometimes you make your decisions as if updating on your conscious experience, but sometimes you don't, and you always get the right answer if you <em>don't</em> update in the first place. Indeed, I expect that the \"correct\" design for an AI is to fundamentally use (more precisely: approximate) updateless decision theory (though I also expect that probabilities updated on the AI's sensory input will be useful for many intermediate computations), and \"I compute, therefore I am\"-style reasoning will play no fundamental role in the AI. And I think the same is true for humans' decisions &mdash; the correct way to act is given by updateless reasoning. But as a human, I find myself unsatisfied by not being able to have a picture of what the physical world probably looks like. I may not need one to figure out how I should act; I still want one, not for instrumental reasons, but because I want one. In a small universe where most mathematically possible humans are l-zombies, the argument in this post seems to give me a justification to say \"I think, therefore I am, therefore probably I either live in a simulation or what I've learned about the laws of physics describes how the real world works (even though there are many l-zombies who are thinking similar thoughts but are wrong about them).\"</p>\n<p>And because of this, even though I disagree with my 2010 post, I also still disagree with <a href=\"/lw/214/late_great_filter_is_not_bad_news/\">Wei Dai's 2010 post arguing that a late Great Filter is good news</a>, which my own 2010 post was trying to argue against. Wei argued that if Omega gave you a choice between (A) destroying the world <em>now</em> and (B) having Omega destroy the world a million years ago (so that you are never instantiated as a conscious being, though your choice as an l-zombie still influences the real world), then you would choose (A), to give humanity at least the time it's had so far. Wei concluded that this means that if you learned that the Great Filter is in our future, rather than our past, that must be <em>good news</em>, since if you could choose where to place the filter, you should place it in the future. I now agree with Wei that (A) is the right choice, but I don't think that you should be <em>happy</em> about it. And similarly, I don't think you should be happy about news that tells you that the Great Filter is later than you might have expected.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"uLqT8mmFiA8NeytTi": 3, "PbShukhzpLsWpGXkM": 1, "dPPATLhRmhdJtJM2t": 1, "XSryTypw5Hszpa4TS": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7nAxgQYGYrEY5ZCAD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 50, "extendedScore": null, "score": 0.00015, "legacy": true, "legacyId": "25366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["88vuFDw3dCX7hC6uW", "Bnv7mxzsgNjYuLcAy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T19:07:43.241Z", "modifiedAt": null, "url": null, "title": "MEETUP: February 9, Philadelphia", "slug": "meetup-february-9-philadelphia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.004Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qm48wsDFzDfNxwqKN/meetup-february-9-philadelphia", "pageUrlRelative": "/posts/qm48wsDFzDfNxwqKN/meetup-february-9-philadelphia", "linkUrl": "https://www.lesswrong.com/posts/qm48wsDFzDfNxwqKN/meetup-february-9-philadelphia", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MEETUP%3A%20February%209%2C%20Philadelphia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMEETUP%3A%20February%209%2C%20Philadelphia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqm48wsDFzDfNxwqKN%2Fmeetup-february-9-philadelphia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MEETUP%3A%20February%209%2C%20Philadelphia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqm48wsDFzDfNxwqKN%2Fmeetup-february-9-philadelphia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqm48wsDFzDfNxwqKN%2Fmeetup-february-9-philadelphia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>February 9th, <a href=\"http://www.namphuongphilly.com/\">Nam Phuong</a> at 11th and Washington (a little south of the intersection) at 1PM.</p>\n<p>The discussion prompt is Gwern's <a href=\"http://www.gwern.net/The%20Narrowing%20Circle\">The Narrowing Circle</a>.</p>\n<blockquote>\"The &ldquo;expanding circle&rdquo; historical thesis ignores all instances in which modern ethics narrowed the set of beings to be morally regarded, and assumes its conclusion.\"</blockquote>\n<p>If you'd like to get email notifications, there's a forum with email sign-up <a href=\"https://groups.google.com/forum/#!forum/lesswrong-philadelphia\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qm48wsDFzDfNxwqKN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "25452", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-07T19:17:55.841Z", "modifiedAt": null, "url": null, "title": "Meetup : London VOI meetup 16/2, plus socials 9/2 and 23/2", "slug": "meetup-london-voi-meetup-16-2-plus-socials-9-2-and-23-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qZNaRszd5KwCHJsRa/meetup-london-voi-meetup-16-2-plus-socials-9-2-and-23-2", "pageUrlRelative": "/posts/qZNaRszd5KwCHJsRa/meetup-london-voi-meetup-16-2-plus-socials-9-2-and-23-2", "linkUrl": "https://www.lesswrong.com/posts/qZNaRszd5KwCHJsRa/meetup-london-voi-meetup-16-2-plus-socials-9-2-and-23-2", "postedAtFormatted": "Friday, February 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20VOI%20meetup%2016%2F2%2C%20plus%20socials%209%2F2%20and%2023%2F2&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20VOI%20meetup%2016%2F2%2C%20plus%20socials%209%2F2%20and%2023%2F2%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZNaRszd5KwCHJsRa%2Fmeetup-london-voi-meetup-16-2-plus-socials-9-2-and-23-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20VOI%20meetup%2016%2F2%2C%20plus%20socials%209%2F2%20and%2023%2F2%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZNaRszd5KwCHJsRa%2Fmeetup-london-voi-meetup-16-2-plus-socials-9-2-and-23-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZNaRszd5KwCHJsRa%2Fmeetup-london-voi-meetup-16-2-plus-socials-9-2-and-23-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wm'>London VOI meetup 16/2, plus socials 9/2 and 23/2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 February 2014 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next not-purely-social London meetup will be on the 15th of February, on the subject of value of information calculations: what they are, when to do them, how to do them, things we've previously used them for (successfully or not), etc.</p>\n\n<p>As usual, we'll be in the Shakespeare's Head from 2pm. If you have trouble finding us, you can reach me on 07792009646.</p>\n\n<p>We are also continuing our regime of weekly meetings, with social meetups on the 9th and 23rd of February, at the same time and place. To avoid spamming the discussion list, I'm not posting them separately, but they will be happening. One of them is this Sunday! Come along!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wm'>London VOI meetup 16/2, plus socials 9/2 and 23/2</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qZNaRszd5KwCHJsRa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "25453", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_VOI_meetup_16_2__plus_socials_9_2_and_23_2\">Discussion article for the meetup : <a href=\"/meetups/wm\">London VOI meetup 16/2, plus socials 9/2 and 23/2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 February 2014 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, Africa House, 64-68 Kingsway, London WC2B 6BG, UK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next not-purely-social London meetup will be on the 15th of February, on the subject of value of information calculations: what they are, when to do them, how to do them, things we've previously used them for (successfully or not), etc.</p>\n\n<p>As usual, we'll be in the Shakespeare's Head from 2pm. If you have trouble finding us, you can reach me on 07792009646.</p>\n\n<p>We are also continuing our regime of weekly meetings, with social meetups on the 9th and 23rd of February, at the same time and place. To avoid spamming the discussion list, I'm not posting them separately, but they will be happening. One of them is this Sunday! Come along!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_VOI_meetup_16_2__plus_socials_9_2_and_23_21\">Discussion article for the meetup : <a href=\"/meetups/wm\">London VOI meetup 16/2, plus socials 9/2 and 23/2</a></h2>", "sections": [{"title": "Discussion article for the meetup : London VOI meetup 16/2, plus socials 9/2 and 23/2", "anchor": "Discussion_article_for_the_meetup___London_VOI_meetup_16_2__plus_socials_9_2_and_23_2", "level": 1}, {"title": "Discussion article for the meetup : London VOI meetup 16/2, plus socials 9/2 and 23/2", "anchor": "Discussion_article_for_the_meetup___London_VOI_meetup_16_2__plus_socials_9_2_and_23_21", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-08T01:20:29.528Z", "modifiedAt": null, "url": null, "title": "White Lies", "slug": "white-lies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:35.850Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kg6hkNENWa8e84rAE/white-lies", "pageUrlRelative": "/posts/Kg6hkNENWa8e84rAE/white-lies", "linkUrl": "https://www.lesswrong.com/posts/Kg6hkNENWa8e84rAE/white-lies", "postedAtFormatted": "Saturday, February 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20White%20Lies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhite%20Lies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKg6hkNENWa8e84rAE%2Fwhite-lies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=White%20Lies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKg6hkNENWa8e84rAE%2Fwhite-lies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKg6hkNENWa8e84rAE%2Fwhite-lies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1626, "htmlBody": "<p><strong>Background: </strong>As can be seen from some of the comments on this post, many people in the LessWrong community take an extreme stance on lying. A few days before I posted this, I was at a meetup where we played the game&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Resistance_(game)\">Resistance</a>, and one guy announced before the game began that he had a policy of never lying <em>even when playing games like that. </em>It's such members of the LessWrong community that this post was written for. I'm <em>not </em>trying to encourage basically honest people with the normal view of white lies that they need to give up being basically honest.</p>\n<hr />\n<blockquote>\n<p>Mr. Potter, you sometimes make a game of lying with truths, playing with words to conceal your meanings in plain sight. I, too, have been known to find that amusing. But if I so much as <em>tell</em> you what I hope we shall do this day, Mr. Potter, you will <em>lie</em> about it. You will lie straight out, without hesitation, without wordplay or hints, to anyone who asks about it, be they foe or closest friend. You will lie to Malfoy, to Granger, and to McGonagall. You will speak, always and without hesitation, in exactly the fashion you would speak if you knew nothing, with no concern for your honor. That also is how it must be.</p>\n</blockquote>\n<p>- Rational!Quirrell, <em><a href=\"http://hpmor.com/chapter/51\">Harry Potter and the Methods of Rationality</a></em></p>\n<p>This post isn't about HMPOR, so I won't comment on the fictional situation the quote comes from. But in many real-world situations, it's excellent advice.</p>\n<p>If you're a gay teenager with homophobic parents, and there's a real chance they'd throw you out on the street if they found out you were gay, you should probably lie to them about it. Even in college, if you're still financially dependent on them, I think it's okay to lie. The minute you're no longer financially dependent on them, you should absolutely come out for your sake and the sake of the world. But it's OK to lie if you need to to keep your education on-track.</p>\n<p>Oh, maybe you could get away with just shutting up and hoping the topic doesn't come up. When asked about dating, you could try to evade while being technically truthful: \"There just aren't any girls at my school I really like.\" \"What about _____? Why don't you ask her out?\" \"We're just friends.\" That might work. But when asked directly \"are you gay?\" and the wrong answer could seriously screw-up your life, I wouldn't bet too much on your ability to \"lie with truths,\" as Quirrell would say.</p>\n<p>I start with this example because the discussions I've seen on the ethics of lying on LessWrong (and everywhere, actually) tend to focus on the extreme cases: the now-clich&eacute; \"Nazis at the door\" example, or even discussion of whether you'd lie with the world at stake. The \"teen with homophobic parents\" case, on the other hand, might have actually happened to someone you know. But even this case is extreme compared to most of the lies people tell on a regular basis.</p>\n<p>Widely-cited statistics claim that <a href=\"http://smg.media.mit.edu/library/DePauloEtAl.LyingEverydayLife.pdf\">the average person lies once per day</a>. I recently saw a new study (that I can't find at the moment) that disputed this, and claimed most people lie rather less often than that, but it still found most people lie fairly often. These lies are mostly \"white lies\" to, say, spare others' feelings. Most people have no qualms about those kind of lies. So why do discussions of the ethics of lying so often focus on the extreme cases, as if those were the only ones where lying is maybe possibly morally permissible?</p>\n<p>At LessWrong there've been discussions of <a href=\"/lw/j9/radical_honesty/\">several different views all described as \"radical honesty.\"</a>&nbsp;No one I know of, though, has advocated Radical Honesty as defined by psychotherapist Brad Blanton, which (among other things) demands that people share every negative thought they have about other people. (If you haven't, I recommend reading <a href=\"http://www.esquire.com/print-this/honesty0707?x\">A. J. Jacobs on Blanton's movement</a>.) While I'm glad no one here is thinks Blanton's version of radical honesty is a good idea, a strict no-lies policy can sometimes have effects that are just as disastrous.</p>\n<p>A few years ago, for example, when I went to see the play my girlfriend had done stage crew for, and she asked what I thought of it. She wasn't satisfied with my initial noncommittal answers, so she pressed for more. Not in a \"trying to start a fight\" way; I just wasn't doing a good job of being evasive. I eventually gave in and explained why I thought the acting had sucked, which did not make her happy. I think incidents like that must have contributed to our breaking up shortly thereafter. The breakup was a good thing for other reasons, but I still regret not lying to her about what I thought of the play.</p>\n<p>Yes, there are probably things I could've said in that situation that would have been not-lies and also would have avoided upsetting her. Sam Harris, in his book <em><a href=\"http://www.amazon.com/Lying-Sam-Harris-ebook/dp/B00G1SRB6Q\">Lying</a>,&nbsp;</em>spends a lot of arguing against lying in that way: he takes situations where most people would be tempted to tell a white lie, and suggesting ways around it. But for that to work, you need to be good at striking the delicate balance between saying too little and saying too much, and framing hard truths diplomatically. Are people who lie because they lack that skill really less moral than people who are able to avoid lying because they have it?</p>\n<p>Notice the signaling issue here: Sam Harris' book is a subtle brag that <em>he </em>has the skills to tell people the truth without too much backlash. This is especially true when Harris gives examples from his own life, like the time he told a friend \"No one would ever call you 'fat,' but I think you could probably lose twenty-five pounds.\" and his friend went and did it rather than getting angry. Conspicuous honesty also overlaps with conspicuous outrage, the signaling move that announces (as Steven Pinker put it) \"I'm so talented, wealthy, popular, or well-connected that I can afford to offend you.\"</p>\n<p>If you're highly averse to lying, I'm not going to spend a lot of time trying to convince you to tell white lies more often. But I will implore you to do one thing:<strong>&nbsp;</strong><em>accept other people's right to lie to you. </em>About some topics, anyway. Accept that some things are none of your business, and sometimes that includes the fact that there's something which is none of your business.</p>\n<p>Or: suppose you ask someone for something, they say \"no,\" and you suspect their reason for saying \"no\" is a lie. When that happens, don't get mad or press them for the real reason. Among other things, they may be operating on the assumptions of <a href=\"/lw/jis/tell_culture/\">guess culture</a>, where your request means you strongly expected a \"yes\" and you might not think their real reason for saying \"no\" was good enough. Maybe <em>you</em> know you'd take an honest refusal well (even if it's \"I don't want to and don't think I owe you that\"), but they don't necessarily know that. And maybe you <em>think </em>you'd take an honest refusal well, but what if you're lying to yourself?</p>\n<p>If it helps to be more concrete: Some men will react badly to being turned down for a date. Some women too, but probably more men, so I'll make this gendered. And also because dealing with someone who won't take \"no\" for an answer is a scarier experience with the asker is a man and the person saying \"no\" is a woman. So I sympathize with women who give made-up reasons for saying \"no\" to dates, to make saying \"no\" easier.</p>\n<p>Is it always the wisest decision? Probably not. But sometimes, I suspect, it is. And I'd advise men to accept that women doing that is OK. Not only that, I wouldn't want to be part of a community with lots of men who didn't get things like that. That's the kind of thing I have in mind when I say to respect other people's right to lie to you.</p>\n<p>All this needs the disclaimer that some domains should be lie-free zones. I value the truth and despise those who would corrupt intellectual discourse with lies. Or, as Eliezer <a href=\"/lw/j9/radical_honesty/\">once put it</a>:</p>\n<blockquote>\n<p>We believe that scientists should always tell the whole truth&nbsp;<em>about science.</em>&nbsp;It's one thing to lie in everyday life, lie to your boss, lie to the police, lie to your lover; but whoever lies&nbsp;<em>in a journal article</em>&nbsp;is guilty of utter heresy and will be excommunicated.</p>\n</blockquote>\n<p>I worry this post will be dismissed as trivial. I simultaneously worry that, even with the above disclaimer, someone is going to respond, \"Chris admits to thinking lying is often okay, now we can't trust anything he says!\" If you're thinking of saying that, that's your problem, not mine. Most people will lie to you occasionally, and if you get upset about it you're setting yourself up for a lot of unhappiness. And refusing to trust someone who lies <em>sometimes </em>isn't actually very rational; all but the most prolific liars don't lie anything like half the time, so what they say is still significant evidence, most of the time. (Maybe such declarations-of-refusal-to-trust shouldn't be taken as <em>arguments </em>so much as <em>threats </em>meant to coerce more honesty than most people feel bound to give.)</p>\n<p>On the other hand, if we ever meet in person, I hope you realize I might lie to you. Failure to realize a statement could be a white lie can create some <em>terribly </em>awkward situations.</p>\n<p><em>Edits: </em>Changed title, added background, clarified the section on accepting other people's right to lie to you (partly cutting and pasting from <a href=\"/lw/jkr/white_lies/aiwx\">this comment</a>).</p>\n<p><em>Edit round 2: </em>Added link to paper supporting claim that the average person lies once per day.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nANxo5C4sPG9HQHzr": 1, "cHoCqtfE9cF7aSs9d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kg6hkNENWa8e84rAE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 59, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "25371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 902, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GMhzDb3uAFYLwmXtY", "rEBXN3x6kXgD4pLxs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-08T01:34:03.772Z", "modifiedAt": null, "url": null, "title": "Preferences without Existence", "slug": "preferences-without-existence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NvwJMQvfu9hbBdG6d/preferences-without-existence", "pageUrlRelative": "/posts/NvwJMQvfu9hbBdG6d/preferences-without-existence", "linkUrl": "https://www.lesswrong.com/posts/NvwJMQvfu9hbBdG6d/preferences-without-existence", "postedAtFormatted": "Saturday, February 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Preferences%20without%20Existence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APreferences%20without%20Existence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNvwJMQvfu9hbBdG6d%2Fpreferences-without-existence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Preferences%20without%20Existence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNvwJMQvfu9hbBdG6d%2Fpreferences-without-existence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNvwJMQvfu9hbBdG6d%2Fpreferences-without-existence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1058, "htmlBody": "<p><a href=\"http://bywayofcontradiction.com/preferences-without-existence/\">Cross-posted</a> on <a href=\"http://bywayofcontradiction.com/\">By Way of Contradiction</a></p>\n<p>My current beliefs say that there is a Tegmark 4 (or larger) multiverse, but there is no meaningful &ldquo;reality fluid&rdquo; or &ldquo;probability&rdquo; measure on it. We are all in this infinite multiverse, but there is no sense in which some parts of it exist more or are more likely than any other part. I have tried to illustrate these beliefs as an imaginary conversation between two people. My goal is to either share this belief, or more likely to get help from you in understanding why it is completely wrong.</p>\n<p>A: Do you know what the game of life is?</p>\n<p>B: Yes, of course, it is a cellular automaton. You start with a configuration of cells, and they update following a simple deterministic rule. It is a simple kind of simulated universe.</p>\n<p>A: Did you know that when you run the game of life on an initial condition of a 2791 by 2791 square of live cells, and run it for long enough, creatures start to evolve. (Not true)</p>\n<p>B: No. That&rsquo;s amazing!</p>\n<p>A: Yeah, these creatures have developed language and civilization. Time step 1,578,891,000,000,000 seems like it is a very important era for them, They have developed much technology, and it someone has developed the theory of a doomsday device that will kill everyone in their universe, and replace the entire thing with emptyness, but at the same time, many people are working hard on developing a way to stop him.</p>\n<p>B:How do you know all this?</p>\n<p>A: We have been simulating them on our computers. We have simulated up to that crucial time.</p>\n<p>B: Wow, let me know what happens. I hope they find a way to stop him</p>\n<p>A: Actually, the whole project is top secret now. The simulation will still be run, but nobody will ever know what happens.</p>\n<p>B: Thats too bad. I was curious, but I still hope the creatures live long, happy, interesting lives.</p>\n<p>A: What? Why do you hope that? It will never have any effect over you.</p>\n<p>B: My utility function includes preferences between different universes even if I never get to know the result.</p>\n<p>A: Oh, wait, I was wrong. It says here the whole project is canceled, and they have stopped simulating.</p>\n<p>B: That is to bad, but I still hope they survive.</p>\n<p>A: They won&rsquo;t survive, we are not simulating them any more.</p>\n<p>B: No, I am not talking about the simulation, I am talking about the simple set of mathematical laws that determine their world. I hope that those mathematical laws if run long enough do interesting things.</p>\n<p>A: Even though you will never know, and it will never even be run in the real universe.</p>\n<p>B: Yeah. It would still be beautiful if it never gets run and no one ever sees it.</p>\n<p>A: Oh, wait. I missed something. It is not actually the game of life. It is a different cellular automaton they used. It says here that it is like the game of life, but the actual rules are really complicated, and take millions of bits to describe.</p>\n<p>B: That is too bad. I still hope they survive, but not nearly as much.</p>\n<p>A: Why not?</p>\n<p>B: I think information theoretically simpler things are more important and more beautiful. It is a personal preference. It is much more desirable to me to have a complex interesting world come from simple initial conditions.</p>\n<p>A: What if I told you I lied, and none of these simulations were run at all and never would be run. Would you have a preference over whether the simple configuration or the complex configuration had the life?</p>\n<p>B: Yes, I would prefer if the simple configuration to have the life.</p>\n<p>A: Is this some sort of Solomonoff probability measure thing?</p>\n<p>B: No actually. It is independent of that. If the only existing things were this universe, I would still want laws of math to have creatures with long happy interesting lives arise from simple initial conditions.</p>\n<p>A: Hmm, I guess I want that too. However, that is negligible compared to my preferences about things that really do exist.</p>\n<p>B: That statement doesn&rsquo;t mean much to me, because I don&rsquo;t think this existence you are talking about is a real thing.</p>\n<p>A: What? That doesn&rsquo;t make any sense.</p>\n<p>B: Actually, it all adds up to normality.</p>\n<p>A: I see why you can still have preferences without existence, but what about beliefs?</p>\n<p>B: What do you mean?</p>\n<p>A:  Without a concept of existence, you cannot have Solomonoff induction to tell you how likely different worlds are to exist.</p>\n<p>B: I do not need it. I said I care more about simple universes than complicated ones, so I already make my decisions to maximize utility weighted by simplicity. It comes out exactly the same, I do not need to believe simple things exist more, because I already believe simple things matter more.</p>\n<p>A: But then you don&rsquo;t actually anticipate that you will observe simple things rather than complicated things.</p>\n<p>B: I care about my actions more in the cases where I observe simple things, so I prepare for simple things to happen. What is the difference between that and anticipation?</p>\n<p>A: I feel like there is something different, but I can&rsquo;t quite put my finger on it. Do you care more about this world than that game of life world?</p>\n<p>B: Well, I am not sure which one is simpler, so I don&rsquo;t know, but it doesn&rsquo;t matter. It is a lot easier for me to change our world than it is for me to change the game of life world. I therefore will make choices that roughly maximizes preferences about the future of this world in the simplest models.</p>\n<p>A: Wait, if simplicity changes preferences, but does not change the level of existence, how do you explain the fact that we appear to be in a world that is simple? Isn&rsquo;t that a priori extremely unlikely?</p>\n<p>B: This is where it gets a little bit fuzzy, but I do not think that question makes sense. Unlikely by what measure? You are presupposing an existence measure on the collection of theoretical worlds just to ask that question.</p>\n<p>A: Okay, it seems plausible, but kind of depressing to think that we do not exist.</p>\n<p>B: Oh, I disagree! I am still a mind with free will, and I have the power to use that will to change my own little piece of mathematics &mdash; the output of my decision procedure. To me that feels incredibly  beautiful, eternal, and important.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NvwJMQvfu9hbBdG6d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 29, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "25454", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-08T02:20:46.279Z", "modifiedAt": null, "url": null, "title": "LessWrong Hamburg First Meetup Notes: Starting small", "slug": "lesswrong-hamburg-first-meetup-notes-starting-small", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.101Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/guYSP8Km3hChHD4ja/lesswrong-hamburg-first-meetup-notes-starting-small", "pageUrlRelative": "/posts/guYSP8Km3hChHD4ja/lesswrong-hamburg-first-meetup-notes-starting-small", "linkUrl": "https://www.lesswrong.com/posts/guYSP8Km3hChHD4ja/lesswrong-hamburg-first-meetup-notes-starting-small", "postedAtFormatted": "Saturday, February 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Hamburg%20First%20Meetup%20Notes%3A%20Starting%20small&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Hamburg%20First%20Meetup%20Notes%3A%20Starting%20small%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FguYSP8Km3hChHD4ja%2Flesswrong-hamburg-first-meetup-notes-starting-small%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Hamburg%20First%20Meetup%20Notes%3A%20Starting%20small%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FguYSP8Km3hChHD4ja%2Flesswrong-hamburg-first-meetup-notes-starting-small", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FguYSP8Km3hChHD4ja%2Flesswrong-hamburg-first-meetup-notes-starting-small", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p><strong>Review of our&nbsp;<a href=\"/meetups/vz\">LessWrong Hamburg First Meetup</a>:</strong></p>\n<p>I arrived early and the location was somewhat crowded and the reserved table in the back had been replaced by a center table - but I managed to switch for a better one.</p>\n<p>Then I put up some books and a sign and was quickly greated by the first LWer.</p>\n<p>We started with smalltalk, finding common background quickly.&nbsp;</p>\n<p>I had brought some books from the LW reading list I had in my collection and surprise: Half of them were recognized and the remaining quickly started a discussion (the Kahneman I later lent to C.F.).</p>\n<p>Some friends trickled in and after some introduction we played Wits and Wagers and then Pandemic (without biasing because the game was new to most). &nbsp;</p>\n<p>Summary: The Meetup was a success for me. We introduced some friends to LW ideas and we enjoyed a lively discussion not without controversy. I adapted to the Meetup format easily. &nbsp; &nbsp;</p>\n<p>One idea I had was a Meetup Diary which I used to plan the Meetup and take notes in. I think it still beats digital for things like quick notes and diagrams. I had it handy to check our schedule, draw Bayes diagrams and write down a telephone number. I plan to have it around and maybe lend it to later Meetup organizers.</p>\n<p>Outlook for the next Meetup: We planned it for Friday 21th in company offices somewhere in Hamburg, Altona (thanks to F.R. who will confirm the location later with a separate Post).</p>\n<p>Topics will be then</p>\n<ul>\n<li>Procrastination (I committed to read up and present some techniques in exchange for C.F. committing to use beeminder).</li>\n<li>More discussion of LW topics, most likely: effective altruism</li>\n<li>I will bring books and games again.</li>\n</ul>\n<p>More Meetups will likely follow roughly every fourtnight and alternating Friday and weekends.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "guYSP8Km3hChHD4ja", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.552809749776055e-06, "legacy": true, "legacyId": "25455", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-08T06:39:36.824Z", "modifiedAt": null, "url": null, "title": "Logic as Probability", "slug": "logic-as-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:34.363Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WKkDD6u79pzmvyQ6a/logic-as-probability", "pageUrlRelative": "/posts/WKkDD6u79pzmvyQ6a/logic-as-probability", "linkUrl": "https://www.lesswrong.com/posts/WKkDD6u79pzmvyQ6a/logic-as-probability", "postedAtFormatted": "Saturday, February 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logic%20as%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogic%20as%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWKkDD6u79pzmvyQ6a%2Flogic-as-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logic%20as%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWKkDD6u79pzmvyQ6a%2Flogic-as-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWKkDD6u79pzmvyQ6a%2Flogic-as-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 887, "htmlBody": "<p><strong>Followup To:</strong>&nbsp;<a href=\"/lw/jfl/putting_in_the_numbers/\">Putting in the Numbers</a></p>\n<p>Before talking about logical uncertainty, our final topic is the relationship between probabilistic logic and classical logic. A robot running on probabilistic logic stores probabilities of events, e.g. that the grass is wet outside, P(wet), and then if they collect new evidence they update that probability to P(wet|evidence). Classical logic robots, on the other hand, deduce the truth of statements from axioms and observations. Maybe our robot starts out not being able to deduce whether the grass is wet, but then they observe that it is raining, and so they use an axiom about rain causing wetness to deduce that \"the grass is wet\" is true.</p>\n<p>Classical logic relies on complete certainty in its axioms and observations, and makes completely certain deductions. This is unrealistic when applied to rain, but we're going to apply this to (<a href=\"/lw/g1y/godels_completeness_and_incompleteness_theorems/\">first order</a>, for starters) math later, which a better fit for classical logic.</p>\n<p>The general pattern of the deduction \"It's raining, and when it rains the grass is wet, therefore the grass is wet\" was&nbsp;modus ponens: if 'U implies R' is true, and U is true, then R must be true. There is also&nbsp;modus tollens: if 'U implies R' is true, and R is false, then U has to be false too. Third, there is the law of non-contradiction: \"It's simultaneously raining and not-raining outside\" is always false.</p>\n<p>We can imagine a robot that does classical logic as if it were writing in a notebook. Axioms are entered in the notebook at the start. Then our robot starts writing down statements that can be deduced by modus ponens&nbsp;or modus tollens. Eventually, the notebook is filled with statements deducible from the axioms. Modus tollens and modus ponens can be thought of as consistency conditions that apply to the contents of the notebook.</p>\n<p><a id=\"more\"></a>Doing math is one important application of our classical-logic robot. The robot can read from its notebook \"If variable A is a number, A=A+0\" and \"SS0 is a number,\" and then write down \"SS0=SS0+0.\"</p>\n<p>Note that this requires the robot to interpret variable A differently than symbol SS0. This is one of many upgrades we can make to the basic robot so that it can interpret math more easily. We also want to program in special responses to symbols like 'and', so that if A and B are in the notebook our robot will write 'A and B', and if 'A and B' is in the notebook it will add in A and B. In this light, modus ponens is just the robot having a programmed response to the 'implies' symbol.</p>\n<p>Certainty about our axioms is what lets us use classical logic, but you can represent complete certainty in probabilistic logic too, by the probabilities 1 and 0. These two methods of reasoning shouldn't contradict each other - if a classical logic robot can deduce that it's raining out, a probabilistic logic robot with the same information should assign P(rain)=1.</p>\n<p>If it's raining out, then my grass is wet. In the language of probabilities, this is P(wet|rain)=1. If I look outside and see rain, P(rain)=1, and then the product rule says that P(wet and rain) = P(rain)&middot;P(wet|rain), and that's equal to 1, so my grass must be wet too. Hey, that's modus ponens!</p>\n<p>The rules of probability can also behave like modus tollens (if P(B)=0, and P(B|A)=1, P(A)=0) and the law of the excluded middle (P(A|not-A)=0). Thus, when we're completely certain, probabilistic logic and classical logic give the same answers.</p>\n<p>There's a very short way to prove this, which is that one of <a href=\"/lw/jfx/foundations_of_probability/\">Cox's desiderata</a> for how probabilities must behave was \"when you're completely certain, your plausibilities should satisfy the rules of classical logic.\"</p>\n<p>In <a href=\"/lw/jfx/foundations_of_probability/\">Foundations of Probability</a>, I alluded to the idea that we should be able to apply probabilities to math. Dutch book arguments work because our robot must act as if it had probabilities in order to avoid losing money. Savage's theorem applies because the results of our robot's actions might depend on mathematical results. Cox's theorem applies because beliefs about math behave like other beliefs.</p>\n<p>This is completely correct. Math follows the rules of probability, and thus can be described with probabilities, because classical logic is the same as probabilistic logic when you're certain.</p>\n<p>We can even use this correspondence to figure out what numbers the probabilities take on:</p>\n<p>1 for every statement that follows from the axioms, 0 for their negations.</p>\n<p>&nbsp;</p>\n<p>This raises an issue: what about betting on the last digit of the 3^^^3'th prime? We dragged probability into this mess because it was supposed to help our robot stop trying to prove the answer and just bet as if P(last digit is 1)=1/4. But it turns out that there is one true probability distribution over mathematical statements, given the axioms. The right distribution is obtained by straightforward application of the product rule - never mind that it takes 4^^^3 steps - and if you deviate from the right distribution that means you violate the product rule at some point.</p>\n<p>This is why logical uncertainty is different. Even though our robot doesn't have enough resources to find the right answer, using logical uncertainty violates <a href=\"/lw/jfx/foundations_of_probability/\">Savage's theorem and Cox's theorem</a>. If we want our robot to act as if it has some \"logical probability,\" it's going to need a stranger sort of foundation.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence&nbsp;<em>Logical Uncertainty</em></p>\n<p style=\"text-align:right\">Previous Post: <a href=\"/lw/jfl/putting_in_the_numbers/\">Putting in the Numbers</a></p>\n<p style=\"text-align:right\">Next post: <a href=\"/lw/jjl/approaching_logical_probability/\">Approaching Logical Uncertainty</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WKkDD6u79pzmvyQ6a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 18, "extendedScore": null, "score": 1.55310599299841e-06, "legacy": true, "legacyId": "25328", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bnFP4yWmsFxaKjg3E", "GZjGtd35vhCnzSQKy", "EQ33emneF3Fh62Nn2", "GcWjDFsAit7CmzYka"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-09T09:51:04.335Z", "modifiedAt": null, "url": null, "title": "Asking about polyamory in Melbourne", "slug": "asking-about-polyamory-in-melbourne", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.980Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vGdcSCKDLopW86eXS/asking-about-polyamory-in-melbourne", "pageUrlRelative": "/posts/vGdcSCKDLopW86eXS/asking-about-polyamory-in-melbourne", "linkUrl": "https://www.lesswrong.com/posts/vGdcSCKDLopW86eXS/asking-about-polyamory-in-melbourne", "postedAtFormatted": "Sunday, February 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Asking%20about%20polyamory%20in%20Melbourne&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsking%20about%20polyamory%20in%20Melbourne%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvGdcSCKDLopW86eXS%2Fasking-about-polyamory-in-melbourne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Asking%20about%20polyamory%20in%20Melbourne%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvGdcSCKDLopW86eXS%2Fasking-about-polyamory-in-melbourne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvGdcSCKDLopW86eXS%2Fasking-about-polyamory-in-melbourne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<p>What communities are there where one can find polyamorous dating in Melbourne? I've decided to give it a try, primarily because my poor social skills mean I should go for the highest possible chance of success rather than anything else for the time being. If there is some degree to which I have options, requesting the best choice for somebody with good academic skills but very poor social skills and Aspergers Syndrome.</p>\n<p>Noting that I consider success to be very unlikely here under my circumstances, but given this is Lesswrong we should all be aware that sometimes it is worth attempting something unlikely dependent on potential payoffs. At the very least, a lower potential acceptance threshold means in the worst case scenario it's metaphorical training wheels for monogamous dating.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vGdcSCKDLopW86eXS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -14, "extendedScore": null, "score": -4.2e-05, "legacy": true, "legacyId": "25460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-09T20:47:30.847Z", "modifiedAt": null, "url": null, "title": "Finance as a career option", "slug": "finance-as-a-career-option", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gw7jsRYzAv4n8uAwF/finance-as-a-career-option", "pageUrlRelative": "/posts/gw7jsRYzAv4n8uAwF/finance-as-a-career-option", "linkUrl": "https://www.lesswrong.com/posts/gw7jsRYzAv4n8uAwF/finance-as-a-career-option", "postedAtFormatted": "Sunday, February 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Finance%20as%20a%20career%20option&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFinance%20as%20a%20career%20option%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgw7jsRYzAv4n8uAwF%2Ffinance-as-a-career-option%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Finance%20as%20a%20career%20option%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgw7jsRYzAv4n8uAwF%2Ffinance-as-a-career-option", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgw7jsRYzAv4n8uAwF%2Ffinance-as-a-career-option", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1638, "htmlBody": "<p style=\"margin: 0.4em 0px 0.5em;\">As a part of our research for <a href=\"/lw/jee/cognito_mentoring_an_advising_service_for/\">Cognito Mentoring</a>, Vipul Naik and I compiled a draft of a page on finance as a career option. Because some Less Wrongers are planning on <a href=\"http://en.wikipedia.org/wiki/Earning_to_give\">earning to give</a>&nbsp;and finance is a commonly considered career option for those who are earning to give, I thought that it might be of interest to the Less Wrong community.&nbsp;&nbsp;See also 80,000 Hours' <a href=\"http://80000hours.org/blog?category=12\">blog posts on finance as a career</a>.&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><a id=\"more\"></a>&nbsp;<span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Finance is a popular career option amongst graduates from elite universities: with </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://economix.blogs.nytimes.com/2011/12/21/out-of-harvard-and-into-finance/?_php=true&amp;_type=blogs&amp;_r=0 \">about 20%</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;of Harvard, Princeton and Yale graduates getting jobs in the field. Economist and New York Times columnist Tyler Cowen has </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://marginalrevolution.com/marginalrevolution/2012/01/a-simple-theory-of-why-so-many-smart-young-people-go-into-finance-law-and-consulting.html\">suggested</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;that people with high intelligence have a significant edge in the field.</span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Finance has a number of sectors. Careers in Finance <a href=\"http://careers-in-finance.com/\">breaks finance down</a> into Commercial Banking, Corporate Finance, Financial Planning, Hedge Funds, Insurance, Investment Banking, Money Management, Private Equity and Real Estate. The nature of jobs in finance varies considerably from sector to sector.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Our remarks below concern jobs in higher paying jobs in finance, such as jobs in investment banking, private equity and at hedge funds.</span></span></p>\n<h2>Compensation</h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Salaries in investment banking, private equity and hedge funds can be very high:</span></span></p>\n<ul>\n<li><span style=\"font-family: sans-serif; line-height: 19.200000762939453px;\">Careers in Finance <a href=\"http://www.careers-in-finance.com/ibsal.htm \">reports</a> that somebody with ~5 years of experience at an investment bank typically makes ~$450K/year.</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">In 2006, Richard Rusczyk (formerly an employee at hedge fund DE Shaw) <a href=\"http://www.artofproblemsolving.com/Forum/viewtopic.php?f=338&amp;t=83897\">wrote</a> \"While it's not expected that you'll make a million dollars in year 5, neither is it impossible. If you're not making at least middle six-digits by year 6-8 as a quant in a hedge fund, then something has gone very wrong for you.\"</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Stock market trader Joe Mela <a href=\"http://80000hours.org/blog/265-why-consider-becoming-a-trader\">wrote</a>&nbsp;that \"If you&rsquo;re good at [being a trader], you can make millions 5 years down the line.\"</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Some of the most wealthy people in the world, such as <a href=\"http://en.wikipedia.org/wiki/George_Soros\">George Soros</a>&nbsp;(net worth $23 billion) and <a href=\"http://en.wikipedia.org/wiki/James_Harris_Simons\">James Simons</a> (net worth 11.7 billion) made their money in these fields.</span></li>\n</ul>\n<div><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">James Miller <a href=\"/lw/jna/finance_as_a_career_option/aj42\">points out</a> that the high income is moderated by high marginal taxes as well as the high cost of living in New York city (where most finance firms are).</span></span></div>\n<h2><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Work-life balance</span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">The high pay in investment banking should be viewed in the context of the grueling hours on the job. <a href=\"http://www.ibankingfaq.com/category/banking-lifestyle/\">According</a> to IBankingFAQ,&nbsp;</span></span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Analysts can routinely expect to work 90-100 hours per week or even more. A typical work day during the week might be 10:00 am until 2:00 am. Analysts will also typically work both days on the weekend. During a particularly busy time [...] it is not uncommon for Analysts to work all night...</span></span></p>\n</blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><a href=\"http://www.quora.com/Investment-Banking/What-is-it-like-to-be-a-first-year-analyst-at-an-investment-bank\">According</a>&nbsp;to a highly upvoted Quora response:&nbsp;</span></span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Your physical health will almost certainly suffer. The extent to which it suffers depends on how careful you are with your diet, whether you make time to exercise, how much sleep you get, and how well or poorly you deal with stress. Most people have at least partial burnouts.</span></p>\n</blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">The quotations are referring to the hours of work in entry level positions. We have not been able to find substantive information on number of hours that more senior employees work per week. Our impression is that the number is smaller, but not dramatically so.&nbsp;</span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">The number of hours per week that employees at hedge funds and proprietary trading firms appear to be smaller. In 2006, Richard Rusczyk (formerly an employee at hedge fund DE Shaw) <a href=\"http://www.artofproblemsolving.com/Forum/viewtopic.php?f=338&amp;t=90486\">wrote</a>:</span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">I would say the average work week at places like Shaw or Jane Street is closer to 55 hours, maybe lower (unless things have changed dramatically).</span></p>\n</blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">This is in consonance with what we've heard from two other acquaintances who have worked at a hedge funds and proprietary trading firms.</span></span></p>\n<h2><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Job security</span></span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Job security in the more lucrative sectors of finance may be poor. At the 80,000 Hours blog, Carl Shulman <a href=\"http://80000hours.org/blog/24-5-ways-to-be-misled-by-salary-rankings\">wrote</a>:</span></span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">While a physician will usually remain a physician throughout her career, lucrative jobs in investment banking and management consulting often come with &ldquo;up or out&rdquo; career paths. Either one is promoted &ldquo;up,&rdquo; with incomes growing exponentially, as one can see in these links for banks and consultancies, or one is fired &ldquo;out&rdquo; and must seek work at a lesser firm or leave the industry. Since most employees will not be around for very long, one must take into account one&rsquo;s &ldquo;exit options&rdquo; in deciding whether to enter.</span></p>\n</blockquote>\n<h2><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Culture</span></span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Some people have characterized finance as having a very abrasive culture. Others have disputed this characterization. The culture of finance firms probably varies substantially from sector to sector and firm to firm.&nbsp;</span></span></p>\n<ul>\n<li><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://en.wikipedia.org/wiki/Liar's_Poker\">Liar's Poker</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;by Michael Lewis gives \"an unflattering portrayal of Wall Street traders and salesmen, their personalities, their beliefs, and their work practices.\" The book reports on the situation in the 1980's, and may be out of date.</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Former DE Shaw employee Cathy O'Neil gives an unflattering characterization of the culture at DE Shaw </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://mathbabe.org/2011/06/24/working-with-larry-summers-part-2/\">at her blog</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">. (However, see <a href=\"/r/discussion/lw/jna/finance_as_a_career_option/aj4k\">this comment</a> by Ben Kuhn.</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Trader Joe Mela </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://80000hours.org/blog/265-why-consider-becoming-a-trader\">wrote</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;</span><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">at the 80,000 Hours blog \"As a rule, traders are highly switched-on, pleasant to talk to, and are great people to learn from. I do not think trading is the optimal career path if you&rsquo;re trying to meet highly altruistic people, but the Gordon Gekko stereotype is pretty far from the truth.\"</span></li>\n</ul>\n<h2><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Social value</span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Actors in finance produce both social value and social disvalue, and it seems difficult to make a general statement about whether the typical worker at an investment bank (for example) does more good or harm. The situation probably varies from sector to sector of finance. We give some relevant considerations below.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><strong>The correlation between income and social value</strong></span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">In general, there's a correlation between income and social value contributed. The fact that the earnings of people who work in finance are high raises the possibility that workers in finance contribute high social value.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">People and organizations sometimes have a temporary need for money to accomplish their goals, and people and organizations are sometimes willing to lend money for a fee. Actors in finance who enable these transactions benefit both the borrower and the lender, and are paid accordingly. Similarly, actors in finance who lend money themselves benefit the borrowers and are paid accordingly. The proportion of activity in finance that fits this basic model is unclear. Many of the transactions in finance are many steps removed from the basic activity of enabling borrowers and lenders to connect. Some of these transactions indirectly enable borrowers and lenders to connect, and others don't. It can be very difficult to tell which are which from the outside.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\"><strong>Unproductively increasing the efficiency of the market</strong></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">If a company is looking for an investor and nobody is willing to invest, this is bad for the company. If the company is deserving of an investment, you spot this, and nobody is willing to invest, then you can benefit the company and make a profit by investing.&nbsp;</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">But suppose there are actors who are willing to invest in the company, and you invest in the company a tiny bit faster than the other actors. The company doesn't benefit much from this, because it would have gotten an investment anyway. The other people who would have invested are harmed by this, because they can't make a profit. So the social value that you contribute is much smaller than it would have been if nobody had been willing to invest within the same rough timeframe.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Some activity in finance takes this form. High frequency trading is a candidate for a sector of finance that makes money through buying and selling stocks a little bit faster than others, without contributing much social value. The transactions that high frequency trading firms make occur on a time scale of a fraction of a second, and it's unclear that enabling people to buy or sell a stock a fraction of a second faster helps them to an appreciable degree, even after taking into account the number of people involved.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\"><strong>Pushing off tail risk onto the government</strong></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Some firms in finance are <a href=\"http://en.wikipedia.org/wiki/Too_big_to_fail\">\"too big to fail\"</a>&nbsp;in the sense that if they were to go bankrupt, the whole economy would suffer enormously, because of their interconnectedness. When they're in danger of bankruptcy, the government will often lend or give them money to keep them afloat. Because the firms are aware that they'll likely be supported by the government in the event that they make bad investments, they'll sometimes make very risky investments, that have high upside to them if they pan out well, with the expectation that if they pan out poorly, the government will cover their losses. Such actors effectively make their money at the expense of the taxpayers, thereby contributing negative social value.</span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Not all actors in finance behave in this way.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><strong>Causing financial crises</strong></span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">As above, sometimes \"too big to fail\" firms will take risks that they're not able to handle, with the expectation that the government will cover their losses. If they're in danger of bankruptcy and the government ''doesn't'' cover their losses, this can precipitate a financial crisis. In particular, the collapse of <a href=\"http://en.wikipedia.org/wiki/Lehman_Brothers\">Lehman Brothers</a>&nbsp;is thought to have played a major role in the 2008 financial crisis. In this way, actors in finance may be able to cause damage far out of proportion with their earnings.&nbsp;</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">As above, not all actors in finance behave in this way.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><a href=\"http://business.time.com/2012/08/08/high-frequency-trading-wall-streets-doomsday-machine/ \">Some people</a>&nbsp;have raised the possibility that high-frequency trading could cause a financial crisis on account of increasing the stock market's volatility, but others have disputed it, or even claimed that high-frequency trading reduces the stock market's volatility.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><strong>Earning to give</strong></span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Because the earnings are high in finance, finance has been highlighted as a promising career track for people who want to <a href=\"http://en.wikipedia.org/wiki/Earning_to_give\">earn to give</a> large amounts of money to charities. 80,000 Hours Executive Director Ben Todd has <a href=\"http://80000hours.org/blog/239-show-me-the-harm\">argued</a>&nbsp;that the harm one might do in finance is small relative to the good that one can do by donating 50+% of one's income to highly effective charities.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gw7jsRYzAv4n8uAwF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 26, "extendedScore": null, "score": 6.7e-05, "legacy": true, "legacyId": "25462", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"margin: 0.4em 0px 0.5em;\">As a part of our research for <a href=\"/lw/jee/cognito_mentoring_an_advising_service_for/\">Cognito Mentoring</a>, Vipul Naik and I compiled a draft of a page on finance as a career option. Because some Less Wrongers are planning on <a href=\"http://en.wikipedia.org/wiki/Earning_to_give\">earning to give</a>&nbsp;and finance is a commonly considered career option for those who are earning to give, I thought that it might be of interest to the Less Wrong community.&nbsp;&nbsp;See also 80,000 Hours' <a href=\"http://80000hours.org/blog?category=12\">blog posts on finance as a career</a>.&nbsp;</p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><a id=\"more\"></a>&nbsp;<span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Finance is a popular career option amongst graduates from elite universities: with </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://economix.blogs.nytimes.com/2011/12/21/out-of-harvard-and-into-finance/?_php=true&amp;_type=blogs&amp;_r=0 \">about 20%</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;of Harvard, Princeton and Yale graduates getting jobs in the field. Economist and New York Times columnist Tyler Cowen has </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://marginalrevolution.com/marginalrevolution/2012/01/a-simple-theory-of-why-so-many-smart-young-people-go-into-finance-law-and-consulting.html\">suggested</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;that people with high intelligence have a significant edge in the field.</span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Finance has a number of sectors. Careers in Finance <a href=\"http://careers-in-finance.com/\">breaks finance down</a> into Commercial Banking, Corporate Finance, Financial Planning, Hedge Funds, Insurance, Investment Banking, Money Management, Private Equity and Real Estate. The nature of jobs in finance varies considerably from sector to sector.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Our remarks below concern jobs in higher paying jobs in finance, such as jobs in investment banking, private equity and at hedge funds.</span></span></p>\n<h2 id=\"Compensation\">Compensation</h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Salaries in investment banking, private equity and hedge funds can be very high:</span></span></p>\n<ul>\n<li><span style=\"font-family: sans-serif; line-height: 19.200000762939453px;\">Careers in Finance <a href=\"http://www.careers-in-finance.com/ibsal.htm \">reports</a> that somebody with ~5 years of experience at an investment bank typically makes ~$450K/year.</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">In 2006, Richard Rusczyk (formerly an employee at hedge fund DE Shaw) <a href=\"http://www.artofproblemsolving.com/Forum/viewtopic.php?f=338&amp;t=83897\">wrote</a> \"While it's not expected that you'll make a million dollars in year 5, neither is it impossible. If you're not making at least middle six-digits by year 6-8 as a quant in a hedge fund, then something has gone very wrong for you.\"</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Stock market trader Joe Mela <a href=\"http://80000hours.org/blog/265-why-consider-becoming-a-trader\">wrote</a>&nbsp;that \"If you\u2019re good at [being a trader], you can make millions 5 years down the line.\"</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Some of the most wealthy people in the world, such as <a href=\"http://en.wikipedia.org/wiki/George_Soros\">George Soros</a>&nbsp;(net worth $23 billion) and <a href=\"http://en.wikipedia.org/wiki/James_Harris_Simons\">James Simons</a> (net worth 11.7 billion) made their money in these fields.</span></li>\n</ul>\n<div><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">James Miller <a href=\"/lw/jna/finance_as_a_career_option/aj42\">points out</a> that the high income is moderated by high marginal taxes as well as the high cost of living in New York city (where most finance firms are).</span></span></div>\n<h2 id=\"Work_life_balance\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Work-life balance</span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">The high pay in investment banking should be viewed in the context of the grueling hours on the job. <a href=\"http://www.ibankingfaq.com/category/banking-lifestyle/\">According</a> to IBankingFAQ,&nbsp;</span></span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Analysts can routinely expect to work 90-100 hours per week or even more. A typical work day during the week might be 10:00 am until 2:00 am. Analysts will also typically work both days on the weekend. During a particularly busy time [...] it is not uncommon for Analysts to work all night...</span></span></p>\n</blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><a href=\"http://www.quora.com/Investment-Banking/What-is-it-like-to-be-a-first-year-analyst-at-an-investment-bank\">According</a>&nbsp;to a highly upvoted Quora response:&nbsp;</span></span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Your physical health will almost certainly suffer. The extent to which it suffers depends on how careful you are with your diet, whether you make time to exercise, how much sleep you get, and how well or poorly you deal with stress. Most people have at least partial burnouts.</span></p>\n</blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">The quotations are referring to the hours of work in entry level positions. We have not been able to find substantive information on number of hours that more senior employees work per week. Our impression is that the number is smaller, but not dramatically so.&nbsp;</span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">The number of hours per week that employees at hedge funds and proprietary trading firms appear to be smaller. In 2006, Richard Rusczyk (formerly an employee at hedge fund DE Shaw) <a href=\"http://www.artofproblemsolving.com/Forum/viewtopic.php?f=338&amp;t=90486\">wrote</a>:</span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">I would say the average work week at places like Shaw or Jane Street is closer to 55 hours, maybe lower (unless things have changed dramatically).</span></p>\n</blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">This is in consonance with what we've heard from two other acquaintances who have worked at a hedge funds and proprietary trading firms.</span></span></p>\n<h2 id=\"Job_security\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Job security</span></span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Job security in the more lucrative sectors of finance may be poor. At the 80,000 Hours blog, Carl Shulman <a href=\"http://80000hours.org/blog/24-5-ways-to-be-misled-by-salary-rankings\">wrote</a>:</span></span></p>\n<blockquote>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">While a physician will usually remain a physician throughout her career, lucrative jobs in investment banking and management consulting often come with \u201cup or out\u201d career paths. Either one is promoted \u201cup,\u201d with incomes growing exponentially, as one can see in these links for banks and consultancies, or one is fired \u201cout\u201d and must seek work at a lesser firm or leave the industry. Since most employees will not be around for very long, one must take into account one\u2019s \u201cexit options\u201d in deciding whether to enter.</span></p>\n</blockquote>\n<h2 id=\"Culture\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Culture</span></span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Some people have characterized finance as having a very abrasive culture. Others have disputed this characterization. The culture of finance firms probably varies substantially from sector to sector and firm to firm.&nbsp;</span></span></p>\n<ul>\n<li><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://en.wikipedia.org/wiki/Liar's_Poker\">Liar's Poker</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;by Michael Lewis gives \"an unflattering portrayal of Wall Street traders and salesmen, their personalities, their beliefs, and their work practices.\" The book reports on the situation in the 1980's, and may be out of date.</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Former DE Shaw employee Cathy O'Neil gives an unflattering characterization of the culture at DE Shaw </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://mathbabe.org/2011/06/24/working-with-larry-summers-part-2/\">at her blog</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">. (However, see <a href=\"/r/discussion/lw/jna/finance_as_a_career_option/aj4k\">this comment</a> by Ben Kuhn.</span></li>\n<li><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Trader Joe Mela </span><a style=\"line-height: 19.200000762939453px; font-family: sans-serif;\" href=\"http://80000hours.org/blog/265-why-consider-becoming-a-trader\">wrote</a><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">&nbsp;</span><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">at the 80,000 Hours blog \"As a rule, traders are highly switched-on, pleasant to talk to, and are great people to learn from. I do not think trading is the optimal career path if you\u2019re trying to meet highly altruistic people, but the Gordon Gekko stereotype is pretty far from the truth.\"</span></li>\n</ul>\n<h2 id=\"Social_value\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Social value</span></h2>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Actors in finance produce both social value and social disvalue, and it seems difficult to make a general statement about whether the typical worker at an investment bank (for example) does more good or harm. The situation probably varies from sector to sector of finance. We give some relevant considerations below.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><strong>The correlation between income and social value</strong></span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">In general, there's a correlation between income and social value contributed. The fact that the earnings of people who work in finance are high raises the possibility that workers in finance contribute high social value.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">People and organizations sometimes have a temporary need for money to accomplish their goals, and people and organizations are sometimes willing to lend money for a fee. Actors in finance who enable these transactions benefit both the borrower and the lender, and are paid accordingly. Similarly, actors in finance who lend money themselves benefit the borrowers and are paid accordingly. The proportion of activity in finance that fits this basic model is unclear. Many of the transactions in finance are many steps removed from the basic activity of enabling borrowers and lenders to connect. Some of these transactions indirectly enable borrowers and lenders to connect, and others don't. It can be very difficult to tell which are which from the outside.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\"><strong>Unproductively increasing the efficiency of the market</strong></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">If a company is looking for an investor and nobody is willing to invest, this is bad for the company. If the company is deserving of an investment, you spot this, and nobody is willing to invest, then you can benefit the company and make a profit by investing.&nbsp;</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">But suppose there are actors who are willing to invest in the company, and you invest in the company a tiny bit faster than the other actors. The company doesn't benefit much from this, because it would have gotten an investment anyway. The other people who would have invested are harmed by this, because they can't make a profit. So the social value that you contribute is much smaller than it would have been if nobody had been willing to invest within the same rough timeframe.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Some activity in finance takes this form. High frequency trading is a candidate for a sector of finance that makes money through buying and selling stocks a little bit faster than others, without contributing much social value. The transactions that high frequency trading firms make occur on a time scale of a fraction of a second, and it's unclear that enabling people to buy or sell a stock a fraction of a second faster helps them to an appreciable degree, even after taking into account the number of people involved.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\"><strong>Pushing off tail risk onto the government</strong></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Some firms in finance are <a href=\"http://en.wikipedia.org/wiki/Too_big_to_fail\">\"too big to fail\"</a>&nbsp;in the sense that if they were to go bankrupt, the whole economy would suffer enormously, because of their interconnectedness. When they're in danger of bankruptcy, the government will often lend or give them money to keep them afloat. Because the firms are aware that they'll likely be supported by the government in the event that they make bad investments, they'll sometimes make very risky investments, that have high upside to them if they pan out well, with the expectation that if they pan out poorly, the government will cover their losses. Such actors effectively make their money at the expense of the taxpayers, thereby contributing negative social value.</span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">Not all actors in finance behave in this way.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><strong>Causing financial crises</strong></span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">As above, sometimes \"too big to fail\" firms will take risks that they're not able to handle, with the expectation that the government will cover their losses. If they're in danger of bankruptcy and the government ''doesn't'' cover their losses, this can precipitate a financial crisis. In particular, the collapse of <a href=\"http://en.wikipedia.org/wiki/Lehman_Brothers\">Lehman Brothers</a>&nbsp;is thought to have played a major role in the 2008 financial crisis. In this way, actors in finance may be able to cause damage far out of proportion with their earnings.&nbsp;</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\">As above, not all actors in finance behave in this way.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><a href=\"http://business.time.com/2012/08/08/high-frequency-trading-wall-streets-doomsday-machine/ \">Some people</a>&nbsp;have raised the possibility that high-frequency trading could cause a financial crisis on account of increasing the stock market's volatility, but others have disputed it, or even claimed that high-frequency trading reduces the stock market's volatility.</span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19.200000762939453px;\"><strong>Earning to give</strong></span></span></p>\n<p style=\"margin: 0.4em 0px 0.5em;\"><span style=\"line-height: 19.200000762939453px; font-family: sans-serif;\">Because the earnings are high in finance, finance has been highlighted as a promising career track for people who want to <a href=\"http://en.wikipedia.org/wiki/Earning_to_give\">earn to give</a> large amounts of money to charities. 80,000 Hours Executive Director Ben Todd has <a href=\"http://80000hours.org/blog/239-show-me-the-harm\">argued</a>&nbsp;that the harm one might do in finance is small relative to the good that one can do by donating 50+% of one's income to highly effective charities.</span></p>", "sections": [{"title": "Compensation", "anchor": "Compensation", "level": 1}, {"title": "Work-life balance", "anchor": "Work_life_balance", "level": 1}, {"title": "Job security", "anchor": "Job_security", "level": 1}, {"title": "Culture", "anchor": "Culture", "level": 1}, {"title": "Social value", "anchor": "Social_value", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "90 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hphGa6xfad3m4imCs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-10T03:13:22.828Z", "modifiedAt": null, "url": null, "title": "The rationality of splitting donations", "slug": "the-rationality-of-splitting-donations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L5DNXiiXrAmQz63SK/the-rationality-of-splitting-donations", "pageUrlRelative": "/posts/L5DNXiiXrAmQz63SK/the-rationality-of-splitting-donations", "linkUrl": "https://www.lesswrong.com/posts/L5DNXiiXrAmQz63SK/the-rationality-of-splitting-donations", "postedAtFormatted": "Monday, February 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20rationality%20of%20splitting%20donations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20rationality%20of%20splitting%20donations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL5DNXiiXrAmQz63SK%2Fthe-rationality-of-splitting-donations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20rationality%20of%20splitting%20donations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL5DNXiiXrAmQz63SK%2Fthe-rationality-of-splitting-donations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL5DNXiiXrAmQz63SK%2Fthe-rationality-of-splitting-donations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 584, "htmlBody": "<p><em>Here are some tentative thoughts that I haven't run by anyone to check for soundness. They're not genuinely original to me &ndash; they've been floating around the effective altruism community in some form or other for a while &ndash; I just hadn't thought them through in sufficient detail to take them to their logical conclusion. I'd appreciate any feedback.</em></p>\n<p>Suppose that the expected number of lives saved per additional dollar donated to charity A is x and the expected value of lives saved per additional dollar donated to charity B is y, where x and y are constants, and x &gt; y. Then if you're trying to maximize the expected number of lives saved (c.f. <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">The \"Intuitions\" Behind \"Utilitarianism\"</a>), you should make all of your charitable contributions to charity A.</p>\n<p>In practice, x and y will not be constant, because of <a href=\"http://en.wikipedia.org/wiki/Room_for_more_funding\">room for more funding</a> issues. So splitting one's donations can maximize number of lives saved, if x is <em>sometimes</em> smaller than y.</p>\n<p>But suppose that you're donating $d, where increasing the charities' budgets by $d would leave the condition x &gt; y unaltered. A common view is that one should then donate all $d to charity A.</p>\n<p>However, this doesn't take into account <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</a>. If all donors to charities A and B were identical to you, then your decision to donate $d to charity A would be equivalent to a decision for all donors' funds to go to charity A rather than charity B, effectively constituting a decision for charity A to get a little bit more money in exchange for charity B existing altogether. If x &gt; y doesn't always hold, this is not expected value maximizing. The other donors aren't identical to you, but their decisions are still correlated with yours on account of the <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">psychological unity of humankind</a>, and shared cultural backgrounds.&nbsp;</p>\n<p>Suppose that x &gt; y is not always true. For simplicity, suppose that the total amount that donors will donate to charities A and B is fixed and known to you.&nbsp;If there were no correlation between your decision making and that of other donors, then it would be that you should give all of your money to charity A. If the correlation between your decision making and that of the other donors was perfect, then your ratio of donations to charity A to charity B should be the same as the ratio of the total amount of funding that you think charity A should get to the total amount of funding that you think charity B should get. This raises the possibility that your actual split of donations should be somewhere between these two extremes, and in particular, that you should split donations.</p>\n<p>In practice it won't always make sense to split donations: it might be that for any given charity A, there are many charities B with the property that x &gt; y is not always true, such that it would be a logistical hassle to split one's donations between all of them. But when one has a small handful of charities that one is considering donating to, it may make sense to split one's donations, even when one is a small donor.</p>\n<p>Moreover, charities&nbsp;<a href=\"http://utilitarian-essays.com/robustness-against-uncertainty.html\">are closer in cost-effectiveness than might initially meet the eye</a>, so that the condition that x &gt; y is not <em>always</em>&nbsp;true holds more often than might initially meet the eye. So the case is for splitting donations is stronger than might initially meet the eye, and the split should be more even than might initially meet the eye.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L5DNXiiXrAmQz63SK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 1.5561719603373664e-06, "legacy": true, "legacyId": "25465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r5MSQ83gtbjWRBDWJ", "Cyj6wQLW6SeF6aGLy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-10T06:11:27.667Z", "modifiedAt": null, "url": null, "title": "Meetup : Yale: Initial Meetup", "slug": "meetup-yale-initial-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.325Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lambda", "createdAt": "2012-02-04T03:49:39.010Z", "isAdmin": false, "displayName": "Lambda"}, "userId": "zbF8miyEeS43MJqBz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7QgsTLYdh5ZH4fLLi/meetup-yale-initial-meetup", "pageUrlRelative": "/posts/7QgsTLYdh5ZH4fLLi/meetup-yale-initial-meetup", "linkUrl": "https://www.lesswrong.com/posts/7QgsTLYdh5ZH4fLLi/meetup-yale-initial-meetup", "postedAtFormatted": "Monday, February 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Yale%3A%20Initial%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Yale%3A%20Initial%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7QgsTLYdh5ZH4fLLi%2Fmeetup-yale-initial-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Yale%3A%20Initial%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7QgsTLYdh5ZH4fLLi%2Fmeetup-yale-initial-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7QgsTLYdh5ZH4fLLi%2Fmeetup-yale-initial-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wn'>Yale: Initial Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bass Library Cafe, 110 Wall St, New Haven, CT 06511</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi. If anyone who goes to Yale is interested in meeting up, I'll be in Bass Cafe on Sunday, February 16, from 2 to 4 pm. I'll bring my copy of <em>Good and Real</em> for identification purposes.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wn'>Yale: Initial Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7QgsTLYdh5ZH4fLLi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 1.5563765430021891e-06, "legacy": true, "legacyId": "25467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Yale__Initial_Meetup\">Discussion article for the meetup : <a href=\"/meetups/wn\">Yale: Initial Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bass Library Cafe, 110 Wall St, New Haven, CT 06511</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi. If anyone who goes to Yale is interested in meeting up, I'll be in Bass Cafe on Sunday, February 16, from 2 to 4 pm. I'll bring my copy of <em>Good and Real</em> for identification purposes.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Yale__Initial_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/wn\">Yale: Initial Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Yale: Initial Meetup", "anchor": "Discussion_article_for_the_meetup___Yale__Initial_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Yale: Initial Meetup", "anchor": "Discussion_article_for_the_meetup___Yale__Initial_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-10T10:13:04.023Z", "modifiedAt": null, "url": null, "title": "Baldness", "slug": "baldness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:34.340Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TraderJoe", "createdAt": "2012-03-02T17:26:49.458Z", "isAdmin": false, "displayName": "TraderJoe"}, "userId": "PvoRSceD7dHzEway5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h8iNuENzwTBJbwmSJ/baldness", "pageUrlRelative": "/posts/h8iNuENzwTBJbwmSJ/baldness", "linkUrl": "https://www.lesswrong.com/posts/h8iNuENzwTBJbwmSJ/baldness", "postedAtFormatted": "Monday, February 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Baldness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABaldness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh8iNuENzwTBJbwmSJ%2Fbaldness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Baldness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh8iNuENzwTBJbwmSJ%2Fbaldness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh8iNuENzwTBJbwmSJ%2Fbaldness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p class=\"MsoNormal\">I have been reviewing FUE hair transplants, and I would like LWers' opinion. I'm actually surprised this isn't covered, as it seems relevant to many users.&nbsp;</p>\n<p class=\"MsoNormal\">As far as I can tell, the downsides are:<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Mild scarring on the back of the head<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Doesn&rsquo;t prevent continued hair loss, so if you get e.g. a bald spot filled in, then you will in a few years have a spot of hair in an oasis<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Cost<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Mild pain/hassle in the initial weeks.<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Possibility of finding a dodgy surgeon</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">The scarring is basically covered if you have a few two days&rsquo; hair growth there and I am fine with that as a long-term solution. he continued hair loss is potentially dealt with by a repeated transplant and more certainly dealt with by getting the initial transplant &ldquo;all over&rdquo;, i.e. thickening hair, rather than just moving the hairline forward. But it is the area I am most uncertain about. I should add that I am 29 with male pattern baldness on both sides of my family, Norwood level 4, and have seen hair loss stabilised (I have been taking propecia for the last year).</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Ignoring the cost, my questions are:<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Is anyone aware of any other problems besides these?<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Do you think this solution works?<br />-<span style=\"font-size: 7pt;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Any ideas on how to pick the right surgeon (using someone in Singapore most probably)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h8iNuENzwTBJbwmSJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -9, "extendedScore": null, "score": -8e-06, "legacy": true, "legacyId": "25469", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-10T17:38:21.961Z", "modifiedAt": null, "url": null, "title": "Meetup : Saint Petersburg sunday meetup", "slug": "meetup-saint-petersburg-sunday-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "efim", "createdAt": "2013-04-14T00:57:28.743Z", "isAdmin": false, "displayName": "efim"}, "userId": "Y8azdhZD6fvWdGwaB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AL893pDTqycioexka/meetup-saint-petersburg-sunday-meetup-0", "pageUrlRelative": "/posts/AL893pDTqycioexka/meetup-saint-petersburg-sunday-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/AL893pDTqycioexka/meetup-saint-petersburg-sunday-meetup-0", "postedAtFormatted": "Monday, February 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saint%20Petersburg%20sunday%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saint%20Petersburg%20sunday%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAL893pDTqycioexka%2Fmeetup-saint-petersburg-sunday-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saint%20Petersburg%20sunday%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAL893pDTqycioexka%2Fmeetup-saint-petersburg-sunday-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAL893pDTqycioexka%2Fmeetup-saint-petersburg-sunday-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wo'>Saint Petersburg sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We most likely will be trying out Pandemia and biased board gaming.</p>\n\n<p>Also I expect some discussions to arrise around topics of congitive biases, utility, or other zeitgeist stuff.</p>\n\n<p>If you are russian lesswronger who sees one of my ads for the first time, please check out  our newsletter or vk group: newsletter or <a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a> for more detailed descriptions.</p>\n\n<p>If you are a foreign guest in Saint Petersburg - we also would all be glad to see you and to meet you - at lest some of our attendees speak english.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wo'>Saint Petersburg sunday meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AL893pDTqycioexka", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5571661192771927e-06, "legacy": true, "legacyId": "25471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup\">Discussion article for the meetup : <a href=\"/meetups/wo\">Saint Petersburg sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We most likely will be trying out Pandemia and biased board gaming.</p>\n\n<p>Also I expect some discussions to arrise around topics of congitive biases, utility, or other zeitgeist stuff.</p>\n\n<p>If you are russian lesswronger who sees one of my ads for the first time, please check out  our newsletter or vk group: newsletter or <a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a> for more detailed descriptions.</p>\n\n<p>If you are a foreign guest in Saint Petersburg - we also would all be glad to see you and to meet you - at lest some of our attendees speak english.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup1\">Discussion article for the meetup : <a href=\"/meetups/wo\">Saint Petersburg sunday meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saint Petersburg sunday meetup", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup", "level": 1}, {"title": "Discussion article for the meetup : Saint Petersburg sunday meetup", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-10T21:27:19.545Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Hamburg - about Procrastination", "slug": "meetup-lesswrong-hamburg-about-procrastination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.938Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CwfvDnZYjsvMKjmhD/meetup-lesswrong-hamburg-about-procrastination", "pageUrlRelative": "/posts/CwfvDnZYjsvMKjmhD/meetup-lesswrong-hamburg-about-procrastination", "linkUrl": "https://www.lesswrong.com/posts/CwfvDnZYjsvMKjmhD/meetup-lesswrong-hamburg-about-procrastination", "postedAtFormatted": "Monday, February 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Hamburg%20-%20about%20Procrastination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Hamburg%20-%20about%20Procrastination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwfvDnZYjsvMKjmhD%2Fmeetup-lesswrong-hamburg-about-procrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Hamburg%20-%20about%20Procrastination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwfvDnZYjsvMKjmhD%2Fmeetup-lesswrong-hamburg-about-procrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwfvDnZYjsvMKjmhD%2Fmeetup-lesswrong-hamburg-about-procrastination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/wp\">LessWrong Hamburg - about Procrastination</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 February 2014 07:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Gau&szlig;strasse 190B, 22765 Hamburg </span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>As our first Meetup in Hamburg went well we planned a next one. Here it is.</p>\n<p>We will be at a new location this time. Thanks to F.R. We are in some company offices in Gau&szlig;strasse 190B, 22765 Hamburg. Look for doorbell Altocon, sign go7seas TransGlobalTours on the 2nd floor.</p>\n<p>The main topic is procrastination.</p>\n<ul>\n<li>There will be a short presentation about procrastination and dealing with it.</li>\n<li>There will be a report about using beeminder.</li>\n<li>We will probably play Liars Dice aka Bluff, Set (the game) or Pandemic</li>\n</ul>\n<p>If you want to eat or drink something you have to bring it yourself.</p>\n<p>The Meetup is open-ended (again) which judging from last time means significantly past midnight.</p>\n<p>The mailing list: <a rel=\"nofollow\" href=\"https://groups.google.com/forum/#!forum/lesswrong-hamburg\">https://groups.google.com/forum/#!forum/lesswrong-hamburg</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/wp\">LessWrong Hamburg - about Procrastination</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CwfvDnZYjsvMKjmhD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.5574294590180377e-06, "legacy": true, "legacyId": "25472", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Hamburg___about_Procrastination\">Discussion article for the meetup : <a href=\"/meetups/wp\">LessWrong Hamburg - about Procrastination</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 February 2014 07:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Gau\u00dfstrasse 190B, 22765 Hamburg </span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>As our first Meetup in Hamburg went well we planned a next one. Here it is.</p>\n<p>We will be at a new location this time. Thanks to F.R. We are in some company offices in Gau\u00dfstrasse 190B, 22765 Hamburg. Look for doorbell Altocon, sign go7seas TransGlobalTours on the 2nd floor.</p>\n<p>The main topic is procrastination.</p>\n<ul>\n<li>There will be a short presentation about procrastination and dealing with it.</li>\n<li>There will be a report about using beeminder.</li>\n<li>We will probably play Liars Dice aka Bluff, Set (the game) or Pandemic</li>\n</ul>\n<p>If you want to eat or drink something you have to bring it yourself.</p>\n<p>The Meetup is open-ended (again) which judging from last time means significantly past midnight.</p>\n<p>The mailing list: <a rel=\"nofollow\" href=\"https://groups.google.com/forum/#!forum/lesswrong-hamburg\">https://groups.google.com/forum/#!forum/lesswrong-hamburg</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Hamburg___about_Procrastination1\">Discussion article for the meetup : <a href=\"/meetups/wp\">LessWrong Hamburg - about Procrastination</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Hamburg - about Procrastination", "anchor": "Discussion_article_for_the_meetup___LessWrong_Hamburg___about_Procrastination", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Hamburg - about Procrastination", "anchor": "Discussion_article_for_the_meetup___LessWrong_Hamburg___about_Procrastination1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-11T11:18:39.009Z", "modifiedAt": null, "url": null, "title": "Useful Personality Tests", "slug": "useful-personality-tests", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "seez", "createdAt": "2013-07-31T22:30:37.599Z", "isAdmin": false, "displayName": "seez"}, "userId": "njhbHuqFGd9r6ivMv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/beAxXaPmPDowKZF2d/useful-personality-tests", "pageUrlRelative": "/posts/beAxXaPmPDowKZF2d/useful-personality-tests", "linkUrl": "https://www.lesswrong.com/posts/beAxXaPmPDowKZF2d/useful-personality-tests", "postedAtFormatted": "Tuesday, February 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Useful%20Personality%20Tests&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUseful%20Personality%20Tests%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbeAxXaPmPDowKZF2d%2Fuseful-personality-tests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Useful%20Personality%20Tests%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbeAxXaPmPDowKZF2d%2Fuseful-personality-tests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbeAxXaPmPDowKZF2d%2Fuseful-personality-tests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p>Have you ever taken a personality quiz/test that helped you have valuable insights? &nbsp;If so, what were the tests and how were they useful?</p>\n<p>The only useful ones I've found all yielded the same type of insight. &nbsp;They showed me where I stand relative to others, which is can be genuinely useful since representative samples of large populations can be hard to come by. &nbsp;This includes IQ tests and tests for mental disorders (in my experience, people are usually aware that they are, for example, smarter than the average (although the Dunning-Kruger effect might complicate this) or have some intrusive thoughts and compulsive rituals, but might be surprised to find that they are three standard deviations above the norm or that their symptoms are sufficiently severe to be considered OCD).&nbsp;</p>\n<p>No remotely reliable (as in, not astrology) test I have ever seen has revealed genuinely surprising information for a moderately self-aware person, outside of ranking. Furthermore, they rarely gather personality data in a remotely subtle or non-transparent way (\"do you like spending lots of time with large groups of people?\" \"yes...\" \"surprise, you're an extrovert!\"), and thus seem super susceptible to test-takers' attempts to confirm a desired identity. &nbsp;</p>\n<p>An example of a more interesting/subtle way to potentially conduct a personality test would be to use question like <a href=\"http://blog.okcupid.com/index.php/the-best-questions-for-first-dates/\">OKTrends'</a>&nbsp;\"do you like beer?\" which clusters strongly with \"do you have sex on the first date,\" and, potentially, sexual openness. Such results might be harder for manipulate (consciously or unconsciously) and could assist with deeper self-awareness. &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Edited</strong> because the first link was broken.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "beAxXaPmPDowKZF2d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.5583862755785284e-06, "legacy": true, "legacyId": "25482", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-11T13:23:17.029Z", "modifiedAt": null, "url": null, "title": "Brainstorming: children's stories", "slug": "brainstorming-children-s-stories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.240Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3PwHvo74QcYZ6QryN/brainstorming-children-s-stories", "pageUrlRelative": "/posts/3PwHvo74QcYZ6QryN/brainstorming-children-s-stories", "linkUrl": "https://www.lesswrong.com/posts/3PwHvo74QcYZ6QryN/brainstorming-children-s-stories", "postedAtFormatted": "Tuesday, February 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brainstorming%3A%20children's%20stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrainstorming%3A%20children's%20stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PwHvo74QcYZ6QryN%2Fbrainstorming-children-s-stories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brainstorming%3A%20children's%20stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PwHvo74QcYZ6QryN%2Fbrainstorming-children-s-stories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3PwHvo74QcYZ6QryN%2Fbrainstorming-children-s-stories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>So I have a three-year old kid, and will usually read or tell him a bedtime story.</p>\n<p>That is a nice opportunity to introduce new concepts, but my capacity for improvisation is limited, especially towards the end of the day. So I'm asking the good people on LessWrong for ideas. How would you wrap various lesswrongish ideas in a short story a little kid would pay attention to?</p>\n<p>I'm mostly interested in the aspects of \"practical rationality\" that aren't going to be taught at school or in children's books or children's TV shows - so things like Sunk Costs, taking the outside view, wondering which side is true instead of arguing for a side, etc.</p>\n<p>Pointers to outside sources of such stories are welcome too!</p>\n<p><em>Edit</em>: actually, if you want to share ideas of games or activities of the same kind, go ahead! :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3PwHvo74QcYZ6QryN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 14, "extendedScore": null, "score": 1.5585298129292575e-06, "legacy": true, "legacyId": "25483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-11T15:34:52.374Z", "modifiedAt": null, "url": null, "title": "Weighting the probability of being a mind by the quantity of the matter composing the computer that calculates that mind", "slug": "weighting-the-probability-of-being-a-mind-by-the-quantity-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:34.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yttrium", "createdAt": "2011-11-26T23:30:29.876Z", "isAdmin": false, "displayName": "yttrium"}, "userId": "BBzE4abSkhSfkQ89D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GXYg9wKABbZEAt74z/weighting-the-probability-of-being-a-mind-by-the-quantity-of", "pageUrlRelative": "/posts/GXYg9wKABbZEAt74z/weighting-the-probability-of-being-a-mind-by-the-quantity-of", "linkUrl": "https://www.lesswrong.com/posts/GXYg9wKABbZEAt74z/weighting-the-probability-of-being-a-mind-by-the-quantity-of", "postedAtFormatted": "Tuesday, February 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weighting%20the%20probability%20of%20being%20a%20mind%20by%20the%20quantity%20of%20the%20matter%20composing%20the%20computer%20that%20calculates%20that%20mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeighting%20the%20probability%20of%20being%20a%20mind%20by%20the%20quantity%20of%20the%20matter%20composing%20the%20computer%20that%20calculates%20that%20mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGXYg9wKABbZEAt74z%2Fweighting-the-probability-of-being-a-mind-by-the-quantity-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weighting%20the%20probability%20of%20being%20a%20mind%20by%20the%20quantity%20of%20the%20matter%20composing%20the%20computer%20that%20calculates%20that%20mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGXYg9wKABbZEAt74z%2Fweighting-the-probability-of-being-a-mind-by-the-quantity-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGXYg9wKABbZEAt74z%2Fweighting-the-probability-of-being-a-mind-by-the-quantity-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 722, "htmlBody": "<p>TL;DR by lavalamp: Treating \"computers running minds\" as discrete objects might cause a paradox in probability calculations that involve self-location. \"The probability of being a certain mind\" is probably an extensive physical quantity, i.e. rises proportionally to the size of the physical system doing the associated computations.</p>\n<p>There are two computers simulating two minds. At some time, one of the minds is being shown a red light, and the other one is shown a green one (call this \"Situation 1\"). Conditioned on you being one of the minds, what is the probability you should assign to seeing red?</p>\n<p>Naively, the answer seems to be 1/2, which comes from assigning being each of the minds an equal probability. If one had three computers and showed two of them a red light and the third one a green one, the probability would be calculated as 2/3, even if the red-seeing computers will be in exactly the same computational state at all times (call this \"Situation 2\").</p>\n<p>However, I think that taking this point of view leads to paradoxes.</p>\n<p>An example: Consider an electrical circuit made of (ideal) wires, resistors, capacitors and transistors (sufficient in principle to build a computer); the supply voltage comes from outside of the circuit considered. Under assumptions regarding the physical implementation of this circuit that do not restrict the possible circuit diagrams, it is possible to split the matter composing it into two part that both comprise working circuits reproducing the original circuit's behavior independently of the other part, in an analogous fashion to how the <a title=\"Ebborian's\" href=\"/lw/ps/where_physics_meets_experience/\">Ebborian's</a> brains are split.* To clarify, what I have in mind is cutting up the wires and resistors orthogonally to their cross-sections - after the splitting, equivalent wires should be on equivalent potentials at the same time, but the currents flowing will be reduced by some factor.</p>\n<p>Now imagine the circuit is a computer, simulating the mind that is going to see red in Situation 1 (the mind that will see green still exists). If one splits the circuit as described, one suddenly ends up with two circuits simulating the same mind, i.e. Situation 2 (let's imagine that the computers are split before they are turned on for the first time, so that stream-of-consciousness-considerations will not influence the calculated probability, like e.g. Deda answering 1/2 to Yu'el's question in the linked article). However, it is not clear how far the circuit components need to be apart from each other so that they should be considered \"split\". I.e., if one fixes a direction in which the circuits are moved apart and then defines P(d) as \"the probability one should assign to seeing red, as a function of the distance by which the circuits have been moved apart), P(0) would be 1/2 and P(&infin;) would be 2/3 in the naive model, but there seems to be no intuitive way how the function should look like in between.</p>\n<p>I think that therefore, it is more plausible that a way closer to the correct one to calculate the probability of having one mind's experiences involves somehow weighting this probability by the amount (maybe mass or electron count) of matter that calculates the mind. If one does this, after splitting, the matter comprising each of the parts will add up exactly to the matter of the original circuit, so P(d) would be constant over all distances.</p>\n<p>What do you think?</p>\n<p>*Namely, the resistors could be full cylinders with the wires protruding along the axes - one could then split them by a plane surface surface that includes the cylinder's axis and would end up with two resistors that have twice the resistance.</p>\n<p>The capacitors could look exactly like in this picture and could then be split up along a plane that includes the wires, so that the capacitance is halved.</p>\n<p>The transistors could look exactly like in this picture (being homogeneous in the z-direction), and be split up in half across a plane that is parallel to the picture shown).</p>\n<p>If one does all of those splittings and splits up the wires so that the parts of each electronic component are connected in the same way as the original circuit was connected, and then operates the resulting circuits with the same supply voltage as one operated the original circuit, the voltages of all wires will always be the same as in the original circuit, and currents will be halved.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GXYg9wKABbZEAt74z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 0, "extendedScore": null, "score": 1.5586813860210846e-06, "legacy": true, "legacyId": "25473", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WajiC3YWeJutyAXTn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-11T17:37:31.270Z", "modifiedAt": null, "url": null, "title": "Native Russian speakers wanted, for help with translation of LW texts", "slug": "native-russian-speakers-wanted-for-help-with-translation-of", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "listic", "createdAt": "2009-03-11T10:06:44.719Z", "isAdmin": false, "displayName": "listic"}, "userId": "qTfheLmwMdCtMb5ZT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m85jai8Eq9S56xFag/native-russian-speakers-wanted-for-help-with-translation-of", "pageUrlRelative": "/posts/m85jai8Eq9S56xFag/native-russian-speakers-wanted-for-help-with-translation-of", "linkUrl": "https://www.lesswrong.com/posts/m85jai8Eq9S56xFag/native-russian-speakers-wanted-for-help-with-translation-of", "postedAtFormatted": "Tuesday, February 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Native%20Russian%20speakers%20wanted%2C%20for%20help%20with%20translation%20of%20LW%20texts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANative%20Russian%20speakers%20wanted%2C%20for%20help%20with%20translation%20of%20LW%20texts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm85jai8Eq9S56xFag%2Fnative-russian-speakers-wanted-for-help-with-translation-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Native%20Russian%20speakers%20wanted%2C%20for%20help%20with%20translation%20of%20LW%20texts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm85jai8Eq9S56xFag%2Fnative-russian-speakers-wanted-for-help-with-translation-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm85jai8Eq9S56xFag%2Fnative-russian-speakers-wanted-for-help-with-translation-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>If you are native Russian speaker and are willing to help MIRI with bringing its information to Russian-speaking audience by helping with translation of key&nbsp;materials, e.g. the forthcoming ebook <em>Smarter Than Us</em>, please contact me via pm or email:&nbsp;<img style=\"vertical-align: baseline;\" src=\"http://i298.photobucket.com/albums/mm249/hrenistic/email_zps178901a4.gif\" alt=\"\" width=\"150\" height=\"18\" /></p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m85jai8Eq9S56xFag", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 1.5588226839648673e-06, "legacy": true, "legacyId": "25484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-11T18:08:23.934Z", "modifiedAt": null, "url": null, "title": "Open Thread for February 11 - 17", "slug": "open-thread-for-february-11-17", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:00.439Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jD5yw654riqqEiyr3/open-thread-for-february-11-17", "pageUrlRelative": "/posts/jD5yw654riqqEiyr3/open-thread-for-february-11-17", "linkUrl": "https://www.lesswrong.com/posts/jD5yw654riqqEiyr3/open-thread-for-february-11-17", "postedAtFormatted": "Tuesday, February 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20for%20February%2011%20-%2017&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20for%20February%2011%20-%2017%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjD5yw654riqqEiyr3%2Fopen-thread-for-february-11-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20for%20February%2011%20-%2017%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjD5yw654riqqEiyr3%2Fopen-thread-for-february-11-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjD5yw654riqqEiyr3%2Fopen-thread-for-february-11-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jD5yw654riqqEiyr3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.5588582604727433e-06, "legacy": true, "legacyId": "25485", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 335, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-11T19:55:01.061Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup X.", "slug": "meetup-bratislava-meetup-x", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KAKZi4LCf8ezwvtba/meetup-bratislava-meetup-x", "pageUrlRelative": "/posts/KAKZi4LCf8ezwvtba/meetup-bratislava-meetup-x", "linkUrl": "https://www.lesswrong.com/posts/KAKZi4LCf8ezwvtba/meetup-bratislava-meetup-x", "postedAtFormatted": "Tuesday, February 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20X.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20X.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAKZi4LCf8ezwvtba%2Fmeetup-bratislava-meetup-x%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20X.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAKZi4LCf8ezwvtba%2Fmeetup-bratislava-meetup-x", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKAKZi4LCf8ezwvtba%2Fmeetup-bratislava-meetup-x", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wq'>Bratislava Meetup X.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 February 2014 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The same place; discussion about HP:MoR, and possibly other topics.</p>\n\n<p>Na rovnakom mieste a v rovnakom \u010dase ako zvy\u010dajne, okr\u00fahle desiate bratislavsk\u00e9 stretnutie. T\u00e9ma: Harry Potter a met\u00f3dy racionality. Mo\u017en\u00e9 s\u00fa aj \u010fal\u0161ie t\u00e9my, pod\u013ea z\u00e1ujmu.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wq'>Bratislava Meetup X.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KAKZi4LCf8ezwvtba", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5589811149073206e-06, "legacy": true, "legacyId": "25486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_X_\">Discussion article for the meetup : <a href=\"/meetups/wq\">Bratislava Meetup X.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 February 2014 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The same place; discussion about HP:MoR, and possibly other topics.</p>\n\n<p>Na rovnakom mieste a v rovnakom \u010dase ako zvy\u010dajne, okr\u00fahle desiate bratislavsk\u00e9 stretnutie. T\u00e9ma: Harry Potter a met\u00f3dy racionality. Mo\u017en\u00e9 s\u00fa aj \u010fal\u0161ie t\u00e9my, pod\u013ea z\u00e1ujmu.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_X_1\">Discussion article for the meetup : <a href=\"/meetups/wq\">Bratislava Meetup X.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup X.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_X_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup X.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_X_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-12T01:16:44.750Z", "modifiedAt": null, "url": null, "title": "Calorie Restriction: My Theory and Practice", "slug": "calorie-restriction-my-theory-and-practice", "viewCount": null, "lastCommentedAt": "2018-10-28T21:31:40.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "brazil84", "createdAt": "2009-12-06T23:07:50.570Z", "isAdmin": false, "displayName": "brazil84"}, "userId": "umdvopePmiD3o3nGf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4vXws5KXAnYiHDs9E/calorie-restriction-my-theory-and-practice", "pageUrlRelative": "/posts/4vXws5KXAnYiHDs9E/calorie-restriction-my-theory-and-practice", "linkUrl": "https://www.lesswrong.com/posts/4vXws5KXAnYiHDs9E/calorie-restriction-my-theory-and-practice", "postedAtFormatted": "Wednesday, February 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calorie%20Restriction%3A%20My%20Theory%20and%20Practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalorie%20Restriction%3A%20My%20Theory%20and%20Practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vXws5KXAnYiHDs9E%2Fcalorie-restriction-my-theory-and-practice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calorie%20Restriction%3A%20My%20Theory%20and%20Practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vXws5KXAnYiHDs9E%2Fcalorie-restriction-my-theory-and-practice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vXws5KXAnYiHDs9E%2Fcalorie-restriction-my-theory-and-practice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1909, "htmlBody": "<p>Like most futurist-oriented people, I am fascinated by the idea of long-term life extension -- the notion that eventually people will have life expectancies of hundreds of years; thousands of years; or even more.&nbsp; Although medicine has a ways to go in this area, one obvious approach is to take low tech steps to increase one's lifespan in hopes of living long enough to take advantage of possible future advances.&nbsp; i.e., to roll with +1 dice.&nbsp; Besides the obvious steps like wearing seat belts; getting regular exercise; eating a lot of fruits and vegetables, calorie restriction presents itself as an intriguing possible method of life extension.</p>\n<p>In this essay, I will attempt to briefly define calorie restriction; assess how useful it might be; speculate about how it might be useful; and use the foregoing to justify my own personal approach to calorie restriction, which I will describe presently.&nbsp; Of course I welcome comments and criticisms, especially since I am messing around with my own health.</p>\n<p>I should note at the outset that I have no formal training or credentials in medicine nutrition or anything like that.&nbsp; I'm just an attorney.</p>\n<p>I should also add that my approach to calorie restriction is not a diet in the sense of being a weight loss strategy for people who cannot control their eating.&nbsp; This is not a weight loss post!&nbsp; I do not describe in this essay how I control my eating; control is assumed.</p>\n<p>Last, my general approach is one of no regret.&nbsp; i.e. My main priority in calorie-restricting myself is to avoid doing anything too radical in terms of loss of quality of life or risk to my health.</p>\n<p><strong>I. What is Calorie Restriction?</strong></p>\n<p>Wikipedia defines \"calorie restriction\" as follows:</p>\n<blockquote>\n<p><strong>Caloric restriction</strong> (CR), or <strong>calorie restriction</strong>, is a dietary regiment that is based on low calorie intake. \"Low\" can be defined relative to the subject's previous intake before intentionally restricting calories, or relative to an average person of similar body type.</p>\n</blockquote>\n<p>So immediately we see a problem -- the concept of calorie restriction is ambiguous.&nbsp; How am I supposed to evaluate and possibly implement calorie restriction in my life if I am not even clear on what it means?&nbsp; This is not just a problem for laymen like me.&nbsp; Imagine you are a researcher who is studying the effects of calorie restrictions in lab chimps.&nbsp; How do you feed your control group of lab chimps?&nbsp; Do you let them eat donuts and potato chips ad libitum?&nbsp; Do you limit them to chimp chow?&nbsp; Without a clear definition, this is a bit of a conundrum.</p>\n<p>In fact, one individual has argued that the difference in treatment of control animals may be part of the reason why two studies on calorie restriction in monkeys had different results:</p>\n<blockquote>\n<p>Further, the NIA study control monkeys were not truly fed <em>ad libitum, </em>unlike the WNPRC study. The regulated portioning of food for the NIA control monkeys may be a slight restriction and, thus, largely prevented obesity. Studies of 10% CR have been reported to increase lifespan in rats compared to <em>ad libitum</em> controls &ndash; even more than 25% and 40% CR<sup>20</sup>. The NIA control monkeys may experience survival benefits from this slight restriction.</p>\n</blockquote>\n<p>http://www.crsociety.org/science/nia_monkey_study</p>\n<p>Another individual states as follows:</p>\n<blockquote>\n<p>\"<strong>Both the NIA and U Wisc studies need to be considered together for proper interpretation.</strong> It is clear that the U Wisc \"controls\" differ from the U Wisc CR group and BOTH NIA groups, and are probably most like the general populations of developed countries.</p>\n<p>Because we at NIA wanted to avoid the criticism leveled at many rodent CR studies that controls are overweight and sedentary, we specifically designed our dietary conditions to supply an adequate, but not OVERadequate, caloric intake.</p>\n<p><strong>The bottom line is that, for most people (who are more like the U Wisc controls), CR may indeed provide both health (BOTH studies agree on THIS) and longevity benefits.....and of course, most important.....more \"healthy years.\"</strong></p>\n</blockquote>\n<p>https://www.crsociety.org/index.php?/topic/2939-dr-george-roth-comments-on-calorie-restriction-and-nia-monkey-study/</p>\n<p>For purposes of this essay, I will offer the following definitions:</p>\n<p>1.&nbsp; \"Mild calorie restriction\" = restricting calories sufficiently so that you avoid gaining large amounts of weight.</p>\n<p>2.&nbsp; \"Moderate calorie restriction\" = restricting calories sufficiently so that most of the time you are towards the bottom of your metabolic range.</p>\n<p>3.&nbsp; \"Severe calorie restriction\" = restricting calories sufficiently so that you end up spending your time significantly below typically fit people in terms of muscle mass and/or body fat.</p>\n<p>The first and third definitions are pretty straightforward, although it's worth noting that a lot of people engage in mild calorie restriction unintentionally, just through the operation of their natural system which regulates their appetite/urge to eat/urge to stop eating (John Walker calls this the \"food clock.\")</p>\n<p>The second definition requires a little explanation.&nbsp;&nbsp; From simple observation, it appears that small changes in one's energy intake result in corresponding changes in one's metabolic rate.&nbsp; So that if your weight is stable but you eat a little more or less than usual, you might notice that you are a little warmer or cooler than usual.&nbsp; Evidently the body can and does make small adjustments to its metabolic rate in response to changes in food intake.&nbsp; This is also consistent with dieters' reports that they feel cold when dieting.</p>\n<p><strong>II.&nbsp;&nbsp;&nbsp;&nbsp; Does Calorie Restriction Work in Humans?</strong></p>\n<p>It seems quite likely that mild calorie restriction works in humans based on the observation that fat people have significantly greater mortality than thin people.</p>\n<p>For example, as illustrated by the charts here:</p>\n<p>http://www.nejm.org/action/showImage?doi=10.1056%2FNEJMoa1000367&amp;iid=f01&amp;</p>\n<p>Of course one cannot know this for sure since there is no ethical way to do a large controlled experiment, but still it's reasonable to infer cause and effect:&nbsp; Common sense says that being fat puts a lot of abnormal extra strain on your system almost all the time.&nbsp; In any event, there seems to be little downside to mild calorie restriction.</p>\n<p>A more interesting question is whether moderate calorie restriction works in humans.&nbsp; Common sense says that it ought to be beneficial based on the idea that slowing one's metabolism ought to slow the aging process, all things being equal.&nbsp; One interesting area of research is studies which look at the effect of modest weight loss among obese people.&nbsp; Is someone who goes from 250 pounds to 225 pounds and stays there more healthy than someone who goes from 210 pounds to 220 pounds and continues to gain weight?&nbsp; If so, part of the difference might be that the second person is towards the top of his metabolic range while the first person is towards the middle or bottom.</p>\n<p>The Calorie Restriction Society web site links to a couple presentations which argue that cancer is actually a metabolic disease related to having too much energy in play.&nbsp; I'm a bit skeptical of this claim, but it does seem to me that you are inviting trouble by having extra energy floating around in your system.</p>\n<p>As for severe calorie restriction, the jury is still out.&nbsp; I don't put too much stock in the left side of the J-shaped curves comparing body weight to mortality.&nbsp; Surely a lot of underweight people have serious latent health problems.&nbsp; What's more interesting to me is that the curves flatten out between BMI of about 19 and 23.5.&nbsp; This suggests to me that one can realize most of the benefits of reduced body mass by being normal weight and that after that, if there are any benefits, it's diminishing returns.</p>\n<p><strong>III.&nbsp;&nbsp; My Approach to Calorie Restriction</strong></p>\n<p>I have decided to adopt an intermediate approach to calorie restriction, i.e. the aim is to stay thin and be towards the bottom of my metabolic range most of the time.&nbsp; The health benefits to staying thin are pretty clear; there doesn't seem to be much downside; and frankly there are a lot of social benefits.&nbsp; The benefits of staying towards the bottom of my metabolic range are more iffy, but again there doesn't seem to be much downside to it.&nbsp; (Putting aside issues of health, the main downside is that it happens pretty frequently that I will have a meal and eat less food than I would have liked to eat.)</p>\n<p>Severe calorie restriction seems too speculative to me to be worth the trouble.&nbsp; Particularly given the social costs and the likely diminishing returns problem.&nbsp; I like having a somewhat muscular appearance as opposed to a gaunt appearance.&nbsp; Since my main priority is to avoid regrets, I am not willing to go this route without pretty solid evidence of benefit.</p>\n<p><strong>IIIa.&nbsp; The Nuts and Bolts</strong></p>\n<p>What I do is this:&nbsp; I have a basic daily diet which I believe is reasonably healthy and well-balanced.&nbsp; Although it is somewhat flexible, it contains roughly the same proportions of macro-nutrients and is roughly the same amount of calories each day.&nbsp; From careful observation, I have determined that my basic daily diet is about 500 to 600 calories short of my actual daily caloric needs.&nbsp; i.e. if I stuck to my basic daily diet and ate nothing more, I would lose about a pound a week.&nbsp; I add a small supplement of extra food to my basic daily diet if I work out at the gym in order to balance out the exercise.&nbsp; (Interestingly, I once measured and it seems my basic daily diet, including the exercise supplement, is about 2800 calories.&nbsp; This seems pretty high for a man who is thin, slightly below average height, and only slightly muscular in build.&nbsp; I'm not sure what to make of it.)</p>\n<p>I weigh myself every morning and calculate a 7-day moving average of my weight.&nbsp; I then subtract this number from a pre-determined reference weight and multiply the result by 100.&nbsp; This is the number of additional calories I consume that day in the form of reasonably healthy foods.&nbsp; The idea is to eat close to the minimum to maintain weight, thus staying thin and towards the bottom of my metabolic range.</p>\n<p>Now and then my weight spikes upward when I have an event which involves a lot of eating; after that it drifts back down again.&nbsp; I've been calorie-restricted in this way for some time now.&nbsp; I feel perfectly fine but after every meal I feel like I could easily eat more.&nbsp; I pretty much never get heartburn anymore.&nbsp; I usually wake up quite hungry.&nbsp; These are about the only effects I have noticed.</p>\n<p><strong>IV.&nbsp; Self-Criticisms of My Approach</strong></p>\n<p>In the interest of rationality, it probably makes sense to offer some self-criticism:</p>\n<p>1. I found the above scientific references only after I had settled on my approach to calorie restriction. &nbsp; So there is probably a certain backwardness about my reasoning.&nbsp; My conclusion is based more on my own intuition, reasoning, observations and common sense than on scientific research.</p>\n<p>2.&nbsp; It never occurred to me to regularly measure my body temperature before and after starting this program.&nbsp; Which is unfortunate because it may have given me some useful information about the effects of my diet on my metabolism.</p>\n<p>3.&nbsp; There's really no way to measure if any of this is having an effect on my rate of aging.&nbsp; Without this sort of feedback, I'm pretty much shooting in the dark.</p>\n<p><strong>V.&nbsp; Conclusion</strong></p>\n<p>So that's about the extent of my self-experiment.&nbsp; It's a bit frightening that I'm putting my own health on the line in the face of so much uncertainty.&nbsp; At same time, it seems like a reasonable, conservative approach which is unlikely to lead to regrets.&nbsp; Of course there is an excellent chance I will never know how much of an impact my lifestyle had on my overall health.</p>\n<p>Anyway, I welcome any intelligent thoughts, suggestions, constructive criticism, etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4vXws5KXAnYiHDs9E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 11, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "25487", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Like most futurist-oriented people, I am fascinated by the idea of long-term life extension -- the notion that eventually people will have life expectancies of hundreds of years; thousands of years; or even more.&nbsp; Although medicine has a ways to go in this area, one obvious approach is to take low tech steps to increase one's lifespan in hopes of living long enough to take advantage of possible future advances.&nbsp; i.e., to roll with +1 dice.&nbsp; Besides the obvious steps like wearing seat belts; getting regular exercise; eating a lot of fruits and vegetables, calorie restriction presents itself as an intriguing possible method of life extension.</p>\n<p>In this essay, I will attempt to briefly define calorie restriction; assess how useful it might be; speculate about how it might be useful; and use the foregoing to justify my own personal approach to calorie restriction, which I will describe presently.&nbsp; Of course I welcome comments and criticisms, especially since I am messing around with my own health.</p>\n<p>I should note at the outset that I have no formal training or credentials in medicine nutrition or anything like that.&nbsp; I'm just an attorney.</p>\n<p>I should also add that my approach to calorie restriction is not a diet in the sense of being a weight loss strategy for people who cannot control their eating.&nbsp; This is not a weight loss post!&nbsp; I do not describe in this essay how I control my eating; control is assumed.</p>\n<p>Last, my general approach is one of no regret.&nbsp; i.e. My main priority in calorie-restricting myself is to avoid doing anything too radical in terms of loss of quality of life or risk to my health.</p>\n<p><strong id=\"I__What_is_Calorie_Restriction_\">I. What is Calorie Restriction?</strong></p>\n<p>Wikipedia defines \"calorie restriction\" as follows:</p>\n<blockquote>\n<p><strong>Caloric restriction</strong> (CR), or <strong>calorie restriction</strong>, is a dietary regiment that is based on low calorie intake. \"Low\" can be defined relative to the subject's previous intake before intentionally restricting calories, or relative to an average person of similar body type.</p>\n</blockquote>\n<p>So immediately we see a problem -- the concept of calorie restriction is ambiguous.&nbsp; How am I supposed to evaluate and possibly implement calorie restriction in my life if I am not even clear on what it means?&nbsp; This is not just a problem for laymen like me.&nbsp; Imagine you are a researcher who is studying the effects of calorie restrictions in lab chimps.&nbsp; How do you feed your control group of lab chimps?&nbsp; Do you let them eat donuts and potato chips ad libitum?&nbsp; Do you limit them to chimp chow?&nbsp; Without a clear definition, this is a bit of a conundrum.</p>\n<p>In fact, one individual has argued that the difference in treatment of control animals may be part of the reason why two studies on calorie restriction in monkeys had different results:</p>\n<blockquote>\n<p>Further, the NIA study control monkeys were not truly fed <em>ad libitum, </em>unlike the WNPRC study. The regulated portioning of food for the NIA control monkeys may be a slight restriction and, thus, largely prevented obesity. Studies of 10% CR have been reported to increase lifespan in rats compared to <em>ad libitum</em> controls \u2013 even more than 25% and 40% CR<sup>20</sup>. The NIA control monkeys may experience survival benefits from this slight restriction.</p>\n</blockquote>\n<p>http://www.crsociety.org/science/nia_monkey_study</p>\n<p>Another individual states as follows:</p>\n<blockquote>\n<p>\"<strong>Both the NIA and U Wisc studies need to be considered together for proper interpretation.</strong> It is clear that the U Wisc \"controls\" differ from the U Wisc CR group and BOTH NIA groups, and are probably most like the general populations of developed countries.</p>\n<p>Because we at NIA wanted to avoid the criticism leveled at many rodent CR studies that controls are overweight and sedentary, we specifically designed our dietary conditions to supply an adequate, but not OVERadequate, caloric intake.</p>\n<p><strong id=\"The_bottom_line_is_that__for_most_people__who_are_more_like_the_U_Wisc_controls___CR_may_indeed_provide_both_health__BOTH_studies_agree_on_THIS__and_longevity_benefits_____and_of_course__most_important_____more__healthy_years__\">The bottom line is that, for most people (who are more like the U Wisc controls), CR may indeed provide both health (BOTH studies agree on THIS) and longevity benefits.....and of course, most important.....more \"healthy years.\"</strong></p>\n</blockquote>\n<p>https://www.crsociety.org/index.php?/topic/2939-dr-george-roth-comments-on-calorie-restriction-and-nia-monkey-study/</p>\n<p>For purposes of this essay, I will offer the following definitions:</p>\n<p>1.&nbsp; \"Mild calorie restriction\" = restricting calories sufficiently so that you avoid gaining large amounts of weight.</p>\n<p>2.&nbsp; \"Moderate calorie restriction\" = restricting calories sufficiently so that most of the time you are towards the bottom of your metabolic range.</p>\n<p>3.&nbsp; \"Severe calorie restriction\" = restricting calories sufficiently so that you end up spending your time significantly below typically fit people in terms of muscle mass and/or body fat.</p>\n<p>The first and third definitions are pretty straightforward, although it's worth noting that a lot of people engage in mild calorie restriction unintentionally, just through the operation of their natural system which regulates their appetite/urge to eat/urge to stop eating (John Walker calls this the \"food clock.\")</p>\n<p>The second definition requires a little explanation.&nbsp;&nbsp; From simple observation, it appears that small changes in one's energy intake result in corresponding changes in one's metabolic rate.&nbsp; So that if your weight is stable but you eat a little more or less than usual, you might notice that you are a little warmer or cooler than usual.&nbsp; Evidently the body can and does make small adjustments to its metabolic rate in response to changes in food intake.&nbsp; This is also consistent with dieters' reports that they feel cold when dieting.</p>\n<p><strong id=\"II______Does_Calorie_Restriction_Work_in_Humans_\">II.&nbsp;&nbsp;&nbsp;&nbsp; Does Calorie Restriction Work in Humans?</strong></p>\n<p>It seems quite likely that mild calorie restriction works in humans based on the observation that fat people have significantly greater mortality than thin people.</p>\n<p>For example, as illustrated by the charts here:</p>\n<p>http://www.nejm.org/action/showImage?doi=10.1056%2FNEJMoa1000367&amp;iid=f01&amp;</p>\n<p>Of course one cannot know this for sure since there is no ethical way to do a large controlled experiment, but still it's reasonable to infer cause and effect:&nbsp; Common sense says that being fat puts a lot of abnormal extra strain on your system almost all the time.&nbsp; In any event, there seems to be little downside to mild calorie restriction.</p>\n<p>A more interesting question is whether moderate calorie restriction works in humans.&nbsp; Common sense says that it ought to be beneficial based on the idea that slowing one's metabolism ought to slow the aging process, all things being equal.&nbsp; One interesting area of research is studies which look at the effect of modest weight loss among obese people.&nbsp; Is someone who goes from 250 pounds to 225 pounds and stays there more healthy than someone who goes from 210 pounds to 220 pounds and continues to gain weight?&nbsp; If so, part of the difference might be that the second person is towards the top of his metabolic range while the first person is towards the middle or bottom.</p>\n<p>The Calorie Restriction Society web site links to a couple presentations which argue that cancer is actually a metabolic disease related to having too much energy in play.&nbsp; I'm a bit skeptical of this claim, but it does seem to me that you are inviting trouble by having extra energy floating around in your system.</p>\n<p>As for severe calorie restriction, the jury is still out.&nbsp; I don't put too much stock in the left side of the J-shaped curves comparing body weight to mortality.&nbsp; Surely a lot of underweight people have serious latent health problems.&nbsp; What's more interesting to me is that the curves flatten out between BMI of about 19 and 23.5.&nbsp; This suggests to me that one can realize most of the benefits of reduced body mass by being normal weight and that after that, if there are any benefits, it's diminishing returns.</p>\n<p><strong id=\"III____My_Approach_to_Calorie_Restriction\">III.&nbsp;&nbsp; My Approach to Calorie Restriction</strong></p>\n<p>I have decided to adopt an intermediate approach to calorie restriction, i.e. the aim is to stay thin and be towards the bottom of my metabolic range most of the time.&nbsp; The health benefits to staying thin are pretty clear; there doesn't seem to be much downside; and frankly there are a lot of social benefits.&nbsp; The benefits of staying towards the bottom of my metabolic range are more iffy, but again there doesn't seem to be much downside to it.&nbsp; (Putting aside issues of health, the main downside is that it happens pretty frequently that I will have a meal and eat less food than I would have liked to eat.)</p>\n<p>Severe calorie restriction seems too speculative to me to be worth the trouble.&nbsp; Particularly given the social costs and the likely diminishing returns problem.&nbsp; I like having a somewhat muscular appearance as opposed to a gaunt appearance.&nbsp; Since my main priority is to avoid regrets, I am not willing to go this route without pretty solid evidence of benefit.</p>\n<p><strong id=\"IIIa___The_Nuts_and_Bolts\">IIIa.&nbsp; The Nuts and Bolts</strong></p>\n<p>What I do is this:&nbsp; I have a basic daily diet which I believe is reasonably healthy and well-balanced.&nbsp; Although it is somewhat flexible, it contains roughly the same proportions of macro-nutrients and is roughly the same amount of calories each day.&nbsp; From careful observation, I have determined that my basic daily diet is about 500 to 600 calories short of my actual daily caloric needs.&nbsp; i.e. if I stuck to my basic daily diet and ate nothing more, I would lose about a pound a week.&nbsp; I add a small supplement of extra food to my basic daily diet if I work out at the gym in order to balance out the exercise.&nbsp; (Interestingly, I once measured and it seems my basic daily diet, including the exercise supplement, is about 2800 calories.&nbsp; This seems pretty high for a man who is thin, slightly below average height, and only slightly muscular in build.&nbsp; I'm not sure what to make of it.)</p>\n<p>I weigh myself every morning and calculate a 7-day moving average of my weight.&nbsp; I then subtract this number from a pre-determined reference weight and multiply the result by 100.&nbsp; This is the number of additional calories I consume that day in the form of reasonably healthy foods.&nbsp; The idea is to eat close to the minimum to maintain weight, thus staying thin and towards the bottom of my metabolic range.</p>\n<p>Now and then my weight spikes upward when I have an event which involves a lot of eating; after that it drifts back down again.&nbsp; I've been calorie-restricted in this way for some time now.&nbsp; I feel perfectly fine but after every meal I feel like I could easily eat more.&nbsp; I pretty much never get heartburn anymore.&nbsp; I usually wake up quite hungry.&nbsp; These are about the only effects I have noticed.</p>\n<p><strong id=\"IV___Self_Criticisms_of_My_Approach\">IV.&nbsp; Self-Criticisms of My Approach</strong></p>\n<p>In the interest of rationality, it probably makes sense to offer some self-criticism:</p>\n<p>1. I found the above scientific references only after I had settled on my approach to calorie restriction. &nbsp; So there is probably a certain backwardness about my reasoning.&nbsp; My conclusion is based more on my own intuition, reasoning, observations and common sense than on scientific research.</p>\n<p>2.&nbsp; It never occurred to me to regularly measure my body temperature before and after starting this program.&nbsp; Which is unfortunate because it may have given me some useful information about the effects of my diet on my metabolism.</p>\n<p>3.&nbsp; There's really no way to measure if any of this is having an effect on my rate of aging.&nbsp; Without this sort of feedback, I'm pretty much shooting in the dark.</p>\n<p><strong id=\"V___Conclusion\">V.&nbsp; Conclusion</strong></p>\n<p>So that's about the extent of my self-experiment.&nbsp; It's a bit frightening that I'm putting my own health on the line in the face of so much uncertainty.&nbsp; At same time, it seems like a reasonable, conservative approach which is unlikely to lead to regrets.&nbsp; Of course there is an excellent chance I will never know how much of an impact my lifestyle had on my overall health.</p>\n<p>Anyway, I welcome any intelligent thoughts, suggestions, constructive criticism, etc.</p>", "sections": [{"title": "I. What is Calorie Restriction?", "anchor": "I__What_is_Calorie_Restriction_", "level": 1}, {"title": "The bottom line is that, for most people (who are more like the U Wisc controls), CR may indeed provide both health (BOTH studies agree on THIS) and longevity benefits.....and of course, most important.....more \"healthy years.\"", "anchor": "The_bottom_line_is_that__for_most_people__who_are_more_like_the_U_Wisc_controls___CR_may_indeed_provide_both_health__BOTH_studies_agree_on_THIS__and_longevity_benefits_____and_of_course__most_important_____more__healthy_years__", "level": 1}, {"title": "II.\u00a0\u00a0\u00a0\u00a0 Does Calorie Restriction Work in Humans?", "anchor": "II______Does_Calorie_Restriction_Work_in_Humans_", "level": 1}, {"title": "III.\u00a0\u00a0 My Approach to Calorie Restriction", "anchor": "III____My_Approach_to_Calorie_Restriction", "level": 1}, {"title": "IIIa.\u00a0 The Nuts and Bolts", "anchor": "IIIa___The_Nuts_and_Bolts", "level": 1}, {"title": "IV.\u00a0 Self-Criticisms of My Approach", "anchor": "IV___Self_Criticisms_of_My_Approach", "level": 1}, {"title": "V.\u00a0 Conclusion", "anchor": "V___Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "34 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-12T19:16:04.347Z", "modifiedAt": null, "url": null, "title": "Meetup : CANCELLED - Montreal Less Wrong - Easy Lifehacks", "slug": "meetup-cancelled-montreal-less-wrong-easy-lifehacks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:36.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bartimaeus", "createdAt": "2013-05-07T17:14:04.389Z", "isAdmin": false, "displayName": "bartimaeus"}, "userId": "mqWrbcZHzhfPLnJqg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p4FrzixZ2ooAiZDkM/meetup-cancelled-montreal-less-wrong-easy-lifehacks", "pageUrlRelative": "/posts/p4FrzixZ2ooAiZDkM/meetup-cancelled-montreal-less-wrong-easy-lifehacks", "linkUrl": "https://www.lesswrong.com/posts/p4FrzixZ2ooAiZDkM/meetup-cancelled-montreal-less-wrong-easy-lifehacks", "postedAtFormatted": "Wednesday, February 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20CANCELLED%20-%20Montreal%20Less%20Wrong%20-%20Easy%20Lifehacks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20CANCELLED%20-%20Montreal%20Less%20Wrong%20-%20Easy%20Lifehacks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4FrzixZ2ooAiZDkM%2Fmeetup-cancelled-montreal-less-wrong-easy-lifehacks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20CANCELLED%20-%20Montreal%20Less%20Wrong%20-%20Easy%20Lifehacks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4FrzixZ2ooAiZDkM%2Fmeetup-cancelled-montreal-less-wrong-easy-lifehacks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp4FrzixZ2ooAiZDkM%2Fmeetup-cancelled-montreal-less-wrong-easy-lifehacks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wr'>CANCELLED - Montreal Less Wrong - Easy Lifehacks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 February 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">3459 McTavish, Montr\u00e9al, qc, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Unfortunately, the meetup has to be cancelled; hopefully it can be done soon.  Apologies for the last-second cancellation.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wr'>CANCELLED - Montreal Less Wrong - Easy Lifehacks</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p4FrzixZ2ooAiZDkM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.5605971120431994e-06, "legacy": true, "legacyId": "25489", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___CANCELLED___Montreal_Less_Wrong___Easy_Lifehacks\">Discussion article for the meetup : <a href=\"/meetups/wr\">CANCELLED - Montreal Less Wrong - Easy Lifehacks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 February 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">3459 McTavish, Montr\u00e9al, qc, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Unfortunately, the meetup has to be cancelled; hopefully it can be done soon.  Apologies for the last-second cancellation.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___CANCELLED___Montreal_Less_Wrong___Easy_Lifehacks1\">Discussion article for the meetup : <a href=\"/meetups/wr\">CANCELLED - Montreal Less Wrong - Easy Lifehacks</a></h2>", "sections": [{"title": "Discussion article for the meetup : CANCELLED - Montreal Less Wrong - Easy Lifehacks", "anchor": "Discussion_article_for_the_meetup___CANCELLED___Montreal_Less_Wrong___Easy_Lifehacks", "level": 1}, {"title": "Discussion article for the meetup : CANCELLED - Montreal Less Wrong - Easy Lifehacks", "anchor": "Discussion_article_for_the_meetup___CANCELLED___Montreal_Less_Wrong___Easy_Lifehacks1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-12T19:56:53.045Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: The Twelve Virtues", "slug": "meetup-berkeley-the-twelve-virtues", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:35.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/88JuFmzbuuYmPb2Ab/meetup-berkeley-the-twelve-virtues", "pageUrlRelative": "/posts/88JuFmzbuuYmPb2Ab/meetup-berkeley-the-twelve-virtues", "linkUrl": "https://www.lesswrong.com/posts/88JuFmzbuuYmPb2Ab/meetup-berkeley-the-twelve-virtues", "postedAtFormatted": "Wednesday, February 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20The%20Twelve%20Virtues&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20The%20Twelve%20Virtues%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88JuFmzbuuYmPb2Ab%2Fmeetup-berkeley-the-twelve-virtues%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20The%20Twelve%20Virtues%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88JuFmzbuuYmPb2Ab%2Fmeetup-berkeley-the-twelve-virtues", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88JuFmzbuuYmPb2Ab%2Fmeetup-berkeley-the-twelve-virtues", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ws'>Berkeley: The Twelve Virtues</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA 94703</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tonight's meetup will feature a critical discussion of the Twelve Virtues of Rationality, an essay that is cited in the Less Wrong Sequences:</p>\n\n<p><a href=\"http://yudkowsky.net/rational/virtues/\" rel=\"nofollow\">http://yudkowsky.net/rational/virtues/</a></p>\n\n<p>In particular, I'd like to see discussion of:</p>\n\n<p>What exactly are these virtues, and what are good examples of them?</p>\n\n<p>Which virtues are overemphasized?</p>\n\n<p>What virtues are underemphasized or absent?</p>\n\n<p>Comparison with other lists of virtues (Catholicism, Benjamin Franklin, etc.)</p>\n\n<p>Whether the idea of \"virtue\" is useful at all.</p>\n\n<p>Please arrive between 7pm and 7:30pm tonight. At 7:30pm we'll review our weekly goals and record goals for the coming week. Then we'll discuss the twelve virtues as long as people are interested.</p>\n\n<p>The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ws'>Berkeley: The Twelve Virtues</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "88JuFmzbuuYmPb2Ab", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5606442291823465e-06, "legacy": true, "legacyId": "25490", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__The_Twelve_Virtues\">Discussion article for the meetup : <a href=\"/meetups/ws\">Berkeley: The Twelve Virtues</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA 94703</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tonight's meetup will feature a critical discussion of the Twelve Virtues of Rationality, an essay that is cited in the Less Wrong Sequences:</p>\n\n<p><a href=\"http://yudkowsky.net/rational/virtues/\" rel=\"nofollow\">http://yudkowsky.net/rational/virtues/</a></p>\n\n<p>In particular, I'd like to see discussion of:</p>\n\n<p>What exactly are these virtues, and what are good examples of them?</p>\n\n<p>Which virtues are overemphasized?</p>\n\n<p>What virtues are underemphasized or absent?</p>\n\n<p>Comparison with other lists of virtues (Catholicism, Benjamin Franklin, etc.)</p>\n\n<p>Whether the idea of \"virtue\" is useful at all.</p>\n\n<p>Please arrive between 7pm and 7:30pm tonight. At 7:30pm we'll review our weekly goals and record goals for the coming week. Then we'll discuss the twelve virtues as long as people are interested.</p>\n\n<p>The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__The_Twelve_Virtues1\">Discussion article for the meetup : <a href=\"/meetups/ws\">Berkeley: The Twelve Virtues</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: The Twelve Virtues", "anchor": "Discussion_article_for_the_meetup___Berkeley__The_Twelve_Virtues", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: The Twelve Virtues", "anchor": "Discussion_article_for_the_meetup___Berkeley__The_Twelve_Virtues1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-12T21:28:23.458Z", "modifiedAt": null, "url": null, "title": "[Meta] Open thread even more often?", "slug": "meta-open-thread-even-more-often", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.339Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "solipsist", "createdAt": "2013-06-09T21:07:31.678Z", "isAdmin": false, "displayName": "solipsist"}, "userId": "PMDZAtysvt35Mni2a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qk6ArDxwC5S9HgTdZ/meta-open-thread-even-more-often", "pageUrlRelative": "/posts/qk6ArDxwC5S9HgTdZ/meta-open-thread-even-more-often", "linkUrl": "https://www.lesswrong.com/posts/qk6ArDxwC5S9HgTdZ/meta-open-thread-even-more-often", "postedAtFormatted": "Wednesday, February 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMeta%5D%20Open%20thread%20even%20more%20often%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMeta%5D%20Open%20thread%20even%20more%20often%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqk6ArDxwC5S9HgTdZ%2Fmeta-open-thread-even-more-often%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMeta%5D%20Open%20thread%20even%20more%20often%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqk6ArDxwC5S9HgTdZ%2Fmeta-open-thread-even-more-often", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqk6ArDxwC5S9HgTdZ%2Fmeta-open-thread-even-more-often", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>This week's <a title=\"Open thread February 11-17\" href=\"/r/discussion/lw/jnx/open_thread_for_february_11_17\">open thread</a>&nbsp;is less than a day old, and has already accumulated more comments than the 15 latest non-open-thread posts combined. &nbsp;I fear the thread will wither and die before Friday.</p>\n<p>Going from monthly to weekly open threads was a big hit. &nbsp;Should we ratchet up open thread frequency even more? &nbsp;Should we add <a href=\"http://wiki.lesswrong.com/wiki/Special_threads#Periodic_threads\">more outlets for comments</a>, or will comments inevitably expand to fill the available room?</p>\n<p><strong>Proposal for discussion</strong>: We follow a regimen of weekly blather threads for the next two weeks, then reassess.</p>\n<ul>\n<li><strong>Stupid questions</strong> (<em>Monday</em>) - Admit your ignorance</li>\n<li><strong>Advice</strong> (<em>Tuesday</em>) - Seek the wisdom of the crowd</li>\n<li><strong>Open Thread</strong> (<em>Wednesday</em>) - Catch-all prattle</li>\n<li><strong>Links</strong> (<em>Friday</em>) - Quality readings. &nbsp;Meme postings punishable by death</li>\n</ul>\n<p>&nbsp;</p>\n<h4>Addendum</h4>\n<p>I posted a <a href=\"/lw/joa/open_thread_links_20140214/\">Links</a> on Friday and a <a href=\"/lw/jp0/open_thread_stupid_questions_20140217/\">Stupid Questions</a> on Monday. &nbsp;In the 5 post-days that they've been up, they've accumulated 32 comments. &nbsp;Based on these numbers, <em>it seems unlikely that these topical open threads will relieve pressure on the main Open Thread</em>&nbsp;and so I've stopped the experiment.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qk6ArDxwC5S9HgTdZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 20, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "25491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jD5yw654riqqEiyr3", "MWqhF9TH7gHQsGyhe", "ETHJmLGfnFR3wrDLw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-12T22:45:04.853Z", "modifiedAt": null, "url": null, "title": "Terminal and Instrumental Beliefs", "slug": "terminal-and-instrumental-beliefs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:56.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GoddoqAvGtqn4SoCY/terminal-and-instrumental-beliefs", "pageUrlRelative": "/posts/GoddoqAvGtqn4SoCY/terminal-and-instrumental-beliefs", "linkUrl": "https://www.lesswrong.com/posts/GoddoqAvGtqn4SoCY/terminal-and-instrumental-beliefs", "postedAtFormatted": "Wednesday, February 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Terminal%20and%20Instrumental%20Beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATerminal%20and%20Instrumental%20Beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGoddoqAvGtqn4SoCY%2Fterminal-and-instrumental-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Terminal%20and%20Instrumental%20Beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGoddoqAvGtqn4SoCY%2Fterminal-and-instrumental-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGoddoqAvGtqn4SoCY%2Fterminal-and-instrumental-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 591, "htmlBody": "<p><a href=\"http://bywayofcontradiction.com/terminal-instrumental-beliefs/\">Cross-Posted</a> on <a href=\"http://bywayofcontradiction.com/\">By Way of Contradiction</a></p>\n<p>As you may know from my past posts, I believe that probabilities should not be viewed as uncertainty, but instead as weights on how much you care about different possible universes. This is a very subjective view of reality. In particular, it seems to imply that when other people have different beliefs than me, there is no sense in which they can be wrong. They just care about the possible futures with different weights than I do. I will now try to argue that this is not a necessary conclusion.</p>\n<p>First, let's be clear what we mean by saying that probabilities are weights on values. Imagine I have an unfair coin which give heads with probability 90%. I care 9 times as much about the possible futures in which the coin comes up heads as I do about the possible futures in which the coins comes up tails. Notice that this does not mean I want to coin to come up heads. What it means is that I would prefer getting a dollar if the coin comes up heads to getting a dollar if the coin comes up tails.</p>\n<p>Now, imagine that you are unaware of the fact that it is an unfair coin. By default, you believe that the coin comes up heads with probability 50%. How can we express the fact that I have a correct belief, and you have an incorrect belief in the language of values?</p>\n<p>We will take advantage of the language of <a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\">terminal and instrumental values</a>. A terminal value is something that you try to get because you want it. An instrumental value is something that you try to get because you believe it will help you get something else that you want.</p>\n<p>If you believe a statement S, that means that you care more about the worlds in which S is true. If you terminally assign a higher value to worlds in which S is true, we will call this belief a terminal belief. On the other hand, if you believe S because you think that S is logically implied by some other terminal belief, T, we will call your belief in S an instrumental belief.</p>\n<p>Instrumental values can be wrong, if you are factually wrong about the fact that the instrumental value will help achieve your terminal values. Similarly, an Instrumental belief can be wrong if you are factually wrong about the fact that it is implied by your terminal belief.</p>\n<p>Your belief that the coin will come up heads with probability 50% is an instrumental belief. You have a terminal belief in some form of Occam's razor. This causes you to believe that coins are likely to behave similarly to how coins have behaved in the past. In this case, that was not valid, because you did not take into consideration the fact that I chose the coin for the purpose of this thought experiment. Your Instrumental belief is in this case wrong. If your belief in Occam's razor is terminal, then it would not be possible for Occam's razor to be wrong.</p>\n<p>This is probably a distinction that you are already familiar with. I am talking about the difference between an axiomatic belief and a deduced belief. So why am I viewing it like this? I am trying to strengthen my understanding of the analogy between beliefs and values. To me, they appear to be two different sides of the same coin, and building up this analogy might allow us to translate some intuitions or results from one view into the other view.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GoddoqAvGtqn4SoCY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 1.5608384394197046e-06, "legacy": true, "legacyId": "25493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-13T10:30:46.492Z", "modifiedAt": null, "url": null, "title": "Embracing the \"sadistic\" conclusion", "slug": "embracing-the-sadistic-conclusion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:35.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZGSd5K5Jzn6wrKckC/embracing-the-sadistic-conclusion", "pageUrlRelative": "/posts/ZGSd5K5Jzn6wrKckC/embracing-the-sadistic-conclusion", "linkUrl": "https://www.lesswrong.com/posts/ZGSd5K5Jzn6wrKckC/embracing-the-sadistic-conclusion", "postedAtFormatted": "Thursday, February 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Embracing%20the%20%22sadistic%22%20conclusion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmbracing%20the%20%22sadistic%22%20conclusion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZGSd5K5Jzn6wrKckC%2Fembracing-the-sadistic-conclusion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Embracing%20the%20%22sadistic%22%20conclusion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZGSd5K5Jzn6wrKckC%2Fembracing-the-sadistic-conclusion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZGSd5K5Jzn6wrKckC%2Fembracing-the-sadistic-conclusion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 687, "htmlBody": "<p>This is not the post I was planning to write. Originally, it was going to be a heroic post where I showed my devotion to philosophical principles by reluctantly but fearlessly biting the bullet on the sadistic conclusion. Except... it turns out to be nothing like that, because the sadistic conclusion is practically void of content and embracing it is trivial.</p>\n<h2>Sadism versus repugnance</h2>\n<p>The sadistic conclusion can be found in\u00a0Gustaf Arrhenius's papers such as \"<a href=\"https://www.iffs.se/media/2264/an-impossibility-theorem-for-welfarist-axiologies-in-ep-2000.pdf\">An Impossibility Theorem for Welfarist Axiologies</a>.\" In it he demonstrated that - modulo a few technical assumptions - any system of population ethics has to embrace either the\u00a0<a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">Repugnant Conclusion</a>, the Anti-Egalitarian Conclusion or the Sadistic conclusion. Astute readers of my <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">blog</a> <a href=\"/lw/j3k/skirting_the_mere_addition_paradox/\">posts</a> may have noticed I'm not the repugnant conclusion's greatest fan, evah! The anti-egalitarian conclusion claims that you can make things better by keeping total happiness/welfare/preference satisfaction constant but redistributing it in a more unequal way. Few systems of ethics embrace this in theory (though many social systems seem to embrace it in practice).</p>\n<p>Remains the sadistic conclusion. A population ethics that accepts this is one where it is sometimes better to create someone whose life is not worth living (call them a \"victim\"), rather a group of people whose lives <em>are </em>worth living. It seems well named - can you not feel the top hatted villain twirl his moustache as he gleefully creates lives condemned to pain and misery, laughing manically as he prevents the intrepid heroes from changing the settings on his incubator machine to \"worth living\"? How could that sadist be in the right, according to any decent system of ethics?</p>\n<h2>Remove the connotations, then the argument</h2>\n<p>But the argument is flawed, for two main reasons: one that strikes at the connotations of \"sadistic\", the other at the heart of the comparison itself.</p>\n<p>The reason the sadistic aspect is a misnomer is that creating a victim is not actually a positive development. Almost all ethical systems would advocate improving the victim's life, if at all possible (or ending it, if appropriate). Indeed some ethical systems which have the \"sadistic conclusion\" (such as <a href=\"http://en.wikipedia.org/wiki/Prioritarianism\">prioritarianism</a> or <a href=\"http://plato.stanford.edu/entries/egalitarianism/\">egalitarianism</a>) would think it <em>more</em> important to improve the victim's life that some ethical systems that don't have the conclusion (such as total utilitarianism). Only if such help is somehow impossible do you get the conclusion. So it's not a gleeful sadist inflicting pain, but a reluctant acceptance that \"if universe conspires to prevent us from helping this victim, then it still may be worth creating them as the least bad option\" (see for instance this <a href=\"/lw/j0l/weak_repugnant_conclusion_need_not_be_so/a4ns\">comment</a>).</p>\n<p>\"The least bad option.\" For the sadistic conclusion is based on a trick, contrasting two bad options and making them seem related (see this <a href=\"/lw/j0l/weak_repugnant_conclusion_need_not_be_so/a4n0\">comment</a>). Consider for example whether it is good to create a large permanent underclass of people with much more limited and miserable lives than all others - but whose lives are nevertheless just above some complicated line of \"worth living\". You may or may not agree that this is bad, but many people and many systems of population ethics do feel it's a negative outcome.</p>\n<p>Then, given that this underclass is a bad outcome (and given a few assumptions as to how outcomes are ranked) then we can find other bad outcomes that are <em>not quite as bad as this one</em>. Such as... a single victim, a tiny bit below the line of \"worth living\". So the sadistic conclusion is not saying anything about the happiness level of a single created population. It's simply saying that sometime (A) creating underclasses with slightly worthwhile lives can sometimes be bad, while (B) creating a victim can sometimes be less bad. But the victim isn't playing a useful role here: they're just an example of a bad outcome better than (A), only linked to (A) through superficial similarity and rhetoric.</p>\n<p>For most systems of population ethics the sadistic conclusion can thus be reduced to \"creating underclasses with slightly worthwhile lives can sometimes be bad.\" But this is the very point that population ethicists are disputing each other about! Wrapping that central point into a misleading \"sadistic conclusion\" is... well, the term \"misleading\" gave it away.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"uG75MELqjCEfciaRp": 2, "nSHiKwWyMZFdZg5qt": 2, "Zs4nYLkNr7Rbo4mAP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZGSd5K5Jzn6wrKckC", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 30, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "24751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is not the post I was planning to write. Originally, it was going to be a heroic post where I showed my devotion to philosophical principles by reluctantly but fearlessly biting the bullet on the sadistic conclusion. Except... it turns out to be nothing like that, because the sadistic conclusion is practically void of content and embracing it is trivial.</p>\n<h2 id=\"Sadism_versus_repugnance\">Sadism versus repugnance</h2>\n<p>The sadistic conclusion can be found in&nbsp;Gustaf Arrhenius's papers such as \"<a href=\"https://www.iffs.se/media/2264/an-impossibility-theorem-for-welfarist-axiologies-in-ep-2000.pdf\">An Impossibility Theorem for Welfarist Axiologies</a>.\" In it he demonstrated that - modulo a few technical assumptions - any system of population ethics has to embrace either the&nbsp;<a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/\">Repugnant Conclusion</a>, the Anti-Egalitarian Conclusion or the Sadistic conclusion. Astute readers of my <a href=\"/lw/d8z/a_small_critique_of_total_utilitarianism/\">blog</a> <a href=\"/lw/j3k/skirting_the_mere_addition_paradox/\">posts</a> may have noticed I'm not the repugnant conclusion's greatest fan, evah! The anti-egalitarian conclusion claims that you can make things better by keeping total happiness/welfare/preference satisfaction constant but redistributing it in a more unequal way. Few systems of ethics embrace this in theory (though many social systems seem to embrace it in practice).</p>\n<p>Remains the sadistic conclusion. A population ethics that accepts this is one where it is sometimes better to create someone whose life is not worth living (call them a \"victim\"), rather a group of people whose lives <em>are </em>worth living. It seems well named - can you not feel the top hatted villain twirl his moustache as he gleefully creates lives condemned to pain and misery, laughing manically as he prevents the intrepid heroes from changing the settings on his incubator machine to \"worth living\"? How could that sadist be in the right, according to any decent system of ethics?</p>\n<h2 id=\"Remove_the_connotations__then_the_argument\">Remove the connotations, then the argument</h2>\n<p>But the argument is flawed, for two main reasons: one that strikes at the connotations of \"sadistic\", the other at the heart of the comparison itself.</p>\n<p>The reason the sadistic aspect is a misnomer is that creating a victim is not actually a positive development. Almost all ethical systems would advocate improving the victim's life, if at all possible (or ending it, if appropriate). Indeed some ethical systems which have the \"sadistic conclusion\" (such as <a href=\"http://en.wikipedia.org/wiki/Prioritarianism\">prioritarianism</a> or <a href=\"http://plato.stanford.edu/entries/egalitarianism/\">egalitarianism</a>) would think it <em>more</em> important to improve the victim's life that some ethical systems that don't have the conclusion (such as total utilitarianism). Only if such help is somehow impossible do you get the conclusion. So it's not a gleeful sadist inflicting pain, but a reluctant acceptance that \"if universe conspires to prevent us from helping this victim, then it still may be worth creating them as the least bad option\" (see for instance this <a href=\"/lw/j0l/weak_repugnant_conclusion_need_not_be_so/a4ns\">comment</a>).</p>\n<p>\"The least bad option.\" For the sadistic conclusion is based on a trick, contrasting two bad options and making them seem related (see this <a href=\"/lw/j0l/weak_repugnant_conclusion_need_not_be_so/a4n0\">comment</a>). Consider for example whether it is good to create a large permanent underclass of people with much more limited and miserable lives than all others - but whose lives are nevertheless just above some complicated line of \"worth living\". You may or may not agree that this is bad, but many people and many systems of population ethics do feel it's a negative outcome.</p>\n<p>Then, given that this underclass is a bad outcome (and given a few assumptions as to how outcomes are ranked) then we can find other bad outcomes that are <em>not quite as bad as this one</em>. Such as... a single victim, a tiny bit below the line of \"worth living\". So the sadistic conclusion is not saying anything about the happiness level of a single created population. It's simply saying that sometime (A) creating underclasses with slightly worthwhile lives can sometimes be bad, while (B) creating a victim can sometimes be less bad. But the victim isn't playing a useful role here: they're just an example of a bad outcome better than (A), only linked to (A) through superficial similarity and rhetoric.</p>\n<p>For most systems of population ethics the sadistic conclusion can thus be reduced to \"creating underclasses with slightly worthwhile lives can sometimes be bad.\" But this is the very point that population ethicists are disputing each other about! Wrapping that central point into a misleading \"sadistic conclusion\" is... well, the term \"misleading\" gave it away.</p>", "sections": [{"title": "Sadism versus repugnance", "anchor": "Sadism_versus_repugnance", "level": 1}, {"title": "Remove the connotations, then the argument", "anchor": "Remove_the_connotations__then_the_argument", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "41 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["pyTuR4ZbLfqpS2oMh", "cFHpWgk8WF7ekrMmq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 20, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-13T17:06:43.216Z", "modifiedAt": null, "url": null, "title": "A few remarks about mass-downvoting", "slug": "a-few-remarks-about-mass-downvoting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:09.379Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gjm", "createdAt": "2009-03-09T01:11:32.668Z", "isAdmin": false, "displayName": "gjm"}, "userId": "977L8MR7JmNrQx6df", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c6Hv472TZNju6zSyE/a-few-remarks-about-mass-downvoting", "pageUrlRelative": "/posts/c6Hv472TZNju6zSyE/a-few-remarks-about-mass-downvoting", "linkUrl": "https://www.lesswrong.com/posts/c6Hv472TZNju6zSyE/a-few-remarks-about-mass-downvoting", "postedAtFormatted": "Thursday, February 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20few%20remarks%20about%20mass-downvoting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20few%20remarks%20about%20mass-downvoting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6Hv472TZNju6zSyE%2Fa-few-remarks-about-mass-downvoting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20few%20remarks%20about%20mass-downvoting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6Hv472TZNju6zSyE%2Fa-few-remarks-about-mass-downvoting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6Hv472TZNju6zSyE%2Fa-few-remarks-about-mass-downvoting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 333, "htmlBody": "<p><em>To whoever has for the last several days been downvoting ~10 of my old comments per day</em>:</p>\n<p>It is possible that your intention is to discourage me from commenting on Less Wrong.</p>\n<p>The actual effect is the reverse. My comments still end up positive on average, and I am therefore motivated to post more of them in order to compensate for the steady karma drain you are causing.</p>\n<p>If you are mass-downvoting other people, the effect on some of them is probably the same.</p>\n<p><em>To the LW admins, if any are reading</em>:</p>\n<p>Look, can we really not do anything about this behaviour? It's childish and stupid, and it makes the karma system less useful (e.g., for comment-sorting), and it gives bad actors a disproportionate influence on Less Wrong. It seems like there are lots of obvious things that would go some way towards helping, many of which have been discussed in past threads about this.</p>\n<p>Failing that, can we at least agree that it's bad behaviour and that it would be good in principle to stop it or make it more visible and/or inconvenient?</p>\n<p>Failing that, can we at least have an official statement from an LW administrator that mass-downvoting is not considered an undesirable behaviour here? I really hope this isn't the opinion of the LW admins, but as the topic has been discussed from time to time with never any admin response I've been thinking it increasingly likely that it is. If so, let's at least be honest about it.</p>\n<p><em>To anyone else reading this</em>:</p>\n<p>If you should happen to notice that a sizeable fraction of my comments are at -1, this is probably why. (Though of course I may just have posted a bunch of silly things. I expect it happens from time to time.)</p>\n<p>My apologies for cluttering up Discussion with this. (But not very many apologies; this sort of mass-downvoting seems to me to be one of the more toxic phenomena on Less Wrong, and I retain some small hope that eventually something may be done about it.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "hGzywXvWhSdJi5F2a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c6Hv472TZNju6zSyE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 34, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "25495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-13T21:28:52.968Z", "modifiedAt": null, "url": null, "title": "Some Tools For Optimizing Our Media Use", "slug": "some-tools-for-optimizing-our-media-use", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:21.285Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mbitton24", "createdAt": "2014-02-04T22:57:52.691Z", "isAdmin": false, "displayName": "mbitton24"}, "userId": "AmBhj854NproW2tDo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/97BGK4ReXf9YQnupr/some-tools-for-optimizing-our-media-use", "pageUrlRelative": "/posts/97BGK4ReXf9YQnupr/some-tools-for-optimizing-our-media-use", "linkUrl": "https://www.lesswrong.com/posts/97BGK4ReXf9YQnupr/some-tools-for-optimizing-our-media-use", "postedAtFormatted": "Thursday, February 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20Tools%20For%20Optimizing%20Our%20Media%20Use&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20Tools%20For%20Optimizing%20Our%20Media%20Use%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97BGK4ReXf9YQnupr%2Fsome-tools-for-optimizing-our-media-use%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20Tools%20For%20Optimizing%20Our%20Media%20Use%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97BGK4ReXf9YQnupr%2Fsome-tools-for-optimizing-our-media-use", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97BGK4ReXf9YQnupr%2Fsome-tools-for-optimizing-our-media-use", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2778, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin:0cm; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Benevolent and malevolent media producers possess the power to influence society in positive and negative ways. They can do this through agenda setting, framing, priming, spreading memes, altering perceptions of groups and individuals, outright propaganda, and other methods.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">I think more attention should be paid to the pathways from content to effects, so that we can optimize our cultural landscape.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Lest this post soon turn into Applause Light Vegas, I&rsquo;ll now get into some methods I think can be used to sway mass opinion in a direction amenable to making the world better. Many of these methods deal with familiar biases, heuristics, and psychological effects.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\"><a id=\"more\"></a><br /></span></p>\n<h3><span style=\"font-family: Arial;\">Media Use Facilitating Positive Social Change</span></h3>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial;\">First, the mass media possesses the power to alter estimates of the likelihood and frequency of specific occurrences. Think back to some of the classic examples of the availability heuristic. </span><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">When <a href=\"http://heatherlench.com/wp-content/uploads/2008/07/johnson-tversky.pdf\" target=\"_blank\">asked</a> to estimate the number of homicides in the USA compared to suicides, people answer that there are far more homicides in the United States, even though<span>&nbsp;</span><a href=\"http://www.worldlifeexpectancy.com/usa-homicide-vs-suicide\" target=\"_blank\">the reverse is true</a>. The mass media report on homicides far more often than they report on suicides, so people have more available instances of homicide in their memories and these come to mind more easily. This influences their beliefs about the real world, which can then be politicized to lead to different stances on gun control and education. The priorities of a culture with a homicide problem are not the priorities of a culture with a suicide problem.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">This effect is consistent with some theoretical models of the mass media&rsquo;s impact on society. <em>Cultivation theorists</em> <a href=\"http://www.zfg.pwsz.krosno.pl/gfx/pwszkrosno/pl/defaultaktualnosci/675/5/1/s08a_lk_cultivation_overview_gerbner.pdf\" target=\"_blank\">understand</a> the media, especially television, as a system of coherent memes and messages reflecting a society&rsquo;s dominant ideology. If we accept the fundamental claim of cultivation theory then we should hypothesize exposure to television to be positively correlated with status quo beliefs and attitudes. We might then expect high exposure to non-</span><span style=\"font-family: Arial;\">fiction television to lead to mean- and scary-world beliefs, given the disproportionate amount of media coverage homicides receive. One cultivation theorist <a href=\"http://www.gwern.net/docs/2008-appel.pdf\" target=\"_blank\">found</a> this result, yet did not find the same effect on heavy fiction viewers.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial;\">Malevolent, benevolent, and clueless media producers could capitalize on the availability heuristic to adjust mass estimates of society&rsquo;s biggest problems, and by extension, mass assessments of social priorities. This ability of the mass media to affect the perceived importance of subjects by representing or not representing them is sometimes called <em><a href=\"http://poq.oxfordjournals.org/content/36/2/176.full.pdf\" target=\"_blank\">agenda setting</a></em>. Want different kinds of people to take existential risks seriously? Maybe get existential risks mentioned in media outlets that different types of people read. Use <a href=\"https://medium.com/\" target=\"_blank\">Medium.com</a>, pitch articles to <a href=\"http://www.theguardian.com/commentisfree/2010/mar/04/you-tell-us\" target=\"_blank\">The Guardian</a>, write an editorial to your local newspaper, increase the representation of <a href=\"http://www.utilitarian-essays.com/wikipedia.html\" target=\"_blank\">important issues on Wikipedia</a>, and so on. You don&rsquo;t even need to convince people that Friendly AI should be a global priority as much as you need to convince them that thinking so doesn&rsquo;t make you crazy. Exposing people to AI concerns without coming off as a clear member of a disliked outgroup (e.g. conspiracy theorist) can play a big role in legitimizing the issue in the public&rsquo;s eyes.</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Politicians and media outlets can also make use of <em><a href=\"http://www.asc.upenn.edu/usr/ogandy/C45405%20resources/Scheufele%20framing%20as%20effects.pdf\" target=\"_blank\">framing</a></em> devices to influence audience perspectives on news stories by tweaking irrelevant factors. A newspaper headline claiming, &ldquo;Public condemnation of democracy should not be allowed&rdquo; will receive more support than will one that claims, &ldquo;It is right to forbid public condemnation of democracy.&rdquo; If you&rsquo;ve ever heard a politician speak, you&rsquo;ve probably noticed how they <a href=\"http://www.youtube.com/watch?v=SKftRlzh2RM\" target=\"_blank\">frame everything they say</a> in a way that makes it sound better than it is. Similarly, a headline will have very different connotations if it describes an event as a &ldquo;strike&rdquo; or as an &ldquo;invasion&rdquo; or as a &ldquo;bombing.&rdquo; (And was it committed against &ldquo;soldiers&rdquo; or &ldquo;forces&rdquo; or &ldquo;rebels&rdquo; or &ldquo;terrorists&rdquo;?)</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Framing isn&rsquo;t purely a word-selection thing. It can be done with audio-visual media as well. Film fans among you may have heard of the </span><em style=\"font-family: Arial;\"><a href=\"https://en.wikipedia.org/wiki/Kuleshov_Effect\" target=\"_blank\">Kuleshov effect</a></em><span style=\"font-family: Arial;\">, discovered with a famous experiment that used and re-used a single close-up of a man&rsquo;s face against a series of different images, such as a bowl of soup, a little girl smiling, a funeral. You can watch a short example <a href=\"http://www.youtube.com/watch?v=_gGl3LJ7vHc\" target=\"_blank\">here</a>. Each time we cut back to the man, his face appears to express a different emotion even though it&rsquo;s actually an identical shot of his face. Soviet Montage filmmakers capitalized on this effect in their movies to express meanings through the </span><em style=\"font-family: Arial;\">juxtaposition</em><span style=\"font-family: Arial;\"> of different shots.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h3><span style=\"font-family: Arial;\">Biases Facilitating Social Stagnation</span></h3>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Media producers have much more to think about than the biases and heuristics that facilitate persuasion. They also have to examine the psychological and cultural factors that entrench ideas in our heads. The mind is the <a href=\"http://www.youtube.com/watch?v=iqODbP1T3nk\" target=\"_blank\">Hotel California</a> of ideas &ndash; once one gets in there, it might never see the light of day again. What are some of those forces that keep us from changing our minds?</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">The first important factor to consider is <em><a href=\"https://en.wikipedia.org/wiki/Selective_exposure_theory\" target=\"_blank\">selective exposure</a></em>. Before an idea can sound persuading to you, it has to get in front of you. This is harder than it seems because people don&rsquo;t want to be confronted with ideas they don&rsquo;t agree with. <a href=\"https://en.wikipedia.org/wiki/Selective_exposure_theory\" target=\"_blank\">Confirmation bias</a> predisposes them to crave ideas they already agree with. If I&rsquo;m an atheist surfing YouTube, am I going to click on &ldquo;Creationist moron DESTROYED with a Hitchslap&rdquo; or on &ldquo;How to prove atheism wrong in 8 seconds&rdquo;? People avoid the stuff that doesn&rsquo;t seem like it&rsquo;ll cater to their beliefs.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">If you want to get existential risk and AGI messages in front of new audiences, you need to find ways to make your stances on those issues seem somewhat consistent with a lot of other peoples&rsquo; current views. Getting important undercovered ideas into the public eye will probably mean <em>smuggling</em> them there. A TV station only covering existential risks can easily be ignored by all the people with no interest in existential risks. (LessWrong is sort of an online equivalent to this.) Instead, you may have to smuggle your important ideas into a mixture of more mainstream content.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">The <em><a href=\"https://en.wikipedia.org/wiki/Confirmation_bias#Preference_for_early_information\" target=\"_blank\">primacy effect</a></em> suggests that the earliest information people receive about an issue is likely to form their thinking on that issue, biasing them in favour of that viewpoint. This suggests to me that it might be a good idea to find subjects on which people haven&rsquo;t completely formed their ideas yet. If you can give people good ideas before they get a chance to form bad ideas, they&rsquo;ll be more partial to your ideas than if you try to convince them that their fully-formed \"bad\" ideas are inferior to yours. My impression is, relative to secularism and the dangers of technological progress, that fundamental anti-speciesist and effective altruist ideas are subjects on which people are still forming their ideas.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Mass media agenda setting also works in combination with other biases. The&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Third-person_effect\" target=\"_blank\">third person effect</a></em>&nbsp;is the occurrence of people overestimating the magnitude of the media&rsquo;s influence on other people. Do you ever assume that a political attack ad, or a marketing pitch, or porn, or a violent video game probably affects a lot of people &ndash; while being very confident that you aren&rsquo;t one of those people being affected?&nbsp;<em>You</em>, of course, are much too clever but those&nbsp;<em>other</em>&nbsp;people are surely easy pickings for propagandists. This view is probably closely related to&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Overconfidence_effect\" target=\"_blank\">overconfidence</a></em>&nbsp;and the&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Bias_blind_spot\" target=\"_blank\">bias blind spot</a></em>.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\"><a href=\"http://poq.oxfordjournals.org/content/47/1/1.full.pdf\" target=\"_blank\">Davison</a> points out that the third person effect can play into&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Pluralistic_ignorance\" target=\"_blank\">pluralistic ignorance</a></em>. Misperceptions of public opinion can lead to the majority reinforcing behavioural norms that only the minority of a population agrees with. If we assume that the majority of persuasion tactics we see in the media are successful on other people, then we&rsquo;re going to wind up with skewed ideas of what everyone else believes. In our current era of <a href=\"http://harrisonsteele.com/?p=208\" target=\"_blank\">demassification</a> and social media, we live in so-called &ldquo;cyber ghettos&rdquo; where most of our information comes from people on our social media feeds and others that already agree with us. This probably leads to an overestimation of the popularity and mainstream-ness of our ideas.</span></p>\n<p>&nbsp;</p>\n<h3><span style=\"font-family: Arial;\">Media Use Facilitating Negative Social Change</span></h3>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">So, all sorts of biases and heuristics prevent ideas from leaving Hotel California - and on the scale of a culture, this creates memetic stagnation. Now, let&rsquo;s look at how the mass media can be deliberately used to create <em>negative</em> change. Understanding how this works can help us squash the deliberate spread of misinformation or improve our more benevolent methods of media persuasion.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">There is a field called &ldquo;<a href=\"http://scholar.princeton.edu/rccu/files/Agnotology%20Intro%20Chapter,%20Robert%20Proctor.pdf\" target=\"_blank\">agnotology</a>&rdquo; that is about exactly this: ignorance, how and why it&rsquo;s produced and maintained. When examining a field, a good indicator of whether there&rsquo;s disinformation at work is whether there exists a divide between expert opinion and public opinion. My impression is that this is the case for many of the issues that interest LessWrongers.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">The <a href=\"http://www.weber.edu/wsuimages/geography/Bedford%202010-%20Agnotology%20as%20a%20teaching%20tool.pdf\" target=\"_blank\">strategies</a> of disinformation are well known to those familiar with the debates on climate change and evolution by natural selection. One strategy is to assert the absence of scientific consensus by citing the dissenting opinions of scientists in unrelated fields. Another is to point out past blemishes on science&rsquo;s track record, often using examples taken from the popular literature, rather than from peer-reviewed academic journals. Finally, deceivers can draw attention to fringe parts of a theory that are indeed controversial rather than focusing on the core tenets of the theory that are widely accepted by experts.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Along the same lines as the availability heuristic, media coverage can alter estimates of the extent of scientific consensus on empirical facts. News programs can intentionally or unintentionally contribute to the appearance of scientific controversy by treating both sides of an argument equally, creating the illusion of equal credibility. Further, in attempting to make science palatable to mass audiences, the mainstream media may inadvertently oversimplify or misrepresent scientists, thereby spreading misunderstanding. As a result, it might make more sense to write your own articles instead of going through a middleman that is knowledgeable about journalism but not about your topic.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Hotel California only truly shuts its doors once you&rsquo;ve left the front lobby and gone up to your room. New information is very briefly &ldquo;<a href=\"http://coglab.wjh.harvard.edu/~dtg/Gillbert%20(How%20Mental%20Systems%20Believe).PDF\" target=\"_blank\">believed</a>&rdquo; before it is rejected. When reading a novel, we do not <em>suspend</em> disbelief as much as we <em><a href=\"http://poeticstoday.dukejournals.org/content/25/2/265.full.pdf\" target=\"_blank\">willingly construct</a></em> disbelief immediately after believing. But it doesn&rsquo;t <em>feel</em> that way to <a href=\"http://sourcesofinsight.com/the-elephant-and-the-rider/\" target=\"_blank\">the Rider on the Elephant</a>. Sometimes we err in figuring out which ideas have gone up to their rooms and which have exited Hotel California immediately after entering the lobby. There is a whole literature on <em><a href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/entertainment-education_and_elaboration_likelihood_-_understanding_the_process_of_narrative_persuasion.pdf\" target=\"_blank\">narrative persuasion</a></em> &ndash; how fiction can lead to the absorption of false beliefs. After reading fictional narratives including statements like, &ldquo;mental illness is contagious&rdquo; and &ldquo;brushing your teeth doesn&rsquo;t actually make your teeth cleaner,&rdquo; people are <a href=\"http://download.springer.com/static/pdf/63/art%253A10.3758%252FPBR.15.1.180.pdf?auth66=1391897390_bc812ebcfdfb6c27b53bfb79a3f11652&amp;ext=.pdf\" target=\"_blank\">more likely</a> to reproduce those errors on future tests. This effect is <a href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/persuasive_effects_of_fictional_narratives_increase_over_time.pdf\" target=\"_blank\">even stronger</a> after a two-week gestation period, revealing an <em>absolute sleeper effect</em>.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Even when a retraction immediately follows a statement, it usually fails to eliminate the initial effect. If I tell you, \"Woody Allen's real name is Jacob Allen,\" then I have just poisoned your mind in a sense even if I immediately tell you I made the name up. If you were on Who Wants To Be A Millionaire and Jacob was one of the options, it would sound more familiar to you than the alternatives even though I'm making it perfectly clear I have no idea what Woody Allen's real name is. For all I know, it's Woody Allen.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">One reasonable explanation of this phenomenon is that listeners form mental models of the stories they hear (e.g. Event A leads to Event B leads to Event C). When one of the events of the story is retracted (&ldquo;Actually, I lied: Event B never happened!&rdquo;), the listener&rsquo;s mental model is left with a gaping hole, as Event A would not lead to Event C without the prior occurrence of Event B. Filling this coherence gap with an alternate account of events is a confirmed way to break the continued influence of misinformation. Many other helpful techniques are offered <a href=\"http://bocktherobber.com.cdn.ie/wordpress/wp-content/uploads/2013/04/lewandowsky_et_al_misinformation_pspi_ip.pdf\" target=\"_blank\">here</a>.</span></p>\n<h3><br /></h3>\n<h3><span style=\"font-family: Arial;\">More Ideas for Optimizing Media Use</span></h3>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">One good tool for world-changers to have is a list of memes they&rsquo;d like to spread to a larger audience. Since our uncertainty about the future is high, the selected memes should be very safe messages that are difficult to be abused or to lead society in bad directions if accepted en masse. For instance, a meme like \"Technological progress is good\" may be generally true but it could easily lead to untrue beliefs or bad consequences if accepted too dogmatically. In contrast, \"Racism is bad\" seems almost impossible to be misused.<br /> <!--[endif]--></span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Examples of \"safe\" memes I would expect to have net positive consequences:</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Racism is bad<br /></span><span style=\"font-family: Arial;\">Sexism is bad<br /></span><span style=\"font-family: Arial;\">Speciesism is bad<br /></span><span style=\"font-family: Arial;\">Homophobia is bad<br /></span><span style=\"font-family: Arial;\">Xenophobia is bad<br /></span><span style=\"font-family: Arial;\">Belief without evidence is bad<br /></span><span style=\"font-family: Arial;\">Recycling is good<br /></span><span style=\"font-family: Arial;\">Defining one&rsquo;s terms before an argument is good<br /></span><span style=\"font-family: Arial;\">It&rsquo;s important to be willing to change one&rsquo;s mind<br /></span><span style=\"font-family: Arial;\">One should learn the basic skills of rationality<br /></span><span style=\"font-family: Arial;\">A lack of absolute certainty does not equal a lack of objectivity<br /></span><span style=\"font-family: Arial;\">Moral reasoning can be useful<br /></span><span style=\"font-family: Arial;\">Etc.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">The point of a fluctuating list of good memes is that it hierarchizes ideas and causes one to consider how likely specific memes are to be misinterpreted or misused. It also prevents one from getting off track. If you have a list of memes in mind, you can use it to guide your creative decision-making.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">It could also be helpful to focus on specific political issues that are hot at a given time. For example,</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Party X should win the election<br /></span><span style=\"font-family: Arial;\">War X should not happen<br /></span><span style=\"font-family: Arial;\">Apartheid X should be stopped<br /></span><span style=\"font-family: Arial;\">Abortion should(n&rsquo;t) be legal<br /></span><span style=\"font-family: Arial;\">Gay marriage should(n&rsquo;t) be legal<br /></span><span style=\"font-family: Arial;\">Capital punishment should(n&rsquo;t) be practiced<br /></span><span style=\"font-family: Arial;\">Gun control laws should be stricter/left alone<br /></span><span style=\"font-family: Arial;\">Climate change should(n&rsquo;t) be taken seriously<br /></span><span style=\"font-family: Arial;\">The rich should(n&rsquo;t) be taxed more<br /></span><span style=\"font-family: Arial;\">Etc.</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Some of these issues might be far less important than the media and politicians make them seem, but knocking them down, one by one, could probably pave the way for more meaningful change. Perhaps more importantly, they win a battle of principles and prevent the tides from gaining momentum in the opposite direction.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">More specific to the issue of an intelligence explosion, the <a href=\"http://en.wikipedia.org/wiki/Uncanny_valley\" target=\"_blank\">uncanny valley</a> hypothesis suggests that people experience revulsion at the sight of a humanlike-but-not-human thing. This suggests that if one wishes to spread general resistance toward the development of AGI, it would be wise to make a point of associating AGI with these ugly humanoid depictions. On the other hand, if one wanted to spread general acceptance of AGI, it would be good to avoid such depictions.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Another approach is <em><a href=\"https://www.adbusters.org/\" target=\"_blank\">culture jamming</a></em>. Culture jamming usually means &ldquo;subvertising&rdquo; ads by creating TV commercials and billboards that turn corporate ads on their head. Click <a href=\"https://www.google.ca/search?q=culture+jamming&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ei=7gj0Upz1E4SL2wXnuICoAg&amp;ved=0CAkQ_AUoAQ&amp;biw=1202&amp;bih=673\" target=\"_blank\">here</a> for some basic examples. These campaigns build cynicism against corporations and politicians, fuel dissent, and prime people for more world-changing behaviour.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">It&rsquo;s also important to consider the audience of a given message. The average person may not need a reminder to develop their social skills or learn how to communicate, but maybe the average LessWronger probably does. Similarly, there&rsquo;s no need to convince rationalists that atheism is acceptable because they already believe so &ndash; but it remains, I think, a good meme to spread to the broader public. The outward image of activists to the public should consist mainly of moderate, socially acceptable ideas. These topics are not necessarily more important than the more esoteric topics, but they are more likely to be memetically effective because they are consistent with a wide number of outlooks.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Lastly, an important tool for social change is the &ldquo;<a href=\"https://en.wikipedia.org/wiki/Nudge_(book)\" target=\"_blank\">nudge</a>&rdquo; because it guides people toward better decision-making without removing their freedom to choose. The clearest cases where nudges are effective in shaping culture involve appeals to <em>social proof</em>.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Some examples from Thaler and Sunstein&rsquo;s book: obesity is socially contagious, federal judges are influenced by the votes of their colleagues, 12% of participants choose &ldquo;subversive activities&rdquo; as the biggest current issue when asked in private compared to 48% when asked publically, self-reported musical taste is hugely influenced by the self-reported tastes of others, the amount of food people eat correlates with the number of people they eat with, tax and recycling compliance can be increased by informing people that the compliance level is high, binge drinking and smoking rates can be reduced by informing the public of unexpectedly low drinking and smoking rates, and people can be nudged to reduce their energy use by informing them that their energy use is above average.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Do you have any others to add to this list? Was there anything useful in this post you didn't already know?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "97BGK4ReXf9YQnupr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 6, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "25445", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin:0cm; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Benevolent and malevolent media producers possess the power to influence society in positive and negative ways. They can do this through agenda setting, framing, priming, spreading memes, altering perceptions of groups and individuals, outright propaganda, and other methods.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">I think more attention should be paid to the pathways from content to effects, so that we can optimize our cultural landscape.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Lest this post soon turn into Applause Light Vegas, I\u2019ll now get into some methods I think can be used to sway mass opinion in a direction amenable to making the world better. Many of these methods deal with familiar biases, heuristics, and psychological effects.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\"><a id=\"more\"></a><br></span></p>\n<h3 id=\"Media_Use_Facilitating_Positive_Social_Change\"><span style=\"font-family: Arial;\">Media Use Facilitating Positive Social Change</span></h3>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial;\">First, the mass media possesses the power to alter estimates of the likelihood and frequency of specific occurrences. Think back to some of the classic examples of the availability heuristic. </span><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">When <a href=\"http://heatherlench.com/wp-content/uploads/2008/07/johnson-tversky.pdf\" target=\"_blank\">asked</a> to estimate the number of homicides in the USA compared to suicides, people answer that there are far more homicides in the United States, even though<span>&nbsp;</span><a href=\"http://www.worldlifeexpectancy.com/usa-homicide-vs-suicide\" target=\"_blank\">the reverse is true</a>. The mass media report on homicides far more often than they report on suicides, so people have more available instances of homicide in their memories and these come to mind more easily. This influences their beliefs about the real world, which can then be politicized to lead to different stances on gun control and education. The priorities of a culture with a homicide problem are not the priorities of a culture with a suicide problem.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">This effect is consistent with some theoretical models of the mass media\u2019s impact on society. <em>Cultivation theorists</em> <a href=\"http://www.zfg.pwsz.krosno.pl/gfx/pwszkrosno/pl/defaultaktualnosci/675/5/1/s08a_lk_cultivation_overview_gerbner.pdf\" target=\"_blank\">understand</a> the media, especially television, as a system of coherent memes and messages reflecting a society\u2019s dominant ideology. If we accept the fundamental claim of cultivation theory then we should hypothesize exposure to television to be positively correlated with status quo beliefs and attitudes. We might then expect high exposure to non-</span><span style=\"font-family: Arial;\">fiction television to lead to mean- and scary-world beliefs, given the disproportionate amount of media coverage homicides receive. One cultivation theorist <a href=\"http://www.gwern.net/docs/2008-appel.pdf\" target=\"_blank\">found</a> this result, yet did not find the same effect on heavy fiction viewers.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial;\">Malevolent, benevolent, and clueless media producers could capitalize on the availability heuristic to adjust mass estimates of society\u2019s biggest problems, and by extension, mass assessments of social priorities. This ability of the mass media to affect the perceived importance of subjects by representing or not representing them is sometimes called <em><a href=\"http://poq.oxfordjournals.org/content/36/2/176.full.pdf\" target=\"_blank\">agenda setting</a></em>. Want different kinds of people to take existential risks seriously? Maybe get existential risks mentioned in media outlets that different types of people read. Use <a href=\"https://medium.com/\" target=\"_blank\">Medium.com</a>, pitch articles to <a href=\"http://www.theguardian.com/commentisfree/2010/mar/04/you-tell-us\" target=\"_blank\">The Guardian</a>, write an editorial to your local newspaper, increase the representation of <a href=\"http://www.utilitarian-essays.com/wikipedia.html\" target=\"_blank\">important issues on Wikipedia</a>, and so on. You don\u2019t even need to convince people that Friendly AI should be a global priority as much as you need to convince them that thinking so doesn\u2019t make you crazy. Exposing people to AI concerns without coming off as a clear member of a disliked outgroup (e.g. conspiracy theorist) can play a big role in legitimizing the issue in the public\u2019s eyes.</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Politicians and media outlets can also make use of <em><a href=\"http://www.asc.upenn.edu/usr/ogandy/C45405%20resources/Scheufele%20framing%20as%20effects.pdf\" target=\"_blank\">framing</a></em> devices to influence audience perspectives on news stories by tweaking irrelevant factors. A newspaper headline claiming, \u201cPublic condemnation of democracy should not be allowed\u201d will receive more support than will one that claims, \u201cIt is right to forbid public condemnation of democracy.\u201d If you\u2019ve ever heard a politician speak, you\u2019ve probably noticed how they <a href=\"http://www.youtube.com/watch?v=SKftRlzh2RM\" target=\"_blank\">frame everything they say</a> in a way that makes it sound better than it is. Similarly, a headline will have very different connotations if it describes an event as a \u201cstrike\u201d or as an \u201cinvasion\u201d or as a \u201cbombing.\u201d (And was it committed against \u201csoldiers\u201d or \u201cforces\u201d or \u201crebels\u201d or \u201cterrorists\u201d?)</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Framing isn\u2019t purely a word-selection thing. It can be done with audio-visual media as well. Film fans among you may have heard of the </span><em style=\"font-family: Arial;\"><a href=\"https://en.wikipedia.org/wiki/Kuleshov_Effect\" target=\"_blank\">Kuleshov effect</a></em><span style=\"font-family: Arial;\">, discovered with a famous experiment that used and re-used a single close-up of a man\u2019s face against a series of different images, such as a bowl of soup, a little girl smiling, a funeral. You can watch a short example <a href=\"http://www.youtube.com/watch?v=_gGl3LJ7vHc\" target=\"_blank\">here</a>. Each time we cut back to the man, his face appears to express a different emotion even though it\u2019s actually an identical shot of his face. Soviet Montage filmmakers capitalized on this effect in their movies to express meanings through the </span><em style=\"font-family: Arial;\">juxtaposition</em><span style=\"font-family: Arial;\"> of different shots.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h3 id=\"Biases_Facilitating_Social_Stagnation\"><span style=\"font-family: Arial;\">Biases Facilitating Social Stagnation</span></h3>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Media producers have much more to think about than the biases and heuristics that facilitate persuasion. They also have to examine the psychological and cultural factors that entrench ideas in our heads. The mind is the <a href=\"http://www.youtube.com/watch?v=iqODbP1T3nk\" target=\"_blank\">Hotel California</a> of ideas \u2013 once one gets in there, it might never see the light of day again. What are some of those forces that keep us from changing our minds?</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">The first important factor to consider is <em><a href=\"https://en.wikipedia.org/wiki/Selective_exposure_theory\" target=\"_blank\">selective exposure</a></em>. Before an idea can sound persuading to you, it has to get in front of you. This is harder than it seems because people don\u2019t want to be confronted with ideas they don\u2019t agree with. <a href=\"https://en.wikipedia.org/wiki/Selective_exposure_theory\" target=\"_blank\">Confirmation bias</a> predisposes them to crave ideas they already agree with. If I\u2019m an atheist surfing YouTube, am I going to click on \u201cCreationist moron DESTROYED with a Hitchslap\u201d or on \u201cHow to prove atheism wrong in 8 seconds\u201d? People avoid the stuff that doesn\u2019t seem like it\u2019ll cater to their beliefs.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">If you want to get existential risk and AGI messages in front of new audiences, you need to find ways to make your stances on those issues seem somewhat consistent with a lot of other peoples\u2019 current views. Getting important undercovered ideas into the public eye will probably mean <em>smuggling</em> them there. A TV station only covering existential risks can easily be ignored by all the people with no interest in existential risks. (LessWrong is sort of an online equivalent to this.) Instead, you may have to smuggle your important ideas into a mixture of more mainstream content.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">The <em><a href=\"https://en.wikipedia.org/wiki/Confirmation_bias#Preference_for_early_information\" target=\"_blank\">primacy effect</a></em> suggests that the earliest information people receive about an issue is likely to form their thinking on that issue, biasing them in favour of that viewpoint. This suggests to me that it might be a good idea to find subjects on which people haven\u2019t completely formed their ideas yet. If you can give people good ideas before they get a chance to form bad ideas, they\u2019ll be more partial to your ideas than if you try to convince them that their fully-formed \"bad\" ideas are inferior to yours. My impression is, relative to secularism and the dangers of technological progress, that fundamental anti-speciesist and effective altruist ideas are subjects on which people are still forming their ideas.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Mass media agenda setting also works in combination with other biases. The&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Third-person_effect\" target=\"_blank\">third person effect</a></em>&nbsp;is the occurrence of people overestimating the magnitude of the media\u2019s influence on other people. Do you ever assume that a political attack ad, or a marketing pitch, or porn, or a violent video game probably affects a lot of people \u2013 while being very confident that you aren\u2019t one of those people being affected?&nbsp;<em>You</em>, of course, are much too clever but those&nbsp;<em>other</em>&nbsp;people are surely easy pickings for propagandists. This view is probably closely related to&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Overconfidence_effect\" target=\"_blank\">overconfidence</a></em>&nbsp;and the&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Bias_blind_spot\" target=\"_blank\">bias blind spot</a></em>.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\"><a href=\"http://poq.oxfordjournals.org/content/47/1/1.full.pdf\" target=\"_blank\">Davison</a> points out that the third person effect can play into&nbsp;<em><a href=\"https://en.wikipedia.org/wiki/Pluralistic_ignorance\" target=\"_blank\">pluralistic ignorance</a></em>. Misperceptions of public opinion can lead to the majority reinforcing behavioural norms that only the minority of a population agrees with. If we assume that the majority of persuasion tactics we see in the media are successful on other people, then we\u2019re going to wind up with skewed ideas of what everyone else believes. In our current era of <a href=\"http://harrisonsteele.com/?p=208\" target=\"_blank\">demassification</a> and social media, we live in so-called \u201ccyber ghettos\u201d where most of our information comes from people on our social media feeds and others that already agree with us. This probably leads to an overestimation of the popularity and mainstream-ness of our ideas.</span></p>\n<p>&nbsp;</p>\n<h3 id=\"Media_Use_Facilitating_Negative_Social_Change\"><span style=\"font-family: Arial;\">Media Use Facilitating Negative Social Change</span></h3>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">So, all sorts of biases and heuristics prevent ideas from leaving Hotel California - and on the scale of a culture, this creates memetic stagnation. Now, let\u2019s look at how the mass media can be deliberately used to create <em>negative</em> change. Understanding how this works can help us squash the deliberate spread of misinformation or improve our more benevolent methods of media persuasion.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">There is a field called \u201c<a href=\"http://scholar.princeton.edu/rccu/files/Agnotology%20Intro%20Chapter,%20Robert%20Proctor.pdf\" target=\"_blank\">agnotology</a>\u201d that is about exactly this: ignorance, how and why it\u2019s produced and maintained. When examining a field, a good indicator of whether there\u2019s disinformation at work is whether there exists a divide between expert opinion and public opinion. My impression is that this is the case for many of the issues that interest LessWrongers.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">The <a href=\"http://www.weber.edu/wsuimages/geography/Bedford%202010-%20Agnotology%20as%20a%20teaching%20tool.pdf\" target=\"_blank\">strategies</a> of disinformation are well known to those familiar with the debates on climate change and evolution by natural selection. One strategy is to assert the absence of scientific consensus by citing the dissenting opinions of scientists in unrelated fields. Another is to point out past blemishes on science\u2019s track record, often using examples taken from the popular literature, rather than from peer-reviewed academic journals. Finally, deceivers can draw attention to fringe parts of a theory that are indeed controversial rather than focusing on the core tenets of the theory that are widely accepted by experts.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Along the same lines as the availability heuristic, media coverage can alter estimates of the extent of scientific consensus on empirical facts. News programs can intentionally or unintentionally contribute to the appearance of scientific controversy by treating both sides of an argument equally, creating the illusion of equal credibility. Further, in attempting to make science palatable to mass audiences, the mainstream media may inadvertently oversimplify or misrepresent scientists, thereby spreading misunderstanding. As a result, it might make more sense to write your own articles instead of going through a middleman that is knowledgeable about journalism but not about your topic.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Hotel California only truly shuts its doors once you\u2019ve left the front lobby and gone up to your room. New information is very briefly \u201c<a href=\"http://coglab.wjh.harvard.edu/~dtg/Gillbert%20(How%20Mental%20Systems%20Believe).PDF\" target=\"_blank\">believed</a>\u201d before it is rejected. When reading a novel, we do not <em>suspend</em> disbelief as much as we <em><a href=\"http://poeticstoday.dukejournals.org/content/25/2/265.full.pdf\" target=\"_blank\">willingly construct</a></em> disbelief immediately after believing. But it doesn\u2019t <em>feel</em> that way to <a href=\"http://sourcesofinsight.com/the-elephant-and-the-rider/\" target=\"_blank\">the Rider on the Elephant</a>. Sometimes we err in figuring out which ideas have gone up to their rooms and which have exited Hotel California immediately after entering the lobby. There is a whole literature on <em><a href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/entertainment-education_and_elaboration_likelihood_-_understanding_the_process_of_narrative_persuasion.pdf\" target=\"_blank\">narrative persuasion</a></em> \u2013 how fiction can lead to the absorption of false beliefs. After reading fictional narratives including statements like, \u201cmental illness is contagious\u201d and \u201cbrushing your teeth doesn\u2019t actually make your teeth cleaner,\u201d people are <a href=\"http://download.springer.com/static/pdf/63/art%253A10.3758%252FPBR.15.1.180.pdf?auth66=1391897390_bc812ebcfdfb6c27b53bfb79a3f11652&amp;ext=.pdf\" target=\"_blank\">more likely</a> to reproduce those errors on future tests. This effect is <a href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/persuasive_effects_of_fictional_narratives_increase_over_time.pdf\" target=\"_blank\">even stronger</a> after a two-week gestation period, revealing an <em>absolute sleeper effect</em>.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Even when a retraction immediately follows a statement, it usually fails to eliminate the initial effect. If I tell you, \"Woody Allen's real name is Jacob Allen,\" then I have just poisoned your mind in a sense even if I immediately tell you I made the name up. If you were on Who Wants To Be A Millionaire and Jacob was one of the options, it would sound more familiar to you than the alternatives even though I'm making it perfectly clear I have no idea what Woody Allen's real name is. For all I know, it's Woody Allen.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">One reasonable explanation of this phenomenon is that listeners form mental models of the stories they hear (e.g. Event A leads to Event B leads to Event C). When one of the events of the story is retracted (\u201cActually, I lied: Event B never happened!\u201d), the listener\u2019s mental model is left with a gaping hole, as Event A would not lead to Event C without the prior occurrence of Event B. Filling this coherence gap with an alternate account of events is a confirmed way to break the continued influence of misinformation. Many other helpful techniques are offered <a href=\"http://bocktherobber.com.cdn.ie/wordpress/wp-content/uploads/2013/04/lewandowsky_et_al_misinformation_pspi_ip.pdf\" target=\"_blank\">here</a>.</span></p>\n<h3><br></h3>\n<h3 id=\"More_Ideas_for_Optimizing_Media_Use\"><span style=\"font-family: Arial;\">More Ideas for Optimizing Media Use</span></h3>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">One good tool for world-changers to have is a list of memes they\u2019d like to spread to a larger audience. Since our uncertainty about the future is high, the selected memes should be very safe messages that are difficult to be abused or to lead society in bad directions if accepted en masse. For instance, a meme like \"Technological progress is good\" may be generally true but it could easily lead to untrue beliefs or bad consequences if accepted too dogmatically. In contrast, \"Racism is bad\" seems almost impossible to be misused.<br> <!--[endif]--></span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Examples of \"safe\" memes I would expect to have net positive consequences:</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Racism is bad<br></span><span style=\"font-family: Arial;\">Sexism is bad<br></span><span style=\"font-family: Arial;\">Speciesism is bad<br></span><span style=\"font-family: Arial;\">Homophobia is bad<br></span><span style=\"font-family: Arial;\">Xenophobia is bad<br></span><span style=\"font-family: Arial;\">Belief without evidence is bad<br></span><span style=\"font-family: Arial;\">Recycling is good<br></span><span style=\"font-family: Arial;\">Defining one\u2019s terms before an argument is good<br></span><span style=\"font-family: Arial;\">It\u2019s important to be willing to change one\u2019s mind<br></span><span style=\"font-family: Arial;\">One should learn the basic skills of rationality<br></span><span style=\"font-family: Arial;\">A lack of absolute certainty does not equal a lack of objectivity<br></span><span style=\"font-family: Arial;\">Moral reasoning can be useful<br></span><span style=\"font-family: Arial;\">Etc.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">The point of a fluctuating list of good memes is that it hierarchizes ideas and causes one to consider how likely specific memes are to be misinterpreted or misused. It also prevents one from getting off track. If you have a list of memes in mind, you can use it to guide your creative decision-making.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">It could also be helpful to focus on specific political issues that are hot at a given time. For example,</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Party X should win the election<br></span><span style=\"font-family: Arial;\">War X should not happen<br></span><span style=\"font-family: Arial;\">Apartheid X should be stopped<br></span><span style=\"font-family: Arial;\">Abortion should(n\u2019t) be legal<br></span><span style=\"font-family: Arial;\">Gay marriage should(n\u2019t) be legal<br></span><span style=\"font-family: Arial;\">Capital punishment should(n\u2019t) be practiced<br></span><span style=\"font-family: Arial;\">Gun control laws should be stricter/left alone<br></span><span style=\"font-family: Arial;\">Climate change should(n\u2019t) be taken seriously<br></span><span style=\"font-family: Arial;\">The rich should(n\u2019t) be taxed more<br></span><span style=\"font-family: Arial;\">Etc.</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Some of these issues might be far less important than the media and politicians make them seem, but knocking them down, one by one, could probably pave the way for more meaningful change. Perhaps more importantly, they win a battle of principles and prevent the tides from gaining momentum in the opposite direction.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">More specific to the issue of an intelligence explosion, the <a href=\"http://en.wikipedia.org/wiki/Uncanny_valley\" target=\"_blank\">uncanny valley</a> hypothesis suggests that people experience revulsion at the sight of a humanlike-but-not-human thing. This suggests that if one wishes to spread general resistance toward the development of AGI, it would be wise to make a point of associating AGI with these ugly humanoid depictions. On the other hand, if one wanted to spread general acceptance of AGI, it would be good to avoid such depictions.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Another approach is <em><a href=\"https://www.adbusters.org/\" target=\"_blank\">culture jamming</a></em>. Culture jamming usually means \u201csubvertising\u201d ads by creating TV commercials and billboards that turn corporate ads on their head. Click <a href=\"https://www.google.ca/search?q=culture+jamming&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ei=7gj0Upz1E4SL2wXnuICoAg&amp;ved=0CAkQ_AUoAQ&amp;biw=1202&amp;bih=673\" target=\"_blank\">here</a> for some basic examples. These campaigns build cynicism against corporations and politicians, fuel dissent, and prime people for more world-changing behaviour.</span></p>\n<p class=\"MsoNormal\" style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-family: Arial; mso-fareast-font-family: &quot;Times New Roman&quot;;\">It\u2019s also important to consider the audience of a given message. The average person may not need a reminder to develop their social skills or learn how to communicate, but maybe the average LessWronger probably does. Similarly, there\u2019s no need to convince rationalists that atheism is acceptable because they already believe so \u2013 but it remains, I think, a good meme to spread to the broader public. The outward image of activists to the public should consist mainly of moderate, socially acceptable ideas. These topics are not necessarily more important than the more esoteric topics, but they are more likely to be memetically effective because they are consistent with a wide number of outlooks.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Lastly, an important tool for social change is the \u201c<a href=\"https://en.wikipedia.org/wiki/Nudge_(book)\" target=\"_blank\">nudge</a>\u201d because it guides people toward better decision-making without removing their freedom to choose. The clearest cases where nudges are effective in shaping culture involve appeals to <em>social proof</em>.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Some examples from Thaler and Sunstein\u2019s book: obesity is socially contagious, federal judges are influenced by the votes of their colleagues, 12% of participants choose \u201csubversive activities\u201d as the biggest current issue when asked in private compared to 48% when asked publically, self-reported musical taste is hugely influenced by the self-reported tastes of others, the amount of food people eat correlates with the number of people they eat with, tax and recycling compliance can be increased by informing people that the compliance level is high, binge drinking and smoking rates can be reduced by informing the public of unexpectedly low drinking and smoking rates, and people can be nudged to reduce their energy use by informing them that their energy use is above average.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Do you have any others to add to this list? Was there anything useful in this post you didn't already know?</span></p>", "sections": [{"title": "Media Use Facilitating Positive Social Change", "anchor": "Media_Use_Facilitating_Positive_Social_Change", "level": 1}, {"title": "Biases Facilitating Social Stagnation", "anchor": "Biases_Facilitating_Social_Stagnation", "level": 1}, {"title": "Media Use Facilitating Negative Social Change", "anchor": "Media_Use_Facilitating_Negative_Social_Change", "level": 1}, {"title": "More Ideas for Optimizing Media Use", "anchor": "More_Ideas_for_Optimizing_Media_Use", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T00:00:05.931Z", "modifiedAt": null, "url": null, "title": "Private currency to generate funds for effective altruism", "slug": "private-currency-to-generate-funds-for-effective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.008Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7hbwNkAr8ptXuGKrJ/private-currency-to-generate-funds-for-effective-altruism", "pageUrlRelative": "/posts/7hbwNkAr8ptXuGKrJ/private-currency-to-generate-funds-for-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/7hbwNkAr8ptXuGKrJ/private-currency-to-generate-funds-for-effective-altruism", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Private%20currency%20to%20generate%20funds%20for%20effective%20altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrivate%20currency%20to%20generate%20funds%20for%20effective%20altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7hbwNkAr8ptXuGKrJ%2Fprivate-currency-to-generate-funds-for-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Private%20currency%20to%20generate%20funds%20for%20effective%20altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7hbwNkAr8ptXuGKrJ%2Fprivate-currency-to-generate-funds-for-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7hbwNkAr8ptXuGKrJ%2Fprivate-currency-to-generate-funds-for-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 600, "htmlBody": "<p>In the last few years we have seen two interesting revolutionary ideas on how to change the monetary system. The first is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Bitcoin\">Bitcoin</a>: the most well-known peer-to-peer currency. It has been wildly debated recently and I won't go into the detail of allegations of use in criminal activities etc (for one thing, I don't know much about it). My interest is rather in the <em>money creation </em>part. The people who run the Bitcoin software are rewarded for their work with new Bitcoins - a process called <a href=\"http://en.wikipedia.org/wiki/Bitcoin#Mining\">mining</a>. Now the pace at which new Bitcoins are mined is limited, which means that Bitcoin creation is a zero-sum game: the more one miner contributes to the Bitcoin software, the less Bitcoins other miners get. Unsurprisingly, this has led to an arms race: miners spend nearly as much on running the software as they get back in form of new Bitcoins.</p>\n<p>The second idea is the <a href=\"http://en.wikipedia.org/wiki/The_Chicago_Plan_Revisited\">Chicago Plan</a>, which was debated already in the 30's, after the great crash of 1929, but which <a href=\"http://www.youtube.com/watch?v=tnehf-U527g\">recently was resurrected by Michael Kumhof</a> (senior economist at IMF, of all places). The central idea of the Chicago Plan is to abolish fractional reserve banking - <a href=\"http://en.wikipedia.org/wiki/Fractional_reserve_banking#Money_creation_process\">the system by which private banks in effect create money out of thin air</a>. Instead of lending out most of the depositors' money, banks would effectively have to let them stay in the bank.&nbsp;</p>\n<p>Instead money would be created by the central bank/government, a process that would generate a massive <a href=\"http://en.wikipedia.org/wiki/Seigniorage\">seignorage</a> for the government. According to Kumhof, it would also have other beneficial effects, such as killing off the \"boom-and-bust\"-cycles which he thinks fractional reserve banking are mostly responsible for, and diminishing the wasteful parts of the financial sector.</p>\n<p>Kumhof ideas' have not been well received. Overall, it is remarkable how little reform there has been of the financial and monetary system given that the world had a major financial meltdown 2008 (and was close to an even greater one, in my understanding). Governments won't challenge the financial system radically in the near future, that's for sure.</p>\n<p>Instead radical reforms can only come from private hands. Let us now compare the two ideas. In the Bitcoin system money is created by private hands, but in wasteful ways, which effectively means that there is very little seignorage. Under the Chicago plan, money is created by the government in much more efficient ways, which leads to a large seignorage. Now my idea is to take the best part of both of these ideas: let a private player - more exactly, an altruistic organization such as <a href=\"http://home.centreforeffectivealtruism.org/\">CEA</a> - produce the money centrally, Chicago plan-like, and let the seignorage be used for altruistic purposes. (Of course, there would be some costs of running the system, but if the system was sufficiently large, these would be negligible in relation to the seignorage.)</p>\n<p>If the altruistic organization that did this had a sufficiently good reputation, chances are greater that people would trust the system. Of course, it would try to stop the currency from being used to launder money, drug trade etc.&nbsp;</p>\n<p>Generally, people would be suspicious of private currencies where the central authority collected a seignorage, but if this seignorage was used for charitable and other altruistic purposes (and people really trusted that that would be the case), this would, I hope, be less of a problem.</p>\n<p>What do you think? I'd be happy to get comments from people who know more about the Bitcoin system, since I don't really know it (though I find it interesting). Perhaps there is some info concerning Bitcoins that tells against this proposal; if so, I'd be interested in that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7hbwNkAr8ptXuGKrJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 4, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "25496", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T05:32:26.543Z", "modifiedAt": null, "url": null, "title": "[Open Thread] Links (2014-02-14)", "slug": "open-thread-links-2014-02-14", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.883Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "solipsist", "createdAt": "2013-06-09T21:07:31.678Z", "isAdmin": false, "displayName": "solipsist"}, "userId": "PMDZAtysvt35Mni2a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MWqhF9TH7gHQsGyhe/open-thread-links-2014-02-14", "pageUrlRelative": "/posts/MWqhF9TH7gHQsGyhe/open-thread-links-2014-02-14", "linkUrl": "https://www.lesswrong.com/posts/MWqhF9TH7gHQsGyhe/open-thread-links-2014-02-14", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BOpen%20Thread%5D%20Links%20(2014-02-14)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BOpen%20Thread%5D%20Links%20(2014-02-14)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWqhF9TH7gHQsGyhe%2Fopen-thread-links-2014-02-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BOpen%20Thread%5D%20Links%20(2014-02-14)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWqhF9TH7gHQsGyhe%2Fopen-thread-links-2014-02-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWqhF9TH7gHQsGyhe%2Fopen-thread-links-2014-02-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p><em>This is part of a <a title=\"Even More Open Threads? Discussion\" href=\"/r/discussion/lw/jo3/meta_open_thread_even_more_often/\">two-week experiment on having more open threads</a>.</em></p>\n<p>A good read, good site, something that made you think. &nbsp;If you really want to share it but don't think it's worthy of a post, here's the place. &nbsp;<strong>Please include a summary.</strong></p>\n<p>Other similar threads include:</p>\n<ul>\n<li><a href=\"/r/discussion/lw/jnx/open_thread_for_february_11_17/\">Open Thread</a></li>\n<li><a href=\"/lw/jlu/february_2014_media_thread/\">Media recommendations</a></li>\n<li><a href=\"/r/discussion/lw/jp0/open_thread_stupid_questions_20140217/\">Stupid questions</a></li>\n<li>Advice (Not yet posted)</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Special_threads\">Other Special Threads</a></li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MWqhF9TH7gHQsGyhe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 4, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "25498", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qk6ArDxwC5S9HgTdZ", "jD5yw654riqqEiyr3", "fJTHc6samuJwHSzXe", "ETHJmLGfnFR3wrDLw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T06:16:32.499Z", "modifiedAt": null, "url": null, "title": "How to illustrate that society is mostly irrational, and how rationality would be beneficial", "slug": "how-to-illustrate-that-society-is-mostly-irrational-and-how", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.707Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RTDQL6g2Eu4Mam9zn/how-to-illustrate-that-society-is-mostly-irrational-and-how", "pageUrlRelative": "/posts/RTDQL6g2Eu4Mam9zn/how-to-illustrate-that-society-is-mostly-irrational-and-how", "linkUrl": "https://www.lesswrong.com/posts/RTDQL6g2Eu4Mam9zn/how-to-illustrate-that-society-is-mostly-irrational-and-how", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20illustrate%20that%20society%20is%20mostly%20irrational%2C%20and%20how%20rationality%20would%20be%20beneficial&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20illustrate%20that%20society%20is%20mostly%20irrational%2C%20and%20how%20rationality%20would%20be%20beneficial%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTDQL6g2Eu4Mam9zn%2Fhow-to-illustrate-that-society-is-mostly-irrational-and-how%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20illustrate%20that%20society%20is%20mostly%20irrational%2C%20and%20how%20rationality%20would%20be%20beneficial%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTDQL6g2Eu4Mam9zn%2Fhow-to-illustrate-that-society-is-mostly-irrational-and-how", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTDQL6g2Eu4Mam9zn%2Fhow-to-illustrate-that-society-is-mostly-irrational-and-how", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<p>Does anyone know of a good article that illustrates how society is generally irrational, and how making society more rational would have huge benefits, because it'd be a very <a href=\"/lw/58g/levels_of_action/\">high level action</a>?</p>\n<p>I'm writing an essay about how to improve education, and one of my proposals is that a core part of the curriculum should be rationality. I believe that doing this would have huge benefits to society, and want to explain why I think this, but I'm having trouble. Any thoughts?</p>\n<p>Edit: Part of <a href=\"/lw/1e/raising_the_sanity_waterline/\">Raising the Sanity Waterline</a>&nbsp;talks about common ways in which people are irrational. However, they're all links to longer Less Wrong articles. Preferably, I'd like to illustrate it in a few sentences/paragraphs.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RTDQL6g2Eu4Mam9zn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -4, "extendedScore": null, "score": 1.5630253828559828e-06, "legacy": true, "legacyId": "25499", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["guDcrPqLsnhEjrPZj", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T17:03:41.279Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-47", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bux5GvEQXoQL3dfnY/weekly-lw-meetups-47", "pageUrlRelative": "/posts/Bux5GvEQXoQL3dfnY/weekly-lw-meetups-47", "linkUrl": "https://www.lesswrong.com/posts/Bux5GvEQXoQL3dfnY/weekly-lw-meetups-47", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBux5GvEQXoQL3dfnY%2Fweekly-lw-meetups-47%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBux5GvEQXoQL3dfnY%2Fweekly-lw-meetups-47", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBux5GvEQXoQL3dfnY%2Fweekly-lw-meetups-47", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 549, "htmlBody": "<p><strong>This summary was posted to LW Main on February 7th. The following week's summary is <a href=\"/lw/joe/new_lw_meetup_yale/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/wc\">Inaugural Canberra meetup:&nbsp;<span class=\"date\">12 February 2014 07:30PM</span></a></li>\n<li><a href=\"/meetups/vz\">Meetup: First Meetup in Hamburg, Germany:&nbsp;<span class=\"date\">07 February 2014 07:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/w2\">Frankfurt meetup:&nbsp;<span class=\"date\">09 February 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/wh\">Moscow, Scholarship and Happiness:&nbsp;<span class=\"date\">09 February 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/wg\">Princeton NJ Meetup:&nbsp;<span class=\"date\">22 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/w4\">Sydney Meetup: February:&nbsp;<span class=\"date\">26 February 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/wj\">Urbana-Champaign: Logical uncertainty:&nbsp;<span class=\"date\">09 February 2014 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">08 February 2020 01:30PM</span></a></li>\n<li><a href=\"/meetups/w6\">[Berlin] Community Weekend in Berlin:&nbsp;<span class=\"date\">11 April 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/vo\">Brussels: Morality - also cake:&nbsp;<span class=\"date\">08 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/w9\">Vienna:&nbsp;<span class=\"date\">15 February 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/wi\">Washington DC Book Swap meetup:&nbsp;<span class=\"date\">09 February 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/wl\">[ALERT] West LA [ALERT] Location Change!!:&nbsp;<span class=\"date\">12 February 2014 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bux5GvEQXoQL3dfnY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5637748710876459e-06, "legacy": true, "legacyId": "25450", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["S2xbQSYvTjyQZQJyT", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T19:31:52.824Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston: In Defence of the Cathedral", "slug": "meetup-boston-in-defence-of-the-cathedral", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tKDLrBrw5gegivhgT/meetup-boston-in-defence-of-the-cathedral", "pageUrlRelative": "/posts/tKDLrBrw5gegivhgT/meetup-boston-in-defence-of-the-cathedral", "linkUrl": "https://www.lesswrong.com/posts/tKDLrBrw5gegivhgT/meetup-boston-in-defence-of-the-cathedral", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%3A%20In%20Defence%20of%20the%20Cathedral&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%3A%20In%20Defence%20of%20the%20Cathedral%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKDLrBrw5gegivhgT%2Fmeetup-boston-in-defence-of-the-cathedral%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%3A%20In%20Defence%20of%20the%20Cathedral%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKDLrBrw5gegivhgT%2Fmeetup-boston-in-defence-of-the-cathedral", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKDLrBrw5gegivhgT%2Fmeetup-boston-in-defence-of-the-cathedral", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wt'>Boston: In Defence of the Cathedral</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anders Huitfeldt (Anders_H) will give a presentation called \"In Defence of the Cathedral\", inspired by the discussion between blogger Scott Alexander and neoreactionaries. If you have time, you may want to first read Scott Alexander's essay \"Reactionary Philosophy in an enormous, planet-sized nutshell\", but this is not required.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 2pm, and have an alternating location:</p>\n\n<ul>\n<li><p>2nd and 4th Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n<li><p>Meetups on other weeks are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n</ul>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 2:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wt'>Boston: In Defence of the Cathedral</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tKDLrBrw5gegivhgT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "25503", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston__In_Defence_of_the_Cathedral\">Discussion article for the meetup : <a href=\"/meetups/wt\">Boston: In Defence of the Cathedral</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anders Huitfeldt (Anders_H) will give a presentation called \"In Defence of the Cathedral\", inspired by the discussion between blogger Scott Alexander and neoreactionaries. If you have time, you may want to first read Scott Alexander's essay \"Reactionary Philosophy in an enormous, planet-sized nutshell\", but this is not required.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 2pm, and have an alternating location:</p>\n\n<ul>\n<li><p>2nd and 4th Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n<li><p>Meetups on other weeks are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n</ul>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 2:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston__In_Defence_of_the_Cathedral1\">Discussion article for the meetup : <a href=\"/meetups/wt\">Boston: In Defence of the Cathedral</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston: In Defence of the Cathedral", "anchor": "Discussion_article_for_the_meetup___Boston__In_Defence_of_the_Cathedral", "level": 1}, {"title": "Discussion article for the meetup : Boston: In Defence of the Cathedral", "anchor": "Discussion_article_for_the_meetup___Boston__In_Defence_of_the_Cathedral1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T19:39:31.632Z", "modifiedAt": null, "url": null, "title": "Cambridge (England) lecture: Existential Risk: Surviving the 21st Century, 26th February ", "slug": "cambridge-england-lecture-existential-risk-surviving-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:36.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sean_o_h", "createdAt": "2012-11-27T20:56:04.066Z", "isAdmin": false, "displayName": "Sean_o_h"}, "userId": "7ntNTAoctZqY9Nuwa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yeqe7DHT5dHDXiD2k/cambridge-england-lecture-existential-risk-surviving-the", "pageUrlRelative": "/posts/yeqe7DHT5dHDXiD2k/cambridge-england-lecture-existential-risk-surviving-the", "linkUrl": "https://www.lesswrong.com/posts/yeqe7DHT5dHDXiD2k/cambridge-england-lecture-existential-risk-surviving-the", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20(England)%20lecture%3A%20Existential%20Risk%3A%20Surviving%20the%2021st%20Century%2C%2026th%20February%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20(England)%20lecture%3A%20Existential%20Risk%3A%20Surviving%20the%2021st%20Century%2C%2026th%20February%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyeqe7DHT5dHDXiD2k%2Fcambridge-england-lecture-existential-risk-surviving-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20(England)%20lecture%3A%20Existential%20Risk%3A%20Surviving%20the%2021st%20Century%2C%2026th%20February%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyeqe7DHT5dHDXiD2k%2Fcambridge-england-lecture-existential-risk-surviving-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyeqe7DHT5dHDXiD2k%2Fcambridge-england-lecture-existential-risk-surviving-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>The <a href=\"http://www.cser.org\">Centre for the Study of Existential Risk</a> will be holding a public lecture on \"Existential Risk: Surviving the 21st Century\" in collaboration with <a href=\"https://www.facebook.com/80kCambridge\">80,000 Hours: Cambridge</a> and <a href=\"https://www.facebook.com/GivingWhatWeCanCambridge\">Giving What We Can: Cambridge</a>&nbsp;on the 26th of February in Cambridge (United Kingdom).</p>\n<p>Lady Mitchell Hall, Sidgwick Site, Cambridge. 5:30pm-6:45pm, with drinks reception to follow.</p>\n<p>SPEAKERS:</p>\n<p>Lord Martin Rees, Astronomer Royal</p>\n<p>Jaan Tallinn, co-founder of Skype</p>\n<p>Huw Price, Bertrand Russell Professor of Philosophy at Cambridge</p>\n<p>\"In the coming century, the greatest threats to human survival may come from our own technological developments. However, if we can safely navigate the pitfalls, the benefits that technology promises are enormous. A philosopher, an astronomer, and an entrepreneur have come together to form the Centre for the Study of Existential Risk. The goal: to bring a fraction of humanity&rsquo;s talents to bear on the task of ensuring our long-term survival. In this lecture, Huw Price, Martin Rees and Jaan Tallinn will outline humanity&rsquo;s greatest challenge: surviving the 21st century.\"</p>\n<p>This event is free and open to all.</p>\n<p>Facebook event notice is <a href=\"https://www.facebook.com/events/1407072239542324\">here</a>.</p>\n<p>In other news, I hope to be posting a general update on progress with the Centre's establishment fairly shortly for those who are interested, although it's still an ongoing process. Things have become quite busy and there are a lot of opportunities to follow up on, so I'll be taking a leave of absence from the Future of Humanity Institute for 6 months in April to work full-time on the project to establish the centre.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yeqe7DHT5dHDXiD2k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 1.5639554502084317e-06, "legacy": true, "legacyId": "25504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T19:51:42.131Z", "modifiedAt": null, "url": null, "title": "I like simplicity, but not THAT much", "slug": "i-like-simplicity-but-not-that-much", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.133Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jZvTWB6Z75cv2q3Nw/i-like-simplicity-but-not-that-much", "pageUrlRelative": "/posts/jZvTWB6Z75cv2q3Nw/i-like-simplicity-but-not-that-much", "linkUrl": "https://www.lesswrong.com/posts/jZvTWB6Z75cv2q3Nw/i-like-simplicity-but-not-that-much", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20like%20simplicity%2C%20but%20not%20THAT%20much&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20like%20simplicity%2C%20but%20not%20THAT%20much%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZvTWB6Z75cv2q3Nw%2Fi-like-simplicity-but-not-that-much%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20like%20simplicity%2C%20but%20not%20THAT%20much%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZvTWB6Z75cv2q3Nw%2Fi-like-simplicity-but-not-that-much", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZvTWB6Z75cv2q3Nw%2Fi-like-simplicity-but-not-that-much", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2396, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/r/discussion/lw/jkm/lzombies_lzombies/\">L-zombies! (L-zombies?)</a> <br /><strong>Reply to:</strong> Coscott's <a href=\"/r/discussion/lw/jn2/preferences_without_existence/\">Preferences without Existence</a>; Paul Christiano's <a href=\"/r/discussion/lw/jkm/lzombies_lzombies/ainx\">comment on my l-zombies post</a></p>\n<p>In my <a href=\"/r/discussion/lw/jkm/lzombies_lzombies/\">previous post</a>, I introduced the idea of an \"l-zombie\", or <em>logical philosophical zombie</em>: A Turing machine that would simulate a conscious human being <em>if it were run</em>, but that is never run in the real, physical world, so that the experiences that this human <em>would have had</em>, if the Turing machine <em>were</em> run, aren't actually consciously experienced.</p>\n<p>One common reply to this is to deny the possibility of logical philosophical zombies just like the possibility of physical philosophical zombies: to say that every mathematically possible conscious experience is in fact consciously experienced, and that there is no kind of \"magical reality fluid\" that makes some of these be experienced \"more\" than others. In other words, we live in the Tegmark Level IV universe, <em>except</em> that unlike Tegmark argues in his paper, there's no objective measure on the collection of all mathematical structures, according to which some mathematical structures somehow \"exist more\" than others (and, although IIRC that's not part of Tegmark's argument, according to which the conscious experiences in some mathematical structures could be \"experienced more\" than those in other structures). All mathematically possible experiences are experienced, and to the same \"degree\".</p>\n<p>So why is our world so orderly? There's a mathematically possible continuation of the world that you seem to be living in, where purple pumpkins are about to start falling from the sky. Or the light we observe coming in from outside our galaxy is suddenly replaced by white noise. Why don't you remember ever seeing anything as obviously disorderly as that?</p>\n<p>And the answer to <em>that</em>, of course, is that among all the possible experiences that get experienced in this multiverse, there are orderly ones as well as non-orderly ones, so the fact that <em>you</em> happen to have orderly experiences isn't in conflict with the hypothesis; after all, the orderly experiences have to be experienced as well.</p>\n<p>One might be tempted to argue that it's somehow more likely that <em>you</em> will observe an orderly world if <em>everybody</em> who has conscious experiences at all, or if at least <em>most</em> conscious observers, see an orderly world. (The \"most observers\" version of the argument assumes that there is a measure on the conscious observers, a.k.a. some kind of magical reality fluid.) But this requires the use of anthropic probabilities, and there is simply no (known) system of anthropic probabilities that gives reasonable answers in general. Fortunately, we have an alternative: Wei Dai's <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">updateless decision theory</a> (which was <a href=\"/lw/cs9/list_of_problems_that_motivated_udt/\">motivated</a> in part exactly by the problem of how to act in this kind of multiverse). The basic idea is simple (though the details do contain devils): We have a prior over what the world looks like; we have some preferences about what we would like the world to look like; and we come up with a plan for what we should do in any circumstance we might find ourselves in that maximizes our expected utility, given our prior.</p>\n<p align=\"center\">*</p>\n<p>In this framework, <a href=\"/r/discussion/lw/jn2/preferences_without_existence/\">Coscott</a> and <a href=\"/r/discussion/lw/jkm/lzombies_lzombies/ainx\">Paul</a> suggest, everything adds up to normality if, instead of saying that some experiences <em>objectively</em> exist more, we happen to <em>care</em> more about some experiences than about others. (That's not a new idea, of course, or the first time this has appeared on LW -- for example, Wei Dai's <a href=\"/lw/1iy/what_are_probabilities_anyway/\">What are probabilities, anyway?</a> comes to mind.) In particular, suppose we just <em>care more</em> about experiences in mathematically really simple worlds -- or more precisely, places in mathematically simple worlds that are mathematically simple to describe (since there's a simple program that runs all Turing machines, and therefore all mathematically possible human experiences, always assuming that human brains are computable). Then, even though there's a version of you that's about to see purple pumpkins rain from the sky, you act in a way that's best in the world where that <em>doesn't</em> happen, because that world has so much lower K-complexity, and because you therefore care so much more about what happens in that world.</p>\n<p>There's something unsettling about that, which I think deserves to be mentioned, even though I do <em>not</em> think it's a good counterargument to this view. This unsettling thing is that on priors, it's very unlikely that the world <em>you</em> experience arises from a really simple mathematical description. (This is a version of a point I also made in my previous post.) Even if the physicists had already figured out the simple Theory of Everything, which is a super-simple cellular automaton that accords really well with experiments, you don't know that this simple cellular automaton, if you ran it, would really produce <em>you</em>. After all, imagine that somebody intervened in Earth's history so that orchids never evolved, but otherwise left the laws of physics the same; there might still be humans, or something like humans, and they would still run experiments and find that they match the predictions of the simple cellular automaton, so they would assume that if you ran that cellular automaton, it would compute <em>them</em> -- except it wouldn't, it would compute us, with orchids and all. Unless, of course, it <em>does</em> compute them, and a special intervention is required to get the orchids.</p>\n<p>So you <em>don't</em> know that you live in a simple world. But, goes the obvious reply, you <em>care</em> much more about what happens if you <em>do</em> happen to live in the simple world. On priors, it's probably not true; but it's best, according to your values, if all people like you <em>act</em> as if they live in the simple world (unless they're in a counterfactual mugging type of situation, where they can influence what happens in the simple world even if they're not in the simple world themselves), because if the <em>actual</em> people in the simple world act like that, that gives the highest utility.</p>\n<p>You can adapt an argument that I was making in my l-zombies post to this setting: Given these preferences, it's fine for everybody to <em>believe</em> that they're in a simple world, because this will increase the correspondence between map and territory for the people that <em>do</em> live in simple worlds, and that's who you care most about.</p>\n<p align=\"center\">*</p>\n<p>I mostly agree with this reasoning. I agree that Tegmark IV without a measure seems like the most obvious and reasonable hypothesis about what the world looks like. I agree that there seems no reason for there to be a \"magical reality fluid\". I agree, therefore, that on the priors that I'd put into my UDT calculation for how I should act, it's much more likely that true reality is a measureless Tegmark IV than that it has some objective measure according to which some experiences are \"experienced less\" than others, or not experienced at all. I don't think I understand things well enough to be <em>extremely</em> confident in this, but my odds would certainly be in favor of it.</p>\n<p>Moreover, I agree that if this <em>is</em> the case, then my preferences <em>are</em> to care more about the simpler worlds, making things add up to normality; I'd want to act as if purple pumpkins are <em>not</em> about to start falling from the sky, precisely because I care more about the consequences my actions have in more orderly worlds.</p>\n<p>But.</p>\n<p align=\"center\">*</p>\n<p>Imagine this: Once you finish reading this article, you hear a bell ringing, and then a sonorous voice announces: \"You do indeed live in a Tegmark IV multiverse without a measure. You had better deal with it.\" And then it turns out that it's not just you who's heard that voice: <em>Every single human being on the planet</em> (who didn't sleep through it, isn't deaf etc.) has heard those same words.</p>\n<p>On the hypothesis, this is of course about to happen to you, though only in one of those worlds with high K-complexity that you don't care about very much.</p>\n<p>So let's consider the following possible plan of action: You <em>could</em> act as if there is some difference between \"existence\" and \"non-existence\", or perhaps some graded degree of existence, <em>until</em> you hear those words and confirm that everybody else has heard them as well, or until you've experienced one similarly obviously \"disorderly\" event. So until that happens, you do things like invest time and energy into trying to figure out what the best way to act is if it turns out that there <em>is</em> some magical reality fluid, and into trying to figure out what a non-confused version of something <em>like</em> a measure on conscious experience could look like, and you act in ways that don't kill you if we happen to <em>not</em> live in a measureless Tegmark IV. But once you've had a disorderly experience, just a single one, you switch over to optimizing for the measureless mathematical multiverse.</p>\n<p>If the degree to which you care about worlds is really proportional to their K-complexity, with respect to what you and I would consider a \"simple\" universal Turing machine, then this would be a silly plan; there is very little to be gained from being right in worlds that have <em>that</em> much higher K-complexity. But when I query my intuitions, it seems like a rather good plan:</p>\n<ul>\n<li>Yes, I care less about those disorderly worlds. But not <em>as</em> much less as if I valued them by their K-complexity. I seem to be willing to tap into my complex human intuitions to refer to the notion of \"single obviously disorderly event\", and assign the worlds with a single such event, and <em>otherwise</em> low K-complexity, not <em>that</em> much lower importance than the worlds with <em>actual</em> low K-complexity.</li>\n<li>And if I imagine that the confused-seeming notions of \"really physically exists\" and \"actually experienced\" <em>do</em> have some objective meaning independent of my preferences, then I care <em>much</em> more about the difference between \"I get to 'actually experience' a tomorrow\" and \"I 'really physically' get hit by a car today\" than I care about the difference between the world with true low K-complexity and the worlds with a single disorderly event.</li>\n</ul>\n<p>In other words, I agree that on the priors I put into my UDT calculation, it's <em>much more likely</em> that we live in measureless Tegmark IV; but my confidence in this isn't extreme, and if we don't, then the difference between \"exists\" and \"doesn't exist\" (or \"is experienced a lot\" and \"is experienced only infinitesimally\") is <em>very</em> important; much more important than the difference between \"simple world\" and \"simple world plus one disorderly event\" according to my preferences if we <em>do</em> live in a Tegmark IV universe. If I act optimally according to the Tegmark IV hypothesis in the latter worlds, that still gives me most of the utility that acting optimally in the truly simple worlds would give me -- or, more precisely, the utility differential isn't nearly as large as if there <em>is</em> something else going on, and I should be doing something about it, and I'm not.</p>\n<p>This is the reason why I'm trying to think seriously about things like l-zombies and magical reality fluid. I mean, I don't even think that these are particularly likely to be exactly right even if the measureless Tegmark IV hypothesis is wrong; I expect that there would be some new insight that makes even more sense than Tegmark IV, and makes all the confusion go away. But trying to grapple with the confused intuitions we currently have seems at least a possible way to make progress on this, if it should be the case that there is in fact progress to be made.</p>\n<p align=\"center\">*</p>\n<p>Here's one avenue of investigation that seems worthwhile to me, and wouldn't without the above argument. One thing I could <em>imagine</em> finding, that could make the confusion go away, would be that the intuitive notion of \"all possible Turing machines\" is just <em>wrong</em>, and leads to outright contradictions (e.g., to inconsistencies in Peano Arithmetic, or something similarly convincing). Lots of people have entertained the idea that concepts like the real numbers don't \"really\" exist, and only the behavior of computable functions is \"real\"; perhaps not even that is real, and true reality is more restricted? (You can reinterpret many results about real numbers as results about computable functions, so maybe you could reinterpret results about computable functions as results about these hypothetical weaker objects that would <em>actually</em> make mathematical sense.) So it <em>wouldn't</em> be the case after all that there is some Turing machine that computes the conscious experiences you would have if pumpkins started falling from the sky.</p>\n<p>Does the above make sense? Probably not. But I'd say that there's a small chance that maybe yes, and that if we understood the right kind of math, it would seem very obvious that <em>not</em> all intuitively possible human experiences are actually <em>mathematically</em> possible (just as obvious as it is today, with hindsight, that there is no Turing machine which takes a program as input and outputs whether this program halts). Moreover, it seems plausible that this could have consequences for how we should act. This, together with my argument above, make me think that this sort of thing is worth investigating -- even if my priors are heavily on the side of expecting that all experiences exist to the same degree, and ordinarily this difference in probabilities would make me think that our time would be better spent on investigating other, more likely hypotheses.</p>\n<p align=\"center\">*</p>\n<p>Leaving aside the question of how I should <em>act</em>, though, does all of this mean that I should <em>believe</em> that I live in a universe with l-zombies and magical reality fluid, until such time as I hear that voice speaking to me?</p>\n<p>I do feel tempted to try to invoke my argument from the l-zombies post that I prefer the map-territory correspondences of <em>actually existing</em> humans to be correct, and don't care about whether l-zombies have their map match up with the territory. But I'm not sure that I care much more about actually existing humans being correct, if the measureless mathematical multiverse hypothesis is wrong, than I care about humans in simple worlds being correct, if that hypothesis is right. So I think that the right thing to do may be to have a subjective belief that I most likely do live in the measureless Tegmark IV, as long as that's the view that seems by far the least confused -- but continue to spend resources on investigating alternatives, because on priors they don't seem sufficiently unlikely to make up for the potential great importance of getting this right.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jZvTWB6Z75cv2q3Nw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 38, "extendedScore": null, "score": 9.8e-05, "legacy": true, "legacyId": "25501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7nAxgQYGYrEY5ZCAD", "NvwJMQvfu9hbBdG6d", "4kvaocbkDDS2AMoPG", "J7Gkz8aDxxSEQKXTN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T20:27:27.578Z", "modifiedAt": null, "url": null, "title": "Thoughts on Death", "slug": "thoughts-on-death", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:31.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BlackNoise", "createdAt": "2011-07-26T14:08:00.498Z", "isAdmin": false, "displayName": "BlackNoise"}, "userId": "zasqWCWXarngC7fu8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oB8eipAD4ditPrEe4/thoughts-on-death", "pageUrlRelative": "/posts/oB8eipAD4ditPrEe4/thoughts-on-death", "linkUrl": "https://www.lesswrong.com/posts/oB8eipAD4ditPrEe4/thoughts-on-death", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20Death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20Death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoB8eipAD4ditPrEe4%2Fthoughts-on-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20Death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoB8eipAD4ditPrEe4%2Fthoughts-on-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoB8eipAD4ditPrEe4%2Fthoughts-on-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 956, "htmlBody": "<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">Death sucks.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">Today (14/2/2014) my mothers&rsquo; father died after struggling with cancer for about a year.<br /> What pains me is the loss, but more so how it affects my mother, especially my imagination being &lsquo;useful&rsquo; in imagining how losing her would be like.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">The tragedy as I see it has a slightly different flavor than that of my other family members: For them it&rsquo;s probably seen as an ultimately inevitable end, and few perhaps hold some hope/notion of an afterlife or maybe just never thought too hard about what death entails.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">For me, as one who identifies with Transhuman ideas, and believes in at least the feasibility (if not high likelihood) of preservation and future restoration, this feels like an ultimately preventable tragedy. Where my mother will grieve, I will have uncertain regret and doubt.</span></p>\n<p><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">With that as background, I&rsquo;ve felt the need to write out some of my thoughts regarding identity, anthropics and existence and death.</span></p>\n<p>&nbsp;</p>\n<p><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">First off, what is a person?</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">The way I see it, everything a person is, is the algorithm and information structure contained in some fashion within the brain (most likely in its structure), which means a person isn&rsquo;t limited to biology as a substrate. If the functional relations and information structure is preserved, there is nothing preventing one from recreating them on a different substrate or even in a simulated environment as an upload.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">Moreover, a person isn&rsquo;t a single continuous entity; the &lsquo;me&rsquo; of today is not quite the &lsquo;me&rsquo; of yesterday, which in turn isn&rsquo;t the &lsquo;me&rsquo; of a year ago, Rather, a person is a series of &lsquo;Person-instances&rsquo;, connected causally between themselves and the world.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">In this context/worldview, certain philosophical problems get obvious solutions: <br /> Destructive teleport for example, preserves identity by virtue of maintaining the causal connection, even if the teleport is done by destructively scanning a person then recreating them years later; from inside it&rsquo;d seem like one was teleported into the future.<br /> For non-destructive teleportation or mind-cloning, the answer to &ldquo;which is &lsquo;you&rsquo;&rdquo; is &lsquo;both&rsquo; (or &lsquo;yes&rsquo;), since both satisfy the condition of preserving the identity-information-structure while being causally related to the person-instance of before. However, from that point onward, both &lsquo;you&rsquo; instances now have a nearly identical and slowly diverging clone/sibling that over time grows more distinct.<br /> Looking at how the subjective experience would look like supports this, since both would feel like being the same person from before.</span></p>\n<p><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">In general, identifying with separate person-instances of yourself should be a question of degree rather than a binary yes/no. Especially considering that person-instances can be separated by more than just time, if any multiverse-type ideas are true.</span></p>\n<p>&nbsp;</p>\n<p>This brings me to the second point: Metaphysics.</p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">Not too long ago, I&rsquo;ve encountered the ideas of Max Tegmark about the nature of existence. The really short version is (if I understand correctly) that existence is, at its highest/lowest level, how intelligent-life-supporting mathematical structures look like from inside.<br /> The idea struck me as a beautiful way to close the explanation chain, providing at least qualitatively a consistent model of existence and reality that contains a path explaining the existence of one to ask and understand it.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">Combine that, and various simulation-type arguments with anthropic thinking, and you get an identity spread across the multiverse in a forest of causal trees, with the occasional Boltzmann brain containing the causal &lsquo;back/forward&rsquo; links arising purely by chance, and you get a very peculiar view of how being a person looks like from inside, specifically at points close to branch-ends:<br /> Like with quantum suicide, even if the measure of realities in which you die far outweighs those in which you don&rsquo;t and assuming some smoothness in that there&rsquo;s no lower probability/measure limit to what still feels like an existence, then &lsquo;you&rsquo; still get a continuation of experience, even if at a much lower measure. </span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">This requires a rethinking/reworking on the specifics of why death sucks and the fact is there are still branch-ends. Even if there is a last moment minor probability split and continuation corresponding to things like reality as given being an ancestral simulation or something, the loss of measure feels like a really bad thing in and of itself, beyond which there are the many realities in which you are now dead, which hurts any others that care about you in all those worlds, not to mention the circumstances surrounding branch-ends aren&rsquo;t likely to be pleasant.</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">Overall though, it seems that there&rsquo;s a subjective kind of immortality, combined with a gradual thinning out over realities, where death still sucks and should be avoided at all cost, and will probably happen to everyone besides you.<br /> Note that horrific injury and survival are still very much a possibility, and the question of what you ought to expect is to me at least somewhat confusing, especially regarding things like cryonics in that you&rsquo;ll only expect a continuation of identity in the events it works, but you&rsquo;d only prefer it in the events it worked and the future doesn&rsquo;t suck, and if you find yourself in the branch with the &lsquo;future sucks&rsquo;, getting to one where it doesn&rsquo;t seems kind of... difficult.</span></p>\n<p><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">Definitely recommend acting as if death = cessation of existence, which is objectively true within any single reality (unless that reality is extra weird), and think about the subjective continuation-of-identity thinking for special cases like when deciding for/against signing up for cryonics, and in general the whole measure thing is kind of confusing, though thinking about it in context of what to expect seems like a useful direction.</span></p>\n<p>&nbsp;</p>\n<p><span style=\"mso-ansi-language:EN-US\" lang=\"EN-US\">So, A bit of a mess of only somewhat coherent ideas, I&rsquo;d appreciate any corrections regarding the metaphysics and any other oversights, but otherwise just thought I&rsquo;d let this out. Hope at least someone besides myself derives some<span style=\"mso-spacerun: yes;\">&nbsp; </span>use from it.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oB8eipAD4ditPrEe4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "25505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-14T22:59:50.766Z", "modifiedAt": null, "url": null, "title": "Effective public college tuition vs. private college tuition", "slug": "effective-public-college-tuition-vs-private-college-tuition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.923Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6heMWKWGwHprQwxet/effective-public-college-tuition-vs-private-college-tuition", "pageUrlRelative": "/posts/6heMWKWGwHprQwxet/effective-public-college-tuition-vs-private-college-tuition", "linkUrl": "https://www.lesswrong.com/posts/6heMWKWGwHprQwxet/effective-public-college-tuition-vs-private-college-tuition", "postedAtFormatted": "Friday, February 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Effective%20public%20college%20tuition%20vs.%20private%20college%20tuition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEffective%20public%20college%20tuition%20vs.%20private%20college%20tuition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6heMWKWGwHprQwxet%2Feffective-public-college-tuition-vs-private-college-tuition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Effective%20public%20college%20tuition%20vs.%20private%20college%20tuition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6heMWKWGwHprQwxet%2Feffective-public-college-tuition-vs-private-college-tuition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6heMWKWGwHprQwxet%2Feffective-public-college-tuition-vs-private-college-tuition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 434, "htmlBody": "<p>Harvard charges $42k/year and $61k/year including room, board, books and personal expenses. UC Berkeley charges $13k/year in tuition and $33k/year including room, board, books and personal expenses. As a part of research for <a href=\"http://cognitomentoring.org/\">Cognito Mentoring</a>, I looked at <a href=\"http://calculator.berkeley.edu/\">UC Berkeley's financial aid calculator</a>&nbsp;and <a href=\"https://college.harvard.edu/financial-aid/net-price-calculator\">Harvard's financial aid calculator</a>&nbsp;to get a sense for the effective costs of attending the two institutions, and was surprised that it appears as though for people whose parents make between 80k/year and $160k/year, when financial aid is factored in, it's more expensive to attend Berkeley than Harvard. See the figures below.</p>\n<p>A cursory glance at financial aid calculators for other elite private universities and public universities gives seems to give broadly similar to those of Harvard and UC Berkeley respectively. I've made a number of simplifying assumptions in filling out the forms I may be making errors of one kind or another (whether conceptual or otherwise), and would appreciate any feedback.&nbsp;</p>\n<p>About 83% of American families make less than $160k/year, so it seems that for 4 out of 5 Americans, were they admitted to an elite college, cost wouldn't be a deterrent. In practice, students who go to elite colleges come from disproportionately wealthy families: http://collegeapps.about.com/ reports that 62% of Harvard students get financial aid in the form of grants at all, which is less than the figure of 83% (as one would expect). The average grant size is $41,555, and Harvard reports that 20% of students don't pay at all, suggesting that the amount of financial aid that students receive is very high variance.</p>\n<p>It would seem that the people for whom the UC Berkeley vs. Harvard choice would be the hardest on financial grounds are families that make e.g. $200k/year and have $300k in saving.</p>\n<p><strong>Harvard</strong></p>\n<p>No parental savings</p>\n<p>Parental income / effective cost</p>\n<ul>\n<li>$80k / $8,600</li>\n<li>$100k / &nbsp;$12,600</li>\n<li>$120k / $16,600</li>\n<li>$140k / $18,600</li>\n<li>$160k / $24,600</li>\n<li>$180k / $34,300</li>\n<li>$200k / $49,200</li>\n<li>$220k / $60,850</li>\n</ul>\n<div>$300k parental savings</div>\n<div><br /></div>\n<div>Parental income / effective cost</div>\n<div>\n<ul>\n<li>$80k / $18,600</li>\n<li>$100k / $22,600</li>\n<li>$120k /&nbsp;$26,600</li>\n<li>$140k / $28,600</li>\n<li>$160k / $34,600</li>\n<li>$180k /&nbsp;$44,300</li>\n<li>$200k /&nbsp;$60,850</li>\n</ul>\n</div>\n<p><strong>UC Berkeley</strong></p>\n<p>Here I needed to enter the amount that parents were taxed, and used a figure of 25% of gross income.</p>\n<p>No parental savings</p>\n<ul>\n<li>$80k / $12,972</li>\n<li>$100k /&nbsp;$17,419</li>\n<li>$120k /&nbsp;$23,092</li>\n<li>$140k / $28,765</li>\n<li>$160k /&nbsp;$32,706</li>\n</ul>\n<div>$300k parental savings&nbsp;</div>\n<div>\n<ul>\n<li>$80k / $26,337</li>\n<li>$100k / $32,009</li>\n<li>$120k / $32,706</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6heMWKWGwHprQwxet", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.564187624434898e-06, "legacy": true, "legacyId": "25506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Harvard charges $42k/year and $61k/year including room, board, books and personal expenses. UC Berkeley charges $13k/year in tuition and $33k/year including room, board, books and personal expenses. As a part of research for <a href=\"http://cognitomentoring.org/\">Cognito Mentoring</a>, I looked at <a href=\"http://calculator.berkeley.edu/\">UC Berkeley's financial aid calculator</a>&nbsp;and <a href=\"https://college.harvard.edu/financial-aid/net-price-calculator\">Harvard's financial aid calculator</a>&nbsp;to get a sense for the effective costs of attending the two institutions, and was surprised that it appears as though for people whose parents make between 80k/year and $160k/year, when financial aid is factored in, it's more expensive to attend Berkeley than Harvard. See the figures below.</p>\n<p>A cursory glance at financial aid calculators for other elite private universities and public universities gives seems to give broadly similar to those of Harvard and UC Berkeley respectively. I've made a number of simplifying assumptions in filling out the forms I may be making errors of one kind or another (whether conceptual or otherwise), and would appreciate any feedback.&nbsp;</p>\n<p>About 83% of American families make less than $160k/year, so it seems that for 4 out of 5 Americans, were they admitted to an elite college, cost wouldn't be a deterrent. In practice, students who go to elite colleges come from disproportionately wealthy families: http://collegeapps.about.com/ reports that 62% of Harvard students get financial aid in the form of grants at all, which is less than the figure of 83% (as one would expect). The average grant size is $41,555, and Harvard reports that 20% of students don't pay at all, suggesting that the amount of financial aid that students receive is very high variance.</p>\n<p>It would seem that the people for whom the UC Berkeley vs. Harvard choice would be the hardest on financial grounds are families that make e.g. $200k/year and have $300k in saving.</p>\n<p><strong id=\"Harvard\">Harvard</strong></p>\n<p>No parental savings</p>\n<p>Parental income / effective cost</p>\n<ul>\n<li>$80k / $8,600</li>\n<li>$100k / &nbsp;$12,600</li>\n<li>$120k / $16,600</li>\n<li>$140k / $18,600</li>\n<li>$160k / $24,600</li>\n<li>$180k / $34,300</li>\n<li>$200k / $49,200</li>\n<li>$220k / $60,850</li>\n</ul>\n<div>$300k parental savings</div>\n<div><br></div>\n<div>Parental income / effective cost</div>\n<div>\n<ul>\n<li>$80k / $18,600</li>\n<li>$100k / $22,600</li>\n<li>$120k /&nbsp;$26,600</li>\n<li>$140k / $28,600</li>\n<li>$160k / $34,600</li>\n<li>$180k /&nbsp;$44,300</li>\n<li>$200k /&nbsp;$60,850</li>\n</ul>\n</div>\n<p><strong id=\"UC_Berkeley\">UC Berkeley</strong></p>\n<p>Here I needed to enter the amount that parents were taxed, and used a figure of 25% of gross income.</p>\n<p>No parental savings</p>\n<ul>\n<li>$80k / $12,972</li>\n<li>$100k /&nbsp;$17,419</li>\n<li>$120k /&nbsp;$23,092</li>\n<li>$140k / $28,765</li>\n<li>$160k /&nbsp;$32,706</li>\n</ul>\n<div>$300k parental savings&nbsp;</div>\n<div>\n<ul>\n<li>$80k / $26,337</li>\n<li>$100k / $32,009</li>\n<li>$120k / $32,706</li>\n</ul>\n</div>", "sections": [{"title": "Harvard", "anchor": "Harvard", "level": 1}, {"title": "UC Berkeley", "anchor": "UC_Berkeley", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-15T02:30:28.436Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Politics and the English Language", "slug": "meetup-urbana-champaign-politics-and-the-english-language", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.007Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y4oA2sEYhHYHXJjam/meetup-urbana-champaign-politics-and-the-english-language", "pageUrlRelative": "/posts/Y4oA2sEYhHYHXJjam/meetup-urbana-champaign-politics-and-the-english-language", "linkUrl": "https://www.lesswrong.com/posts/Y4oA2sEYhHYHXJjam/meetup-urbana-champaign-politics-and-the-english-language", "postedAtFormatted": "Saturday, February 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Politics%20and%20the%20English%20Language&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Politics%20and%20the%20English%20Language%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4oA2sEYhHYHXJjam%2Fmeetup-urbana-champaign-politics-and-the-english-language%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Politics%20and%20the%20English%20Language%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4oA2sEYhHYHXJjam%2Fmeetup-urbana-champaign-politics-and-the-english-language", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4oA2sEYhHYHXJjam%2Fmeetup-urbana-champaign-politics-and-the-english-language", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wu'>Urbana-Champaign: Politics and the English Language</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> 40.109545,-88.2273</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Read this:\n<a href=\"https://www.mtholyoke.edu/acad/intrel/orwell46.htm\" rel=\"nofollow\">https://www.mtholyoke.edu/acad/intrel/orwell46.htm</a></p>\n\n<p>And then show up and discuss it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wu'>Urbana-Champaign: Politics and the English Language</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y4oA2sEYhHYHXJjam", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5644318128305997e-06, "legacy": true, "legacyId": "25507", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Politics_and_the_English_Language\">Discussion article for the meetup : <a href=\"/meetups/wu\">Urbana-Champaign: Politics and the English Language</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> 40.109545,-88.2273</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Read this:\n<a href=\"https://www.mtholyoke.edu/acad/intrel/orwell46.htm\" rel=\"nofollow\">https://www.mtholyoke.edu/acad/intrel/orwell46.htm</a></p>\n\n<p>And then show up and discuss it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Politics_and_the_English_Language1\">Discussion article for the meetup : <a href=\"/meetups/wu\">Urbana-Champaign: Politics and the English Language</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Politics and the English Language", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Politics_and_the_English_Language", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Politics and the English Language", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Politics_and_the_English_Language1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-15T02:38:58.030Z", "modifiedAt": null, "url": null, "title": "Mental Subvocalization --\"Saying\" Words In Your Mind As You Read", "slug": "mental-subvocalization-saying-words-in-your-mind-as-you-read", "viewCount": null, "lastCommentedAt": "2018-06-20T21:30:34.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Torello", "createdAt": "2013-07-01T17:38:37.441Z", "isAdmin": false, "displayName": "Torello"}, "userId": "xoRpeFN7K5MgDRcvM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7h7C3C5nHWordPENM/mental-subvocalization-saying-words-in-your-mind-as-you-read", "pageUrlRelative": "/posts/7h7C3C5nHWordPENM/mental-subvocalization-saying-words-in-your-mind-as-you-read", "linkUrl": "https://www.lesswrong.com/posts/7h7C3C5nHWordPENM/mental-subvocalization-saying-words-in-your-mind-as-you-read", "postedAtFormatted": "Saturday, February 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mental%20Subvocalization%20--%22Saying%22%20Words%20In%20Your%20Mind%20As%20You%20Read&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMental%20Subvocalization%20--%22Saying%22%20Words%20In%20Your%20Mind%20As%20You%20Read%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7h7C3C5nHWordPENM%2Fmental-subvocalization-saying-words-in-your-mind-as-you-read%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mental%20Subvocalization%20--%22Saying%22%20Words%20In%20Your%20Mind%20As%20You%20Read%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7h7C3C5nHWordPENM%2Fmental-subvocalization-saying-words-in-your-mind-as-you-read", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7h7C3C5nHWordPENM%2Fmental-subvocalization-saying-words-in-your-mind-as-you-read", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>http://en.wikipedia.org/wiki/Subvocalization</p>\n<p>I'm curious about how often or to what degree visitors to this site subvocalize as they read.&nbsp; I was originally interested in reducing subvocalizations as a way to increase reading speed, as the idea is mentioned in multiple pieces I've read about speed reading.</p>\n<p>The Wikipedia entry seems to focus on subtle throat and muscle movements, but I'm more interested to know if you \"say\" or \"hear\" the words in your head as you read.</p>\n<p>Since reading about subvocalization recently, I seem to notice that I \"say/hear\" what I'm reading quite frequently.&nbsp; I'm not sure if this is causal (in the way that the command \"don't think of pink elephants\" obliges you to do so), or if I just notice it more now, or both.&nbsp;</p>\n<p>When I'm very engrossed in a book either I don't notice the subvocalizations or they stop happening, so seems that it could either be a cause or a symptom of distractedness.</p>\n<p>In the comments, please describe your mental subvocalizations (or lack of them) and if they are related to how engrossed you are in the book.&nbsp; Any other comments relevant comments about speed reading or subvocalizations are welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7h7C3C5nHWordPENM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.5644416607356754e-06, "legacy": true, "legacyId": "25508", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-15T05:22:11.067Z", "modifiedAt": null, "url": null, "title": "Rethinking Education", "slug": "rethinking-education", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:08.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qyjzTZpBNkNsmettN/rethinking-education", "pageUrlRelative": "/posts/qyjzTZpBNkNsmettN/rethinking-education", "linkUrl": "https://www.lesswrong.com/posts/qyjzTZpBNkNsmettN/rethinking-education", "postedAtFormatted": "Saturday, February 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rethinking%20Education&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARethinking%20Education%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqyjzTZpBNkNsmettN%2Frethinking-education%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rethinking%20Education%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqyjzTZpBNkNsmettN%2Frethinking-education", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqyjzTZpBNkNsmettN%2Frethinking-education", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1610, "htmlBody": "<h2>Problems</h2>\n<p class=\"p4\"><span class=\"s1\">Problems have bottlenecks. To solve problems, you need to overcome each bottleneck. If you fail to overcome just one bottleneck, the problem will go unsolved, and your effort will have been fruitless.</span></p>\n<p class=\"p4\"><span class=\"s1\">In reality, it&rsquo;s a little bit more complicated than that. Some bottlenecks are tighter than others, and some progress might leak through, but it usually isn&rsquo;t anything notable.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Education</span></h2>\n<p class=\"p4\"><span class=\"s1\">There is a lot wrong with education. Attempts are being made to improve it, but they&rsquo;re glossing over important bottlenecks. Consequently, progress is slowly dripping through. I think that it&rsquo;d be a better use of our time to take the time to think through each bottleneck, and how it can be addressed.</span></p>\n<p class=\"p4\"><span class=\"s1\">I have a theory of how we can overcome enough bottlenecks such that progress will fall through, instead of drip through.</span></p>\n<p class=\"p4\"><span class=\"s1\">Consider how we learn. Say that you want to learn parent concept A. To do this, it&rsquo;ll require you to understand a bunch of other things first&nbsp;</span></p>\n<p class=\"p4\"><span class=\"s1\">My groundbreaking idea: make sure that students know A</span><span class=\"s2\"><sub>1</sub></span><span class=\"s1\">&hellip;A</span><span class=\"s2\"><sub>n</sub></span><span class=\"s1\"> before teaching them A.</span></p>\n<p class=\"p2\"><img src=\"https://www.dropbox.com/s/4gnwamufalg5gqo/learning.jpg\" alt=\"\" />https://www.dropbox.com/s/4gnwamufalg5gqo/learning.jpg</p>\n<p class=\"p4\"><span class=\"s1\">The bottlenecks to understanding A are A</span><span class=\"s2\"><sub>1</sub></span><span class=\"s1\">&hellip;A</span><span class=\"s2\"><sub>n</sub></span><span class=\"s1\">. Some of these bottlenecks are tighter than others, and in reality, there are constraints on our ability to teach, so it&rsquo;s probably best to focus on the tighter bottlenecks. Regardless, this is the approach we&rsquo;ll need to take if we want to truly change education.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">How would this work?</span></h2>\n<p class=\"p4\"><span class=\"s1\">1) Create a dependency tree.</span></p>\n<p class=\"p4\"><span class=\"s1\">2) Explain each cell in the tree.</span></p>\n<p class=\"p4\"><span class=\"s1\">3) Devise a test of understanding for each cell in the tree.</span></p>\n<p class=\"p4\"><span class=\"s1\">4) Teach accordingly.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Where does our system fail us?</span></h2>\n<ul>\n<li><span class=\"s1\">When you&rsquo;re in class and the teacher is explaining A when you still don&rsquo;t get, say A</span><span class=\"s2\"><sub>2 </sub></span><span class=\"s1\">and A</span><span class=\"s2\"><sub>5</sub></span><span class=\"s1\">.</span></li>\n<li><span class=\"s1\">When you&rsquo;re in class and the teacher is explaining A, when she never thought to explain A</span><span class=\"s2\"><sub>2 </sub></span><span class=\"s1\">and A</span><span class=\"s2\"><sub>5</sub></span><span class=\"s1\">.</span></li>\n<li>When you&rsquo;re reading the textbook and you&rsquo;re confused, but you don&rsquo;t even know what child concepts you&rsquo;re confused about.</li>\n<li>When you memorize for the test/assignment instead of properly filling out your dependency tree.</li>\n<li>When being too far ahead or behind the class leads to a lack of motivation.</li>\n<li>When lack of interest in the material leads to lack of motivation.</li>\n<li>When physical distractions divert your attention (tired, uncomfortable, hungry&hellip;).</li>\n</ul>\n<p>&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">My proposal</span></h2>\n<p class=\"p4\"><span class=\"s1\">I propose that we pool all of our resources and make a perfect educational web app. It would have the dependency trees, have explanations for each cell in each tree, and have a test of understanding for each cell in each tree. It would test the user to establish what it is that he does and doesn&rsquo;t know, and would proceed with lessons accordingly.</span></p>\n<p class=\"p4\"><span class=\"s1\">In other words, usage of this web app would be mastery-based: you&rsquo;d only proceed to a parent concept when you&rsquo;ve mastered the child concepts.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Motivation</span></h2>\n<p class=\"p4\"><span class=\"s1\">Motivation would be another thing to optimize.</span></p>\n<p class=\"p4\"><span class=\"s1\">One way to do this would be to teach things to students at the right times. Lack of interest is often due to lack of understanding of child concepts, and thus lack of appreciation for the beauty and significance of a parent concept. By teaching things to students when they&rsquo;re able to appreciate them, we could increase students&rsquo; motivation.&nbsp;</span></p>\n<p class=\"p4\"><span class=\"s1\">Another way to optimize motivation would be to do a better job of teaching students things that are useful to them (or things that are likely to be useful to them). In todays system, students are often times forced to memorize lots of details that are unlikely to ever be useful to them.</span></p>\n<p class=\"p4\"><span class=\"s1\">By making teaching more effective, I think motivation will naturally increase as well (it&rsquo;ll eliminate the lack of motivation that comes with the frustration of bad teaching).</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Pooling of resources</span></h2>\n<p class=\"p4\"><span class=\"s1\">The pooling of resources to create this web app is <a href=\"http://www.youtube.com/watch?v=vDwzmJpI4io&amp;feature=youtu.be&amp;t=37m55s\"><span class=\"s3\">analogous</span></a> to how resources were pooled for Christopher Nolan to make a really cool movie. When you pool resources, a lot more becomes possible. When you don&rsquo;t pool resources, the product often sucks. Imagine what would happen if you tried to reproduce Batman at a local high school. This is analogous to what we&rsquo;re trying to do with education now.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">How would this look?</span></h2>\n<p class=\"p4\"><span class=\"s1\">I&rsquo;m not quite sure. Technically, kids could just sit at home on their computers and work through the lessons that the web app gives them&hellip; but I sense that that wouldn&rsquo;t be such a good idea. It&rsquo;d probably be best to require kids to go to a &ldquo;school-like institution&rdquo;. Kids could work through the lessons by themselves, ask each other for help, work together on projects, compete with each other on projects etc.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Certificates</span></h2>\n<p class=\"p4\"><span class=\"s1\">I envision that credentials would be certificate-based. You&rsquo;d get smaller certificates that indicate that you have mastered a certain subject. Today, the credentials you get are for passing a grade, or passing a class, or getting a degree. They&rsquo;re too big and inflexible. For example, maybe the plant unit in intro to biology isn&rsquo;t necessary for you. Smaller certificates allow for more flexibility.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Deadlines</span></h2>\n<p class=\"p4\"><span class=\"s1\">Deadlines are a tough issue. If they exist, there&rsquo;s a possibility that you have to cram to meet the deadline, and cramming isn&rsquo;t optimal for learning. However, if they don&rsquo;t exist, students probably won&rsquo;t have the incentive to learn. For this reason, I think that they probably do have to exist.</span></p>\n<p class=\"p4\"><span class=\"s1\">My first thought is that deadlines should be personalized. For example, if I moved 50 steps and the deadline was at 100 steps, the next deadline should be based on where I am now (step 50), not where the deadline was (step 100).</span></p>\n<p class=\"p4\"><span class=\"s1\">My second thought is that deadlines should be rather loose, because I think that flexibility and personalization are important, and that deadlines sacrifice those things.</span></p>\n<p class=\"p4\"><span class=\"s1\">My third thought, is that students should be given credit for going faster. In our one-size-fits-all system now, you can&rsquo;t get credit for moving faster than your class. I think that if you want to work harder and make faster progress, you should be able to and you should be given credentials for the knowledge that you&rsquo;ve acquired. Given the chance, I think that many students would do this. I think this would allow students to really thrive and pursue their interests.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Tutoring</span></h2>\n<p class=\"p4\"><span class=\"s1\">I think that it&rsquo;d be a good idea to require tutoring. Say, in order to get a certificate, after passing the tests, you&rsquo;d have to tutor for x hours.</span></p>\n<p class=\"p4\"><span class=\"s1\">Tutoring helps you to master the concept, because having to explain something will expose the holes in your understanding. See <a href=\"http://www.youtube.com/watch?v=FrNqSLPaZLc\"><span class=\"s3\">The Feynman Technique</span></a>.</span></p>\n<p class=\"p4\"><span class=\"s1\">Tutoring allows for social interaction, which is important.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Social Atmosphere</span></h2>\n<p class=\"p4\"><span class=\"s1\">The social atmosphere in these &ldquo;schools&rdquo; would also be something to optimize. It's not something that people think too much about, but it has a huge impact on how people develop, and thus on how society develops.</span></p>\n<p class=\"p4\"><span class=\"s1\">I&rsquo;m not sure exactly what would be best, but I have a few thoughts:</span></p>\n<p class=\"p4\"><span class=\"s1\">The idea of social value is horrible. In schools today, you grow up caring way too much about how you look, who you&rsquo;re friends with, how athletic you are, how smart you are, how much success you have with the opposite sex&hellip; how &ldquo;good&rdquo; you are. This bleeds into our society, and does a lot to cause unhappiness. It should be avoided, if possible.</span></p>\n<p class=\"p4\"><span class=\"s1\">Relationships are based largely on <a href=\"http://www.nytimes.com/2012/07/15/fashion/the-challenge-of-making-friends-as-an-adult.html?pagewanted=all&amp;_r=0\"><span class=\"s3\">repeated, unplanned interactions + an environment that encourages you to let your guard down</span></a>. I think that schools should actively provide these situations to students, and should allow you to experience these situations with a variety of types of people (right now you only get these repeated, unplanned interactions with the cohort of students you happen to be with, which limits&nbsp; you in a lot of ways).</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Rationality</span></h2>\n<p class=\"p4\"><span class=\"s1\">I propose that rationality be a core part of the curriculum (the benefits of making people better at reasoning would trickle down into many aspects of life). I think that this should be done in two ways: the first is by teaching the ideas of rationality, and the second is by using them.</span></p>\n<p class=\"p4\"><span class=\"s1\">The ideas of rationality can be found right <a href=\"http://wiki.lesswrong.com/wiki/Rationality_materials\"><span class=\"s3\">here</span></a>. Some examples:</span></p>\n<ul>\n<li>Your beliefs have to be about <a href=\"http://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\"><span class=\"s3\">anticipated experiences</span></a>.</li>\n<li>Don&rsquo;t commit the <a href=\"http://wiki.lesswrong.com/wiki/Fallacy_of_gray\"><span class=\"s3\">fallacy of gray</span></a>.</li>\n<li>Understanding that you should optimize the <a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\"><span class=\"s3\">terminal value</span></a>.</li>\n<li>Don&rsquo;t <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\"><span class=\"s3\">treat arguments like war</span></a>.</li>\n<li>Disagree by <a href=\"http://paulgraham.com/disagree.html\"><span class=\"s3\">refuting the central point</span></a>.</li>\n<li><span class=\"s4\"><a href=\"/lw/bc3/sotw_be_specific/\">Be specific</a></span><span class=\"s1\">.</span></li>\n</ul>\n<p class=\"p4\"><span class=\"s1\">After the ideas are taught, they should be practiced. The best way that I could think of to do this is to have kids write and critique essays (writing is just thought on paper, and it&rsquo;s often easier to argue in writing than it is in verbal conversation). Students could pick a topic that they want to talk about, make claims, and argue for them. And then they could read each others&rsquo; essays, and point out what they think are mistakes in each others&rsquo; reasoning (this should all be supervised by a teacher, who should probably be more of a benevolent dictator, and who should also contribute points to the discussions).</span></p>\n<p class=\"p4\"><span class=\"s1\">I think that some competition and social pressure could be useful too; maybe it&rsquo;d be a good idea to divide students into classes, where the most insightful points are voted upon, and the number of mistakes committed would be tallied and posted.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Writing</span></h2>\n<p class=\"p4\"><span class=\"s1\">Right now, essays in schools are a joke. No one takes them seriously. Students b.s. them, and teachers barely read them, and hardly give any feedback. And they&rsquo;re also always on english literature, which sends a bad message to kids about what an essay <a href=\"http://www.paulgraham.com/essay.html\"><span class=\"s3\">really is</span></a>. <a href=\"https://medium.com/writers-on-writing/3ce57e1fce82\"><span class=\"s3\">Good writing</span></a> isn&rsquo;t taught or practiced, and it should be.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2><span class=\"s1\">Levels of Action</span></h2>\n<p class=\"p4\"><span class=\"s1\">Certain <a href=\"/lw/58g/levels_of_action/\"><span class=\"s3\">levels of action</span></a> have impacts that are orders of magnitude bigger than others. I think that improving education this much would be a high level action, and have many positive effects that&rsquo;ll trickle down into many aspects of society. I&rsquo;ll let you speculate on what they are.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qyjzTZpBNkNsmettN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": -1, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "25509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Problems\">Problems</h2>\n<p class=\"p4\"><span class=\"s1\">Problems have bottlenecks. To solve problems, you need to overcome each bottleneck. If you fail to overcome just one bottleneck, the problem will go unsolved, and your effort will have been fruitless.</span></p>\n<p class=\"p4\"><span class=\"s1\">In reality, it\u2019s a little bit more complicated than that. Some bottlenecks are tighter than others, and some progress might leak through, but it usually isn\u2019t anything notable.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Education\"><span class=\"s1\">Education</span></h2>\n<p class=\"p4\"><span class=\"s1\">There is a lot wrong with education. Attempts are being made to improve it, but they\u2019re glossing over important bottlenecks. Consequently, progress is slowly dripping through. I think that it\u2019d be a better use of our time to take the time to think through each bottleneck, and how it can be addressed.</span></p>\n<p class=\"p4\"><span class=\"s1\">I have a theory of how we can overcome enough bottlenecks such that progress will fall through, instead of drip through.</span></p>\n<p class=\"p4\"><span class=\"s1\">Consider how we learn. Say that you want to learn parent concept A. To do this, it\u2019ll require you to understand a bunch of other things first&nbsp;</span></p>\n<p class=\"p4\"><span class=\"s1\">My groundbreaking idea: make sure that students know A</span><span class=\"s2\"><sub>1</sub></span><span class=\"s1\">\u2026A</span><span class=\"s2\"><sub>n</sub></span><span class=\"s1\"> before teaching them A.</span></p>\n<p class=\"p2\"><img src=\"https://www.dropbox.com/s/4gnwamufalg5gqo/learning.jpg\" alt=\"\">https://www.dropbox.com/s/4gnwamufalg5gqo/learning.jpg</p>\n<p class=\"p4\"><span class=\"s1\">The bottlenecks to understanding A are A</span><span class=\"s2\"><sub>1</sub></span><span class=\"s1\">\u2026A</span><span class=\"s2\"><sub>n</sub></span><span class=\"s1\">. Some of these bottlenecks are tighter than others, and in reality, there are constraints on our ability to teach, so it\u2019s probably best to focus on the tighter bottlenecks. Regardless, this is the approach we\u2019ll need to take if we want to truly change education.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"How_would_this_work_\"><span class=\"s1\">How would this work?</span></h2>\n<p class=\"p4\"><span class=\"s1\">1) Create a dependency tree.</span></p>\n<p class=\"p4\"><span class=\"s1\">2) Explain each cell in the tree.</span></p>\n<p class=\"p4\"><span class=\"s1\">3) Devise a test of understanding for each cell in the tree.</span></p>\n<p class=\"p4\"><span class=\"s1\">4) Teach accordingly.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Where_does_our_system_fail_us_\"><span class=\"s1\">Where does our system fail us?</span></h2>\n<ul>\n<li><span class=\"s1\">When you\u2019re in class and the teacher is explaining A when you still don\u2019t get, say A</span><span class=\"s2\"><sub>2 </sub></span><span class=\"s1\">and A</span><span class=\"s2\"><sub>5</sub></span><span class=\"s1\">.</span></li>\n<li><span class=\"s1\">When you\u2019re in class and the teacher is explaining A, when she never thought to explain A</span><span class=\"s2\"><sub>2 </sub></span><span class=\"s1\">and A</span><span class=\"s2\"><sub>5</sub></span><span class=\"s1\">.</span></li>\n<li>When you\u2019re reading the textbook and you\u2019re confused, but you don\u2019t even know what child concepts you\u2019re confused about.</li>\n<li>When you memorize for the test/assignment instead of properly filling out your dependency tree.</li>\n<li>When being too far ahead or behind the class leads to a lack of motivation.</li>\n<li>When lack of interest in the material leads to lack of motivation.</li>\n<li>When physical distractions divert your attention (tired, uncomfortable, hungry\u2026).</li>\n</ul>\n<p>&nbsp;</p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"My_proposal\"><span class=\"s1\">My proposal</span></h2>\n<p class=\"p4\"><span class=\"s1\">I propose that we pool all of our resources and make a perfect educational web app. It would have the dependency trees, have explanations for each cell in each tree, and have a test of understanding for each cell in each tree. It would test the user to establish what it is that he does and doesn\u2019t know, and would proceed with lessons accordingly.</span></p>\n<p class=\"p4\"><span class=\"s1\">In other words, usage of this web app would be mastery-based: you\u2019d only proceed to a parent concept when you\u2019ve mastered the child concepts.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Motivation\"><span class=\"s1\">Motivation</span></h2>\n<p class=\"p4\"><span class=\"s1\">Motivation would be another thing to optimize.</span></p>\n<p class=\"p4\"><span class=\"s1\">One way to do this would be to teach things to students at the right times. Lack of interest is often due to lack of understanding of child concepts, and thus lack of appreciation for the beauty and significance of a parent concept. By teaching things to students when they\u2019re able to appreciate them, we could increase students\u2019 motivation.&nbsp;</span></p>\n<p class=\"p4\"><span class=\"s1\">Another way to optimize motivation would be to do a better job of teaching students things that are useful to them (or things that are likely to be useful to them). In todays system, students are often times forced to memorize lots of details that are unlikely to ever be useful to them.</span></p>\n<p class=\"p4\"><span class=\"s1\">By making teaching more effective, I think motivation will naturally increase as well (it\u2019ll eliminate the lack of motivation that comes with the frustration of bad teaching).</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Pooling_of_resources\"><span class=\"s1\">Pooling of resources</span></h2>\n<p class=\"p4\"><span class=\"s1\">The pooling of resources to create this web app is <a href=\"http://www.youtube.com/watch?v=vDwzmJpI4io&amp;feature=youtu.be&amp;t=37m55s\"><span class=\"s3\">analogous</span></a> to how resources were pooled for Christopher Nolan to make a really cool movie. When you pool resources, a lot more becomes possible. When you don\u2019t pool resources, the product often sucks. Imagine what would happen if you tried to reproduce Batman at a local high school. This is analogous to what we\u2019re trying to do with education now.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"How_would_this_look_\"><span class=\"s1\">How would this look?</span></h2>\n<p class=\"p4\"><span class=\"s1\">I\u2019m not quite sure. Technically, kids could just sit at home on their computers and work through the lessons that the web app gives them\u2026 but I sense that that wouldn\u2019t be such a good idea. It\u2019d probably be best to require kids to go to a \u201cschool-like institution\u201d. Kids could work through the lessons by themselves, ask each other for help, work together on projects, compete with each other on projects etc.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Certificates\"><span class=\"s1\">Certificates</span></h2>\n<p class=\"p4\"><span class=\"s1\">I envision that credentials would be certificate-based. You\u2019d get smaller certificates that indicate that you have mastered a certain subject. Today, the credentials you get are for passing a grade, or passing a class, or getting a degree. They\u2019re too big and inflexible. For example, maybe the plant unit in intro to biology isn\u2019t necessary for you. Smaller certificates allow for more flexibility.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Deadlines\"><span class=\"s1\">Deadlines</span></h2>\n<p class=\"p4\"><span class=\"s1\">Deadlines are a tough issue. If they exist, there\u2019s a possibility that you have to cram to meet the deadline, and cramming isn\u2019t optimal for learning. However, if they don\u2019t exist, students probably won\u2019t have the incentive to learn. For this reason, I think that they probably do have to exist.</span></p>\n<p class=\"p4\"><span class=\"s1\">My first thought is that deadlines should be personalized. For example, if I moved 50 steps and the deadline was at 100 steps, the next deadline should be based on where I am now (step 50), not where the deadline was (step 100).</span></p>\n<p class=\"p4\"><span class=\"s1\">My second thought is that deadlines should be rather loose, because I think that flexibility and personalization are important, and that deadlines sacrifice those things.</span></p>\n<p class=\"p4\"><span class=\"s1\">My third thought, is that students should be given credit for going faster. In our one-size-fits-all system now, you can\u2019t get credit for moving faster than your class. I think that if you want to work harder and make faster progress, you should be able to and you should be given credentials for the knowledge that you\u2019ve acquired. Given the chance, I think that many students would do this. I think this would allow students to really thrive and pursue their interests.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Tutoring\"><span class=\"s1\">Tutoring</span></h2>\n<p class=\"p4\"><span class=\"s1\">I think that it\u2019d be a good idea to require tutoring. Say, in order to get a certificate, after passing the tests, you\u2019d have to tutor for x hours.</span></p>\n<p class=\"p4\"><span class=\"s1\">Tutoring helps you to master the concept, because having to explain something will expose the holes in your understanding. See <a href=\"http://www.youtube.com/watch?v=FrNqSLPaZLc\"><span class=\"s3\">The Feynman Technique</span></a>.</span></p>\n<p class=\"p4\"><span class=\"s1\">Tutoring allows for social interaction, which is important.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Social_Atmosphere\"><span class=\"s1\">Social Atmosphere</span></h2>\n<p class=\"p4\"><span class=\"s1\">The social atmosphere in these \u201cschools\u201d would also be something to optimize. It's not something that people think too much about, but it has a huge impact on how people develop, and thus on how society develops.</span></p>\n<p class=\"p4\"><span class=\"s1\">I\u2019m not sure exactly what would be best, but I have a few thoughts:</span></p>\n<p class=\"p4\"><span class=\"s1\">The idea of social value is horrible. In schools today, you grow up caring way too much about how you look, who you\u2019re friends with, how athletic you are, how smart you are, how much success you have with the opposite sex\u2026 how \u201cgood\u201d you are. This bleeds into our society, and does a lot to cause unhappiness. It should be avoided, if possible.</span></p>\n<p class=\"p4\"><span class=\"s1\">Relationships are based largely on <a href=\"http://www.nytimes.com/2012/07/15/fashion/the-challenge-of-making-friends-as-an-adult.html?pagewanted=all&amp;_r=0\"><span class=\"s3\">repeated, unplanned interactions + an environment that encourages you to let your guard down</span></a>. I think that schools should actively provide these situations to students, and should allow you to experience these situations with a variety of types of people (right now you only get these repeated, unplanned interactions with the cohort of students you happen to be with, which limits&nbsp; you in a lot of ways).</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Rationality\"><span class=\"s1\">Rationality</span></h2>\n<p class=\"p4\"><span class=\"s1\">I propose that rationality be a core part of the curriculum (the benefits of making people better at reasoning would trickle down into many aspects of life). I think that this should be done in two ways: the first is by teaching the ideas of rationality, and the second is by using them.</span></p>\n<p class=\"p4\"><span class=\"s1\">The ideas of rationality can be found right <a href=\"http://wiki.lesswrong.com/wiki/Rationality_materials\"><span class=\"s3\">here</span></a>. Some examples:</span></p>\n<ul>\n<li>Your beliefs have to be about <a href=\"http://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\"><span class=\"s3\">anticipated experiences</span></a>.</li>\n<li>Don\u2019t commit the <a href=\"http://wiki.lesswrong.com/wiki/Fallacy_of_gray\"><span class=\"s3\">fallacy of gray</span></a>.</li>\n<li>Understanding that you should optimize the <a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\"><span class=\"s3\">terminal value</span></a>.</li>\n<li>Don\u2019t <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\"><span class=\"s3\">treat arguments like war</span></a>.</li>\n<li>Disagree by <a href=\"http://paulgraham.com/disagree.html\"><span class=\"s3\">refuting the central point</span></a>.</li>\n<li><span class=\"s4\"><a href=\"/lw/bc3/sotw_be_specific/\">Be specific</a></span><span class=\"s1\">.</span></li>\n</ul>\n<p class=\"p4\"><span class=\"s1\">After the ideas are taught, they should be practiced. The best way that I could think of to do this is to have kids write and critique essays (writing is just thought on paper, and it\u2019s often easier to argue in writing than it is in verbal conversation). Students could pick a topic that they want to talk about, make claims, and argue for them. And then they could read each others\u2019 essays, and point out what they think are mistakes in each others\u2019 reasoning (this should all be supervised by a teacher, who should probably be more of a benevolent dictator, and who should also contribute points to the discussions).</span></p>\n<p class=\"p4\"><span class=\"s1\">I think that some competition and social pressure could be useful too; maybe it\u2019d be a good idea to divide students into classes, where the most insightful points are voted upon, and the number of mistakes committed would be tallied and posted.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Writing\"><span class=\"s1\">Writing</span></h2>\n<p class=\"p4\"><span class=\"s1\">Right now, essays in schools are a joke. No one takes them seriously. Students b.s. them, and teachers barely read them, and hardly give any feedback. And they\u2019re also always on english literature, which sends a bad message to kids about what an essay <a href=\"http://www.paulgraham.com/essay.html\"><span class=\"s3\">really is</span></a>. <a href=\"https://medium.com/writers-on-writing/3ce57e1fce82\"><span class=\"s3\">Good writing</span></a> isn\u2019t taught or practiced, and it should be.</span></p>\n<p class=\"p2\">&nbsp;</p>\n<h2 id=\"Levels_of_Action\"><span class=\"s1\">Levels of Action</span></h2>\n<p class=\"p4\"><span class=\"s1\">Certain <a href=\"/lw/58g/levels_of_action/\"><span class=\"s3\">levels of action</span></a> have impacts that are orders of magnitude bigger than others. I think that improving education this much would be a high level action, and have many positive effects that\u2019ll trickle down into many aspects of society. I\u2019ll let you speculate on what they are.</span></p>", "sections": [{"title": "Problems", "anchor": "Problems", "level": 1}, {"title": "Education", "anchor": "Education", "level": 1}, {"title": "How would this work?", "anchor": "How_would_this_work_", "level": 1}, {"title": "Where does our system fail us?", "anchor": "Where_does_our_system_fail_us_", "level": 1}, {"title": "My proposal", "anchor": "My_proposal", "level": 1}, {"title": "Motivation", "anchor": "Motivation", "level": 1}, {"title": "Pooling of resources", "anchor": "Pooling_of_resources", "level": 1}, {"title": "How would this look?", "anchor": "How_would_this_look_", "level": 1}, {"title": "Certificates", "anchor": "Certificates", "level": 1}, {"title": "Deadlines", "anchor": "Deadlines", "level": 1}, {"title": "Tutoring", "anchor": "Tutoring", "level": 1}, {"title": "Social Atmosphere", "anchor": "Social_Atmosphere", "level": 1}, {"title": "Rationality", "anchor": "Rationality", "level": 1}, {"title": "Writing", "anchor": "Writing", "level": 1}, {"title": "Levels of Action", "anchor": "Levels_of_Action", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "87 comments"}], "headingsCount": 17}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A", "NgtYDP3ZtLJaM248W", "guDcrPqLsnhEjrPZj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-15T18:41:42.580Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-9", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YnnKXGenR7LzxTnbv/meetup-washington-dc-fun-and-games-meetup-9", "pageUrlRelative": "/posts/YnnKXGenR7LzxTnbv/meetup-washington-dc-fun-and-games-meetup-9", "linkUrl": "https://www.lesswrong.com/posts/YnnKXGenR7LzxTnbv/meetup-washington-dc-fun-and-games-meetup-9", "postedAtFormatted": "Saturday, February 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnnKXGenR7LzxTnbv%2Fmeetup-washington-dc-fun-and-games-meetup-9%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnnKXGenR7LzxTnbv%2Fmeetup-washington-dc-fun-and-games-meetup-9", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnnKXGenR7LzxTnbv%2Fmeetup-washington-dc-fun-and-games-meetup-9", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wv'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 February 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">national portrait gallaey</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wv'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YnnKXGenR7LzxTnbv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5655586741116126e-06, "legacy": true, "legacyId": "25512", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/wv\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 February 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">national portrait gallaey</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/wv\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-16T19:47:57.357Z", "modifiedAt": null, "url": null, "title": "A defense of Senexism (Deathism)", "slug": "a-defense-of-senexism-deathism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:36.872Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fn8Kru2kaGRZCGTKx/a-defense-of-senexism-deathism", "pageUrlRelative": "/posts/Fn8Kru2kaGRZCGTKx/a-defense-of-senexism-deathism", "linkUrl": "https://www.lesswrong.com/posts/Fn8Kru2kaGRZCGTKx/a-defense-of-senexism-deathism", "postedAtFormatted": "Sunday, February 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20defense%20of%20Senexism%20(Deathism)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20defense%20of%20Senexism%20(Deathism)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFn8Kru2kaGRZCGTKx%2Fa-defense-of-senexism-deathism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20defense%20of%20Senexism%20(Deathism)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFn8Kru2kaGRZCGTKx%2Fa-defense-of-senexism-deathism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFn8Kru2kaGRZCGTKx%2Fa-defense-of-senexism-deathism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1854, "htmlBody": "<p>EDIT: Incorporated suggestions from comments: Moved off-topic parts into comments, improved formatting, corrected links.</p>\n<h2>Definition</h2>\n<p>The LW post <a href=\"/lw/2zj/value_deathism/\">Value Deathism</a> differntiates between the illusory nature of death and the 'desirability' of death called deathism proper. This post is about the latter. Where desirability is meant in a general sense and not (only) in the sense of desirable for an individual.</p>\n<p>I propose a different more neutral term for deathism: <strong>Senexism</strong> - from the latin adjective senex - old. &nbsp;I propose this because death is only the end of an aging process and by focussing on the ultimate and emotionally disturbing result one loads the topic with negative connotations. Senescence on the other hand - though unwanted - has also positive connotations of experience and humility. This also nicely splits off (or reduces applicability of) death by accident.&nbsp;</p>\n<h2>Outline</h2>\n<p>My defense is twofold. First I address the (emotional) pain and loss death causes and point out adaptive affects of the coping mechanisms humans have. Second I address the actual benefits senescence and death has - not for the individual but for the group. Thus the latter is an utilitarian argument for death actually.</p>\n<p>I will provide current research results for these points. At the end I will conclude with an opinion piece on what this means for rationalists and an outlook how this applies in light of the singularity.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2>Fear of Death</h2>\n<p>How does (fear of) death affect you?</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Terror_management_theory\">Terror Management Theory</a>&nbsp;(TMT) posits that</p>\n<blockquote>\n<p>people cope with mortality by creating beliefs and values that promise a sense of immortality.</p>\n<p>And research supports the premise that these beliefs are</p>\n<p>(a) defended more when people are reminded of death and</p>\n<p>(b) protect people from mortality concerns.</p>\n</blockquote>\n<p>Some more scientifically validated claims of TMT are (nicely presented by&nbsp;<a href=\"http://www.psychologytoday.com/blog/the-big-questions/201206/how-we-cope-death\">psychology today</a>):</p>\n<blockquote>\n<p>Death reminders cause people to&nbsp;self-enhance and protect self-esteem, such as by agreeing more with positive feedback and taking more credit for success and identify more with members of their own group, and even to rate them as more unique from other animals.</p>\n</blockquote>\n<p>One can see this even here on LW e.g. in links from <a href=\"http://wiki.lesswrong.com/wiki/Death\">Death</a>&nbsp;and also in the defenses of cryonics - which <a href=\"/lw/jfc/link_how_do_good_ideas_spread/aa6p\"><em>look</em> like an afterlife meme</a>.&nbsp;</p>\n<p>Applied to this post this means that you are likely to&nbsp;</p>\n<blockquote>\n<p>1) defend [your] cultural worldviews more strongly.&nbsp;</p>\n</blockquote>\n<p>(here e.g. denial of death via cryonics) thus I objectively risk karma.</p>\n<p>This is the reason I started this post with a positive confirmation. I hacked you dammit. I used this fact:&nbsp;</p>\n<blockquote>\n<p>9) Defending any of these things (relationships, beliefs, etc) prior to being reminded of death, or taking away people's anxiety, reduces the effects that mortality thoughts have.</p>\n</blockquote>\n<p>Western thinking of coping with death is confused with beliefs of coping with death. Probably due to the above effect itself.&nbsp;</p>\n<p>We seem to believe that (when you read this ask yourself: Do you agree with this?)</p>\n<blockquote><ol>\n<li>Bereaved persons are expected to exhibit significant distress following a major loss, and the failure to experience such distress tends to be seen as indicative of a problem.</li>\n<li>Positive emotions are implicitly assumed to be absent during this period. If they are expressed, they tend to be viewed as an indication that people are denying or covering up their distress.</li>\n<li>Following the loss of a loved one, the bereaved must confront and &ldquo;work through&rdquo; their feelings about the loss. Efforts to avoid or deny feelings are maladaptive in the long run.</li>\n<li>It is important for the bereaved to break down their attachment to the deceased loved one.</li>\n<li>Within a year or two, the bereaved will be able to come to terms with what has happened, recover from the loss, and resume their earlier level of functioning.</li>\n</ol></blockquote>\n<p>Do you agree?</p>\n<p>Yes?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>No! These are all&nbsp;<a href=\"http://www.psychology.sunysb.edu/cwortman-/papers/Wortman_Boerner.pdf\">Myths of coping with death</a>!</p>\n<p>It is true that</p>\n<blockquote>\n<p>Unlike many stressful life experiences, bereavement cannot be altered by the coping efforts of survivors. Indeed, the major coping task faced by the bereaved is to reconcile themselves to a situation that cannot be changed and find a way to carry on with their own lives.</p>\n</blockquote>\n<p>But this doesn't mean that is must always hurt and take long.</p>\n<h2>Biases and Death</h2>\n<p>Thus from our society and being human we are bound to believe that (we should believe that) death is horrible and we should suffer from encountering it. &nbsp;</p>\n<p>For an efficiently working brain (that is set on the track of avoiding death at all cost) it is not hard to spot patterns that support the view that death is only bad.</p>\n<p>This means that among all topics you are most likely to fall prey to one bias or other with respect to death memes e.g. &nbsp;</p>\n<ul>\n<li>availability heuristics (arguments against death are mucb more available obviously)</li>\n<li>confirmation bias (you already believe death to be bad)&nbsp;</li>\n<li>belief bias and wishful thinking (you want death to be bad)</li>\n<li>attentional bias (thought of death can be salient; they are actually continuous repeated by the media)</li>\n<li>and even illusion of control (the belief that you can cheat death)</li>\n</ul>\n<p>There are probably lots others. Take finding them as a homework (or chance for a comment).</p>\n<h2>Coping with Death Adaptively</h2>\n<p>But death and loss may not be as devasting as you make it.</p>\n<p>In particular according to&nbsp;<a href=\"http://link.springer.com/article/10.1007/s11013-007-9050-6#page-1\">Nordanger</a>&nbsp;(2007)</p>\n<blockquote>\n<p>Epidemiological studies indicate that the majority of trauma survivors recover from initial posttraumatic reactions without professional help and their posttraumatic adjustment may be facilitated by indigenous coping resources and socioeconomic structures such as traditional healers, traditions and rituals.</p>\n</blockquote>\n<p>and <a href=\"http://www.researchgate.net/profile/Kate_Murray3/publication/232581337_Resilience_A_new_definition_of_health_for_people_and_communities/file/9fcfd51259d834e134.pdf\">Zautra</a> 2010</p>\n<blockquote>\n<p>It would be most consistent with what we observe in human communities to see resilience as a natural capacity to recover and perhaps even further one&rsquo;s adaptive capacities</p>\n</blockquote>\n<p>I also understand that indigenous tribes which are more acutely affected by harm and death do not suffer the same way from it we do.&nbsp;</p>\n<p>Can it be that anti-deathism is a foul meme we acquired when technology 'robbed' us of 'natural' experience of death?</p>\n<p>With this I close the coping section and move on to the actual benefits.&nbsp;</p>\n<h2>Evolution of Aging</h2>\n<p>The Wikipedia article on aging states that&nbsp;</p>\n<blockquote>\n<p>The evolutionary origin of senescence remains a fundamental unsolved problem in biology.</p>\n</blockquote>\n<p>But gives some hints as to its origin: New results on the old <a href=\"http://en.wikipedia.org/wiki/Evolution_of_aging#Disposable_soma_theory\">disposable soma theory</a>&nbsp;and new <a href=\"http://en.wikipedia.org/wiki/Evolution_of_aging#Ageing_theories_based_on_group_selection\">group selection theories of aging</a>.</p>\n<p>Following up on that you can find that it is likely adaptive even if there is not yet consensus about this.</p>\n<p>For example after Joshua Mitteldorf has</p>\n<blockquote>\n<p><a href=\"http://mathforum.org/~josh/4OwnSake.pdf\">summarized</a> a diverse body of data indicating that senescence is an adaptation selected for its own sake.</p>\n</blockquote>\n<p>he <a href=\"http://mathforum.org/~josh/Epidemics-JTB.pdf\">goes on</a> to that</p>\n<blockquote>&nbsp;the proposed benefit is that senescence protects against infectious epidemics by controlling population density and increasing diversity of the host population.</blockquote>\n<p>and find evidence that</p>\n<blockquote>\n<p>Senescence bene\ufb01ts the rate of evolution, increases diversity, and shortens the effective generation time.</p>\n</blockquote>\n<p>Note that this biological argument also applies to memes.&nbsp;</p>\n<p>You can have 'infectious diseases' of the mind which in a technological society may dominate the biological effects.</p>\n<p>Applying this principle to science might mean that without death scienctific progress might go slower - something we have been already told:</p>\n<blockquote>\n<p>A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.</p>\n</blockquote>\n<p>(Max Planck in his Autobiography)</p>\n<h2>Risk Aversion and Mediocrity</h2>\n<p>This section gives my personal opinion on risk aversion in our society.</p>\n<p>Technological progress in the last century has worked hard on satisfying basic needs. What remains are complex social needs and existential fears.</p>\n<p>Fear of death has led to what I believe overly protecting children (and adults). For fear of injury or abuse children often no longer have the chance to&nbsp;</p>\n<ul>\n<li>aquire basic motor skills (balance, climbing, even running)</li>\n<li>follow their curiosity to explore (animals, chemical/physical experiments, geography)</li>\n<li>aquire social skills (talk to strangers, meet with friends)&nbsp;</li>\n<li>train immune defenses (playing in/eat dirt, pets)&nbsp;</li>\n</ul>\n<p>A comparable list could also be given for adults. Please feel free to comment on this.&nbsp;</p>\n<p>All of this protection surely leads to some (minor?) reduction of health risks. But all of this also leads to a reduction of efficiency. Some of this protection even pose other (longer term) health risks which are less salient (yet?). This is a promotion of mediocrity. Sometimes I think our society could benefit from a bit more harm. Wouldn't we value life more and make more out of it? &nbsp;</p>\n<p>Even if you do not agree with me on this one, maybe you do on the following.</p>\n<h2>Risk Aversion and Death</h2>\n<p>Sometimes it is necessary to&nbsp;<a href=\"/lw/uo/make_an_extraordinary_effort/\">Make an Extraordinary Effort</a>&nbsp;or even to&nbsp;<a href=\"/lw/up/shut_up_and_do_the_impossible/\">Shut up and do the impossible!</a>&nbsp;This implies setting aside some of your mental barriers. Barriers that protect you from danger, exhaustion and possibly death (not necessarily immediate but possibly speeding up senescence).</p>\n<p>Some say that there are areas where this may be necessary:</p>\n<blockquote>\n<p>if nobody [] dies for space exploration we are cheating humanity. We are just not trying hard enough to get off this planet and into space. I firmly believe that moving into space is really important to the future of my species. We are going to penetrate space and become a space-faring race or we are going to stagnate and pass on.\"</p>\n</blockquote>\n<p><a href=\"http://leepers.us/mtvoid/2003/VOID0207.htm\">http://leepers.us/mtvoid/2003/VOID0207.htm</a> (section Acceptable risk)</p>\n<blockquote>\n<p>They used to say \"if people are not dying, we're not trying hard enough\".&nbsp;</p>\n</blockquote>\n<p><a href=\"http://forum.nasaspaceflight.com/index.php?topic=31452.30\">http://forum.nasaspaceflight.com/index.php?topic=31452.30</a></p>\n<p>This may also apply for other human endeavors.</p>\n<h2>Death and Transhumanism</h2>\n<p>Now that we have reached the edge of human progress I want to drive my argument a bit beyond its applicability. The evolutionary biological benefit of senescence and death may not apply once humans can fully engineer biology. What if we \"if we knew more, thought faster, were more the people we wished we were\"? Does this stop the argument? Any group-benefit argument continues to apply if a population of distinct minds remains. If the minds incorporate mutual experience than the minds either converge to multiple identical minds or the minds maintain a difference in which case the group benefit argument may continue to hold.</p>\n<p>Independent of whether you want to avoid becoming identical to all other minds - being a single mind makes it a single point of failure. Death - of a certain kind - may be necessary even for parts of a super intelligence.&nbsp;</p>\n<h2>References</h2>\n<p>Mentioned above and some more:</p>\n<ul>\n<li>Wikipedia: <a href=\"http://en.wikipedia.org/wiki/Terror_management_theory\">Terror Management Theory</a>&nbsp;</li>\n<li><a href=\"http://www.psychologytoday.com/blog/the-big-questions/201206/how-we-cope-death\">How We Cope with Death, Nathan Heflick, 2012</a></li>\n<li><a href=\"http://link.springer.com/article/10.1007/s11013-007-9050-6#page-1\">Discourses of Loss and Bereavement in Tigray, Ethiopia, Dag Nordanger, 2007</a></li>\n<li><a href=\"http://www.researchgate.net/profile/Kate_Murray3/publication/232581337_Resilience_A_new_definition_of_health_for_people_and_communities/file/9fcfd51259d834e134.pdf\">Resilience: A New Definition of Health for People and Communities, Zautra 2010</a></li>\n<li><a href=\"http://www.ashgate.com/pdf/SamplePages/Mortality_Mourning_and_Mortuary_Practices_in_Indigenous_Australia_Intro.pdf\">Indigenous Ways of Death in Australia, V. Burbank et al&nbsp;</a></li>\n<li>Wikipedia: <a href=\"http://en.wikipedia.org/wiki/Evolution_of_aging\">Evolution of aging</a>&nbsp; &nbsp;</li>\n<li><a href=\"http://mathforum.org/~josh/4OwnSake.pdf\">Ageing selected for its own sake, Joshua Mitteldorf, 2004</a></li>\n<li><a href=\"http://mathforum.org/~josh/Epidemics-JTB.pdf\">Senescence as an adaptation to limit the spread of disease, Joshua Mitteldorf, 2009</a></li>\n<li>More from Mitteldorf <a href=\"http://www.programmed-aging.org/theory-3/mitteldorf.html\">here</a></li>\n<li><a href=\"http://dspace.library.drexel.edu/bitstream/1860/4110/1/Kriete-2013.pdf\">Robustness and Aging &ndash; A Systems-Level Perspective, Kriete 2013</a> <span style=\"white-space:pre\"> </span></li>\n<li><a href=\"http://books.google.de/books/about/Death_Hope_and_Sex.html?id=GRa4WbuQC5EC&amp;redir_esc=y\">Death, Hope and Sex Chisholm 1999</a></li>\n</ul>\n<p>For background you might consult the&nbsp;<a href=\"/lw/ii5/baseline_of_my_opinion_on_lw_topics/\">Baseline of my opinion on LW topics</a>.&nbsp;</p>\n<h2>Summary</h2>\n<p>For the TLDR crowd:</p>\n<p>Humans have powerful mental adaptations to cope with death/loss (they often actually learn from it and get out of it stronger).&nbsp;</p>\n<p>Death/senescence/loss is adaptive for the group providing real benefits an utilitarian should see and build on.</p>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fn8Kru2kaGRZCGTKx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": -12, "extendedScore": null, "score": -3.3e-05, "legacy": true, "legacyId": "25513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>EDIT: Incorporated suggestions from comments: Moved off-topic parts into comments, improved formatting, corrected links.</p>\n<h2 id=\"Definition\">Definition</h2>\n<p>The LW post <a href=\"/lw/2zj/value_deathism/\">Value Deathism</a> differntiates between the illusory nature of death and the 'desirability' of death called deathism proper. This post is about the latter. Where desirability is meant in a general sense and not (only) in the sense of desirable for an individual.</p>\n<p>I propose a different more neutral term for deathism: <strong>Senexism</strong> - from the latin adjective senex - old. &nbsp;I propose this because death is only the end of an aging process and by focussing on the ultimate and emotionally disturbing result one loads the topic with negative connotations. Senescence on the other hand - though unwanted - has also positive connotations of experience and humility. This also nicely splits off (or reduces applicability of) death by accident.&nbsp;</p>\n<h2 id=\"Outline\">Outline</h2>\n<p>My defense is twofold. First I address the (emotional) pain and loss death causes and point out adaptive affects of the coping mechanisms humans have. Second I address the actual benefits senescence and death has - not for the individual but for the group. Thus the latter is an utilitarian argument for death actually.</p>\n<p>I will provide current research results for these points. At the end I will conclude with an opinion piece on what this means for rationalists and an outlook how this applies in light of the singularity.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Fear_of_Death\">Fear of Death</h2>\n<p>How does (fear of) death affect you?</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Terror_management_theory\">Terror Management Theory</a>&nbsp;(TMT) posits that</p>\n<blockquote>\n<p>people cope with mortality by creating beliefs and values that promise a sense of immortality.</p>\n<p>And research supports the premise that these beliefs are</p>\n<p>(a) defended more when people are reminded of death and</p>\n<p>(b) protect people from mortality concerns.</p>\n</blockquote>\n<p>Some more scientifically validated claims of TMT are (nicely presented by&nbsp;<a href=\"http://www.psychologytoday.com/blog/the-big-questions/201206/how-we-cope-death\">psychology today</a>):</p>\n<blockquote>\n<p>Death reminders cause people to&nbsp;self-enhance and protect self-esteem, such as by agreeing more with positive feedback and taking more credit for success and identify more with members of their own group, and even to rate them as more unique from other animals.</p>\n</blockquote>\n<p>One can see this even here on LW e.g. in links from <a href=\"http://wiki.lesswrong.com/wiki/Death\">Death</a>&nbsp;and also in the defenses of cryonics - which <a href=\"/lw/jfc/link_how_do_good_ideas_spread/aa6p\"><em>look</em> like an afterlife meme</a>.&nbsp;</p>\n<p>Applied to this post this means that you are likely to&nbsp;</p>\n<blockquote>\n<p>1) defend [your] cultural worldviews more strongly.&nbsp;</p>\n</blockquote>\n<p>(here e.g. denial of death via cryonics) thus I objectively risk karma.</p>\n<p>This is the reason I started this post with a positive confirmation. I hacked you dammit. I used this fact:&nbsp;</p>\n<blockquote>\n<p>9) Defending any of these things (relationships, beliefs, etc) prior to being reminded of death, or taking away people's anxiety, reduces the effects that mortality thoughts have.</p>\n</blockquote>\n<p>Western thinking of coping with death is confused with beliefs of coping with death. Probably due to the above effect itself.&nbsp;</p>\n<p>We seem to believe that (when you read this ask yourself: Do you agree with this?)</p>\n<blockquote><ol>\n<li>Bereaved persons are expected to exhibit significant distress following a major loss, and the failure to experience such distress tends to be seen as indicative of a problem.</li>\n<li>Positive emotions are implicitly assumed to be absent during this period. If they are expressed, they tend to be viewed as an indication that people are denying or covering up their distress.</li>\n<li>Following the loss of a loved one, the bereaved must confront and \u201cwork through\u201d their feelings about the loss. Efforts to avoid or deny feelings are maladaptive in the long run.</li>\n<li>It is important for the bereaved to break down their attachment to the deceased loved one.</li>\n<li>Within a year or two, the bereaved will be able to come to terms with what has happened, recover from the loss, and resume their earlier level of functioning.</li>\n</ol></blockquote>\n<p>Do you agree?</p>\n<p>Yes?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>No! These are all&nbsp;<a href=\"http://www.psychology.sunysb.edu/cwortman-/papers/Wortman_Boerner.pdf\">Myths of coping with death</a>!</p>\n<p>It is true that</p>\n<blockquote>\n<p>Unlike many stressful life experiences, bereavement cannot be altered by the coping efforts of survivors. Indeed, the major coping task faced by the bereaved is to reconcile themselves to a situation that cannot be changed and find a way to carry on with their own lives.</p>\n</blockquote>\n<p>But this doesn't mean that is must always hurt and take long.</p>\n<h2 id=\"Biases_and_Death\">Biases and Death</h2>\n<p>Thus from our society and being human we are bound to believe that (we should believe that) death is horrible and we should suffer from encountering it. &nbsp;</p>\n<p>For an efficiently working brain (that is set on the track of avoiding death at all cost) it is not hard to spot patterns that support the view that death is only bad.</p>\n<p>This means that among all topics you are most likely to fall prey to one bias or other with respect to death memes e.g. &nbsp;</p>\n<ul>\n<li>availability heuristics (arguments against death are mucb more available obviously)</li>\n<li>confirmation bias (you already believe death to be bad)&nbsp;</li>\n<li>belief bias and wishful thinking (you want death to be bad)</li>\n<li>attentional bias (thought of death can be salient; they are actually continuous repeated by the media)</li>\n<li>and even illusion of control (the belief that you can cheat death)</li>\n</ul>\n<p>There are probably lots others. Take finding them as a homework (or chance for a comment).</p>\n<h2 id=\"Coping_with_Death_Adaptively\">Coping with Death Adaptively</h2>\n<p>But death and loss may not be as devasting as you make it.</p>\n<p>In particular according to&nbsp;<a href=\"http://link.springer.com/article/10.1007/s11013-007-9050-6#page-1\">Nordanger</a>&nbsp;(2007)</p>\n<blockquote>\n<p>Epidemiological studies indicate that the majority of trauma survivors recover from initial posttraumatic reactions without professional help and their posttraumatic adjustment may be facilitated by indigenous coping resources and socioeconomic structures such as traditional healers, traditions and rituals.</p>\n</blockquote>\n<p>and <a href=\"http://www.researchgate.net/profile/Kate_Murray3/publication/232581337_Resilience_A_new_definition_of_health_for_people_and_communities/file/9fcfd51259d834e134.pdf\">Zautra</a> 2010</p>\n<blockquote>\n<p>It would be most consistent with what we observe in human communities to see resilience as a natural capacity to recover and perhaps even further one\u2019s adaptive capacities</p>\n</blockquote>\n<p>I also understand that indigenous tribes which are more acutely affected by harm and death do not suffer the same way from it we do.&nbsp;</p>\n<p>Can it be that anti-deathism is a foul meme we acquired when technology 'robbed' us of 'natural' experience of death?</p>\n<p>With this I close the coping section and move on to the actual benefits.&nbsp;</p>\n<h2 id=\"Evolution_of_Aging\">Evolution of Aging</h2>\n<p>The Wikipedia article on aging states that&nbsp;</p>\n<blockquote>\n<p>The evolutionary origin of senescence remains a fundamental unsolved problem in biology.</p>\n</blockquote>\n<p>But gives some hints as to its origin: New results on the old <a href=\"http://en.wikipedia.org/wiki/Evolution_of_aging#Disposable_soma_theory\">disposable soma theory</a>&nbsp;and new <a href=\"http://en.wikipedia.org/wiki/Evolution_of_aging#Ageing_theories_based_on_group_selection\">group selection theories of aging</a>.</p>\n<p>Following up on that you can find that it is likely adaptive even if there is not yet consensus about this.</p>\n<p>For example after Joshua Mitteldorf has</p>\n<blockquote>\n<p><a href=\"http://mathforum.org/~josh/4OwnSake.pdf\">summarized</a> a diverse body of data indicating that senescence is an adaptation selected for its own sake.</p>\n</blockquote>\n<p>he <a href=\"http://mathforum.org/~josh/Epidemics-JTB.pdf\">goes on</a> to that</p>\n<blockquote>&nbsp;the proposed benefit is that senescence protects against infectious epidemics by controlling population density and increasing diversity of the host population.</blockquote>\n<p>and find evidence that</p>\n<blockquote>\n<p>Senescence bene\ufb01ts the rate of evolution, increases diversity, and shortens the effective generation time.</p>\n</blockquote>\n<p>Note that this biological argument also applies to memes.&nbsp;</p>\n<p>You can have 'infectious diseases' of the mind which in a technological society may dominate the biological effects.</p>\n<p>Applying this principle to science might mean that without death scienctific progress might go slower - something we have been already told:</p>\n<blockquote>\n<p>A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.</p>\n</blockquote>\n<p>(Max Planck in his Autobiography)</p>\n<h2 id=\"Risk_Aversion_and_Mediocrity\">Risk Aversion and Mediocrity</h2>\n<p>This section gives my personal opinion on risk aversion in our society.</p>\n<p>Technological progress in the last century has worked hard on satisfying basic needs. What remains are complex social needs and existential fears.</p>\n<p>Fear of death has led to what I believe overly protecting children (and adults). For fear of injury or abuse children often no longer have the chance to&nbsp;</p>\n<ul>\n<li>aquire basic motor skills (balance, climbing, even running)</li>\n<li>follow their curiosity to explore (animals, chemical/physical experiments, geography)</li>\n<li>aquire social skills (talk to strangers, meet with friends)&nbsp;</li>\n<li>train immune defenses (playing in/eat dirt, pets)&nbsp;</li>\n</ul>\n<p>A comparable list could also be given for adults. Please feel free to comment on this.&nbsp;</p>\n<p>All of this protection surely leads to some (minor?) reduction of health risks. But all of this also leads to a reduction of efficiency. Some of this protection even pose other (longer term) health risks which are less salient (yet?). This is a promotion of mediocrity. Sometimes I think our society could benefit from a bit more harm. Wouldn't we value life more and make more out of it? &nbsp;</p>\n<p>Even if you do not agree with me on this one, maybe you do on the following.</p>\n<h2 id=\"Risk_Aversion_and_Death\">Risk Aversion and Death</h2>\n<p>Sometimes it is necessary to&nbsp;<a href=\"/lw/uo/make_an_extraordinary_effort/\">Make an Extraordinary Effort</a>&nbsp;or even to&nbsp;<a href=\"/lw/up/shut_up_and_do_the_impossible/\">Shut up and do the impossible!</a>&nbsp;This implies setting aside some of your mental barriers. Barriers that protect you from danger, exhaustion and possibly death (not necessarily immediate but possibly speeding up senescence).</p>\n<p>Some say that there are areas where this may be necessary:</p>\n<blockquote>\n<p>if nobody [] dies for space exploration we are cheating humanity. We are just not trying hard enough to get off this planet and into space. I firmly believe that moving into space is really important to the future of my species. We are going to penetrate space and become a space-faring race or we are going to stagnate and pass on.\"</p>\n</blockquote>\n<p><a href=\"http://leepers.us/mtvoid/2003/VOID0207.htm\">http://leepers.us/mtvoid/2003/VOID0207.htm</a> (section Acceptable risk)</p>\n<blockquote>\n<p>They used to say \"if people are not dying, we're not trying hard enough\".&nbsp;</p>\n</blockquote>\n<p><a href=\"http://forum.nasaspaceflight.com/index.php?topic=31452.30\">http://forum.nasaspaceflight.com/index.php?topic=31452.30</a></p>\n<p>This may also apply for other human endeavors.</p>\n<h2 id=\"Death_and_Transhumanism\">Death and Transhumanism</h2>\n<p>Now that we have reached the edge of human progress I want to drive my argument a bit beyond its applicability. The evolutionary biological benefit of senescence and death may not apply once humans can fully engineer biology. What if we \"if we knew more, thought faster, were more the people we wished we were\"? Does this stop the argument? Any group-benefit argument continues to apply if a population of distinct minds remains. If the minds incorporate mutual experience than the minds either converge to multiple identical minds or the minds maintain a difference in which case the group benefit argument may continue to hold.</p>\n<p>Independent of whether you want to avoid becoming identical to all other minds - being a single mind makes it a single point of failure. Death - of a certain kind - may be necessary even for parts of a super intelligence.&nbsp;</p>\n<h2 id=\"References\">References</h2>\n<p>Mentioned above and some more:</p>\n<ul>\n<li>Wikipedia: <a href=\"http://en.wikipedia.org/wiki/Terror_management_theory\">Terror Management Theory</a>&nbsp;</li>\n<li><a href=\"http://www.psychologytoday.com/blog/the-big-questions/201206/how-we-cope-death\">How We Cope with Death, Nathan Heflick, 2012</a></li>\n<li><a href=\"http://link.springer.com/article/10.1007/s11013-007-9050-6#page-1\">Discourses of Loss and Bereavement in Tigray, Ethiopia, Dag Nordanger, 2007</a></li>\n<li><a href=\"http://www.researchgate.net/profile/Kate_Murray3/publication/232581337_Resilience_A_new_definition_of_health_for_people_and_communities/file/9fcfd51259d834e134.pdf\">Resilience: A New Definition of Health for People and Communities, Zautra 2010</a></li>\n<li><a href=\"http://www.ashgate.com/pdf/SamplePages/Mortality_Mourning_and_Mortuary_Practices_in_Indigenous_Australia_Intro.pdf\">Indigenous Ways of Death in Australia, V. Burbank et al&nbsp;</a></li>\n<li>Wikipedia: <a href=\"http://en.wikipedia.org/wiki/Evolution_of_aging\">Evolution of aging</a>&nbsp; &nbsp;</li>\n<li><a href=\"http://mathforum.org/~josh/4OwnSake.pdf\">Ageing selected for its own sake, Joshua Mitteldorf, 2004</a></li>\n<li><a href=\"http://mathforum.org/~josh/Epidemics-JTB.pdf\">Senescence as an adaptation to limit the spread of disease, Joshua Mitteldorf, 2009</a></li>\n<li>More from Mitteldorf <a href=\"http://www.programmed-aging.org/theory-3/mitteldorf.html\">here</a></li>\n<li><a href=\"http://dspace.library.drexel.edu/bitstream/1860/4110/1/Kriete-2013.pdf\">Robustness and Aging \u2013 A Systems-Level Perspective, Kriete 2013</a> <span style=\"white-space:pre\"> </span></li>\n<li><a href=\"http://books.google.de/books/about/Death_Hope_and_Sex.html?id=GRa4WbuQC5EC&amp;redir_esc=y\">Death, Hope and Sex Chisholm 1999</a></li>\n</ul>\n<p>For background you might consult the&nbsp;<a href=\"/lw/ii5/baseline_of_my_opinion_on_lw_topics/\">Baseline of my opinion on LW topics</a>.&nbsp;</p>\n<h2 id=\"Summary\">Summary</h2>\n<p>For the TLDR crowd:</p>\n<p>Humans have powerful mental adaptations to cope with death/loss (they often actually learn from it and get out of it stronger).&nbsp;</p>\n<p>Death/senescence/loss is adaptive for the group providing real benefits an utilitarian should see and build on.</p>\n<div><br></div>", "sections": [{"title": "Definition", "anchor": "Definition", "level": 1}, {"title": "Outline", "anchor": "Outline", "level": 1}, {"title": "Fear of Death", "anchor": "Fear_of_Death", "level": 1}, {"title": "Biases and Death", "anchor": "Biases_and_Death", "level": 1}, {"title": "Coping with Death Adaptively", "anchor": "Coping_with_Death_Adaptively", "level": 1}, {"title": "Evolution of Aging", "anchor": "Evolution_of_Aging", "level": 1}, {"title": "Risk Aversion and Mediocrity", "anchor": "Risk_Aversion_and_Mediocrity", "level": 1}, {"title": "Risk Aversion and Death", "anchor": "Risk_Aversion_and_Death", "level": 1}, {"title": "Death and Transhumanism", "anchor": "Death_and_Transhumanism", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "89 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GMyjNQe5ZgkXJChbg", "GuEsfTpSDSbXFiseH", "nCvvhFBaayaXyuBiD", "tHv3gb3cwPg5EJxSx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-16T23:46:40.591Z", "modifiedAt": null, "url": null, "title": "Meetup report: London LW paranoid debating session", "slug": "meetup-report-london-lw-paranoid-debating-session", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:40.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/maw5DkXSNdihAWbcJ/meetup-report-london-lw-paranoid-debating-session", "pageUrlRelative": "/posts/maw5DkXSNdihAWbcJ/meetup-report-london-lw-paranoid-debating-session", "linkUrl": "https://www.lesswrong.com/posts/maw5DkXSNdihAWbcJ/meetup-report-london-lw-paranoid-debating-session", "postedAtFormatted": "Sunday, February 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20report%3A%20London%20LW%20paranoid%20debating%20session&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20report%3A%20London%20LW%20paranoid%20debating%20session%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmaw5DkXSNdihAWbcJ%2Fmeetup-report-london-lw-paranoid-debating-session%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20report%3A%20London%20LW%20paranoid%20debating%20session%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmaw5DkXSNdihAWbcJ%2Fmeetup-report-london-lw-paranoid-debating-session", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmaw5DkXSNdihAWbcJ%2Fmeetup-report-london-lw-paranoid-debating-session", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 916, "htmlBody": "<p><img src=\"http://reasonableapproximation.net/images/lw-meetup-20140209-photo.jpg\" alt=\"photo\" width=\"685\" height=\"512\" /> <em></em></p>\n<p><em>A photo from a (different) recent LW London meetup</em></p>\n<p><em><a href=\"http://reasonableapproximation.net/2014/02/16/london-lw-paranoid-debating.html\">Cross-posted from my blog.</a><br /></em></p>\n<p>I wasn't going to bother writing this up, but then I remembered it's important to publish negative results.</p>\n<p>LessWrong London played a few rounds of paranoid debating at our meetup on 02/02/14. I'm not sure we got too much from the experience, except that it was fun. (I enjoyed it, at any rate.)</p>\n<p>There were nine of us, which was unwieldy, so we split into two groups. Our first questions probably weren't very good: we wanted the height of the third-highest mountain in the world, and the length of the third-longest river. (The groups had different questions so that someone on the other group could verify that the answer was well-defined and easy to discover. I had intended to ask \"tallness of the third-tallest mountain\", but the wikipedia page I found sorted by height, so I went with that.)</p>\n<p>I was on the \"river\" question, and we did pretty badly. None of us really knew what ballpark we were searching in. I made the mistake of saying an actual number that was in my head but I didn't know where from and I didn't trust it, that the longest river was something like 1,800 miles long. Despite my unconfidence, we became anchored there. Someone else suggested that the thing to do would be to take a quarter of the circumference of earth (which comes to 6,200 miles) as a baseline and adjust for the fact that rivers wiggle. I thought, that's crazy, you must be the mole. I think I answered 1500 miles.</p>\n<p>In reality, the longest river is 4,100 miles, the third longest is 3900 miles, and the mole decided that 1800 was dumb and he didn't need to do anything to sabotage us. (I don't remember what the circumference-over-four person submitted. I have a recollection that he was closer than I was, but I also had a recollection that circumference-over-four was actually pretty close, which it isn't especially.)</p>\n<p>The other team did considerably better, getting answers in the 8,000s for a true answer of 8,600.</p>\n<p>I'd devised a scoring system, where every player submits their own answer, and non-moles score proportional to -|log(given answer / actual answer)|; the mole scores the negative mean of everyone else's score. But after calculating it for a few people, we decided we didn't really care, and we probably wouldn't be playing enough rounds for it to become meaningful.</p>\n<p>Those questions weren't so great, because we felt there wasn't much you could do to approach them beyond having some idea of the correct answer. For round two we tried to pick questions more amenable to Fermi estimates: annual U.S. electricity consumption (sourced from Wolfram Alpha), and the number of pennies that could fit inside St. Paul's Cathedral. This round, we gave the correct answers to the moles.</p>\n<p>I was on team Cathedral, and again did pretty badly. We started by measuring pennies using notepaper for scale, and calculating packing densities, to work out pennies per cubic metre. (I don't remember the answer, but we got pretty close.) But after that it was just a matter of knowing how large St. Paul's Cathedral was likely to be.</p>\n<p>I had been stood outside St. Paul's Cathedral a few weeks back, but mostly facing in the wrong direction while tourists took photos of the group I was with. From that vague recollection I thought maybe it was about a thousand metres square at the base, and four stories so about twelve metres high? (Later I looked at a picture of the Cathedral, and realized that I was probably thinking of the dimensions of the entrance hall.) Someone else, who had actually been inside the cathedral, was giving much higher numbers, especially for the base, and someone else was agreeing with his general ballpark. And the other people weren't saying much, so I figured one of those two had to be the mole, and decided to not update very much in that direction. Those numbers were pretty reasonable, and mine were pretty bad. One of them was the mole (not the one I most suspected); I don't remember what he said his strategy was.</p>\n<p>Again, I'm not too sure it was a great question; pennies-per-cubic-metre is a question of geometry rather than estimation, and the interior dimensions of St. Paul's Cathedral don't seem much more Fermi estimable than the river question.</p>\n<p>The other team got very close to the answer I'd given the mole. Apparently they actually came up with a number off the top of someone's head that was pretty damn close. Embarassingly, the answer I gave the mole was an order of magnitude too high.... I'd sourced it from Wolfram Alpha ahead of time, but then someone asked me to clarify whether it was total energy usage, or just electricity usage. I looked again to check, and I think I used a different query, saw that it said \"electricity usage\", and didn't see that the number was different. The answer I actually gave was for energy usage.</p>\n<p>The mole on that team reported that it wasn't really helpful to know the correct answer without any intermediate steps. That mechanic might be worth experimenting further, but currently I think it doesn't add much, and it's a mild extra hassle when setting up.</p>\n<p>I had fun, and hopefully in future I will put much less trust in my estimates of the dimensions of things, but I wouldn't say the session was a particular success. Not a failure either, just kind of \"it happened\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "maw5DkXSNdihAWbcJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "25515", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-17T01:07:45.973Z", "modifiedAt": null, "url": null, "title": "Meta: social influence bias and the karma system", "slug": "meta-social-influence-bias-and-the-karma-system", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:59.262Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "9rEm7xdDZvArDyWgo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7Hq4SNdDBcYJo9jjT/meta-social-influence-bias-and-the-karma-system", "pageUrlRelative": "/posts/7Hq4SNdDBcYJo9jjT/meta-social-influence-bias-and-the-karma-system", "linkUrl": "https://www.lesswrong.com/posts/7Hq4SNdDBcYJo9jjT/meta-social-influence-bias-and-the-karma-system", "postedAtFormatted": "Monday, February 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%3A%20social%20influence%20bias%20and%20the%20karma%20system&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%3A%20social%20influence%20bias%20and%20the%20karma%20system%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Hq4SNdDBcYJo9jjT%2Fmeta-social-influence-bias-and-the-karma-system%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%3A%20social%20influence%20bias%20and%20the%20karma%20system%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Hq4SNdDBcYJo9jjT%2Fmeta-social-influence-bias-and-the-karma-system", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Hq4SNdDBcYJo9jjT%2Fmeta-social-influence-bias-and-the-karma-system", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 536, "htmlBody": "<p>Given LW&rsquo;s keen interest in bias, it would seem pertinent to be aware of the biases engendered by the karma system. Note: I used to be strictly opposed to comment scoring mechanisms, but witnessing the general effectiveness in which LWers use karma has largely redeemed the system for me.</p>\n<p>In <a href=\"http://web.natur.cuni.cz/~houdek3/papers/Muchnik%20et%20al%202013.pdf\">&ldquo;Social Influence Bias: A Randomized Experiment&rdquo;</a> by Muchnik <em>et al</em>, random comments on a &ldquo;social news aggregation Web site&rdquo; were up-voted after being posted. The likelihood of such rigged comments receiving additional up-votes were quantified in comparison to a control group. The results show that users were significantly biased towards the randomly up-voted posts:</p>\n<blockquote>The up-vote treatment significantly increased the probability of up-voting by the first viewer by 32% over the control group ... Uptreated comments were not down-voted significantly more or less frequently than the control group, so users did not tend to correct the upward manipulation. In the absence of a correction, positive herding accumulated over time.</blockquote>\n<p>At the end of their five month testing period, the comments that had artificially received an up-vote had an average rating 25% higher than the control group. Interestingly, the severity of the bias was largely dependent on the topic of discussion:</p>\n<blockquote>We found significant positive herding effects for comment ratings in &ldquo;politics,&rdquo; &ldquo;culture and society,&rdquo; and &ldquo;business,&rdquo; but no detectable herding behavior for comments in &ldquo;economics,&rdquo; &ldquo;IT,&rdquo; &ldquo;fun,&rdquo; and &ldquo;general news&rdquo;.</blockquote>\n<p>The herding behavior outlined in the paper seems rather intuitive to me. If before I read a post, I see a little green &lsquo;1&rsquo; next to it, I&rsquo;m probably going to read the post in a better light than if I hadn't seen that little green &lsquo;1&rsquo; next to it. Similarly, if I see a post that has a negative score, I&rsquo;ll probably see flaws in it much more readily. One might say that this is <em>the point of the rating system</em>, as it allows the group as a whole to evaluate the content. However, I&rsquo;m still unsettled by just how easily popular opinion was swayed in the experiment.</p>\n<p>This certainly doesn't necessitate that we reprogram the site and eschew the karma system. Moreover, understanding the biases inherent in such a system will allow us to use it much more effectively. Discussion on how this bias affects LW in particular would be welcomed. Here are some questions to begin with:</p>\n<ul>\n<li>Should we worry about this bias at all? Are its effects negligible in the scheme of things?</li>\n<li>How does the culture of LW contribute to this herding behavior? Is it positive or negative?</li>\n<li>If there are damages, how can we mitigate them?</li>\n</ul>\n<h4>Notes:</h4>\n<p>In the paper, they mentioned that comments were not sorted by popularity, therefore &ldquo;mitigating the selection bias.&rdquo; This of course implies that the bias would be more severe on forums where comments are sorted by popularity, such as this one.</p>\n<p>For those interested, another enlightening paper is <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2369332\">&ldquo;Overcoming the J-shaped distribution of product reviews&rdquo;</a> by Nan Hu <em>et al</em>, which discusses rating biases on websites such as amazon. User <a href=\"/user/gwern/\">gwern</a> has also recommended a longer 2007 paper by the same authors which the one above is based upon: <a href=\"http://www.researchgate.net/publication/228604596_Why_do_online_product_reviews_have_a_j-shaped_distribution_overcoming_biases_in_online_word-of-mouth_communication\">\"Why do Online Product Reviews have a J-shaped Distribution? Overcoming Biases in Online Word-of-Mouth Communication\"</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7Hq4SNdDBcYJo9jjT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 1.5676812206379605e-06, "legacy": true, "legacyId": "25517", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-17T03:19:51.622Z", "modifiedAt": null, "url": null, "title": "Meetup Report Thread: February 2014", "slug": "meetup-report-thread-february-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WHPnsAQCEiruKN5QN/meetup-report-thread-february-2014", "pageUrlRelative": "/posts/WHPnsAQCEiruKN5QN/meetup-report-thread-february-2014", "linkUrl": "https://www.lesswrong.com/posts/WHPnsAQCEiruKN5QN/meetup-report-thread-february-2014", "postedAtFormatted": "Monday, February 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Report%20Thread%3A%20February%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Report%20Thread%3A%20February%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHPnsAQCEiruKN5QN%2Fmeetup-report-thread-february-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Report%20Thread%3A%20February%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHPnsAQCEiruKN5QN%2Fmeetup-report-thread-february-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHPnsAQCEiruKN5QN%2Fmeetup-report-thread-february-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>A month ago, a new type of thread was proposed: <a href=\"/lw/jh2/new_proposal_for_monthly_thread_meetup_reports/\">a monthly page for meetup reports</a>. The idea is that meetup attendees, or organizers, who wanted to share information about how the meetup went could do so in the comments of this thread. This is so information is dispersed, but without the need for anyone, and/or everyone, to dedicate their own thread to the report. The idea worked for January, and nobody had objections. So, we'll do this every month.</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">If you had an interesting Less Wrong meetup recently, but don't have the time to write up a big report to post to Discussion, feel free to write a comment here.&nbsp; Even if it's just a couple lines about what you did and how people felt about it, it might encourage some people to attend meetups or start meetups in their area.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">If you have the time, you can also describe what types of exercises you did, what worked and what didn't.&nbsp; This could help inspire meetups to try new things and improve themselves in various ways.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">If you're inspired by what's posted below and want to organize a meetup, check out&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\" target=\"_blank\">this page</a>&nbsp;for some resources to get started!&nbsp; You can also check FrankAdamek's weekly post on meetups for the week</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WHPnsAQCEiruKN5QN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 1.5678349597388225e-06, "legacy": true, "legacyId": "25525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nHcuKaEPvFdJgpex4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-17T05:34:51.540Z", "modifiedAt": null, "url": null, "title": "[Open Thread] Stupid Questions (2014-02-17)", "slug": "open-thread-stupid-questions-2014-02-17", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:09.784Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "solipsist", "createdAt": "2013-06-09T21:07:31.678Z", "isAdmin": false, "displayName": "solipsist"}, "userId": "PMDZAtysvt35Mni2a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ETHJmLGfnFR3wrDLw/open-thread-stupid-questions-2014-02-17", "pageUrlRelative": "/posts/ETHJmLGfnFR3wrDLw/open-thread-stupid-questions-2014-02-17", "linkUrl": "https://www.lesswrong.com/posts/ETHJmLGfnFR3wrDLw/open-thread-stupid-questions-2014-02-17", "postedAtFormatted": "Monday, February 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BOpen%20Thread%5D%20Stupid%20Questions%20(2014-02-17)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BOpen%20Thread%5D%20Stupid%20Questions%20(2014-02-17)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FETHJmLGfnFR3wrDLw%2Fopen-thread-stupid-questions-2014-02-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BOpen%20Thread%5D%20Stupid%20Questions%20(2014-02-17)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FETHJmLGfnFR3wrDLw%2Fopen-thread-stupid-questions-2014-02-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FETHJmLGfnFR3wrDLw%2Fopen-thread-stupid-questions-2014-02-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p class=\"p1\"><span class=\"s1\"><em>This is part of a&nbsp;</em><a href=\"/r/discussion/lw/jo3/meta_open_thread_even_more_often/\"><span class=\"s2\"><em>two-week experiment on having more open threads</em></span></a><em>.</em></span></p>\n<p class=\"p2\">Obvious answers aren't always obvious. &nbsp;If you feel silly for not understanding something, you're not alone. &nbsp;Ask a question here.</p>\n<p class=\"p2\">Previous <a href=\"/lw/jh8/stupid_questions_thread_january_2014/\">stupid questions</a></p>\n<p class=\"p2\">Other similar threads include:</p>\n<ul class=\"ul1\">\n<li class=\"li3\"><a href=\"/r/discussion/lw/jnx/open_thread_for_february_11_17/\"><span class=\"s2\">Open Thread</span></a></li>\n<li class=\"li3\"><a href=\"/lw/jlu/february_2014_media_thread/\"><span class=\"s2\">Media recommendations</span></a></li>\n<li class=\"li4\"><a href=\"/lw/joa/open_thread_links_20140214/\">Links</a></li>\n<li class=\"li4\">Advice (Not yet posted)</li>\n<li class=\"li3\"><a href=\"http://wiki.lesswrong.com/wiki/Special_threads\"><span class=\"s2\">Other Special Threads</span></a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ETHJmLGfnFR3wrDLw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "25524", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 140, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qk6ArDxwC5S9HgTdZ", "XxJDvPyqDN47uttCR", "jD5yw654riqqEiyr3", "fJTHc6samuJwHSzXe", "MWqhF9TH7gHQsGyhe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-17T07:17:19.181Z", "modifiedAt": null, "url": null, "title": "Steelmanning Young Earth Creationism", "slug": "steelmanning-young-earth-creationism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:59.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YCYPKwFrxz94KppwK/steelmanning-young-earth-creationism", "pageUrlRelative": "/posts/YCYPKwFrxz94KppwK/steelmanning-young-earth-creationism", "linkUrl": "https://www.lesswrong.com/posts/YCYPKwFrxz94KppwK/steelmanning-young-earth-creationism", "postedAtFormatted": "Monday, February 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Steelmanning%20Young%20Earth%20Creationism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASteelmanning%20Young%20Earth%20Creationism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCYPKwFrxz94KppwK%2Fsteelmanning-young-earth-creationism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Steelmanning%20Young%20Earth%20Creationism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCYPKwFrxz94KppwK%2Fsteelmanning-young-earth-creationism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCYPKwFrxz94KppwK%2Fsteelmanning-young-earth-creationism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1019, "htmlBody": "<p>In a recent discussion of steelmanning, I&nbsp;<a href=\"/lw/jhb/dangers_of_steelmanning_principle_of_charity/adg0\">observed</a>&nbsp;that few people around here seem interested in steelmanning young earth creationism. In the resulting subthread, someone suggested&nbsp;<a href=\"http://squid314.livejournal.com/327646.html\">Yvain's steelman</a>&nbsp;of the&nbsp;<a href=\"http://www.timecube.com/\">Time Cube</a>&nbsp;as illustrating what a steelman of of young earth creationism might look like. But steelmanning the Time Cube may be too much of a stretch, it's a little&nbsp;<em>too&nbsp;</em>incoherent to steelman effectively without changing it into something else entirely. In contrast, once I thought about it it wasn't hard to come up with some ways to steelman young earth creationism in a way that was very much in keeping with the spirit of real young earth creationist writings.</p>\n<p>The way to do it, I think, is to approach it from a philosophy of science angle. For example, here's a quote from an article titled <a href=\"http://www.answersingenesis.org/articles/cm/v22/n1/creation-proof\">\"Creation: 'Where's the Proof?'\"</a>&nbsp;on Answers in Genesis, a website run by young earth creationist Ken Ham (who recently debated science guy Bill Nye):</p>\n<blockquote>\n<p>Creationists and evolutionists, Christians and non-Christians all have the same evidence&mdash;the same facts. Think about it: we all have the same earth, the same fossil layers, the same animals and plants, the same stars&mdash;the facts are all the same.</p>\n<p>The difference is in the way we all interpret the facts. And why do we interpret facts differently? Because we start with different presuppositions. These are things that are assumed to be true, without being able to prove them. These then become the basis for other conclusions. All reasoning is based on presuppositions (also called axioms). This becomes especially relevant when dealing with past events.</p>\n</blockquote>\n<p>You find a lot of this kind of thing on Answers in Genesis. For example, they're willing to concede that on certain assumptions, <a href=\"http://www.answersingenesis.org/articles/nab/does-c14-disprove-the-bible\">radiometric dating</a>&nbsp;is a strong argument that the earth is a lot more than 10,000 years old, but they deny that they need to accept those assumptions.</p>\n<p>I honestly don't think it's much of a stretch to steelman this into something that would look a lot like some of the things the philosophers of science I studied in graduate school said. I started writing a long post spelling this out, but then I started worrying I was going too far in playing <a href=\"/lw/r3/against_devils_advocacy/\">devil's advocacy</a> for creationism (even for an exercise in exploring the weaknesses of steelmanning). So instead, I'll just mention some places too look for material in such a project: The <a href=\"http://en.wikipedia.org/wiki/Quine-Duhem_thesis\">Duhem-Quine thesis</a>, <a href=\"http://en.wikipedia.org/wiki/Confirmation_holism\">confirmation holism</a>, <a href=\"http://plato.stanford.edu/entries/scientific-underdetermination/\">underdetermination of scientific theory</a>. Fun fact: Pierre Duhem regarded the existence of atoms as a metaphysical question that could not be settled by experiment, and this has not stopped him from being regarded as an important contributor to philosophy of science. I suppose&nbsp;<a href=\"http://www.calpoly.edu/~fotoole/321.1/feyer.html\">Feyerabend</a>&nbsp;belongs on the list too, but that's almost too easy (even though, yes, I did have to study Feyerabend in grad school).</p>\n<p>Oh, and I could even find material for my steelmanning of young earth creationism in the writings of Robert Pennock, a philosopher of science who testified against Intelligent Design at the <em>Dover </em>trial. Some philosophers, while thinking creationism is dead wrong, have criticized the reasoning used in that and other court decisions that have kept anti-evolutionism out of public schools in the US. Pennock wrote a response, titled <a href=\"http://ge.tt/4dsfg7K1/v/0\">\"Can&rsquo;t philosophers tell the difference between science and religion?\"</a> where he argued, among other things, that methodological naturalism (MN) is essential to science and supernatural claims are inherently untestable. A relevant quote:</p>\n<blockquote>\n<p>The second misunderstanding arises in a different way, with ID proponents and even some opponents... claiming that science can indeed test the supernatural... For instance, both Laudan and Quinn cite the young-earth creationist view that God created the earth is 6,000 to 10,000 years ago as a hypothesis that is testable and found to be false. But this and other examples that are offered to show the possibility of tests of the supernatural invariably build in naturalistic assumptions that creationists do not share... The point here is that we cannot overlook or ignore, as Laudan and company regularly do, the fact that creationists have a fundamentally different notion from science of what constitutes proper evidential grounds for warranted belief. The young earth view is certainly disconfirmed if we are considering matters under MN, but if one takes the supernatural aspect of the claim seriously, then one loses any ground upon which to test the claim.</p>\n</blockquote>\n<p>So on Pennock's view, testing young-earth creationism and thereby demonstrating it to be false is not possible without relying on naturalistic assumptions. This creates an opening for the creationist to question whether science needs to rely on naturalistic assumptions, and argue that one could create an equally valid version of science based on (fundamentalist) Christian assumptions.</p>\n<p>You might conclude Pennock is wrong about this, and young-earth creationism really has been straightforwardly refuted by science, but this creates a different opening for the creationist: argue that if Pennock is wrong, his ideas really shouldn't be the basis of court decisions about whether creationism can be taught in public schools. Evolutionists could respond by arguing for some other philosophical basis for rejecting creationism, but then they'd probably have to make some contentious philosophical claims and we shouldn't be determining what children can learn based on contentious philosophical claims either.</p>\n<p>An argument along the above lines could also be used for a different purpose, by someone who rejected creationism but wanted to make a show of being fair-minded towards their opponents and generally more rational than most of their peers. The thing to do is to say that while young earth creationism can be decisively refuted, most scientists and philosophers botch the philosophy required to do that, and this indicates their rejection of creationism is mostly tribalistic, and the young-earth creationists don't actually come out looking so bad by comparison.</p>\n<p>Speaking as Chris Hallquist and <em>not </em>some hypothetical alter-ego, I think that if a philosophy of science makes young earth creationism come out looking good, that's a <em>reductio </em>for that philosophy. I think educated people who reject young earth creationism are generally rational to do so, even if their philosophy isn't that hot. Still, I'd be curious to know what else people on LessWrong can come up with in the way of steelmanning young earth creationism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YCYPKwFrxz94KppwK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 6, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "25500", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PxN5iwS2CTCYi4oAP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-17T19:02:04.148Z", "modifiedAt": null, "url": null, "title": "Methods for treating depression", "slug": "methods-for-treating-depression", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:46.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sagbSbm3KdH6tHQKP/methods-for-treating-depression", "pageUrlRelative": "/posts/sagbSbm3KdH6tHQKP/methods-for-treating-depression", "linkUrl": "https://www.lesswrong.com/posts/sagbSbm3KdH6tHQKP/methods-for-treating-depression", "postedAtFormatted": "Monday, February 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Methods%20for%20treating%20depression&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMethods%20for%20treating%20depression%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsagbSbm3KdH6tHQKP%2Fmethods-for-treating-depression%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Methods%20for%20treating%20depression%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsagbSbm3KdH6tHQKP%2Fmethods-for-treating-depression", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsagbSbm3KdH6tHQKP%2Fmethods-for-treating-depression", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1130, "htmlBody": "<p>Many people struggle with depression, and I've been trying to formulate some general advice for treating it as a part of my work for <a href=\"http://cognitomentoring.org/\">Cognito Mentoring</a>. I'm hesitant to write about the subject on account of lacking professional expertise, and so am especially interested in getting feedback on my thinking on the subject. I've written up some tentative thoughts below. The reader being addressed is somebody who's struggling with depression, with a special focus on high school students.</p>\n<hr />\n<p>The research on the efficacy of different depression treatments is only moderately strong. I'm not confident in my remarks below: they reflect an attempt to come to the best conclusion possible with the evidence available.</p>\n<ul>\n<li>Any given treatment of depression only works for a fraction of depressed people, suggesting that causes of depression may be diverse.&nbsp;</li>\n<li>Cognitive behavioral therapy (CBT) stands out for its combination of efficacy, potential for producing lasting changes, low cost, and absence of averse side effects.</li>\n<li>It's worth experimenting with different treatment methods to see which works best for you, with the possible exception of antidepressants.</li>\n<li>Combining treatment methods may be more impactful than applying one individually.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<h2>Cognitive behavioral therapy</h2>\n<p>According to <a href=\"http://stantatkin.com/wp-content/uploads/2012/09/CogBehav-Therapy-Butler06.pdf\">The empirical status of cognitive-behavioral therapy: A review of meta-analyses</a>, there's a strong base of evidence that CBT has a large effect of reducing depression on average. There's evidence that the benefits extend beyond the duration of the treatment. Studies generally seem to show that CBT is as effective as antidepressants at reducing depression (some find that CBT is more effective, but the evidence is unclear).</p>\n<p>CBT has the advantage that one can learn to do the exercises on one's own, without the expense of a therapist or a psychiatrist. The evidence for the efficacy of self-help CBT materials is weaker than the evidence for the efficacy of therapist-administered CBT, but this may reflect insufficient commitment on the part of patients who were assigned to use self-help CBT materials. <a href=\"http://www.nrepp.samhsa.gov/pdfs/cbt_review2.pdf\">The clinical effectiveness of CBT-based guided self-help interventions for anxiety and depressive disorders: A systematic review</a> finds that for <em>self-selected</em>&nbsp;users of self-help CBT materials, the treatment was efficacious (though the quality of the studies was not high). If you're sufficiently committed, the expected benefits that you stand to gain from self-help CBT may be enhanced substantially.</p>\n<p>A book for learning CBT on your own is <a href=\"http://www.amazon.com/Feeling-Good-New-Mood-Therapy-ebook/dp/B009UW5X4C/ref=sr_sp-atf_title_1_1?ie=UTF8&amp;qid=1392587360&amp;sr=8-1&amp;keywords=feeling+good+new+mood+therapy\">Feeling Good: The New Mood Therapy</a>&nbsp;by David Burns.</p>\n<h2>Exercise</h2>\n<p>It's widely believed that exercise alleviates depression. There's an intuitive basis for thinking this: exercise often gives one a <a href=\"http://en.wikipedia.org/wiki/Endorphins#Runner.27s_high runner's high\">runner's high</a>.</p>\n<p>In the Cochrane review <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/14651858.CD004366.pub5/abstract\">Exercise for depression</a>, the authors find that on average, studies show a moderate-sized effect, but that when one restricts consideration to the highest quality studies, one sees a significantly smaller effect, suggesting that the efficacy of exercise for treating depression may be overstated.</p>\n<p>The main downside to exercise is that it takes time, but it may be worth it even if the effect size is small if alleviating depression is sufficiently high priority for you.</p>\n<h2>Talk therapy</h2>\n<p>Talk therapy has been shown to reduce depression on average. However:</p>\n<ul>\n<li>Professional therapists are expensive, often charging on order of $120/week if one's insurance doesn't cover them.</li>\n<li>Anecdotally, highly intelligent people find therapy less useful than the average person does, perhaps because there's a gap in intelligence between them and most therapists that makes it difficult for the therapist to understand them.</li>\n</ul>\n<p><a href=\"http://www.amazon.com/House-Cards-Robyn-Dawes/dp/0684830914\">House of Cards</a>&nbsp;by Robyn Dawes argues that there's no evidence that licensed therapists are better at performing therapy than minimally trained laypeople. The evidence therein raises the possibility that one can derive the benefits of seeing a therapist from talking to a friend.</p>\n<p>This requires that one has a friend who</p>\n<ul>\n<li>is willing to talk with you about your emotions on a regular basis</li>\n<li>you trust to the point of feeling comfortable sharing your emotions</li>\n</ul>\n<p>Some reasons to think that talking with a friend may not carry the full benefits of talking with a therapist are</p>\n<ul>\n<li><strong>Conflict of interest</strong>&nbsp;&mdash; Your friend may be biased for reasons having to do with your pre-existing relationship &ndash; for example, he or she might be unwilling to ask certain questions or offer certain feedback out of concern of offending you and damaging your friendship.</li>\n<li><strong>Risk of damaged relationship dynamics</strong> &mdash; There's a possibility of your friend feeling burdened by a sense of obligation to help you, creating feelings of resentment, and/or of you feeling guilty.</li>\n<li><strong>Risk of breach of confidentiality</strong> &mdash; Since you and your friend know people in common, there's a possibility that your friend will reveal things that you say to others who you know, that you might not want to be known. In contrast, a therapist generally won't know people in common with you, and is professionally obliged to keep what you say confidential.</li>\n</ul>\n<p>Depending on the friend and on the nature of help that you need, these factors may be non-issues, but they're worth considering when deciding between seeing a therapist and talking with a friend.</p>\n<h2>Light therapy</h2>\n<p>If your depression is seasonal in nature, you may benefit from <a href=\"http://en.wikipedia.org/wiki/Light_therapy\">light therapy</a>.&nbsp;According to <a href=\"http://journals.psychiatryonline.org/article.aspx?articleid=177447\">The Efficacy of Light Therapy in the Treatment of Mood Disorders: A Review and Meta-Analysis of the Evidence</a></p>\n<blockquote>\n<p><a href=\"http://journals.psychiatryonline.org/article.aspx?articleid=177447\"></a>Randomized, controlled trials suggests that bright light treatment and dawn simulation for seasonal affective disorder and bright light for nonseasonal depression are efficacious, with effect sizes equivalent to those in most antidepressant pharmacotherapy trials.</p>\n</blockquote>\n<p>The Cochrane review <a href=\"http://www.bibliotecacochrane.com/pdf/CD004050.pdf\">Light therapy for non-seasonal depression</a>&nbsp;finds that even for non-seasonal depression, light therapy reduces depression on average, though the effect is modest.</p>\n<h2>Antidepressants</h2>\n<p>The Cochrane review <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/23152227\">Newer generation antidepressants for depressive disorders in children and adolescents</a>&nbsp;found that antidepressants increased recover rates from 38.0% to 44.8% (over a specified duration) relative to a placebo. This understates the capacity for anti-depressants to reduce depression, because placebo treatment is also better than no treatment, and if one antidepressant doesn't work, you can try another one.</p>\n<p>If you're an adolescent, the case for using an antidepressant is weakened by the fact that antidepressants are thought to increase the risk of suicide in adolescents. Some evidence for this comes from the Cochrane review above, which found that antidepressants increased suicide rates by 58%. The Food and Drug Administration requires that manufacturers of antidepressants include a warning that antidepressants can increase the risk of suicide in children, adolescents and adults under age 25. See <a href=\"http://en.wikipedia.org/wiki/Antidepressants_and_suicide_risk\">antidepressants and suicide risk</a>&nbsp;for more information. The size of the increased risk in \"absolute\" terms varies from person to person, because some people are more likely to commit suicide than others. But in a given case, the increased risk of suicide may not be worth the potential benefits.&nbsp;</p>\n<p>If you're under 25 years old, particularly if you're an adolescent, it seems reasonable to try other methods of treatment before considering antidepressants.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6v2FHy8dtyCYg9Kz4": 2, "BAhM42jvzuWMzTDxR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sagbSbm3KdH6tHQKP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 25, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "25526", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Many people struggle with depression, and I've been trying to formulate some general advice for treating it as a part of my work for <a href=\"http://cognitomentoring.org/\">Cognito Mentoring</a>. I'm hesitant to write about the subject on account of lacking professional expertise, and so am especially interested in getting feedback on my thinking on the subject. I've written up some tentative thoughts below. The reader being addressed is somebody who's struggling with depression, with a special focus on high school students.</p>\n<hr>\n<p>The research on the efficacy of different depression treatments is only moderately strong. I'm not confident in my remarks below: they reflect an attempt to come to the best conclusion possible with the evidence available.</p>\n<ul>\n<li>Any given treatment of depression only works for a fraction of depressed people, suggesting that causes of depression may be diverse.&nbsp;</li>\n<li>Cognitive behavioral therapy (CBT) stands out for its combination of efficacy, potential for producing lasting changes, low cost, and absence of averse side effects.</li>\n<li>It's worth experimenting with different treatment methods to see which works best for you, with the possible exception of antidepressants.</li>\n<li>Combining treatment methods may be more impactful than applying one individually.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Cognitive_behavioral_therapy\">Cognitive behavioral therapy</h2>\n<p>According to <a href=\"http://stantatkin.com/wp-content/uploads/2012/09/CogBehav-Therapy-Butler06.pdf\">The empirical status of cognitive-behavioral therapy: A review of meta-analyses</a>, there's a strong base of evidence that CBT has a large effect of reducing depression on average. There's evidence that the benefits extend beyond the duration of the treatment. Studies generally seem to show that CBT is as effective as antidepressants at reducing depression (some find that CBT is more effective, but the evidence is unclear).</p>\n<p>CBT has the advantage that one can learn to do the exercises on one's own, without the expense of a therapist or a psychiatrist. The evidence for the efficacy of self-help CBT materials is weaker than the evidence for the efficacy of therapist-administered CBT, but this may reflect insufficient commitment on the part of patients who were assigned to use self-help CBT materials. <a href=\"http://www.nrepp.samhsa.gov/pdfs/cbt_review2.pdf\">The clinical effectiveness of CBT-based guided self-help interventions for anxiety and depressive disorders: A systematic review</a> finds that for <em>self-selected</em>&nbsp;users of self-help CBT materials, the treatment was efficacious (though the quality of the studies was not high). If you're sufficiently committed, the expected benefits that you stand to gain from self-help CBT may be enhanced substantially.</p>\n<p>A book for learning CBT on your own is <a href=\"http://www.amazon.com/Feeling-Good-New-Mood-Therapy-ebook/dp/B009UW5X4C/ref=sr_sp-atf_title_1_1?ie=UTF8&amp;qid=1392587360&amp;sr=8-1&amp;keywords=feeling+good+new+mood+therapy\">Feeling Good: The New Mood Therapy</a>&nbsp;by David Burns.</p>\n<h2 id=\"Exercise\">Exercise</h2>\n<p>It's widely believed that exercise alleviates depression. There's an intuitive basis for thinking this: exercise often gives one a <a href=\"http://en.wikipedia.org/wiki/Endorphins#Runner.27s_high runner's high\">runner's high</a>.</p>\n<p>In the Cochrane review <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/14651858.CD004366.pub5/abstract\">Exercise for depression</a>, the authors find that on average, studies show a moderate-sized effect, but that when one restricts consideration to the highest quality studies, one sees a significantly smaller effect, suggesting that the efficacy of exercise for treating depression may be overstated.</p>\n<p>The main downside to exercise is that it takes time, but it may be worth it even if the effect size is small if alleviating depression is sufficiently high priority for you.</p>\n<h2 id=\"Talk_therapy\">Talk therapy</h2>\n<p>Talk therapy has been shown to reduce depression on average. However:</p>\n<ul>\n<li>Professional therapists are expensive, often charging on order of $120/week if one's insurance doesn't cover them.</li>\n<li>Anecdotally, highly intelligent people find therapy less useful than the average person does, perhaps because there's a gap in intelligence between them and most therapists that makes it difficult for the therapist to understand them.</li>\n</ul>\n<p><a href=\"http://www.amazon.com/House-Cards-Robyn-Dawes/dp/0684830914\">House of Cards</a>&nbsp;by Robyn Dawes argues that there's no evidence that licensed therapists are better at performing therapy than minimally trained laypeople. The evidence therein raises the possibility that one can derive the benefits of seeing a therapist from talking to a friend.</p>\n<p>This requires that one has a friend who</p>\n<ul>\n<li>is willing to talk with you about your emotions on a regular basis</li>\n<li>you trust to the point of feeling comfortable sharing your emotions</li>\n</ul>\n<p>Some reasons to think that talking with a friend may not carry the full benefits of talking with a therapist are</p>\n<ul>\n<li><strong>Conflict of interest</strong>&nbsp;\u2014 Your friend may be biased for reasons having to do with your pre-existing relationship \u2013 for example, he or she might be unwilling to ask certain questions or offer certain feedback out of concern of offending you and damaging your friendship.</li>\n<li><strong>Risk of damaged relationship dynamics</strong> \u2014 There's a possibility of your friend feeling burdened by a sense of obligation to help you, creating feelings of resentment, and/or of you feeling guilty.</li>\n<li><strong>Risk of breach of confidentiality</strong> \u2014 Since you and your friend know people in common, there's a possibility that your friend will reveal things that you say to others who you know, that you might not want to be known. In contrast, a therapist generally won't know people in common with you, and is professionally obliged to keep what you say confidential.</li>\n</ul>\n<p>Depending on the friend and on the nature of help that you need, these factors may be non-issues, but they're worth considering when deciding between seeing a therapist and talking with a friend.</p>\n<h2 id=\"Light_therapy\">Light therapy</h2>\n<p>If your depression is seasonal in nature, you may benefit from <a href=\"http://en.wikipedia.org/wiki/Light_therapy\">light therapy</a>.&nbsp;According to <a href=\"http://journals.psychiatryonline.org/article.aspx?articleid=177447\">The Efficacy of Light Therapy in the Treatment of Mood Disorders: A Review and Meta-Analysis of the Evidence</a></p>\n<blockquote>\n<p><a href=\"http://journals.psychiatryonline.org/article.aspx?articleid=177447\"></a>Randomized, controlled trials suggests that bright light treatment and dawn simulation for seasonal affective disorder and bright light for nonseasonal depression are efficacious, with effect sizes equivalent to those in most antidepressant pharmacotherapy trials.</p>\n</blockquote>\n<p>The Cochrane review <a href=\"http://www.bibliotecacochrane.com/pdf/CD004050.pdf\">Light therapy for non-seasonal depression</a>&nbsp;finds that even for non-seasonal depression, light therapy reduces depression on average, though the effect is modest.</p>\n<h2 id=\"Antidepressants\">Antidepressants</h2>\n<p>The Cochrane review <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/23152227\">Newer generation antidepressants for depressive disorders in children and adolescents</a>&nbsp;found that antidepressants increased recover rates from 38.0% to 44.8% (over a specified duration) relative to a placebo. This understates the capacity for anti-depressants to reduce depression, because placebo treatment is also better than no treatment, and if one antidepressant doesn't work, you can try another one.</p>\n<p>If you're an adolescent, the case for using an antidepressant is weakened by the fact that antidepressants are thought to increase the risk of suicide in adolescents. Some evidence for this comes from the Cochrane review above, which found that antidepressants increased suicide rates by 58%. The Food and Drug Administration requires that manufacturers of antidepressants include a warning that antidepressants can increase the risk of suicide in children, adolescents and adults under age 25. See <a href=\"http://en.wikipedia.org/wiki/Antidepressants_and_suicide_risk\">antidepressants and suicide risk</a>&nbsp;for more information. The size of the increased risk in \"absolute\" terms varies from person to person, because some people are more likely to commit suicide than others. But in a given case, the increased risk of suicide may not be worth the potential benefits.&nbsp;</p>\n<p>If you're under 25 years old, particularly if you're an adolescent, it seems reasonable to try other methods of treatment before considering antidepressants.</p>", "sections": [{"title": "Cognitive behavioral therapy", "anchor": "Cognitive_behavioral_therapy", "level": 1}, {"title": "Exercise", "anchor": "Exercise", "level": 1}, {"title": "Talk therapy", "anchor": "Talk_therapy", "level": 1}, {"title": "Light therapy", "anchor": "Light_therapy", "level": 1}, {"title": "Antidepressants", "anchor": "Antidepressants", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "51 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-17T20:52:42.699Z", "modifiedAt": "2022-03-13T20:39:02.798Z", "url": null, "title": "Book Review: Linear Algebra Done Right (MIRI course list)", "slug": "book-review-linear-algebra-done-right-miri-course-list", "viewCount": null, "lastCommentedAt": "2015-09-10T12:09:46.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DWmrLjo5CgzK2Xmzk/book-review-linear-algebra-done-right-miri-course-list", "pageUrlRelative": "/posts/DWmrLjo5CgzK2Xmzk/book-review-linear-algebra-done-right-miri-course-list", "linkUrl": "https://www.lesswrong.com/posts/DWmrLjo5CgzK2Xmzk/book-review-linear-algebra-done-right-miri-course-list", "postedAtFormatted": "Monday, February 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20Linear%20Algebra%20Done%20Right%20(MIRI%20course%20list)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20Linear%20Algebra%20Done%20Right%20(MIRI%20course%20list)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWmrLjo5CgzK2Xmzk%2Fbook-review-linear-algebra-done-right-miri-course-list%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20Linear%20Algebra%20Done%20Right%20(MIRI%20course%20list)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWmrLjo5CgzK2Xmzk%2Fbook-review-linear-algebra-done-right-miri-course-list", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDWmrLjo5CgzK2Xmzk%2Fbook-review-linear-algebra-done-right-miri-course-list", "socialPreviewImageUrl": "http://images.betterworldbooks.com/038/Linear-Algebra-Done-Right-9780387982588.jpg", "question": false, "authorIsUnreviewed": false, "wordCount": 2245, "htmlBody": "<p>I'm reviewing the books in the MIRI course list.</p>\n<p>It's been a while since I did a book review. The last book I reviewed was <a href=\"/lw/j4r/book_review_computability_and_logic/\">Computation and Logic</a>, which I read in November. After that, I spent a few weeks brushing up on specific topics in preparation for my first MIRI math workshop. I read about half of <a href=\"http://www.amazon.com/The-Logic-Provability-George-Boolos/dp/0521483255\">The Logic of Provability</a> and studied a little topology. I also worked my way through some <a href=\"/lw/jca/walkthrough_of_the_tiling_agents_for/\">relevant</a> <a href=\"/r/lesswrong/lw/jbe/walkthrough_of_definability_of_truth_in/\">papers</a>.</p>\n<p>After the workshop, I took some time off around the holidays and <a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">wrote a bit about my experience</a>. I'm finally back into Study Mode. This week I finished <a href=\"http://smile.amazon.com/gp/product/0387982582/ref=smi_www_rcol_go_smi?ie=UTF8&amp;*Version*=1&amp;*entries*=0\">Linear Algebra Done Right</a>, by Sheldon Axler.</p>\n<p style=\"text-align: center\"><img src=\"http://images.betterworldbooks.com/038/Linear-Algebra-Done-Right-9780387982588.jpg\" alt=\"\" /></p>\n<p>I quite enjoyed the book. Linear algebra has far-reaching impact, and while I learned it in college, I was mostly just <a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">memorizing passwords</a>. There are a few important concepts in linear algebra that seem prone to poor explanations.&nbsp;<em>Linear Algebra Done Right</em>&nbsp;derives these concepts intuitively. My understanding of Linear Algebra improved drastically as I read this book.</p>\n<p>Below, I'll review the contents of the text before giving a more detailed overview of the book as a whole.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h1 id=\"mybackground\">My Background</h1>\n<p>When reading a review of a textbook, it's important to know the reviewer's background. I studied Linear Algebra briefly in college, not in a Linear Algebra course, but as a subsection of a Discrete Mathematics course. I knew about vector spaces, I understood what 'linearity' entailed, and I was acquainted with the standard tools of linear algebra. I had a vague idea that matrices encoded multiple linear equations at the same time. I could solve problems mechanically using the usual tools, but I didn't fully understand them.</p>\n<p>The book was an easy read (as I already knew the answers), but was still quite informative (as I didn't yet understand them).</p>\n<h1 id=\"contents\">Contents</h1>\n<ol>\n<li><a href=\"#vectorspaces\">Vector Spaces</a></li>\n<li><a href=\"#finitedimensionalvectorspaces\">Finite Dimensional Vector Spaces</a></li>\n<li><a href=\"#linearmaps\">Linear Maps</a></li>\n<li><a href=\"#polynomials\">Polynomials</a></li>\n<li><a href=\"#eigenvectorsandeigenvalues\">Eigenvectors and Eigenvalues</a></li>\n<li><a href=\"#innerproductspaces\">Inner-Product Spaces</a></li>\n<li><a href=\"#operatorsoninnerproductspaces\">Operators on Inner-Product Spaces</a></li>\n<li><a href=\"#operatorsoncomplexvectorspaces\">Operators on Complex Vector Spaces</a></li>\n<li><a href=\"#operatorsonrealvectorspaces\">Operators on Real Vector Spaces</a></li>\n<li><a href=\"#traceanddeterminant\">Trace and Determinant</a></li>\n</ol>\n<h3 id=\"vectorspaces\">Vector Spaces</h3>\n<p>This chapter briefly explains complex numbers, vectors (providing both geometric and non-geometric interpretations), and vector spaces. It's short, and it's a nice review. If you don't know what vector spaces are, this is as good a way to learn as any. Even if you are already familiar with vector spaces, this chapter is worth a skim to learn the specific notation used in this particular book.</p>\n<h3 id=\"finitedimensionalvectorspaces\">Finite Dimensional Vector Spaces</h3>\n<p>This chapter covers span, linear independence, bases, and dimension. There are some interesting results here if you're new to the field (for example, any two vectors which are not scalar multiples of each other, no matter how close they are to each other, span an entire plane). Mostly, though, this section is about generalizing the basic geometric intuition for vector spaces into more-than-three dimensions.</p>\n<p>Again, this chapter is probably a good introduction for people who have never seen Linear Algebra before, but it doesn't cover much that is counter-intuitive.</p>\n<h3 id=\"linearmaps\">Linear Maps</h3>\n<p>This is the first chapter that started explaining things in ways I hadn't heard before. It covers linear maps from one vector space to another. It spends some time discussing null spaces (the vector spaces which a linear map maps to zero) and ranges (the subspace of the target space that the map maps the source space onto). These are fairly simple concepts that turn out to be far more useful than I anticipated when it comes to building up intuition about linear maps.</p>\n<p>Next, matrices are explored. Given a linear map and a basis for each of the source and the target spaces, we can completely describe the linear map by seeing what it does to each basis vector. No matter how complicated the map is, no matter what gymnastics it is doing, linearity guarantees that we can always fully capture the behavior by pre-computing only what it does to the basis vectors. This pre-computation, ignoring the 'actual function' and seeing what it does to two specific bases, is precisely a matrix of the linear map (with respect to those bases).</p>\n<p>That was a neat realization. The chapter then covers surjectivity, injectivity, and invertability, none of which are particularly surprising.</p>\n<h3 id=\"polynomials\">Polynomials</h3>\n<p>This chapter is spent exploring polynomials in both real and complex fields. This was a nice refresher, as polynomials become quite important (unexpectedly so) later on in the book.</p>\n<h3 id=\"eigenvectorsandeigenvalues\">Eigenvectors and Eigenvalues</h3>\n<p>The book now turns to operators (linear maps from a vector space onto itself), and starts analyzing their structure. It introduces invariant subspaces (spaces which, under the operator, map to some scalar multiple of themselves) and more specifically eigenvectors (one-dimensional invariant subspaces) and eigenvalues (the corresponding scalar multiples).</p>\n<p>Interestingly, if T is an operator with eigenvalue &lambda; then the null space of (T - &lambda;I) includes the corresponding eigenvector. This seemingly simple fact turns out to have far-reaching implications. This leads to our first taste of applying polynomials to operators, which is a portent of things to come.</p>\n<p>The chapter then discusses some \"well-behaved\" operator/basis combinations and methods for finding bases under which operators are well behaved. This leads to upper-triangular and diagonal matrices.</p>\n<p>The chapter concludes with a discussion of invariant subspaces on real vector spaces. This introduces some technical difficulties that arise in real spaces (namely stemming from the fact that not every polynomial has real roots). The methods for dealing with real spaces introduced here are repeated frequently in different contexts for the remainder of the book.</p>\n<h3 id=\"innerproductspaces\">Inner-Product Spaces</h3>\n<p>This chapter introduces inner products, which essentially allows us to start measuring the 'size' of vectors (for varying definitions of 'size'). This leads to a discussion of orthonormal bases (orthogonal bases where each basis vector has norm 1). Some of the very nice properties offered by orthogonal / orthonormal bases are then explored.</p>\n<p>The chapter moves on to discuss linear functionals (linear maps onto a one-dimensional space) and adjoints. The adjoint of an operator is analogous to the complex conjugate of a complex number. Adjoints aren't very well motivated in this chapter, but they allow us to discuss self-adjoint operators in the following chapter.</p>\n<h3 id=\"operatorsoninnerproductspaces\">Operators on Inner-Product Spaces</h3>\n<p>This chapter opens with self-adjoint operators, which are essentially operators T such that the inner product of Tv with w is the same as the inner product of v with Tw. To continue with the analogy above, self-adjoint operators (which are \"equal to their conjugate\") are analogous to real numbers.</p>\n<p>Self-adjoint operators are generalized to normal operators, which merely commute with their adjoints. This sets us up for the spectral theorem, which allows us to prove some nice properties exclusive to normal operators on inner-product spaces. (Additional work is required for real spaces, as expected.)</p>\n<p>The chapter moves on to positive operators, which should really be called non-negative operators, which essentially don't turn any vectors around (with respect to the inner product) &mdash; specifically these are operators T such that \u27e8Tv, v\u27e9 &ge; 0. This allows us to start talking about square roots of operators, which are always positive.</p>\n<p>This is followed by isometries, which are operators that preserve norms (norm(Sv) = norm(v)).</p>\n<p>With these two concepts in hand, the chapter shows that every operator on an inner product space can be decomposed into an isometry and a positive operator (which is the square of the square root of the original operator). Intuitively, this shows that every operator on an inner product space can be thought of as one positive operation (no turning vectors around) followed by one isometric operation (no changing lengths). This is called a polar decomposition, for obvious reasons.</p>\n<p>The chapter concludes by showing that the polar decomposition leads to a singular value decomposition, which essentially states that every operator on an inner product space has a diagonal matrix with respect to two orthonormal bases. This is pretty powerful.</p>\n<h3 id=\"operatorsoncomplexvectorspaces\">Operators on Complex Vector Spaces</h3>\n<p>The chapter begins by introducing generalized eigenvectors. Essentially, not every operator has as many eigenvectors as it has dimensions. However, in such operators, there will be some eigenvalues that are \"repeated\": (T - &lambda;)^2 maps additional subspaces to zero (that T - &lambda; did not). More generally, we can assign a 'multiplicity' to each eigenvalue counting the maximum dimension of the null space of (T - &lambda;)^j. Counting multiplicity, each operator on space V has as many eigenvalues as the dimension of V.</p>\n<p>This allows us to characterize every operator via a unique polynomial p(T) = (T - &lambda;_1)&hellip;(T - &lambda;_n) such that p(T) = 0. In other words, we can think of T as a root of this polynomial, which is called the characteristic polynomial of T. This polynomial can tell us much about an operator (as it describes both the eigenvalues of the operator and their multiplicities).</p>\n<p>We can also find a minimal polynomial for T, which is the monic polynomial q of minimal degree such that q(T) = 0. If the degree of q is equal to the dimension of the space that T operates on, then the minimal polynomial is the same as the characteristic polynomial. The minimal polynomial of an operator is also useful for analyzing the operator's structure.&nbsp;</p>\n<p>The chapter concludes by introducing Jordan form, a particularly 'nice' version of upper-triangular form available to 'nilpotent' operators (operators N such that N^p = 0 for some p).</p>\n<h3 id=\"operatorsonrealvectorspaces\">Operators on Real Vector Spaces</h3>\n<p>This chapter derives characteristic polynomials for operators on real spaces. It's essentially a repeat of the corresponding section of chapter 8, but with extra machinery to deal with real vector spaces. Similar (although predictably weaker) results are achieved.</p>\n<h3 id=\"traceanddeterminant\">Trace and Determinant</h3>\n<p>This chapter explains trace (the sum of all eigenvalues, counting multiplicity) and determinant (the product of all eigenvalues, counting multiplicity), which you may also recognize as the second and final coefficients of the characteristic polynomial, respectively. These values can tell you a fair bit about an operator (as they're tightly related to the eigenvalues), and it turns out you can calculate them even when you can't figure out the individual eigenvalues precisely.</p>\n<p>The book then fleshes out methods for calculating trace and determinant from arbitrary matrices. It tries to motivate the standard method of calculating determinants from arbitrary matrices, but this involves a number of arbitrary leaps and feels very much like an accident of history. After this explanation, I understand better what a determinant is, and it is no longer surprising to me that my university courses had trouble motivating them.</p>\n<p>The book concludes by exhibiting some situations where knowing only the determinant of an operator is actually useful.</p>\n<hr />\n<h1 id=\"whoshouldreadthis\">Who should read this?</h1>\n<p>This book did a far better job of introducing the main concepts of linear algebra to me than did my Discrete Mathematics course. I came away with a vastly improved intuition for why the standard tools of linear algebra actually work.</p>\n<p>I can personally attest that <em>Linear Algebra Done Right</em> is a great way to un-memorize passwords and build up that intuition. If you know how to compute a determinant but you have no idea what it <em>means</em>, then I recommend giving this book a shot.</p>\n<p>I imagine that&nbsp;<em>Linear Algebra Done Right</em>&nbsp;would also be a good introduction for someone who hasn't done any linear algebra at all.&nbsp;</p>\n<h1 id=\"whatshouldiread\">What should I read?</h1>\n<p>Chapters 1, 2, and 4 are pretty simple and probably review. They are well-written, short, and at least worth a skim. I recommend reading them unless you feel you know the subjects very well already.</p>\n<p>Chapters 3, 5, and 6 were very helpful, and really helped me build up my intuition for linear algebra. If you're trying to build an intuition for linear algebra, read these three chapters closely and do the exercises (chapter 5 especially, it's probably the most important chapter).</p>\n<p>Chapters 7 and 8 also introduced some very helpful concepts, though things got fairly technical. They build up some good intuition, but they also require some grinding. I recommend reading these chapters and understanding all the concepts therein, but they're pretty heavy on the exercises, which you can probably skip without much trouble. Chapter 8 is where the book deviates most from the \"standard\" way of teaching linear algebra, and might be worth a close read for that reason alone.</p>\n<p>Chapter 9 is largely just a repeat of chapter 8 but on real spaces (which introduces some additional complexity &mdash; har har): none of it is surprising, most of it is mechanical, and I recommend skimming it and skipping the exercises unless you really want the practice.</p>\n<p>Chapter 10 is dedicated to explaining some specific memorized passwords, and does a decent job of it. The sections about trace and determinants of an operator are very useful. The corresponding sections about traces and determinants of matrices are eye-opening from the perspective of someone coming from the \"standard\" classes, and are probably somewhat surprising from a newcomer's perspective as well. (You can torture an impressive amount of information out of a matrix.) However, a good half of chapter 10 is an artifact of the \"standard\" way of teaching linear algebra: in my opinion, it's sufficient to understand what trace and determinants are and know that they <em>can</em> be calculated from matrices in general, without worrying too much about the specific incantations. I'd skim most of this chapter and skip the exercises.</p>\n<h1>Closing Notes</h1>\n<p>This book is well-written. It has minimal typos and isn't afraid to spend a few paragraphs building intuition, but it largely gets to the point and doesn't waste your time. It's a fairly quick read, it's well-paced, and it never feels too difficult.</p>\n<p>I feel this book deserves its place on the course list. Linear algebra is prevalent throughout mathematics, and <em>Linear Algebra Done Right</em>&nbsp;provides a solid overview.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2, "4Kcm4etxAJjmeDkHP": 2, "6nS8oYmSMuFMaiowF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DWmrLjo5CgzK2Xmzk", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 56, "extendedScore": null, "score": 0.000151, "legacy": true, "legacyId": "25528", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "http://images.betterworldbooks.com/038/Linear-Algebra-Done-Right-9780387982588.jpg", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm reviewing the books in the MIRI course list.</p>\n<p>It's been a while since I did a book review. The last book I reviewed was <a href=\"/lw/j4r/book_review_computability_and_logic/\">Computation and Logic</a>, which I read in November. After that, I spent a few weeks brushing up on specific topics in preparation for my first MIRI math workshop. I read about half of <a href=\"http://www.amazon.com/The-Logic-Provability-George-Boolos/dp/0521483255\">The Logic of Provability</a> and studied a little topology. I also worked my way through some <a href=\"/lw/jca/walkthrough_of_the_tiling_agents_for/\">relevant</a> <a href=\"/r/lesswrong/lw/jbe/walkthrough_of_definability_of_truth_in/\">papers</a>.</p>\n<p>After the workshop, I took some time off around the holidays and <a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">wrote a bit about my experience</a>. I'm finally back into Study Mode. This week I finished <a href=\"http://smile.amazon.com/gp/product/0387982582/ref=smi_www_rcol_go_smi?ie=UTF8&amp;*Version*=1&amp;*entries*=0\">Linear Algebra Done Right</a>, by Sheldon Axler.</p>\n<p style=\"text-align: center\"><img src=\"http://images.betterworldbooks.com/038/Linear-Algebra-Done-Right-9780387982588.jpg\" alt=\"\"></p>\n<p>I quite enjoyed the book. Linear algebra has far-reaching impact, and while I learned it in college, I was mostly just <a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">memorizing passwords</a>. There are a few important concepts in linear algebra that seem prone to poor explanations.&nbsp;<em>Linear Algebra Done Right</em>&nbsp;derives these concepts intuitively. My understanding of Linear Algebra improved drastically as I read this book.</p>\n<p>Below, I'll review the contents of the text before giving a more detailed overview of the book as a whole.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h1 id=\"My_Background\">My Background</h1>\n<p>When reading a review of a textbook, it's important to know the reviewer's background. I studied Linear Algebra briefly in college, not in a Linear Algebra course, but as a subsection of a Discrete Mathematics course. I knew about vector spaces, I understood what 'linearity' entailed, and I was acquainted with the standard tools of linear algebra. I had a vague idea that matrices encoded multiple linear equations at the same time. I could solve problems mechanically using the usual tools, but I didn't fully understand them.</p>\n<p>The book was an easy read (as I already knew the answers), but was still quite informative (as I didn't yet understand them).</p>\n<h1 id=\"Contents\">Contents</h1>\n<ol>\n<li><a href=\"#vectorspaces\">Vector Spaces</a></li>\n<li><a href=\"#finitedimensionalvectorspaces\">Finite Dimensional Vector Spaces</a></li>\n<li><a href=\"#linearmaps\">Linear Maps</a></li>\n<li><a href=\"#polynomials\">Polynomials</a></li>\n<li><a href=\"#eigenvectorsandeigenvalues\">Eigenvectors and Eigenvalues</a></li>\n<li><a href=\"#innerproductspaces\">Inner-Product Spaces</a></li>\n<li><a href=\"#operatorsoninnerproductspaces\">Operators on Inner-Product Spaces</a></li>\n<li><a href=\"#operatorsoncomplexvectorspaces\">Operators on Complex Vector Spaces</a></li>\n<li><a href=\"#operatorsonrealvectorspaces\">Operators on Real Vector Spaces</a></li>\n<li><a href=\"#traceanddeterminant\">Trace and Determinant</a></li>\n</ol>\n<h3 id=\"Vector_Spaces\">Vector Spaces</h3>\n<p>This chapter briefly explains complex numbers, vectors (providing both geometric and non-geometric interpretations), and vector spaces. It's short, and it's a nice review. If you don't know what vector spaces are, this is as good a way to learn as any. Even if you are already familiar with vector spaces, this chapter is worth a skim to learn the specific notation used in this particular book.</p>\n<h3 id=\"Finite_Dimensional_Vector_Spaces\">Finite Dimensional Vector Spaces</h3>\n<p>This chapter covers span, linear independence, bases, and dimension. There are some interesting results here if you're new to the field (for example, any two vectors which are not scalar multiples of each other, no matter how close they are to each other, span an entire plane). Mostly, though, this section is about generalizing the basic geometric intuition for vector spaces into more-than-three dimensions.</p>\n<p>Again, this chapter is probably a good introduction for people who have never seen Linear Algebra before, but it doesn't cover much that is counter-intuitive.</p>\n<h3 id=\"Linear_Maps\">Linear Maps</h3>\n<p>This is the first chapter that started explaining things in ways I hadn't heard before. It covers linear maps from one vector space to another. It spends some time discussing null spaces (the vector spaces which a linear map maps to zero) and ranges (the subspace of the target space that the map maps the source space onto). These are fairly simple concepts that turn out to be far more useful than I anticipated when it comes to building up intuition about linear maps.</p>\n<p>Next, matrices are explored. Given a linear map and a basis for each of the source and the target spaces, we can completely describe the linear map by seeing what it does to each basis vector. No matter how complicated the map is, no matter what gymnastics it is doing, linearity guarantees that we can always fully capture the behavior by pre-computing only what it does to the basis vectors. This pre-computation, ignoring the 'actual function' and seeing what it does to two specific bases, is precisely a matrix of the linear map (with respect to those bases).</p>\n<p>That was a neat realization. The chapter then covers surjectivity, injectivity, and invertability, none of which are particularly surprising.</p>\n<h3 id=\"Polynomials\">Polynomials</h3>\n<p>This chapter is spent exploring polynomials in both real and complex fields. This was a nice refresher, as polynomials become quite important (unexpectedly so) later on in the book.</p>\n<h3 id=\"Eigenvectors_and_Eigenvalues\">Eigenvectors and Eigenvalues</h3>\n<p>The book now turns to operators (linear maps from a vector space onto itself), and starts analyzing their structure. It introduces invariant subspaces (spaces which, under the operator, map to some scalar multiple of themselves) and more specifically eigenvectors (one-dimensional invariant subspaces) and eigenvalues (the corresponding scalar multiples).</p>\n<p>Interestingly, if T is an operator with eigenvalue \u03bb then the null space of (T - \u03bbI) includes the corresponding eigenvector. This seemingly simple fact turns out to have far-reaching implications. This leads to our first taste of applying polynomials to operators, which is a portent of things to come.</p>\n<p>The chapter then discusses some \"well-behaved\" operator/basis combinations and methods for finding bases under which operators are well behaved. This leads to upper-triangular and diagonal matrices.</p>\n<p>The chapter concludes with a discussion of invariant subspaces on real vector spaces. This introduces some technical difficulties that arise in real spaces (namely stemming from the fact that not every polynomial has real roots). The methods for dealing with real spaces introduced here are repeated frequently in different contexts for the remainder of the book.</p>\n<h3 id=\"Inner_Product_Spaces\">Inner-Product Spaces</h3>\n<p>This chapter introduces inner products, which essentially allows us to start measuring the 'size' of vectors (for varying definitions of 'size'). This leads to a discussion of orthonormal bases (orthogonal bases where each basis vector has norm 1). Some of the very nice properties offered by orthogonal / orthonormal bases are then explored.</p>\n<p>The chapter moves on to discuss linear functionals (linear maps onto a one-dimensional space) and adjoints. The adjoint of an operator is analogous to the complex conjugate of a complex number. Adjoints aren't very well motivated in this chapter, but they allow us to discuss self-adjoint operators in the following chapter.</p>\n<h3 id=\"Operators_on_Inner_Product_Spaces\">Operators on Inner-Product Spaces</h3>\n<p>This chapter opens with self-adjoint operators, which are essentially operators T such that the inner product of Tv with w is the same as the inner product of v with Tw. To continue with the analogy above, self-adjoint operators (which are \"equal to their conjugate\") are analogous to real numbers.</p>\n<p>Self-adjoint operators are generalized to normal operators, which merely commute with their adjoints. This sets us up for the spectral theorem, which allows us to prove some nice properties exclusive to normal operators on inner-product spaces. (Additional work is required for real spaces, as expected.)</p>\n<p>The chapter moves on to positive operators, which should really be called non-negative operators, which essentially don't turn any vectors around (with respect to the inner product) \u2014 specifically these are operators T such that \u27e8Tv, v\u27e9 \u2265 0. This allows us to start talking about square roots of operators, which are always positive.</p>\n<p>This is followed by isometries, which are operators that preserve norms (norm(Sv) = norm(v)).</p>\n<p>With these two concepts in hand, the chapter shows that every operator on an inner product space can be decomposed into an isometry and a positive operator (which is the square of the square root of the original operator). Intuitively, this shows that every operator on an inner product space can be thought of as one positive operation (no turning vectors around) followed by one isometric operation (no changing lengths). This is called a polar decomposition, for obvious reasons.</p>\n<p>The chapter concludes by showing that the polar decomposition leads to a singular value decomposition, which essentially states that every operator on an inner product space has a diagonal matrix with respect to two orthonormal bases. This is pretty powerful.</p>\n<h3 id=\"Operators_on_Complex_Vector_Spaces\">Operators on Complex Vector Spaces</h3>\n<p>The chapter begins by introducing generalized eigenvectors. Essentially, not every operator has as many eigenvectors as it has dimensions. However, in such operators, there will be some eigenvalues that are \"repeated\": (T - \u03bb)^2 maps additional subspaces to zero (that T - \u03bb did not). More generally, we can assign a 'multiplicity' to each eigenvalue counting the maximum dimension of the null space of (T - \u03bb)^j. Counting multiplicity, each operator on space V has as many eigenvalues as the dimension of V.</p>\n<p>This allows us to characterize every operator via a unique polynomial p(T) = (T - \u03bb_1)\u2026(T - \u03bb_n) such that p(T) = 0. In other words, we can think of T as a root of this polynomial, which is called the characteristic polynomial of T. This polynomial can tell us much about an operator (as it describes both the eigenvalues of the operator and their multiplicities).</p>\n<p>We can also find a minimal polynomial for T, which is the monic polynomial q of minimal degree such that q(T) = 0. If the degree of q is equal to the dimension of the space that T operates on, then the minimal polynomial is the same as the characteristic polynomial. The minimal polynomial of an operator is also useful for analyzing the operator's structure.&nbsp;</p>\n<p>The chapter concludes by introducing Jordan form, a particularly 'nice' version of upper-triangular form available to 'nilpotent' operators (operators N such that N^p = 0 for some p).</p>\n<h3 id=\"Operators_on_Real_Vector_Spaces\">Operators on Real Vector Spaces</h3>\n<p>This chapter derives characteristic polynomials for operators on real spaces. It's essentially a repeat of the corresponding section of chapter 8, but with extra machinery to deal with real vector spaces. Similar (although predictably weaker) results are achieved.</p>\n<h3 id=\"Trace_and_Determinant\">Trace and Determinant</h3>\n<p>This chapter explains trace (the sum of all eigenvalues, counting multiplicity) and determinant (the product of all eigenvalues, counting multiplicity), which you may also recognize as the second and final coefficients of the characteristic polynomial, respectively. These values can tell you a fair bit about an operator (as they're tightly related to the eigenvalues), and it turns out you can calculate them even when you can't figure out the individual eigenvalues precisely.</p>\n<p>The book then fleshes out methods for calculating trace and determinant from arbitrary matrices. It tries to motivate the standard method of calculating determinants from arbitrary matrices, but this involves a number of arbitrary leaps and feels very much like an accident of history. After this explanation, I understand better what a determinant is, and it is no longer surprising to me that my university courses had trouble motivating them.</p>\n<p>The book concludes by exhibiting some situations where knowing only the determinant of an operator is actually useful.</p>\n<hr>\n<h1 id=\"Who_should_read_this_\">Who should read this?</h1>\n<p>This book did a far better job of introducing the main concepts of linear algebra to me than did my Discrete Mathematics course. I came away with a vastly improved intuition for why the standard tools of linear algebra actually work.</p>\n<p>I can personally attest that <em>Linear Algebra Done Right</em> is a great way to un-memorize passwords and build up that intuition. If you know how to compute a determinant but you have no idea what it <em>means</em>, then I recommend giving this book a shot.</p>\n<p>I imagine that&nbsp;<em>Linear Algebra Done Right</em>&nbsp;would also be a good introduction for someone who hasn't done any linear algebra at all.&nbsp;</p>\n<h1 id=\"What_should_I_read_\">What should I read?</h1>\n<p>Chapters 1, 2, and 4 are pretty simple and probably review. They are well-written, short, and at least worth a skim. I recommend reading them unless you feel you know the subjects very well already.</p>\n<p>Chapters 3, 5, and 6 were very helpful, and really helped me build up my intuition for linear algebra. If you're trying to build an intuition for linear algebra, read these three chapters closely and do the exercises (chapter 5 especially, it's probably the most important chapter).</p>\n<p>Chapters 7 and 8 also introduced some very helpful concepts, though things got fairly technical. They build up some good intuition, but they also require some grinding. I recommend reading these chapters and understanding all the concepts therein, but they're pretty heavy on the exercises, which you can probably skip without much trouble. Chapter 8 is where the book deviates most from the \"standard\" way of teaching linear algebra, and might be worth a close read for that reason alone.</p>\n<p>Chapter 9 is largely just a repeat of chapter 8 but on real spaces (which introduces some additional complexity \u2014 har har): none of it is surprising, most of it is mechanical, and I recommend skimming it and skipping the exercises unless you really want the practice.</p>\n<p>Chapter 10 is dedicated to explaining some specific memorized passwords, and does a decent job of it. The sections about trace and determinants of an operator are very useful. The corresponding sections about traces and determinants of matrices are eye-opening from the perspective of someone coming from the \"standard\" classes, and are probably somewhat surprising from a newcomer's perspective as well. (You can torture an impressive amount of information out of a matrix.) However, a good half of chapter 10 is an artifact of the \"standard\" way of teaching linear algebra: in my opinion, it's sufficient to understand what trace and determinants are and know that they <em>can</em> be calculated from matrices in general, without worrying too much about the specific incantations. I'd skim most of this chapter and skip the exercises.</p>\n<h1 id=\"Closing_Notes\">Closing Notes</h1>\n<p>This book is well-written. It has minimal typos and isn't afraid to spend a few paragraphs building intuition, but it largely gets to the point and doesn't waste your time. It's a fairly quick read, it's well-paced, and it never feels too difficult.</p>\n<p>I feel this book deserves its place on the course list. Linear algebra is prevalent throughout mathematics, and <em>Linear Algebra Done Right</em>&nbsp;provides a solid overview.</p>", "sections": [{"title": "My Background", "anchor": "My_Background", "level": 1}, {"title": "Contents", "anchor": "Contents", "level": 1}, {"title": "Vector Spaces", "anchor": "Vector_Spaces", "level": 2}, {"title": "Finite Dimensional Vector Spaces", "anchor": "Finite_Dimensional_Vector_Spaces", "level": 2}, {"title": "Linear Maps", "anchor": "Linear_Maps", "level": 2}, {"title": "Polynomials", "anchor": "Polynomials", "level": 2}, {"title": "Eigenvectors and Eigenvalues", "anchor": "Eigenvectors_and_Eigenvalues", "level": 2}, {"title": "Inner-Product Spaces", "anchor": "Inner_Product_Spaces", "level": 2}, {"title": "Operators on Inner-Product Spaces", "anchor": "Operators_on_Inner_Product_Spaces", "level": 2}, {"title": "Operators on Complex Vector Spaces", "anchor": "Operators_on_Complex_Vector_Spaces", "level": 2}, {"title": "Operators on Real Vector Spaces", "anchor": "Operators_on_Real_Vector_Spaces", "level": 2}, {"title": "Trace and Determinant", "anchor": "Trace_and_Determinant", "level": 2}, {"title": "Who should read this?", "anchor": "Who_should_read_this_", "level": 1}, {"title": "What should I read?", "anchor": "What_should_I_read_", "level": 1}, {"title": "Closing Notes", "anchor": "Closing_Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 17}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CvhPTwSMPqNju7hhw", "QGrX3qK3qxQYK9D4C", "kFDikC8kbukAhSnbe", "uX3HjXo6BWos3Zgy5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-02-17T20:52:42.699Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T01:28:51.571Z", "modifiedAt": null, "url": null, "title": "[RESCHEDULED] NYC Rationality Megameetup and Unconference: 4/5 - 4/6", "slug": "rescheduled-nyc-rationality-megameetup-and-unconference-4-5", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.123Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KenChen", "createdAt": "2011-02-16T18:02:23.420Z", "isAdmin": false, "displayName": "KenChen"}, "userId": "Tay9Y5o7ehACBHeqc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gwNqZchxD2wgHgewj/rescheduled-nyc-rationality-megameetup-and-unconference-4-5", "pageUrlRelative": "/posts/gwNqZchxD2wgHgewj/rescheduled-nyc-rationality-megameetup-and-unconference-4-5", "linkUrl": "https://www.lesswrong.com/posts/gwNqZchxD2wgHgewj/rescheduled-nyc-rationality-megameetup-and-unconference-4-5", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BRESCHEDULED%5D%20NYC%20Rationality%20Megameetup%20and%20Unconference%3A%204%2F5%20-%204%2F6&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BRESCHEDULED%5D%20NYC%20Rationality%20Megameetup%20and%20Unconference%3A%204%2F5%20-%204%2F6%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgwNqZchxD2wgHgewj%2Frescheduled-nyc-rationality-megameetup-and-unconference-4-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BRESCHEDULED%5D%20NYC%20Rationality%20Megameetup%20and%20Unconference%3A%204%2F5%20-%204%2F6%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgwNqZchxD2wgHgewj%2Frescheduled-nyc-rationality-megameetup-and-unconference-4-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgwNqZchxD2wgHgewj%2Frescheduled-nyc-rationality-megameetup-and-unconference-4-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p class=\"p1\">Please see the updated post here:</p>\n<p class=\"p1\">http://lesswrong.com/r/discussion/lw/jqh/rescheduled_nyc_rationality_megameetup_and/</p>\n<p class=\"p1\">On the weekend of April 5-6, the NYC community will be hosting a megameetup and rationality unconference. Everyone who can make the trip is strongly encouraged to come. There will be presentations, interesting discussions, and cake.</p>\n<p class=\"p1\">If you think you might come, please leave a comment and a confidence estimate. For example, if you would bring two guests and are 75% certain you will come, your comment might contain \"me +2, 75%.\" If you need a space to crash at night, please send me a PM.</p>\n<p class=\"p1\"><strong>Location:</strong></p>\n<address><span style=\"font-style: normal;\">Highgarden<br />851 Park Place<br />Brooklyn, NY 11216</span></address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gwNqZchxD2wgHgewj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.5693832155129707e-06, "legacy": true, "legacyId": "25530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T05:48:01.215Z", "modifiedAt": null, "url": null, "title": "Don't rely on the system to guarantee you life satisfaction", "slug": "don-t-rely-on-the-system-to-guarantee-you-life-satisfaction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:59.503Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h5R2gnpqsgXEsDuT7/don-t-rely-on-the-system-to-guarantee-you-life-satisfaction", "pageUrlRelative": "/posts/h5R2gnpqsgXEsDuT7/don-t-rely-on-the-system-to-guarantee-you-life-satisfaction", "linkUrl": "https://www.lesswrong.com/posts/h5R2gnpqsgXEsDuT7/don-t-rely-on-the-system-to-guarantee-you-life-satisfaction", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20rely%20on%20the%20system%20to%20guarantee%20you%20life%20satisfaction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20rely%20on%20the%20system%20to%20guarantee%20you%20life%20satisfaction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5R2gnpqsgXEsDuT7%2Fdon-t-rely-on-the-system-to-guarantee-you-life-satisfaction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20rely%20on%20the%20system%20to%20guarantee%20you%20life%20satisfaction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5R2gnpqsgXEsDuT7%2Fdon-t-rely-on-the-system-to-guarantee-you-life-satisfaction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5R2gnpqsgXEsDuT7%2Fdon-t-rely-on-the-system-to-guarantee-you-life-satisfaction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 514, "htmlBody": "<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><em>A brief essay intended for high school students: any thoughts?</em></span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">If you go to school, take the classes that people tell you to, do your homework, and engage in the extracurricular activities that your peers do, you'll be setting yourself up for an \"okay\" life. But you can do better than that.</span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><a id=\"more\"></a></span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">The school system wasn't designed to help you achieve your goals. It wasn't designed to optimize student welfare in general: it was cobbled together by many different actors with many different goals, from politicians to teachers to parents to colleges. It often suffers from inadequate resources. Even if the school system&nbsp;</span><em style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">were</em><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">&nbsp;optimized on average, the&nbsp;</span><em style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">one-size-fits-all</em><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">&nbsp;approach it takes means that it wouldn't be optimized for people who differ in any relevant respect from average. Compared with what you can achieve by carefully thinking about what your goals are and how you can achieve them, following \"the system\" fares poorly.</span><br style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\" /></p>\n<blockquote style=\"margin: 0px; padding: 0px 1em 0px 1.25em; color: #666666; border-left-width: 3px; border-left-color: #eeeeee; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><strong style=\"margin: 0px; padding: 0px;\">KEEP IN MIND</strong>: Doing well within the system should be treated as a&nbsp;<em style=\"margin: 0px; padding: 0px;\">constraint</em>&nbsp;within which you need to operate, rather than a defining feature of your life.</blockquote>\n<p><em style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">By putting you in this situation, society has fouled you. Yes, as you suspect, a lot of the stuff you learn in your classes is crap. And yes, as you suspect, the college admissions process is largely a charade. But like many fouls, this one was unintentional. So just keep playing. Rebellion is almost as stupid as obedience. In either case you let yourself be defined by what they tell you to do. The best plan, I think, is to step onto an orthogonal vector. Don't just do what they tell you, and don't just refuse to. Instead treat school as a day job. As day jobs go, it's pretty sweet. You're done at 3 o'clock, and you can even work on your own stuff while you're there.&nbsp;</em><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">&mdash; Paul Graham in&nbsp;</span><span class=\"qlink_container\" style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><a class=\"external_link\" style=\"margin: 0px; padding: 0px 12px 0px 0px; text-decoration: none; color: #19558d; background-image: url(data; background-position: 100% 5px; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://paulgraham.com/hs.html\" target=\"_blank\">What You'll Wish You'd Known</a></span><br style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\" /></p>\n<h2><span style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Some subjects are more important than others</span></h2>\n<p>&nbsp;</p>\n<p><strong style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"></strong><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">High school can give an illusion of democracy between the different subjects you study: they all seem to get equal weight in class time and in grades, so you may believe that all subjects are equally important to study. This is not true even in general: some subjects are more important to study overall, and within each subject, some topics may be more important than what school seems to suggest.</span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Because the method of exposition and choice of topics in school is likely suboptimal, it generally makes sense to learn the important topics well ahead of time, and deal with the others as needed to do well on the courses.</span></p>\n<h2><span style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">It's important to choose your extracurriculars well</span></h2>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Many common extra-curricular activities (in particular, many school clubs) are not the most productive uses of time. If you go with the flow and sign up for the activities that the people around you are signing up for, you may sacrifice the opportunity to develop yourself and accomplish far more.</span></p>\n<h2><span style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Consider alternatives to high school</span></h2>\n<p><span style=\"font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; color: #333333;\"><span style=\"font-size: 19.09090805053711px; line-height: 30px;\">Consider homeschooling and online school. Depending on your situation, these may be superior alternatives to regular high school for you.&nbsp;</span></span></p>\n<p><span style=\"font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; color: #333333;\"><span style=\"font-size: 19.09090805053711px; line-height: 30px;\">This post is a modified version of a write-up for <a href=\"http://cognitomentoring.org/\">Cognito Mentoring</a>.</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h5R2gnpqsgXEsDuT7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 21, "extendedScore": null, "score": 1.5696854472771854e-06, "legacy": true, "legacyId": "25533", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><em>A brief essay intended for high school students: any thoughts?</em></span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">If you go to school, take the classes that people tell you to, do your homework, and engage in the extracurricular activities that your peers do, you'll be setting yourself up for an \"okay\" life. But you can do better than that.</span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><a id=\"more\"></a></span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">The school system wasn't designed to help you achieve your goals. It wasn't designed to optimize student welfare in general: it was cobbled together by many different actors with many different goals, from politicians to teachers to parents to colleges. It often suffers from inadequate resources. Even if the school system&nbsp;</span><em style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">were</em><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">&nbsp;optimized on average, the&nbsp;</span><em style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">one-size-fits-all</em><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">&nbsp;approach it takes means that it wouldn't be optimized for people who differ in any relevant respect from average. Compared with what you can achieve by carefully thinking about what your goals are and how you can achieve them, following \"the system\" fares poorly.</span><br style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"></p>\n<blockquote style=\"margin: 0px; padding: 0px 1em 0px 1.25em; color: #666666; border-left-width: 3px; border-left-color: #eeeeee; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><strong style=\"margin: 0px; padding: 0px;\">KEEP IN MIND</strong>: Doing well within the system should be treated as a&nbsp;<em style=\"margin: 0px; padding: 0px;\">constraint</em>&nbsp;within which you need to operate, rather than a defining feature of your life.</blockquote>\n<p><em style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">By putting you in this situation, society has fouled you. Yes, as you suspect, a lot of the stuff you learn in your classes is crap. And yes, as you suspect, the college admissions process is largely a charade. But like many fouls, this one was unintentional. So just keep playing. Rebellion is almost as stupid as obedience. In either case you let yourself be defined by what they tell you to do. The best plan, I think, is to step onto an orthogonal vector. Don't just do what they tell you, and don't just refuse to. Instead treat school as a day job. As day jobs go, it's pretty sweet. You're done at 3 o'clock, and you can even work on your own stuff while you're there.&nbsp;</em><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">\u2014 Paul Graham in&nbsp;</span><span class=\"qlink_container\" style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"><a class=\"external_link\" style=\"margin: 0px; padding: 0px 12px 0px 0px; text-decoration: none; color: #19558d; background-image: url(data; background-position: 100% 5px; background-repeat: no-repeat no-repeat;\" rel=\"nofollow\" href=\"http://paulgraham.com/hs.html\" target=\"_blank\">What You'll Wish You'd Known</a></span><br style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"></p>\n<h2 id=\"Some_subjects_are_more_important_than_others\"><span style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Some subjects are more important than others</span></h2>\n<p>&nbsp;</p>\n<p><strong style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\"></strong><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">High school can give an illusion of democracy between the different subjects you study: they all seem to get equal weight in class time and in grades, so you may believe that all subjects are equally important to study. This is not true even in general: some subjects are more important to study overall, and within each subject, some topics may be more important than what school seems to suggest.</span></p>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Because the method of exposition and choice of topics in school is likely suboptimal, it generally makes sense to learn the important topics well ahead of time, and deal with the others as needed to do well on the courses.</span></p>\n<h2 id=\"It_s_important_to_choose_your_extracurriculars_well\"><span style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">It's important to choose your extracurriculars well</span></h2>\n<p><span style=\"color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Many common extra-curricular activities (in particular, many school clubs) are not the most productive uses of time. If you go with the flow and sign up for the activities that the people around you are signing up for, you may sacrifice the opportunity to develop yourself and accomplish far more.</span></p>\n<h2 id=\"Consider_alternatives_to_high_school\"><span style=\"margin: 0px; padding: 0px; color: #333333; font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; font-size: 18.66666603088379px; line-height: 30px;\">Consider alternatives to high school</span></h2>\n<p><span style=\"font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; color: #333333;\"><span style=\"font-size: 19.09090805053711px; line-height: 30px;\">Consider homeschooling and online school. Depending on your situation, these may be superior alternatives to regular high school for you.&nbsp;</span></span></p>\n<p><span style=\"font-family: 'Palatino Linotype', Palatino, Palladio, 'URW Palladio L', 'Book Antiqua', Baskerville, 'Bookman Old Style', 'Bitstream Charter', 'Nimbus Roman No9 L', Garamond, 'Apple Garamond', 'ITC Garamond Narrow', 'New Century Schoolbook', 'Century Schoolbook', 'Century Schoolbook L', Georgia, serif; color: #333333;\"><span style=\"font-size: 19.09090805053711px; line-height: 30px;\">This post is a modified version of a write-up for <a href=\"http://cognitomentoring.org/\">Cognito Mentoring</a>.</span></span></p>", "sections": [{"title": "Some subjects are more important than others", "anchor": "Some_subjects_are_more_important_than_others", "level": 1}, {"title": "It's important to choose your extracurriculars well", "anchor": "It_s_important_to_choose_your_extracurriculars_well", "level": 1}, {"title": "Consider alternatives to high school", "anchor": "Consider_alternatives_to_high_school", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "64 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T11:35:49.393Z", "modifiedAt": null, "url": null, "title": "Identity and Death", "slug": "identity-and-death", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:05.807Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c6e9ysWLuWMoPdewM/identity-and-death", "pageUrlRelative": "/posts/c6e9ysWLuWMoPdewM/identity-and-death", "linkUrl": "https://www.lesswrong.com/posts/c6e9ysWLuWMoPdewM/identity-and-death", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Identity%20and%20Death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdentity%20and%20Death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6e9ysWLuWMoPdewM%2Fidentity-and-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Identity%20and%20Death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6e9ysWLuWMoPdewM%2Fidentity-and-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc6e9ysWLuWMoPdewM%2Fidentity-and-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 660, "htmlBody": "<p>This recent <a href=\"http://www.smbc-comics.com/index.php?id=3262#comic\">SMBC comic</a> illustrates the old question of what exactly is <em>you</em> by referencing the Star Trek Teleporter Problem. Do you actually get teleported or does the teleporter just kill you before making a copy of you somewhere else?</p>\n<p>Well, the answer that a lot of rationalist seem to accept is <a href=\"http://everything2.com/title/Pattern+Identity+Theory\">Pattern Identity Theory</a> proposed by Hans Moravec (skim the link or do a google search for the theory if you have no idea what I am referring to). I am very sympathetic to this view and it definitely ties with my limited understanding of physics and biology - <a href=\"http://en.wikipedia.org/wiki/Identical_particles\">elementary particles are interchangeable and do not have 'identity'</a>, <a href=\"http://skeptics.stackexchange.com/questions/18427/are-all-the-atoms-in-our-bodies-replaced-on-a-regular-basis\">at least some of the atoms in your body (including some of those who form neurons) get replaced over time</a> etc.</p>\n<p><br />This is all fine and dandy, but if you take this view to its logical extreme it looks like a sufficently modified version of you shouldn't actually qualify as <em>you</em> - the difference in the pattern might be as great or greater than the difference in the patterns of any two random people.</p>\n<p>Let's say something happens to Eliezer and he gets successfully cryo-preserved in 2014. Then 80 years later the singularity hasn't arrived yet but the future is still pretty good - everyone is smart and happy due to enhancements, ageing is a thing of the past and we have the technology to wake cryopreserved people up. The people in that future build Eliezer a new body, restore the information from his brain and apply all the standard enhancements on him and then they wake him up. The person who wakes up remembers all that good old Eliezer did and seems to act like you would expect an enhanced Eliezer to act. However, if you examine things closely the difference between 2014!Eliezer and 2094!Eliezer is actually bigger than the difference between 2014!Eliezer and let's say 2014!Yvain due to having all the new standard enhancements. Does that person <em>really</em> qualify as the same person according to Pattern Identity Theory, then? Sure, he originates from Eliezer and arguably the difference between the two is similar to the difference between kid!Eliezer and adult!Eliezer but is it really the same pattern? If you believe that you really are the pattern then how can you not think of Eliezer!2014 as a dead man?</p>\n<p>Sure, you could argue that continual change (as opposed to the sudden change in the cryo!Eliezer scenario) or 'evolution of the pattern' is in some way relevant but why would that be? The only somewhat reasonable argument for that I've seen is 'because it looks like this is what I care about'. That's fine with me but my personal preference is closer to 'I want to continue existing and experiencing things'; I don't care if anything that looks like me or thinks it's me is experiencing stuff - I want me (whatever that is) to continue living and doing stuff. And so far it looks really plausible that <em>me</em> is the pattern which sadly leaves me to think that maybe changing the pattern is a bad idea.</p>\n<p>I know that this line of thinking can damn you to eternal stagnation but it seems worth exploring before teleporters, uploading, big self-enhancements etc. come along which is why I am starting this discussion. Additionally, a part of the problem might be that there is some confusion about definitions going on but I'd like to see where. Furthermore, 'the difference in the pattern' seems both somehow hard to quantify and more importantly - it doesn't look like something that could have a clear cut-off as in 'if the pattern differs by more than 10% you are a different person'. At any rate, whatever that cut-off is, it still seems pretty clear that tenoke!2000 differs enough from me to be considered dead.</p>\n<p>As an exercise at home I will leave you to think about what this whole line of thinking implies if you combine it with MWII-style quantum immortality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c6e9ysWLuWMoPdewM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 14, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "25461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T17:48:57.854Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Lesswrong Meetup: How to Increase Your Brainpower", "slug": "meetup-atlanta-lesswrong-meetup-how-to-increase-your", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XseWiNo3QGy4tbSE5/meetup-atlanta-lesswrong-meetup-how-to-increase-your", "pageUrlRelative": "/posts/XseWiNo3QGy4tbSE5/meetup-atlanta-lesswrong-meetup-how-to-increase-your", "linkUrl": "https://www.lesswrong.com/posts/XseWiNo3QGy4tbSE5/meetup-atlanta-lesswrong-meetup-how-to-increase-your", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Lesswrong%20Meetup%3A%20How%20to%20Increase%20Your%20Brainpower&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Lesswrong%20Meetup%3A%20How%20to%20Increase%20Your%20Brainpower%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXseWiNo3QGy4tbSE5%2Fmeetup-atlanta-lesswrong-meetup-how-to-increase-your%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Lesswrong%20Meetup%3A%20How%20to%20Increase%20Your%20Brainpower%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXseWiNo3QGy4tbSE5%2Fmeetup-atlanta-lesswrong-meetup-how-to-increase-your", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXseWiNo3QGy4tbSE5%2Fmeetup-atlanta-lesswrong-meetup-how-to-increase-your", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ww'>Atlanta Lesswrong Meetup: How to Increase Your Brainpower</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 February 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Dual N-back, transcranial electrical stimulation, nootropics, what's all that about? What does the research say on what works and what doesn't? Come found out at Atlanta Lesswrong!</p>\n\n<p>A presenter will be giving a summary of the Dan Hurley book, \"Smarter\" (<a href=\"http://www.amazon.com/Smarter-Science-Building-Brain-Power/dp/1594631271\" rel=\"nofollow\">http://www.amazon.com/Smarter-Science-Building-Brain-Power/dp/1594631271</a>) as well as smaller discussions on individual topics.</p>\n\n<p>More presenters are needed and encouraged, if you'd like to talk on your favorite method!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ww'>Atlanta Lesswrong Meetup: How to Increase Your Brainpower</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XseWiNo3QGy4tbSE5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 1.5705267504347254e-06, "legacy": true, "legacyId": "25538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong_Meetup__How_to_Increase_Your_Brainpower\">Discussion article for the meetup : <a href=\"/meetups/ww\">Atlanta Lesswrong Meetup: How to Increase Your Brainpower</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 February 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Dual N-back, transcranial electrical stimulation, nootropics, what's all that about? What does the research say on what works and what doesn't? Come found out at Atlanta Lesswrong!</p>\n\n<p>A presenter will be giving a summary of the Dan Hurley book, \"Smarter\" (<a href=\"http://www.amazon.com/Smarter-Science-Building-Brain-Power/dp/1594631271\" rel=\"nofollow\">http://www.amazon.com/Smarter-Science-Building-Brain-Power/dp/1594631271</a>) as well as smaller discussions on individual topics.</p>\n\n<p>More presenters are needed and encouraged, if you'd like to talk on your favorite method!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong_Meetup__How_to_Increase_Your_Brainpower1\">Discussion article for the meetup : <a href=\"/meetups/ww\">Atlanta Lesswrong Meetup: How to Increase Your Brainpower</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Lesswrong Meetup: How to Increase Your Brainpower", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong_Meetup__How_to_Increase_Your_Brainpower", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Lesswrong Meetup: How to Increase Your Brainpower", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong_Meetup__How_to_Increase_Your_Brainpower1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T18:41:13.935Z", "modifiedAt": null, "url": null, "title": "The January 2013 CFAR workshop: one-year retrospective", "slug": "the-january-2013-cfar-workshop-one-year-retrospective", "viewCount": null, "lastCommentedAt": "2014-03-04T02:32:46.951Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Qiaochu_Yuan", "createdAt": "2012-11-24T08:36:50.547Z", "isAdmin": false, "displayName": "Qiaochu_Yuan"}, "userId": "qgFX9ZhzPCkcduZyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TwD5xm5pErbQ9LTwn/the-january-2013-cfar-workshop-one-year-retrospective", "pageUrlRelative": "/posts/TwD5xm5pErbQ9LTwn/the-january-2013-cfar-workshop-one-year-retrospective", "linkUrl": "https://www.lesswrong.com/posts/TwD5xm5pErbQ9LTwn/the-january-2013-cfar-workshop-one-year-retrospective", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20January%202013%20CFAR%20workshop%3A%20one-year%20retrospective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20January%202013%20CFAR%20workshop%3A%20one-year%20retrospective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwD5xm5pErbQ9LTwn%2Fthe-january-2013-cfar-workshop-one-year-retrospective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20January%202013%20CFAR%20workshop%3A%20one-year%20retrospective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwD5xm5pErbQ9LTwn%2Fthe-january-2013-cfar-workshop-one-year-retrospective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTwD5xm5pErbQ9LTwn%2Fthe-january-2013-cfar-workshop-one-year-retrospective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1528, "htmlBody": "<p><a href=\"/lw/gid/thoughts_on_the_january_cfar_workshop/\">About a year ago</a>, I attended my first CFAR workshop and wrote a post about it here. I mentioned in that post that it was too soon for me to tell if the workshop would have a large positive impact on my life. In the comments to that post, <a href=\"/lw/gid/thoughts_on_the_january_cfar_workshop/8dlg\">I was asked</a> to follow up on that post in a year to better evaluate that impact. So here we are!</p>\n<p>Very short summary: overall I think the workshop had a large and persistent positive impact on my life.&nbsp;</p>\n<h3>Important caveat</h3>\n<p>However, anyone using this post to evaluate the value of going to a CFAR workshop themselves should be aware that I'm local to Berkeley and have had many opportunities to stay connected to CFAR and the rationalist community. More specifically, in addition to the January workshop, I also</p>\n<ul>\n<li>visited the March workshop (and possibly others),</li>\n<li>attended various social events held by members of the community,</li>\n<li>taught at the July workshop, and</li>\n<li>taught at <a href=\"http://sparc-camp.org/\">SPARC</a>.</li>\n</ul>\n<p>These experiences were all very helpful in helping me digest and reinforce the workshop material (which was also improving over time), and a typical workshop participant might not have these advantages.&nbsp;</p>\n<h3>Answering a question</h3>\n<p><a href=\"/lw/jki/open_thread_january_25_february_1/ah9c\">pewpewlasergun</a> wanted me to answer the following question:</p>\n<blockquote>I'd like to know how many techniques you were taught at the meetup you still use regularly. Also which has had the largest effect on your life.</blockquote>\n<p>The short answer is: in some sense very few, but a lot of the value I got out of attending the workshop didn't come from specific techniques.&nbsp;</p>\n<p>In more detail: to be honest, many of the specific techniques are kind of a chore to use (at least as of January 2013). I experimented with a good number of them in the months after the workshop, and most of them haven't stuck (but that isn't so bad; the cost of trying a technique and finding that it doesn't work for you is low, while the benefit of trying a technique and finding that it does work for you can be quite high!). One that has is the idea of a <strong>next action</strong>, which I've found incredibly useful. Next actions are the things that to-do list items should be, say in the context of using <a href=\"http://www.rememberthemilk.com/home/qiaochu/\">Remember The Milk</a>. Many to-do list items you might be tempted to right down are difficult to actually do because they're either too vague or too big and hence trigger <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">ugh fields</a>. For example, you might have an item like</p>\n<ul>\n<li>Do my taxes</li>\n</ul>\n<p>that you don't get around to until right before you have to because you have an ugh field around doing your taxes. This item is both too vague and too big: instead of writing this down, write down the <strong>next physical action</strong>&nbsp;you need to take to make progress on this item, which might be something more like</p>\n<ul>\n<li>Find tax forms and put them on desk</li>\n</ul>\n<p>which is both concrete and small. Thinking in terms of next actions has been a huge upgrade to my <a href=\"http://zenhabits.net/the-getting-things-done-gtd-faq/\">GTD system</a>&nbsp;(as was <a href=\"https://workflowy.com/\">Workflowy</a>, which I also started using because of the workshop) and I do it constantly.&nbsp;</p>\n<p>But as I mentioned, a lot of the value I got out of attending the workshop was not from specific techniques. Much of the value comes from spending time with the workshop instructors and participants, which had effects that I find hard to summarize, but I'll try to describe some of them below:&nbsp;</p>\n<h3>Emotional attitudes</h3>\n<p>The workshop readjusted my&nbsp;emotional attitudes&nbsp;towards several things for the better, and at several meta levels. For example, a short conversation with a workshop alum completely readjusted my emotional attitude towards both nutrition and exercise, and I started paying more attention to what I ate and going to the gym (albeit sporadically) for the first time in my life not long afterwards. I lost about 15 pounds this way (mostly from the eating part, not the gym part, I think).&nbsp;</p>\n<p>At a higher meta level, I did a fair amount of experimenting with various lifestyle changes (cold showers, not shampooing) after the workshop and overall they had the effect of readjusting my emotional attitude towards change. I find it generally easier to change my behavior than I used to because I've had a lot of practice at it lately, and am more enthusiastic about the prospect of such changes.&nbsp;</p>\n<p>(Incidentally, I think emotional attitude adjustment is an underrated component of causing people to change their behavior, at least here on LW.)</p>\n<h3>Using all of my strength</h3>\n<p>The workshop is the first place I really understood, on a gut level, that I could use my brain to&nbsp;think about something other than math. It sounds silly when I phrase it like that, but at some point in the past I had incorporated into my identity that I was good at math but absentminded and silly about real-world matters, and I used it as an excuse not to fully engage intellectually with anything that wasn't math, especially anything practical. One way or another the workshop helped me realize this, and I stopped thinking this way.&nbsp;</p>\n<p>The result is that I constantly apply optimization power to situations I wouldn't have even tried to apply optimization power to before. For example, today I was trying to figure out why the water in my bathroom sink was draining so slowly. At first I thought it was because the strainer had become clogged with gunk, so I cleaned the strainer, but then I found out that even with the strainer removed the water was still draining slowly. In the past I might've given up here. Instead I looked around for something that would fit farther into the sink than my fingers and saw the handle of my plunger. I pumped the handle into the sink a few times and some extra gunk I hadn't known was there came out. The sink is fine now. (This might seem small to people who are more domestically talented than me, but trust me when I say I wasn't doing stuff like this before last year.)</p>\n<h3>Reflection and repair</h3>\n<p>Thanks to the workshop, my GTD system is now robust enough to consistently enable me to reflect on and repair my life (including my GTD system). For example, I'm quicker to attempt to deal with minor medical problems I have than I used to be. I also think more often about what I'm doing and whether I could be doing something better. In this regard I pay a lot of attention in particular to what habits I'm forming, although I don't use the specific techniques in the relevant CFAR unit.</p>\n<p>For example, at some point I had recorded in RTM that I was frustrated by the sensation of hours going by without remembering how I had spent them (usually because I was mindlessly browsing the internet). In response, I started keeping a record of what I was doing every half hour and categorizing each hour according to a combination of how productively and how intentionally I spent it (in the first iteration it was just how productively I spent it, but I found that this was making me feel too guilty about relaxing). For example:</p>\n<ul>\n<li>a half-hour intentionally spent reading a paper is marked green.</li>\n<li>a half-hour half-spent writing up solutions to a problem set and half-spent on Facebook is marked yellow.&nbsp;</li>\n<li>a half-hour intentionally spent playing a video game is marked with no color.</li>\n<li>a half-hour mindlessly browsing the internet when I had intended to do work is marked red.&nbsp;</li>\n</ul>\n<p>The act of doing this every half hour itself helps make me more mindful about how I spend my time, but having a record of how I spend my time has also helped me notice interesting things, like how less of my time is under my direct control than I had thought (but instead is taken up by classes, commuting, eating, etc.). It's also easier for me to get into a <a href=\"/lw/3w3/how_to_beat_procrastination/\">success spiral</a> when I see a lot of green.&nbsp;</p>\n<h3>Stimulation</h3>\n<p>Being around workshop instructors and participants is consistently intellectually stimulating. I don't have a tactful way of saying what I'm about to say next, but: two effects of this are that I think more interesting thoughts than I used to and also that I'm funnier than I used to be. (I realize that these are both hard to quantify.)&nbsp;</p>\n<h3>etc.</h3>\n<p>I worry that I haven't given a complete picture here, but hopefully anything I've left out will be brought up in the comments one way or another. (Edit: this totally happened! Please read <a href=\"/r/discussion/lw/jm3/the_january_2013_cfar_workshop_oneyear/al1z\">Anna Salamon's comment below</a>.)&nbsp;</p>\n<ul>\n</ul>\n<h3>Takeaway for prospective workshop attendees</h3>\n<p>I'm not actually sure what you should take away from all this if your goal is to figure out whether you should <a href=\"http://rationality.org/workshops/\">attend a workshop yourself</a>. My thoughts are roughly this: I think attending a workshop is potentially high-value and therefore that even <a href=\"http://rationality.org/talk/\">talking to CFAR about any questions you might have</a> is potentially high-value, in addition to being relatively low-cost. If you think there's even a small chance you could get a lot of value out of attending a workshop I recommend that you at least take that one step.&nbsp;</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zcvsZQWJBFK6SxK4K": 2, "X7v7Fyp9cgBYaMe2e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TwD5xm5pErbQ9LTwn", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 53, "extendedScore": null, "score": 0.0005264750054754617, "legacy": true, "legacyId": "25419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"/lw/gid/thoughts_on_the_january_cfar_workshop/\">About a year ago</a>, I attended my first CFAR workshop and wrote a post about it here. I mentioned in that post that it was too soon for me to tell if the workshop would have a large positive impact on my life. In the comments to that post, <a href=\"/lw/gid/thoughts_on_the_january_cfar_workshop/8dlg\">I was asked</a> to follow up on that post in a year to better evaluate that impact. So here we are!</p>\n<p>Very short summary: overall I think the workshop had a large and persistent positive impact on my life.&nbsp;</p>\n<h3 id=\"Important_caveat\">Important caveat</h3>\n<p>However, anyone using this post to evaluate the value of going to a CFAR workshop themselves should be aware that I'm local to Berkeley and have had many opportunities to stay connected to CFAR and the rationalist community. More specifically, in addition to the January workshop, I also</p>\n<ul>\n<li>visited the March workshop (and possibly others),</li>\n<li>attended various social events held by members of the community,</li>\n<li>taught at the July workshop, and</li>\n<li>taught at <a href=\"http://sparc-camp.org/\">SPARC</a>.</li>\n</ul>\n<p>These experiences were all very helpful in helping me digest and reinforce the workshop material (which was also improving over time), and a typical workshop participant might not have these advantages.&nbsp;</p>\n<h3 id=\"Answering_a_question\">Answering a question</h3>\n<p><a href=\"/lw/jki/open_thread_january_25_february_1/ah9c\">pewpewlasergun</a> wanted me to answer the following question:</p>\n<blockquote>I'd like to know how many techniques you were taught at the meetup you still use regularly. Also which has had the largest effect on your life.</blockquote>\n<p>The short answer is: in some sense very few, but a lot of the value I got out of attending the workshop didn't come from specific techniques.&nbsp;</p>\n<p>In more detail: to be honest, many of the specific techniques are kind of a chore to use (at least as of January 2013). I experimented with a good number of them in the months after the workshop, and most of them haven't stuck (but that isn't so bad; the cost of trying a technique and finding that it doesn't work for you is low, while the benefit of trying a technique and finding that it does work for you can be quite high!). One that has is the idea of a <strong>next action</strong>, which I've found incredibly useful. Next actions are the things that to-do list items should be, say in the context of using <a href=\"http://www.rememberthemilk.com/home/qiaochu/\">Remember The Milk</a>. Many to-do list items you might be tempted to right down are difficult to actually do because they're either too vague or too big and hence trigger <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">ugh fields</a>. For example, you might have an item like</p>\n<ul>\n<li>Do my taxes</li>\n</ul>\n<p>that you don't get around to until right before you have to because you have an ugh field around doing your taxes. This item is both too vague and too big: instead of writing this down, write down the <strong>next physical action</strong>&nbsp;you need to take to make progress on this item, which might be something more like</p>\n<ul>\n<li>Find tax forms and put them on desk</li>\n</ul>\n<p>which is both concrete and small. Thinking in terms of next actions has been a huge upgrade to my <a href=\"http://zenhabits.net/the-getting-things-done-gtd-faq/\">GTD system</a>&nbsp;(as was <a href=\"https://workflowy.com/\">Workflowy</a>, which I also started using because of the workshop) and I do it constantly.&nbsp;</p>\n<p>But as I mentioned, a lot of the value I got out of attending the workshop was not from specific techniques. Much of the value comes from spending time with the workshop instructors and participants, which had effects that I find hard to summarize, but I'll try to describe some of them below:&nbsp;</p>\n<h3 id=\"Emotional_attitudes\">Emotional attitudes</h3>\n<p>The workshop readjusted my&nbsp;emotional attitudes&nbsp;towards several things for the better, and at several meta levels. For example, a short conversation with a workshop alum completely readjusted my emotional attitude towards both nutrition and exercise, and I started paying more attention to what I ate and going to the gym (albeit sporadically) for the first time in my life not long afterwards. I lost about 15 pounds this way (mostly from the eating part, not the gym part, I think).&nbsp;</p>\n<p>At a higher meta level, I did a fair amount of experimenting with various lifestyle changes (cold showers, not shampooing) after the workshop and overall they had the effect of readjusting my emotional attitude towards change. I find it generally easier to change my behavior than I used to because I've had a lot of practice at it lately, and am more enthusiastic about the prospect of such changes.&nbsp;</p>\n<p>(Incidentally, I think emotional attitude adjustment is an underrated component of causing people to change their behavior, at least here on LW.)</p>\n<h3 id=\"Using_all_of_my_strength\">Using all of my strength</h3>\n<p>The workshop is the first place I really understood, on a gut level, that I could use my brain to&nbsp;think about something other than math. It sounds silly when I phrase it like that, but at some point in the past I had incorporated into my identity that I was good at math but absentminded and silly about real-world matters, and I used it as an excuse not to fully engage intellectually with anything that wasn't math, especially anything practical. One way or another the workshop helped me realize this, and I stopped thinking this way.&nbsp;</p>\n<p>The result is that I constantly apply optimization power to situations I wouldn't have even tried to apply optimization power to before. For example, today I was trying to figure out why the water in my bathroom sink was draining so slowly. At first I thought it was because the strainer had become clogged with gunk, so I cleaned the strainer, but then I found out that even with the strainer removed the water was still draining slowly. In the past I might've given up here. Instead I looked around for something that would fit farther into the sink than my fingers and saw the handle of my plunger. I pumped the handle into the sink a few times and some extra gunk I hadn't known was there came out. The sink is fine now. (This might seem small to people who are more domestically talented than me, but trust me when I say I wasn't doing stuff like this before last year.)</p>\n<h3 id=\"Reflection_and_repair\">Reflection and repair</h3>\n<p>Thanks to the workshop, my GTD system is now robust enough to consistently enable me to reflect on and repair my life (including my GTD system). For example, I'm quicker to attempt to deal with minor medical problems I have than I used to be. I also think more often about what I'm doing and whether I could be doing something better. In this regard I pay a lot of attention in particular to what habits I'm forming, although I don't use the specific techniques in the relevant CFAR unit.</p>\n<p>For example, at some point I had recorded in RTM that I was frustrated by the sensation of hours going by without remembering how I had spent them (usually because I was mindlessly browsing the internet). In response, I started keeping a record of what I was doing every half hour and categorizing each hour according to a combination of how productively and how intentionally I spent it (in the first iteration it was just how productively I spent it, but I found that this was making me feel too guilty about relaxing). For example:</p>\n<ul>\n<li>a half-hour intentionally spent reading a paper is marked green.</li>\n<li>a half-hour half-spent writing up solutions to a problem set and half-spent on Facebook is marked yellow.&nbsp;</li>\n<li>a half-hour intentionally spent playing a video game is marked with no color.</li>\n<li>a half-hour mindlessly browsing the internet when I had intended to do work is marked red.&nbsp;</li>\n</ul>\n<p>The act of doing this every half hour itself helps make me more mindful about how I spend my time, but having a record of how I spend my time has also helped me notice interesting things, like how less of my time is under my direct control than I had thought (but instead is taken up by classes, commuting, eating, etc.). It's also easier for me to get into a <a href=\"/lw/3w3/how_to_beat_procrastination/\">success spiral</a> when I see a lot of green.&nbsp;</p>\n<h3 id=\"Stimulation\">Stimulation</h3>\n<p>Being around workshop instructors and participants is consistently intellectually stimulating. I don't have a tactful way of saying what I'm about to say next, but: two effects of this are that I think more interesting thoughts than I used to and also that I'm funnier than I used to be. (I realize that these are both hard to quantify.)&nbsp;</p>\n<h3 id=\"etc_\">etc.</h3>\n<p>I worry that I haven't given a complete picture here, but hopefully anything I've left out will be brought up in the comments one way or another. (Edit: this totally happened! Please read <a href=\"/r/discussion/lw/jm3/the_january_2013_cfar_workshop_oneyear/al1z\">Anna Salamon's comment below</a>.)&nbsp;</p>\n<ul>\n</ul>\n<h3 id=\"Takeaway_for_prospective_workshop_attendees\">Takeaway for prospective workshop attendees</h3>\n<p>I'm not actually sure what you should take away from all this if your goal is to figure out whether you should <a href=\"http://rationality.org/workshops/\">attend a workshop yourself</a>. My thoughts are roughly this: I think attending a workshop is potentially high-value and therefore that even <a href=\"http://rationality.org/talk/\">talking to CFAR about any questions you might have</a> is potentially high-value, in addition to being relatively low-cost. If you think there's even a small chance you could get a lot of value out of attending a workshop I recommend that you at least take that one step.&nbsp;</p>\n<ul>\n</ul>", "sections": [{"title": "Important caveat", "anchor": "Important_caveat", "level": 1}, {"title": "Answering a question", "anchor": "Answering_a_question", "level": 1}, {"title": "Emotional attitudes", "anchor": "Emotional_attitudes", "level": 1}, {"title": "Using all of my strength", "anchor": "Using_all_of_my_strength", "level": 1}, {"title": "Reflection and repair", "anchor": "Reflection_and_repair", "level": 1}, {"title": "Stimulation", "anchor": "Stimulation", "level": 1}, {"title": "etc.", "anchor": "etc_", "level": 1}, {"title": "Takeaway for prospective workshop attendees", "anchor": "Takeaway_for_prospective_workshop_attendees", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9FfxfaLQN2rRvSjp7", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-02-18T18:41:13.935Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T20:08:48.833Z", "modifiedAt": null, "url": null, "title": "A Fervent Defense of Frequentist Statistics", "slug": "a-fervent-defense-of-frequentist-statistics", "viewCount": null, "lastCommentedAt": "2020-08-02T12:47:35.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KdwP5i6N4E4q6BGkr/a-fervent-defense-of-frequentist-statistics", "pageUrlRelative": "/posts/KdwP5i6N4E4q6BGkr/a-fervent-defense-of-frequentist-statistics", "linkUrl": "https://www.lesswrong.com/posts/KdwP5i6N4E4q6BGkr/a-fervent-defense-of-frequentist-statistics", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Fervent%20Defense%20of%20Frequentist%20Statistics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Fervent%20Defense%20of%20Frequentist%20Statistics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKdwP5i6N4E4q6BGkr%2Fa-fervent-defense-of-frequentist-statistics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Fervent%20Defense%20of%20Frequentist%20Statistics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKdwP5i6N4E4q6BGkr%2Fa-fervent-defense-of-frequentist-statistics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKdwP5i6N4E4q6BGkr%2Fa-fervent-defense-of-frequentist-statistics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4915, "htmlBody": "<p><em> [Highlights for the busy: de-bunking standard \"Bayes is optimal\" arguments; frequentist Solomonoff induction; and a description of the online learning framework. Note: cross-posted from my <a href=\"http://jsteinhardt.wordpress.com/2014/02/10/a-fervent-defense-of-frequentist-statistics/\">blog</a>.]</em></p>\n<p><strong>Short summary.</strong> This essay makes many points, each of which I think is worth reading, but if you are only going to understand one point I think it should be &ldquo;Myth 5&Prime; below, which describes the online learning framework as a response to the claim that frequentist methods need to make strong modeling assumptions. Among other things, online learning allows me to perform the following remarkable feat: if I&rsquo;m betting on horses, and I get to place bets after watching other people bet but before seeing which horse wins the race, then I can guarantee that after a relatively small number of races, I will do almost as well overall as the best other person, even if the number of other people is very large (say, 1 billion), and their performance is correlated in complicated ways.</p>\n<p>If you&rsquo;re only going to understand two points, then also read about the frequentist version of Solomonoff induction, which is described in &ldquo;Myth 6&Prime;.</p>\n<p><strong>Main article.</strong> I&rsquo;ve already written one essay on <a href=\"http://cs.stanford.edu/~jsteinhardt/stats-essay.pdf\">Bayesian vs. frequentist statistics</a>. In that essay, I argued for a balanced, pragmatic approach in which we think of the two families of methods as a collection of tools to be used as appropriate. Since I&rsquo;m currently feeling contrarian, this essay will be far less balanced and will argue explicitly against Bayesian methods and in favor of frequentist methods. I hope this will be forgiven as so much other writing goes in the opposite direction of unabashedly defending Bayes. I should note that this essay is partially inspired by some of Cosma Shalizi&rsquo;s blog posts, such as <a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/612.html\">this</a> one.</p>\n<p>This essay will start by listing a series of myths, then debunk them one-by-one. My main motivation for this is that Bayesian approaches seem to be highly popularized, to the point that one may get the impression that they are the uncontroversially superior method of doing statistics. I actually think the opposite is true: I think most statisticans would for the most part defend frequentist methods, although there are also many departments that are decidedly Bayesian (e.g. many places in England, as well as some U.S. universities like Columbia). I have a lot of respect for many of the people at these universities, such as Andrew Gelman and Philip Dawid, but I worry that many of the other proponents of Bayes (most of them non-statisticians) tend to oversell Bayesian methods or undersell alternative methodologies.</p>\n<p>If you are like me from, say, two years ago, you are firmly convinced that Bayesian methods are superior and that you have knockdown arguments in favor of this. If this is the case, then I hope this essay will give you an experience that I myself found life-altering: the experience of having a way of thinking that seemed unquestionably true slowly dissolve into just one of many imperfect models of reality. This experience helped me gain more explicit appreciation for the skill of viewing the world from many different angles, and of distinguishing between a very successful paradigm and reality.</p>\n<p><a id=\"more\"></a></p>\n<p>If you are not like me, then you may have had the experience of bringing up one of many reasonable objections to normative Bayesian epistemology, and having it shot down by one of many &ldquo;standard&rdquo; arguments that seem wrong but not for easy-to-articulate reasons. I hope to lend some reprieve to those of you in this camp, by providing a collection of &ldquo;standard&rdquo; replies to these standard arguments.</p>\n<p>I will start with the myths (and responses) that I think will require the least technical background and be most interesting to a general audience. Toward the end, I deal with some attacks on frequentist methods that I believe amount to technical claims that are demonstrably false; doing so involves more math. Also, I should note that for the sake of simplicity I&rsquo;ve labeled everything that is non-Bayesian as a &ldquo;frequentist&rdquo; method, even though I think there&rsquo;s actually a fair amount of variation among these methods, although also a fair amount of overlap (e.g. I&rsquo;m throwing in statistical learning theory with minimax estimation, which certainly have a lot of overlap in ideas but were also in some sense developed by different communities).</p>\n<p><strong>The </strong><strong>Myths:</strong></p>\n<ul>\n<li>Bayesian methods are optimal.</li>\n<li>Bayesian methods are optimal except for computational considerations.</li>\n<li>We can deal with computational constraints simply by making approximations to Bayes.</li>\n<li>The prior isn&rsquo;t a big deal because Bayesians can always share likelihood ratios.</li>\n<li>Frequentist methods need to assume their model is correct, or that the data are i.i.d.</li>\n<li>Frequentist methods can only deal with simple models, and make arbitrary cutoffs in model complexity (aka: &ldquo;I&rsquo;m Bayesian because I want to do Solomonoff induction&rdquo;).</li>\n<li>Frequentist methods hide their assumptions while Bayesian methods make assumptions explicit.</li>\n<li>Frequentist methods are fragile, Bayesian methods are robust.</li>\n<li>Frequentist methods are responsible for bad science</li>\n<li>Frequentist methods are unprincipled/hacky.</li>\n<li>Frequentist methods have no promising approach to computationally bounded inference.</li>\n</ul>\n<p><em>Myth </em><em>1</em><em>: </em><em>Bayesian methods are optimal.</em> Presumably when most people say this they are thinking of either Dutch-booking or the complete class theorem. Roughly what these say are the following:</p>\n<p><strong>Dutch-book argument:</strong> Every coherent set of beliefs can be modeled as a subjective probability distribution. (Roughly, coherent means &ldquo;unable to be Dutch-booked&rdquo;.)</p>\n<p><strong>Complete class theorem:</strong> Every non-Bayesian method is worse than some Bayesian method (in the sense of performing deterministically at least as poorly in every possible world).</p>\n<p>Let&rsquo;s unpack both of these. My high-level argument regarding Dutch books is that I would much rather spend my time trying to correspond with reality than trying to be internally consistent. More concretely, the Dutch-book argument says that if for every bet you force me to take one side or the other, then unless I&rsquo;m Bayesian there&rsquo;s a collection of bets that will cause me to lose money for sure. I don&rsquo;t find this very compelling. This seems analogous to the situation where there&rsquo;s some quant at Jane Street, and they&rsquo;re about to run code that will make thousands of dollars trading stocks, and someone comes up to them and says &ldquo;Wait! You should add checks to your code to make sure that no subset of your trades will lose you money!&rdquo; This just doesn&rsquo;t seem worth the quant&rsquo;s time, it will slow down the code substantially, and instead the quant should be writing the next program to make thousands more dollars. This is basically what dutch-booking arguments seem like to me.</p>\n<p>Moving on, the complete class theorem says that for any decision rule, I can do better by replacing it with <em>some </em>Bayesian decision rule. But this injunction is not useful in practice, because it doesn&rsquo;t say anything about <em>which </em>decision rule I should replace it with. Of course, if you hand me a decision rule and give me infinite computational resources, then I can hand you back a Bayesian method that will perform better. But it still might not perform <strong>well</strong>. All the complete class theorem says is that every local optimum is Bayesan. To be a useful theory of epistemology, I need a prescription for how, in the first place, I am to arrive at a <em>good</em> decision rule, <em>not </em>just a locally optimal one. And this is something that frequentist methods do provide, to a far greater extent than Bayesian methods (for instance by using minimax decision rules such as the maximum-entropy example given later). Note also that many frequentist methods <em>do</em> correspond to a Bayesian method for some appropriately chosen prior. But the crucial point is that the frequentist <em>told</em> me how to pick a prior I would be happy with (also, many frequentist methods <em>don&rsquo;t</em> correspond to a Bayesian method for any choice of prior; they nevertheless often perform quite well).</p>\n<p><em>Myth </em><em>2</em><em>: </em><em>Bayesian methods are optimal except for computational considerations.</em> We already covered this in the previous point under the complete class theorem, but to re-iterate: <strong>Bayesian methods are </strong><strong><em>locally </em></strong><strong>optimal, </strong><strong><em>not</em></strong><strong> global optimal. Identifying all the local optima is very different from knowing which of them is the global optimum.</strong> I would much rather have someone hand me something that wasn&rsquo;t a local optimum but was close to the global optimum, than something that was a local optimum but was far from the global optimum.</p>\n<p><em>Myth </em><em>3</em><em>: </em><em>We can deal with computational constraints simply by making approximations to Bayes.</em> I have rarely seen this born out in practice. Here&rsquo;s a challenge: suppose I give you data generated in the following way. There are a collection of vectors <img class=\"latex\" title=\"{x_1}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x_1}\" />, <img class=\"latex\" title=\"{x_2}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx_2%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x_2}\" />, <img class=\"latex\" title=\"{\\ldots}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cldots%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\ldots}\" />, <img class=\"latex\" title=\"{x_{10,000}}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx_%7B10%2C000%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x_{10,000}}\" />, each with <img class=\"latex\" title=\"{10^6}\" src=\"http://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{10^6}\" /> coordinates. I generate outputs <img class=\"latex\" title=\"{y_1}\" src=\"http://s0.wp.com/latex.php?latex=%7By_1%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_1}\" />, <img class=\"latex\" title=\"{y_2}\" src=\"http://s0.wp.com/latex.php?latex=%7By_2%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_2}\" />, <img class=\"latex\" title=\"{\\ldots}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cldots%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\ldots}\" />, <img class=\"latex\" title=\"{y_{10,000}}\" src=\"http://s0.wp.com/latex.php?latex=%7By_%7B10%2C000%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_{10,000}}\" /> in the following way. First I globally select <img class=\"latex\" title=\"{100}\" src=\"http://s0.wp.com/latex.php?latex=%7B100%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{100}\" /> of the <img class=\"latex\" title=\"{10^6}\" src=\"http://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{10^6}\" /> coordinates uniformly at random, then I select a fixed vector <img class=\"latex\" title=\"{u}\" src=\"http://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{u}\" /> such that those <img class=\"latex\" title=\"{100}\" src=\"http://s0.wp.com/latex.php?latex=%7B100%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{100}\" /> coordinates are drawn from i.i.d. Gaussians and the rest of the coordinates are zero. Now I set <img class=\"latex\" title=\"{x_n = u^{\\top}y_n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx_n+%3D+u%5E%7B%5Ctop%7Dy_n%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x_n = u^{\\top}y_n}\" /> (i.e. <img class=\"latex\" title=\"{x_n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx_n%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x_n}\" /> is the dot product of <img class=\"latex\" title=\"{u}\" src=\"http://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{u}\" /> with <img class=\"latex\" title=\"{y_n}\" src=\"http://s0.wp.com/latex.php?latex=%7By_n%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_n}\" />). You are given <img class=\"latex\" title=\"{x}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x}\" /> and <img class=\"latex\" title=\"{y}\" src=\"http://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y}\" />, and your job is to infer <img class=\"latex\" title=\"{u}\" src=\"http://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{u}\" />. This is a completely well-specified problem, the only task remaining is computational. I know people who have solved this problem using Bayesan methods with approximate inference. I have respect for these people, because doing so is no easy task. I think very few of them would say that &ldquo;we can just approximate Bayesian updating and be fine&rdquo;. (Also, this particular problem can be solved trivially with frequentist methods.)</p>\n<p>A particularly egregious example of this is when people talk about &ldquo;computable approximations to Solomonoff induction&rdquo; or &ldquo;computable approximations to AIXI&rdquo; as if such notions were meaningful.</p>\n<p><em>Myth </em><em>4</em><em>: </em><em>the prior isn&rsquo;t a big deal because Bayesians can always share likelihood ratios.</em> Putting aside the practical issue that there would in general be an infinite number of likelihood ratios to share, there is the larger issue that for any hypothesis <img class=\"latex\" title=\"{h}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h}\" />, there is also the hypothesis <img class=\"latex\" title=\"{h'}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh%27%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h'}\" /> that matches <img class=\"latex\" title=\"{h}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h}\" /> exactly up to now, and then predicts the opposite of <img class=\"latex\" title=\"{h}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h}\" /> at all points in the future. You have to constrain model complexity at some point, the question is about how. To put this another way, sharing my likelihood ratios without also constraining model complexity (by focusing on a subset of all logically possible hypotheses) would be equivalent to just sharing all sensory data I&rsquo;ve ever accrued in my life. To the extent that such a notion is even possible, I certainly don&rsquo;t need to be a Bayesian to do such a thing.</p>\n<p><em>Myth 5: frequentist methods need to assume their model is correct or that the data are i.i.d.</em> <strong>Understanding the content of this section is the most important single insight to gain from this essay.</strong> For some reason it&rsquo;s assumed that frequentist methods need to make strong assumptions (such as Gaussianity), whereas Bayesian methods are somehow immune to this. In reality, the opposite is true. While there are many beautiful and deep frequentist formalisms that answer this, I will choose to focus on one of my favorite, which is <strong>online learning</strong>.</p>\n<p>To explain the online learning framework, let us suppose that our data are <img class=\"latex\" title=\"{(x_1, y_1), (x_2, y_2), \\ldots, (x_T, y_T)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%28x_1%2C+y_1%29%2C+%28x_2%2C+y_2%29%2C+%5Cldots%2C+%28x_T%2C+y_T%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{(x_1, y_1), (x_2, y_2), \\ldots, (x_T, y_T)}\" />. We don&rsquo;t observe <img class=\"latex\" title=\"{y_t}\" src=\"http://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_t}\" /> until after making a prediction <img class=\"latex\" title=\"{z_t}\" src=\"http://s0.wp.com/latex.php?latex=%7Bz_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{z_t}\" /> of what <img class=\"latex\" title=\"{y_t}\" src=\"http://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_t}\" /> will be, and then we receive a penalty <img class=\"latex\" title=\"{L(y_t, z_t)}\" src=\"http://s0.wp.com/latex.php?latex=%7BL%28y_t%2C+z_t%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{L(y_t, z_t)}\" /> based on how incorrect we were. So we can think of this as receiving prediction problems one-by-one, and in particular we make no assumptions about the relationship between the different problems; they could be i.i.d., they could be positively correlated, they could be anti-correlated, they could even be adversarially chosen.</p>\n<p>As a running example, suppose that I&rsquo;m betting on horses and before each race there are <img class=\"latex\" title=\"{n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{n}\" /> other people who give me advice on which horse to bet on. I know nothing about horses, so based on this advice I&rsquo;d like to devise a good betting strategy. In this case, <img class=\"latex\" title=\"{x_t}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x_t}\" /> would be the <img class=\"latex\" title=\"{n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{n}\" /> bets that each of the other people recommend, <img class=\"latex\" title=\"{z_t}\" src=\"http://s0.wp.com/latex.php?latex=%7Bz_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{z_t}\" /> would be the horse that I actually bet on, and <img class=\"latex\" title=\"{y_t}\" src=\"http://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_t}\" /> would be the horse that actually wins the race. Then, supposing that <img class=\"latex\" title=\"{y_t = z_t}\" src=\"http://s0.wp.com/latex.php?latex=%7By_t+%3D+z_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_t = z_t}\" /> (i.e., the horse I bet on actually wins), <img class=\"latex\" title=\"{L(y_t, z_t)}\" src=\"http://s0.wp.com/latex.php?latex=%7BL%28y_t%2C+z_t%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{L(y_t, z_t)}\" /> is the negative of the payoff from correctly betting on that horse. Otherwise, if the horse I bet on doesn&rsquo;t win, <img class=\"latex\" title=\"{L(y_t, z_t)}\" src=\"http://s0.wp.com/latex.php?latex=%7BL%28y_t%2C+z_t%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{L(y_t, z_t)}\" /> is the cost I had to pay to place the bet.</p>\n<p>If I&rsquo;m in this setting, what guarantee can I hope for? I might ask for an algorithm that is guaranteed to make good bets &mdash; but this seems impossible unless the people advising me actually know something about horses. Or, at the very least, <em>one</em> of the people advising me knows something. Motivated by this, I define my <strong>regret</strong> to be the difference between my penalty and the penalty of the best of the <img class=\"latex\" title=\"{n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{n}\" /> people (note that I only have access to the latter after all <img class=\"latex\" title=\"{T}\" src=\"http://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{T}\" /> rounds of betting). More formally, given a class <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> of predictors <img class=\"latex\" title=\"{h : x \\mapsto z}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh+%3A+x+%5Cmapsto+z%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h : x \\mapsto z}\" />, I define</p>\n<p align=\"center\"><img class=\"latex\" title=\"\\displaystyle \\mathrm{Regret}(T) = \\frac{1}{T} \\sum_{t=1}^T L(y_t, z_t) - \\min_{h \\in \\mathcal{M}} \\frac{1}{T} \\sum_{t=1}^T L(y_t, h(x_t))\" src=\"http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BRegret%7D%28T%29+%3D+%5Cfrac%7B1%7D%7BT%7D+%5Csum_%7Bt%3D1%7D%5ET+L%28y_t%2C+z_t%29+-+%5Cmin_%7Bh+%5Cin+%5Cmathcal%7BM%7D%7D+%5Cfrac%7B1%7D%7BT%7D+%5Csum_%7Bt%3D1%7D%5ET+L%28y_t%2C+h%28x_t%29%29&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"\\displaystyle \\mathrm{Regret}(T) = \\frac{1}{T} \\sum_{t=1}^T L(y_t, z_t) - \\min_{h \\in \\mathcal{M}} \\frac{1}{T} \\sum_{t=1}^T L(y_t, h(x_t))\" /></p>\n<p>In this case, <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> would have size <img class=\"latex\" title=\"{n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{n}\" /> and the <img class=\"latex\" title=\"{i}\" src=\"http://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{i}\" />th predictor would just always follow the advice of person <img class=\"latex\" title=\"{i}\" src=\"http://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{i}\" />. The regret is then how much worse I do on average than the best expert. A remarkable fact is that, in this case, there is a strategy such that <img class=\"latex\" title=\"{\\mathrm{Regret}(T)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BRegret%7D%28T%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathrm{Regret}(T)}\" /> shrinks at a rate of <img class=\"latex\" title=\"{\\sqrt{\\frac{\\log(n)}{T}}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B%5Cfrac%7B%5Clog%28n%29%7D%7BT%7D%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\sqrt{\\frac{\\log(n)}{T}}}\" />. In other words, I can have an average score within <img class=\"latex\" title=\"{\\epsilon}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\epsilon}\" /> of the best advisor after <img class=\"latex\" title=\"{\\frac{\\log(n)}{\\epsilon^2}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B%5Clog%28n%29%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\frac{\\log(n)}{\\epsilon^2}}\" /> rounds of betting.</p>\n<p>One reason that this is remarkable is that it does not depend at all on how the data are distributed; <strong>the data could be i.i.d., positively correlated, negatively correlated, even adversarial,</strong> and one can still construct an (adaptive) prediction rule that does almost as well as the best predictor in the family.</p>\n<p>To be even more concrete, if we assume that all costs and payoffs are bounded by <img class=\"latex\" title=\"{\\$1}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5C%241%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\$1}\" /> per round, and that there are <img class=\"latex\" title=\"{1,000,000,000}\" src=\"http://s0.wp.com/latex.php?latex=%7B1%2C000%2C000%2C000%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{1,000,000,000}\" /> people in total, then an explicit upper bound is that after <img class=\"latex\" title=\"{28/\\epsilon^2}\" src=\"http://s0.wp.com/latex.php?latex=%7B28%2F%5Cepsilon%5E2%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{28/\\epsilon^2}\" /> rounds, we will be within <img class=\"latex\" title=\"{\\epsilon}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\epsilon}\" /> dollars on average of the best other person. Under slightly stronger assumptions, we can do even better, for instance if the best person has an average variance of <img class=\"latex\" title=\"{0.1}\" src=\"http://s0.wp.com/latex.php?latex=%7B0.1%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{0.1}\" /> about their mean, then the <img class=\"latex\" title=\"{28}\" src=\"http://s0.wp.com/latex.php?latex=%7B28%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{28}\" /> can be replaced with <img class=\"latex\" title=\"{4.5}\" src=\"http://s0.wp.com/latex.php?latex=%7B4.5%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{4.5}\" />.</p>\n<p>It is important to note that the betting scenario is just a running example, and one can still obtain regret bounds under fairly general scenarios; <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> could be continuous and <img class=\"latex\" title=\"{L}\" src=\"http://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{L}\" /> could have quite general structure; the only technical assumption is that <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> be a convex set and that <img class=\"latex\" title=\"{L}\" src=\"http://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{L}\" /> be a convex function of <img class=\"latex\" title=\"{z}\" src=\"http://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{z}\" />. These assumptions tend to be easy to satisfy, though I have run into a few situations where they end up being problematic, mainly for computational reasons. For an <img class=\"latex\" title=\"{n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{n}\" />-dimensional model family, typically <img class=\"latex\" title=\"{\\mathrm{Regret}(T)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BRegret%7D%28T%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathrm{Regret}(T)}\" /> decreases at a rate of <img class=\"latex\" title=\"{\\sqrt{\\frac{n}{T}}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B%5Cfrac%7Bn%7D%7BT%7D%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\sqrt{\\frac{n}{T}}}\" />, although under additional assumptions this can be reduced to <img class=\"latex\" title=\"{\\sqrt{\\frac{\\log(n)}{T}}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B%5Cfrac%7B%5Clog%28n%29%7D%7BT%7D%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\sqrt{\\frac{\\log(n)}{T}}}\" />, as in the betting example above. I would consider this reduction to be one of the crowning results of modern frequentist statistics.</p>\n<p>Yes, these guarantees sound incredibly awesome and perhaps too good to be true. They actually are that awesome, and they are actually true. The work is being done by measuring the error relative to the best model in the model family. We aren&rsquo;t required to do well in an absolute sense, we just need to not do any worse than the best model. Of as long as at least one of the models in our family makes good predictions, that means we will as well. This is really what statistics is meant to be doing: you come up with everything you imagine could possibly be reasonable, and hand it to me, and then I come up with an algorithm that will figure out which of the things you handed me was most reasonable, and will do almost as well as that. As long as at least one of the things you come up with is good, then my algorithm will do well. Importantly, due to the <img class=\"latex\" title=\"{\\log(n)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Clog%28n%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\log(n)}\" /> dependence on the dimension of the model family, you can actually write down extremely broad classes of models and I will still successfully sift through them.</p>\n<p><strong>Let me stress again:</strong> regret bounds are saying that, no matter how the <img class=\"latex\" title=\"{x_t}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x_t}\" /> and <img class=\"latex\" title=\"{y_t}\" src=\"http://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y_t}\" /> are related, no i.i.d. assumptions anywhere in sight, we will do almost as well as any predictor <img class=\"latex\" title=\"{h}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h}\" /> in <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> (in particular, almost as well as the best predictor).</p>\n<p><em>Myth 6: frequentist methods can only deal with simple models and need to make arbitrary cutoffs in model complexity.</em> A naive perusal of the literature might lead one to believe that frequentists only ever consider very simple models, because many discussions center on linear and log-linear models. To dispel this, I will first note that there are just as many discussions that focus on much more general properties such as convexity and smoothness, and that can achieve comparably good bounds in many cases. But more importantly, the reason we focus so much on linear models is because <strong>we have already reduced a large family of problems to (log-)linear regression.</strong> The key insight, and I think one of the most important insights in all of applied mathematics, is that of <strong>featurization</strong>: given a <em>non-linear</em> problem, we can often embed it into a higher-dimensional <em>linear</em> problem, via a feature map <img class=\"latex\" title=\"{\\phi : X \\rightarrow \\mathbb{R}^n}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cphi+%3A+X+%5Crightarrow+%5Cmathbb%7BR%7D%5En%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\phi : X \\rightarrow \\mathbb{R}^n}\" /> (<img class=\"latex\" title=\"{\\mathbb{R}^n}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5En%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathbb{R}^n}\" /> denotes <img class=\"latex\" title=\"{n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{n}\" />-dimensional space, i.e. vectors of real numbers of length <img class=\"latex\" title=\"{n}\" src=\"http://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{n}\" />). For instance, if I think that <img class=\"latex\" title=\"{y}\" src=\"http://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y}\" /> is a polynomial (say cubic) function of <img class=\"latex\" title=\"{x}\" src=\"http://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{x}\" />, I can apply the mapping <img class=\"latex\" title=\"{\\phi(x) = (1, x, x^2, x^3)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cphi%28x%29+%3D+%281%2C+x%2C+x%5E2%2C+x%5E3%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\phi(x) = (1, x, x^2, x^3)}\" />, and now look for a <em>linear</em> relationship between <img class=\"latex\" title=\"{y}\" src=\"http://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{y}\" /> and <img class=\"latex\" title=\"{\\phi(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cphi%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\phi(x)}\" />.</p>\n<p>This insight extends far beyond polynomials. In combinatorial domains such as natural language, it is common to use <em>indicator features</em>: features that are <img class=\"latex\" title=\"{1}\" src=\"http://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{1}\" /> if a certain event occurs and <img class=\"latex\" title=\"{0}\" src=\"http://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{0}\" /> otherwise. For instance, I might have an indicator feature for whether two words appear consecutively in a sentence, whether two parts of speech are adjacent in a syntax tree, or for what part of speech a word has. Almost all state of the art systems in natural language processing work by solving a relatively simple regression task (typically either log-linear or max-margin) over a rich feature space (often involving hundreds of thousands or millions of features, i.e. an embedding into <img class=\"latex\" title=\"{\\mathbb{R}^{10^5}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5E%7B10%5E5%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathbb{R}^{10^5}}\" /> or <img class=\"latex\" title=\"{\\mathbb{R}^{10^6}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5E%7B10%5E6%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathbb{R}^{10^6}}\" />).</p>\n<p>A counter-argument to the previous point could be: &ldquo;Sure, you could create a high-dimensional family of models, but it&rsquo;s still a <em>parameterized family</em>. I don&rsquo;t want to be stuck with a parameterized family, I want my family to include all Turing machines!&rdquo; Putting aside for a second the question of whether &ldquo;all Turing machines&rdquo; is a well-advised model choice, this is something that a frequentist approach can handle just fine, using a tool called <em>regularization</em>, which after featurization is the second most important idea in statistics.</p>\n<p>Specifically, given any sufficiently quickly growing function <img class=\"latex\" title=\"{\\psi(h)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cpsi%28h%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\psi(h)}\" />, one can show that, given <img class=\"latex\" title=\"{T}\" src=\"http://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{T}\" /> data points, there is a strategy whose average error is at most <img class=\"latex\" title=\"{\\sqrt{\\frac{\\psi(h)}{T}}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B%5Cfrac%7B%5Cpsi%28h%29%7D%7BT%7D%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\sqrt{\\frac{\\psi(h)}{T}}}\" /> worse than <em>any</em> estimator <img class=\"latex\" title=\"{h}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h}\" />. This can hold even if the model class <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> is infinite dimensional. For instance, if <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> consists of all probability distributions over Turing machines, and we let <img class=\"latex\" title=\"{h_i}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh_i%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h_i}\" /> denote the probability mass placed on the <img class=\"latex\" title=\"{i}\" src=\"http://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{i}\" />th Turing machine, then a valid regularizer <img class=\"latex\" title=\"{\\psi}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\psi}\" /> would be</p>\n<p align=\"center\"><img class=\"latex\" title=\"\\displaystyle \\psi(h) = \\sum_i h_i \\log(i^2 \\cdot h_i)\" src=\"http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cpsi%28h%29+%3D+%5Csum_i+h_i+%5Clog%28i%5E2+%5Ccdot+h_i%29&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"\\displaystyle \\psi(h) = \\sum_i h_i \\log(i^2 \\cdot h_i)\" /></p>\n<p>If we consider this, then we see that, for any probability distribution over the first <img class=\"latex\" title=\"{2^k}\" src=\"http://s0.wp.com/latex.php?latex=%7B2%5Ek%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{2^k}\" /> Turing machines (i.e. all Turing machines with description length <img class=\"latex\" title=\"{\\leq k}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cleq+k%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\leq k}\" />), the value of <img class=\"latex\" title=\"{\\psi}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\psi}\" /> is at most <img class=\"latex\" title=\"{\\log((2^k)^2) = k\\log(4)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Clog%28%282%5Ek%29%5E2%29+%3D+k%5Clog%284%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\log((2^k)^2) = k\\log(4)}\" />. (Here we use the fact that <img class=\"latex\" title=\"{\\psi(h) \\geq \\sum_i h_i \\log(i^2)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cpsi%28h%29+%5Cgeq+%5Csum_i+h_i+%5Clog%28i%5E2%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\psi(h) \\geq \\sum_i h_i \\log(i^2)}\" />, since <img class=\"latex\" title=\"{h_i \\leq 1}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh_i+%5Cleq+1%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h_i \\leq 1}\" /> and hence <img class=\"latex\" title=\"{h_i\\log(h_i) \\leq 0}\" src=\"http://s0.wp.com/latex.php?latex=%7Bh_i%5Clog%28h_i%29+%5Cleq+0%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{h_i\\log(h_i) \\leq 0}\" />.) <strong>This means that, if we receive roughly <img class=\"latex\" title=\"{\\frac{k}{\\epsilon^2}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bk%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\frac{k}{\\epsilon^2}}\" /> data, we will achieve error within <img class=\"latex\" title=\"{\\epsilon}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\epsilon}\" /> of the best Turing machine that has description length <img class=\"latex\" title=\"{\\leq k}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cleq+k%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\leq k}\" />.</strong></p>\n<p>Let me note several things here:</p>\n<ul>\n<li>This strategy makes no assumptions about the data being i.i.d. It doesn&rsquo;t even assume that the data are computable. It just guarantees that it will perform as well as any Turing machine (or distribution over Turing machines) given the appropriate amount of data.</li>\n<li>This guarantee holds for any given sufficiently smooth measurement of prediction error (the update strategy depends on the particular error measure).</li>\n<li>This guarantee holds deterministically, no randomness required (although predictions may need to consist of probability distributions rather than specific points, but this is also true of Bayesian predictions).</li>\n</ul>\n<p>Interestingly, in the case that the prediction error is given by the negative log probability assigned to the truth, then the corresponding strategy that achieves the error bound is just normal Bayesian updating. But for other measurements of error, we get different update strategies. Although I haven&rsquo;t worked out the math, intuitively this difference could be important if the universe is fundamentally unpredictable but our notion of error is insensitive to the unpredictable aspects.</p>\n<p><em>Myth </em><em>7</em><em>: </em><em>frequentist methods hide their assumptions while </em><em>B</em><em>ayesian methods make assumptions explicit.</em> I&rsquo;m still not really sure where this came from. As we&rsquo;ve seen numerous times so far, a very common flavor among frequentist methods is the following: I have a model class <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" />, I want to do as well as any model in <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" />; or put another way:</p>\n<p><strong>Assumption:</strong> At least one model in <img class=\"latex\" title=\"{\\mathcal{M}}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BM%7D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathcal{M}}\" /> has error at most <img class=\"latex\" title=\"{E}\" src=\"http://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{E}\" />.<br /> <strong>Guarantee:</strong> My method will have error at most <img class=\"latex\" title=\"{E + \\epsilon}\" src=\"http://s0.wp.com/latex.php?latex=%7BE+%2B+%5Cepsilon%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{E + \\epsilon}\" />.</p>\n<p>This seems like a very explicit assumption with a very explicit guarantee. On the other hand, an argument I hear is that Bayesian methods make their assumptions explicit because they have an explicit prior. If I were to write this as an assumption and guarantee, I would write:</p>\n<p><strong>Assumption:</strong> The data were generated from the prior.<br /> <strong>Guarantee:</strong> I will perform at least as well as any other method.</p>\n<p>While I agree that this is an assumption and guarantee of Bayesian methods, there are two problems that I have with drawing the conclusion that &ldquo;Bayesian methods make their assumptions explicit&rdquo;. The first is that it can often be very difficult to understand how a prior behaves; so while we could say &ldquo;The data were generated from the prior&rdquo; is an explicit assumption, it may be unclear what exactly that assumption entails. However, a bigger issue is that &ldquo;The data were generated from the prior&rdquo; is an assumption that very rarely holds; indeed, in many cases the underlying process is deterministic (if you&rsquo;re a subjective Bayesian then this isn&rsquo;t necessarily a problem, but it does certainly mean that the assumption given above doesn&rsquo;t hold). So given that that assumption doesn&rsquo;t hold but Bayesian methods still often perform well in practice, I would say that Bayesian methods are making some other sort of &ldquo;assumption&rdquo; that is far less explicit (indeed, I would be very interested in understanding what this other, more nebulous assumption might be).</p>\n<p><em>Myth </em><em>8</em><em>: </em><em>frequentist methods are fragile, </em><em>Bayesian methods are robust</em><em>.</em> This is another one that&rsquo;s straightforwardly false. First, since frequentist methods often rest on weaker assumptions they are more robust if the assumptions don&rsquo;t quite hold. Secondly, there is an entire area of robust statistics, which focuses on being robust to adversarial errors in the problem data.</p>\n<p><em>Myth </em><em>9</em><em>: </em><em>frequentist methods are responsible for bad science.</em> I will concede that much bad science is done using frequentist statistics. But this is true only because pretty much all science is done using frequentist statistics. I&rsquo;ve heard arguments that using Bayesian methods instead of frequentist methods would fix at least some of the problems with science. I don&rsquo;t think this is particularly likely, as I think many of the problems come from mis-application of statistical tools or from failure to control for multiple hypotheses. If anything, Bayesian methods would exacerbate the former, because they often require more detailed modeling (although in most simple cases the difference doesn&rsquo;t matter at all). I don&rsquo;t think being Bayesian guards against multiple hypothesis testing. Yes, in some sense a prior &ldquo;controls for multiple hypotheses&rdquo;, but in general the issue is that the &ldquo;multiple hypotheses&rdquo; are never written down in the first place, or are written down and then discarded. One could argue that being in the habit of writing down a prior might make practitioners more likely to think about multiple hypotheses, but I&rsquo;m not sure this is the first-order thing to worry about.</p>\n<p><em>Myth 10: </em><em>frequentist methods are unprincipled / hacky. </em>One of the most beautiful theoretical paradigms that I can think of is what I could call the &ldquo;geometric view of statistics&rdquo;. One place that does a particularly good job of show-casing this is <a href=\"http://eprints.pascal-network.org/archive/00004161/\">Shai Shalev-Shwartz&rsquo;s PhD thesis</a>, which was so beautiful that I cried when I read it. I&rsquo;ll try (probably futilely) to convey a tiny amount of the intuition and beauty of this paradigm in the next few paragraphs, although focusing on minimax estimation, rather than online learning as in Shai&rsquo;s thesis.</p>\n<p>The geometric paradigm tends to emphasize a view of measurements (i.e. empirical expected values over observed data) as &ldquo;noisy&rdquo; linear constraints on a model family. We can control the noise by either taking few enough measurements that the total error from the noise is small (classical statistics), or by broadening the linear constraints to convex constraints (robust statistics), or by controlling the Lagrange multipliers on the constraints (regularization). One particularly beautiful result in this vein is the duality between maximum entropy and maximum likelihood. (I can already predict the Jaynesians trying to claim this result for their camp, but (i) Jaynes did not invent maximum entropy; (ii) maximum entropy is not particularly Bayesian (in the sense that frequentists use it as well); and (iii) the view on maximum entropy that I&rsquo;m about to provide is different from the view given in Jaynes or by physicists in general <em>[edit: EHeller thinks this last claim is questionable, see discussion <a href=\"/lw/jne/a_fervent_defense_of_frequentist_statistics/aj8g\">here</a>]</em>.)</p>\n<p>To understand the duality mentioned above, suppose that we have a probability distribution <img class=\"latex\" title=\"{p(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bp%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{p(x)}\" /> and the only information we have about it is the expected value of a certain number of functions, i.e. the information that <img class=\"latex\" title=\"{\\mathbb{E}[\\phi(x)] = \\phi^*}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BE%7D%5B%5Cphi%28x%29%5D+%3D+%5Cphi%5E%2A%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathbb{E}[\\phi(x)] = \\phi^*}\" />, where the expectation is taken with respect to <img class=\"latex\" title=\"{p(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bp%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{p(x)}\" />. We are interested in constructing a probability distribution <img class=\"latex\" title=\"{q(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bq%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{q(x)}\" /> such that no matter what particular value <img class=\"latex\" title=\"{p(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bp%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{p(x)}\" /> takes, <img class=\"latex\" title=\"{q(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bq%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{q(x)}\" /> will still make good predictions. In other words (taking <img class=\"latex\" title=\"{\\log p(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Clog+p%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\log p(x)}\" /> as our measurement of prediction accuracy) we want <img class=\"latex\" title=\"{\\mathbb{E}_{p'}[\\log q(x)]}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BE%7D_%7Bp%27%7D%5B%5Clog+q%28x%29%5D%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathbb{E}_{p'}[\\log q(x)]}\" /> to be large for all distributions <img class=\"latex\" title=\"{p'}\" src=\"http://s0.wp.com/latex.php?latex=%7Bp%27%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{p'}\" /> such that <img class=\"latex\" title=\"{\\mathbb{E}_{p'}[\\phi(x)] = \\phi^*}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BE%7D_%7Bp%27%7D%5B%5Cphi%28x%29%5D+%3D+%5Cphi%5E%2A%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathbb{E}_{p'}[\\phi(x)] = \\phi^*}\" />. Using a technique called Lagrangian duality, we can both find the optimal distribution <img class=\"latex\" title=\"{q}\" src=\"http://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{q}\" /> and compute its worse-case accuracy over all <img class=\"latex\" title=\"{p'}\" src=\"http://s0.wp.com/latex.php?latex=%7Bp%27%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{p'}\" /> with <img class=\"latex\" title=\"{\\mathbb{E}_{p'}[\\phi(x)] = \\phi^*}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BE%7D_%7Bp%27%7D%5B%5Cphi%28x%29%5D+%3D+%5Cphi%5E%2A%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\mathbb{E}_{p'}[\\phi(x)] = \\phi^*}\" />. The characterization is as follows: consider all probability distributions <img class=\"latex\" title=\"{q(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bq%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{q(x)}\" /> that are proportional to <img class=\"latex\" title=\"{\\exp(\\lambda^{\\top}\\phi(x))}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cexp%28%5Clambda%5E%7B%5Ctop%7D%5Cphi%28x%29%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\exp(\\lambda^{\\top}\\phi(x))}\" /> for some vector <img class=\"latex\" title=\"{\\lambda}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\lambda}\" />, i.e. <img class=\"latex\" title=\"{q(x) = \\exp(\\lambda^{\\top}\\phi(x))/Z(\\lambda)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bq%28x%29+%3D+%5Cexp%28%5Clambda%5E%7B%5Ctop%7D%5Cphi%28x%29%29%2FZ%28%5Clambda%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{q(x) = \\exp(\\lambda^{\\top}\\phi(x))/Z(\\lambda)}\" /> for some <img class=\"latex\" title=\"{Z(\\lambda)}\" src=\"http://s0.wp.com/latex.php?latex=%7BZ%28%5Clambda%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{Z(\\lambda)}\" />. Of all of these, take the q(x) with the largest value of <img class=\"latex\" title=\"{\\lambda^{\\top}\\phi^* - \\log Z(\\lambda)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Clambda%5E%7B%5Ctop%7D%5Cphi%5E%2A+-+%5Clog+Z%28%5Clambda%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\lambda^{\\top}\\phi^* - \\log Z(\\lambda)}\" />. Then <img class=\"latex\" title=\"{q(x)}\" src=\"http://s0.wp.com/latex.php?latex=%7Bq%28x%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{q(x)}\" /> will be the optimal distribution and the accuracy for <em>all</em> distributions <img class=\"latex\" title=\"{p'}\" src=\"http://s0.wp.com/latex.php?latex=%7Bp%27%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{p'}\" /> will be exactly <img class=\"latex\" title=\"{\\lambda^{\\top}\\phi^* - \\log Z(\\lambda)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Clambda%5E%7B%5Ctop%7D%5Cphi%5E%2A+-+%5Clog+Z%28%5Clambda%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\lambda^{\\top}\\phi^* - \\log Z(\\lambda)}\" />. Furthermore, if <img class=\"latex\" title=\"{\\phi^*}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Cphi%5E%2A%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\phi^*}\" /> is the empirical expectation given some number of samples, then one can show that <img class=\"latex\" title=\"{\\lambda^{\\top}\\phi^* - \\log Z(\\lambda)}\" src=\"http://s0.wp.com/latex.php?latex=%7B%5Clambda%5E%7B%5Ctop%7D%5Cphi%5E%2A+-+%5Clog+Z%28%5Clambda%29%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{\\lambda^{\\top}\\phi^* - \\log Z(\\lambda)}\" /> is propotional to the log likelihood of <img class=\"latex\" title=\"{q}\" src=\"http://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=f0f0f0&amp;fg=000000&amp;s=0\" alt=\"{q}\" />, which is why I say that maximum entropy and maximum likelihood are dual to each other.</p>\n<p>This is a relatively simple result but it underlies a decent chunk of models used in practice.</p>\n<p><em>Myth </em><em>11</em><em>: </em><em>frequentist methods have no </em><em>promising approach to computationally bounded inference.</em> I would personally argue that frequentist methods are <em>more</em> promising than Bayesian methods at handling computational constraints, although computationally bounded inference is a very cutting edge area and I&rsquo;m sure other experts would disagree. However, one point in favor of the frequentist approach here is that we already have some frameworks, such as the &ldquo;tightening relaxations&rdquo; framework discussed <a href=\"http://people.csail.mit.edu/tommi/papers/Sontag_etal_UAI08.pdf\">here</a>, that provide quite elegant and rigorous ways of handling computationally intractable models.</p>\n<p>&nbsp;</p>\n<p><strong>References</strong></p>\n<p>(Myth 3) Sparse recovery: <a href=\"http://people.csail.mit.edu/indyk/survey-10.pdf\">Sparse recovery using sparse matrices</a><cite class=\"vurls\"></cite><br />(Myth 5) Online learning: <a href=\"http://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf\">Online learning and online convex optimization</a><br />(Myth 8) Robust statistics: see <a href=\"http://hunch.net/?p=197\">this</a> blog post and the <a href=\"http://www.cs.princeton.edu/~mdudik/DudikPhSc04.pdf\">two</a> <a href=\"http://ttic.uchicago.edu/~altun/pubs/AltSmo-COLT06.pdf\">linked</a> papers<br />(Myth 10) Maximum entropy duality: <a href=\"http://projecteuclid.org/euclid.aos/1091626173\">Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KdwP5i6N4E4q6BGkr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 72, "extendedScore": null, "score": 0.000194, "legacy": true, "legacyId": "25466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 129, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T20:23:48.752Z", "modifiedAt": null, "url": null, "title": "Audio Version of \"How to Actually Change Your Mind\"", "slug": "audio-version-of-how-to-actually-change-your-mind", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:41.674Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rick_from_Castify", "createdAt": "2012-12-03T09:33:28.512Z", "isAdmin": false, "displayName": "Rick_from_Castify"}, "userId": "XyTqQupkZ9SW7nGCB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yibwDNWRDeNgXSE3j/audio-version-of-how-to-actually-change-your-mind", "pageUrlRelative": "/posts/yibwDNWRDeNgXSE3j/audio-version-of-how-to-actually-change-your-mind", "linkUrl": "https://www.lesswrong.com/posts/yibwDNWRDeNgXSE3j/audio-version-of-how-to-actually-change-your-mind", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Audio%20Version%20of%20%22How%20to%20Actually%20Change%20Your%20Mind%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAudio%20Version%20of%20%22How%20to%20Actually%20Change%20Your%20Mind%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyibwDNWRDeNgXSE3j%2Faudio-version-of-how-to-actually-change-your-mind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Audio%20Version%20of%20%22How%20to%20Actually%20Change%20Your%20Mind%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyibwDNWRDeNgXSE3j%2Faudio-version-of-how-to-actually-change-your-mind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyibwDNWRDeNgXSE3j%2Faudio-version-of-how-to-actually-change-your-mind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>The audio version of the mega-sequence \"How to Actually Change Your Mind\" is <a href=\"http://castify.co/channels/46-how-to-actually-change-your-mind\">now available</a>.&nbsp; It's the biggest sequence we've done yet, coming in at over 8 hours of audio.&nbsp; Here at Castify we think this sequence is the most important one we've done so far.</p>\n<p>On the wiki it says, \"The most important technique that Less Wrong can offer you is 'How to Actually Change Your Mind'\".&nbsp; I couldn't agree more.&nbsp; If you haven't read it already I'd strongly recommend it.&nbsp; If you have more free \"ear time\" than \"eye time\" or you learn better from listening this audio version is for you.</p>\n<p>Feedback is welcome: support@castify.co</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yibwDNWRDeNgXSE3j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 1.5707075541754463e-06, "legacy": true, "legacyId": "25537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-18T22:03:08.008Z", "modifiedAt": null, "url": null, "title": "Bridge Collapse: Reductionism as Engineering Problem", "slug": "bridge-collapse-reductionism-as-engineering-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:01.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3WuAjWMtxQwTxr2Qn/bridge-collapse-reductionism-as-engineering-problem", "pageUrlRelative": "/posts/3WuAjWMtxQwTxr2Qn/bridge-collapse-reductionism-as-engineering-problem", "linkUrl": "https://www.lesswrong.com/posts/3WuAjWMtxQwTxr2Qn/bridge-collapse-reductionism-as-engineering-problem", "postedAtFormatted": "Tuesday, February 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bridge%20Collapse%3A%20Reductionism%20as%20Engineering%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABridge%20Collapse%3A%20Reductionism%20as%20Engineering%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WuAjWMtxQwTxr2Qn%2Fbridge-collapse-reductionism-as-engineering-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bridge%20Collapse%3A%20Reductionism%20as%20Engineering%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WuAjWMtxQwTxr2Qn%2Fbridge-collapse-reductionism-as-engineering-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WuAjWMtxQwTxr2Qn%2Fbridge-collapse-reductionism-as-engineering-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4571, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/jd9/building_phenomenological_bridges/\">Building Phenomenological Bridges</a></p>\n<p><strong>Summary</strong>: AI theorists often use models in which agents are crisply separated from their environments. This simplifying assumption can be useful, but it leads to trouble when we build machines that presuppose it. A machine that believes it can only interact with its environment in a narrow, fixed set of ways will not understand the value, or the dangers, of self-modification. By analogy with Descartes' mind/body dualism, I refer to agent/environment dualism as&nbsp;<em>Cartesianism</em>. The&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence#Open_problems\">open problem in Friendly AI</a>&nbsp;(OPFAI) I'm calling&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalized induction</a>&nbsp;is the project of replacing Cartesian approaches to scientific induction with reductive, physicalistic ones.</p>\n<hr />\n<p>&nbsp;</p>\n<p>I'll begin with a story about a storyteller.</p>\n<p>Once upon a time &mdash; specifically, 1976 &mdash;&nbsp;there was an AI named TALE-SPIN. This AI told <a href=\"https://en.wikipedia.org/wiki/Interactive_storytelling\">stories</a> by inferring how characters would respond to problems from background knowledge about the characters' traits. One day, TALE-SPIN constructed a most peculiar tale.</p>\n<p style=\"padding-left: 30px;\">Henry Ant was thirsty. He walked over to the river bank where his good friend Bill Bird was sitting. Henry slipped and fell in the river. Gravity drowned.</p>\n<p>Since Henry fell in the river near his friend Bill, TALE-SPIN concluded that Bill rescued Henry. But for Henry to fall in the river, gravity must have pulled Henry. Which means gravity must have been in the river. TALE-SPIN had never been told that gravity knows how to swim; and TALE-SPIN had never been told that gravity has any friends. So gravity drowned.</p>\n<p>TALE-SPIN had previously been programmed to understand involuntary motion in the case of characters being pulled or carried by other characters &mdash; like Bill rescuing Henry. So it was programmed to understand 'character X fell to place Y' as 'gravity moves X to Y', as though gravity were a character in the story.<a name=\"footnote1back\"></a><sup><a href=\"#footnote1\">1</a></sup></p>\n<p>For us, the hypothesis 'gravity drowned' has low prior probability because we know gravity isn't the <em>type </em>of thing that swims or breathes or makes friends. We want agents to seriously consider whether the law of gravity pulls down rocks; we don't want agents to seriously consider whether the law of gravity pulls down the law of electromagnetism. We&nbsp;<a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">may not want</a>&nbsp;an AI to assign&nbsp;<a href=\"/lw/mp/0_and_1_are_not_probabilities/\"><em>zero&nbsp;</em>probability</a>&nbsp;to 'gravity drowned', but we at least want it to neglect the possibility as Ridiculous-By-Default.</p>\n<p>When we introduce deep type distinctions, however, we also introduce new ways our stories can fail.</p>\n<p><a id=\"more\"></a></p>\n<div><br /></div>\n<div><br /></div>\n<h3>Hutter's cybernetic agent model</h3>\n<p>Russell and Norvig's leading AI textbook credits Solomonoff with setting the agenda for the field of AGI: \"AGI looks for a universal algorithm for learning and acting in any environment, and has its roots in the work of Ray Solomonoff[.]\" As an approach to AGI, Solomonoff induction presupposes a model with a strong type distinction between the 'agent' and the 'environment'. To make its intuitive appeal and attendant problems more obvious, I'll sketch out the model.</p>\n<p>A Solomonoff-inspired AI can most easily be represented as a multi-tape&nbsp;<a href=\"http://plato.stanford.edu/entries/turing-machine/\">Turing machine</a>&nbsp;like the one&nbsp;Alex Altair describes in&nbsp;<a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#all_algorithms\">An Intuitive Explanation of Solomonoff Induction</a>. The machine has:</p>\n<p style=\"padding-left: 30px;\">-&nbsp;<span style=\"text-decoration: underline;\">three tapes</span>, labeled 'input', 'work', and 'output'. Each initially has an infinite strip of 0s written in discrete cells.</p>\n<p style=\"padding-left: 30px;\">- <span style=\"text-decoration: underline;\">one</span><span style=\"text-decoration: underline;\">&nbsp;head per tape</span>, with the input head able to read its cell's digit and move to the right, the output head able to write 0 or 1 to its cell and move to the right, and the work head able to read, write, and move in either direction.</p>\n<p style=\"padding-left: 30px;\">- a&nbsp;<span style=\"text-decoration: underline;\">program</span>, consisting of a finite, fixed set of&nbsp;transition rules.&nbsp;Each rule says when heads read, write, move, or do nothing, and how to transition to another rule.</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_0.png?v=8aed0100dd1d9a71648e37faa55d0865\" alt=\"\" width=\"420\" height=\"444\" /></p>\n<p align=\"center\"><em>A three-tape Turing machine.</em></p>\n<p>&nbsp;</p>\n<p>We could imagine two such Turing machines communicating with each other. Call them 'Agent' and 'Environment', or 'Alice' and 'Everett'. Alice and Everett take turns acting. After Everett writes a bit to his output tape, that bit magically appears on Alice's input tape; and likewise, when Alice writes to her output tape, it gets copied to Everett's input tape. AI theorists have used this setup, which <a href=\"http://www.hutter1.net/\">Marcus Hutter</a> calls the <strong>cybernetic agent model</strong>,&nbsp;as an extremely simple representation of an agent that can&nbsp;<strong>perceive</strong>&nbsp;its environment (using the input tape), <strong>think&nbsp;</strong>(using the work tape), and&nbsp;<strong>act</strong>&nbsp;(using the output tape).<a name=\"footnote2aback\"></a><sup><a href=\"#footnote2\">2</a></sup></p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_10.png\" alt=\"\" width=\"660\" height=\"334\" /></p>\n<p align=\"center\"><em>A Turing machine model of agent-environment interactions. At first, the machines differ only in their programs. &lsquo;Alice&rsquo; is the agent we want to build, while &lsquo;Everett&rsquo; stands for everything else that&rsquo;s causally relevant to Alice&rsquo;s success.</em></p>\n<p>&nbsp;</p>\n<p>We can define Alice and Everett's behavior in terms of any bit-producing Turing machines we'd like, including ones that represent probability distributions and do&nbsp;<a href=\"http://yudkowsky.net/rational/bayes\">Bayesian updating</a>. Alice might, for example, use her work tape to track four distinct possibilities and update probabilities over them:<a name=\"footnote3back\"></a><sup><a href=\"#footnote3\">3</a></sup></p>\n<ul>\n<li>(a) Everett always outputs 0.</li>\n<li>(b) Everett always outputs 1.</li>\n<li>(c) Everett outputs its input.</li>\n<li>(d) Everett outputs the opposite of its input.</li>\n</ul>\n<p>Alice starts with a uniform prior, i.e., 25% probability each. If Alice's first output is 1, and Everett responds with 1, then Alice can store those two facts on her work tape and conditionalize on them both, treating them as though they were certain. This results in 0.5 probability each for (b) and (c), 0 probability for (a) and (d).</p>\n<p>We care about an AI's epistemology only because it informs the AI's behavior &mdash; on this model, its bit output. If Alice outputs whatever bits maximize her expected chance of receiving 1s as input, then <a href=\"/lw/6ha/the_blueminimizing_robot/\">we can say</a> that Alice&nbsp;<strong>prefers</strong>&nbsp;to perceive 1. In the example I just gave, such a preference predicts that Alice will proceed to output 1 forever. Further exploration is unnecessary, since she knows of no other importantly different hypotheses to test.</p>\n<p>Enriching Alice's set of hypotheses for how Everett could act will let Alice win more games against a wider variety of Turing machines. The more programs Alice can pick out and assign a probability to, the more Turing machines Alice will be able to identify and intelligently respond to. If we aren't worried about whether it takes Alice ten minutes or a billion years to compute an update, and Everett will always patiently wait his turn, then we can simply have Alice perform perfect Bayesian updates; if her priors are right, and she translates her beliefs into sensible actions, she'll then be able to&nbsp;<a href=\"/lw/mt/beautiful_probability/\">optimally</a>&nbsp;respond to any environmental Turing machine.</p>\n<p>For AI researchers following Solomonoff's lead, that's the name of the game: Figure out the program that will let Alice behave optimally while communicating with as wide a range of Turing machines as possible, and you've at least solved the&nbsp;<em>theoretical&nbsp;</em>problem of picking out the optimal artificial agent from the space of possible reasoners. The agent/environment model here may look simple, but a number of theorists see it as distilling into its most basic form the task of an AGI.<a name=\"footnote2bback\"></a><sup><a href=\"#footnote2\">2</a></sup></p>\n<p>Yet a Turing machine,&nbsp;<a href=\"/lw/jd9/building_phenomenological_bridges/\">like a cellular automaton</a>, is an&nbsp;<a href=\"https://en.wikipedia.org/wiki/Automata_theory\">abstract machine</a>&nbsp;&mdash; a creature of thought experiments and mathematical proofs. Physical computers can act like abstract computers, in just the same sense that&nbsp;<a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/\">heaps of apples</a>&nbsp;can behave like&nbsp;<a href=\"/lw/f4e/logical_pinpointing/\">the abstract objects we call 'numbers'</a>. But computers and apples are&nbsp;<a href=\"/lw/lc/leaky_generalizations/\">high-level generalizations</a>, imperfectly represented by concise equations.<a name=\"footnote4back\"></a><sup><a href=\"#footnote4\">4</a></sup>&nbsp;When we move from our mental models to trying to build an actual AI, we have to pause and ask how well our formalism captures what's going on in reality.</p>\n<p>&nbsp;</p>\n<h3>The problem with Alice</h3>\n<p>'Sensory input' or 'data' is what I call the information Alice conditionalizes on; and 'beliefs' or 'hypotheses' is what I call the resultant probability distribution and representation of possibilities (in Alice's program or work tape). This distinction <a href=\"/lw/jiu/can_we_do_without_bridge_hypotheses/\">seems basic to reasoning</a>, so I endorse programming agents to treat them as two clearly distinct types. But in building such agents, we introduce the possibility of&nbsp;<strong>Cartesianism</strong>.</p>\n<p>Ren&eacute; Descartes held that human minds and brains, although able to causally interact with each other, can each exist in the absence of the other; and, moreover, that the properties of purely material things can never fully explain minds. In his honor, we can call a model or procedure&nbsp;<em>Cartesian</em>&nbsp;if it treats the reasoner as a being separated from the physical universe. Such a being can perceive (and perhaps alter) physical processes, but it can't be identified with any such process.<a name=\"footnote5back\"></a><sup><a href=\"#footnote5\">5</a></sup></p>\n<p>The relevance of Cartesians to AGI work is that we can model them as agents experiencing a strong type distinction between 'mind' and 'matter', and an unshakable belief in the metaphysical independence of those two categories;&nbsp;<em>because&nbsp;</em>they're of such different kinds, they can vary independently. So we end up with AI errors that are the opposite of TALE-SPIN's &mdash; like an induction procedure that distinguishes gravity's type from embodied characters' types&nbsp;<em>so </em>strongly&nbsp;that it cannot hypothesize that, say, particles underlie or mediate both phenomena.</p>\n<p>My claim is that if we plug in 'Alice's sensory data' for 'mind' and 'the stuff Alice hypothesizes as causing the sensory data' for 'matter', then agents that can only model themselves using the cybernetic agent model are Cartesian in the relevant sense.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup></p>\n<p>The model is Cartesian because the agent and its environment&nbsp;<em>can only interact by communicating</em>.&nbsp;That is, their only way of affecting each other is by trading bits printed to tapes.</p>\n<p>If we build an actual AI that believes it's like Alice, it will believe that the environment can't affect it in ways that aren't immediately detectable, can't edit its source code, and can't force it to halt. But that makes the Alice-Everett system almost nothing like a physical agent embedded in a real environment. Under many circumstances, a real AI's environment will alter it directly. E.g., the AI can fall into a volcano. A volcano doesn't harm the agent by feeding unhelpful bits into its environmental sensors. It harms the agent by destroying it.</p>\n<p>A more naturalistic model would say: Alice outputs a bit; Everett reads it; and then Everett does whatever the heck he wants. That might be feeding a new bit into Alice. Or it might be vandalizing Alice's work tape, or smashing Alice flat.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_3.png?v=99c1224a99c75b811f5db5fe92762e63\" alt=\"\" width=\"474\" height=\"422\" /></p>\n<p align=\"center\"><em>A robotic Everett tampering with an agent that mistakenly assumes Cartesianism. A real-world agent&rsquo;s computational states have physical correlates that can be directly edited by the environment. If the agent can't model such scenarios, its reasoning (and resultant decision-making) will suffer.</em></p>\n<p>&nbsp;</p>\n<p>A still more naturalistic approach would be to place Alice&nbsp;<em>inside&nbsp;</em>of Everett, as a subsystem. In the real world, agents are surrounded by their environments. The two form a cohesive whole, bound by the same physical laws, freely interacting and commingling.</p>\n<p>If Alice only worries about whether Everett will output a 0 or 1 to her sensory tape, then no matter how complex an understanding Alice has of Everett's inner workings, Alice will fundamentally misunderstand the situation she's in.&nbsp;Alice won't be able to represent hypotheses about how, for example, a pill might erase her memories or otherwise modify her source code.</p>\n<p>Humans, in contrast, can readily imagine a pill that modifies our memories. It seems childishly easy to hypothesize being changed by avenues other than perceived sensory information. The limitations of the cybernetic agent model aren't immediately obvious, because it isn't easy for us to put ourselves in the shoes of agents with alien blind spots.</p>\n<p>There&nbsp;<em>is&nbsp;</em>an agent-environment distinction, but it's a pragmatic and artificial one. The boundary between the part of the world we call 'agent' and the part we call 'not-agent' (= 'environment') is frequently fuzzy and mutable. If we want to build an agent that's robust across many environments and self-modifications, we can't just design a program that excels at predicting sensory sequences generated by Turing machines. We need an agent that can form accurate beliefs about the actual world it lives in, including accurate beliefs about its own physical underpinnings.</p>\n<p>&nbsp;</p>\n<h3>From Cartesianism to naturalism</h3>\n<p>What would a naturalized self-model, a model of the agent as a process embedded in a lawful universe, look like? As a first attempt, one might point to the pictures of Cai in <a href=\"/lw/jd9/building_phenomenological_bridges/\">Building Phenomenological Bridges</a>.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_5.png?v=a853c43e6b17b53b147daa5b64e58c51\" alt=\"\" width=\"386\" height=\"371\" /></p>\n<p align=\"center\"><em>Cai has a simple physical model of itself as a black tile at the center of a cellular automaton grid. Cai's phenomenological bridge hypotheses relate its sensory data to surrounding tiles' states.</em></p>\n<p>&nbsp;</p>\n<p>But this doesn't yet specify a non-Cartesian agent. To treat Cai as a Cartesian, we could view the tiles surrounding Cai as the work tape of Everett, and the dynamics of Cai's environment as Everett's program. (We can also convert Cai's perceptual experiences into a binary sequence on Alice/Cai's input tape, with a translation like 'cyan = 01, magenta = 10, yellow = 11'.)</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_11.png?v=48ddbf4a34f13d8b4f4c1cc413193470\" alt=\"\" width=\"523\" height=\"294\" /></p>\n<p align=\"center\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><em>Alice/Cai as a cybernetic agent in a Turing machine circuit.</em></span></p>\n<p>&nbsp;</p>\n<p>The problem isn't that Cai's world is Turing-computable, of course. It's that if Cai's hypotheses are solely about what sorts of perception-correlated patterns of environmental change can occur, then Cai's models will be Cartesian.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_4.png?v=c42f2470b3d8ac21cae40cd545afb023\" alt=\"\" width=\"430\" height=\"258\" /></p>\n<p align=\"center\"><em>Cai as a Cartesian treats its sensory experiences as though they exist in a separate world.</em></p>\n<p>&nbsp;</p>\n<p>Cartesian Cai recognizes that its two universes, its sensory experiences and hypothesized environment, can interact. But it thinks they can only do so via a narrow range of stable pathways. No actual agent's mind-matter connections can be that simple and uniform.</p>\n<p>If Cai were a robot in a world resembling its model, it would <em>itself </em>be a complex pattern of tiles. To form accurate predictions, it would need to have self-models and bridge hypotheses that were more sophisticated than any I've considered so far. Humans are the same way: No bridge hypothesis explaining the physical conditions for subjective experience will ever fit on a T-shirt.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_1.png?v=a9eeb910cd4d5c524183df47375183ff\" alt=\"\" width=\"554\" height=\"520\" /></p>\n<p align=\"center\"><em>Cai's world divided up into a 9x9 grid. Cai is the central 3x3 grid. Barely visible: Complex computations like Cai's reasoning are possible in this world because they're implemented by even finer tile patterns at smaller scales.</em></p>\n<p>&nbsp;</p>\n<p>Changing Cai's tiles' states &mdash; from black to white, for example &mdash; could have a large impact on its computations, analogous to changing a human brain from solid to gaseous. But if an agent's hypotheses are all shaped like the cybernetic agent model, 'my input/output algorithm is replaced by a dust cloud' won't be in the hypothesis space.</p>\n<p>If you programmed something to thinks like Cartesian Cai, it might decide that its sequence of visual experiences will persist even if the tiles forming its brain completely change state. It wouldn't be able to entertain thoughts like 'if Cai performs self-modification #381, Cai will experience its environment as smells rather than colors' or 'if Cai falls into a volcano, Cai gets destroyed'. No pattern of perceived colors is identical to a perceived smell, or to the absence of perception.</p>\n<p>To form naturalistic self-models and world-models, Cai needs hypotheses that look less like conversations between independent programs, and more like worlds in which it is a fairly <a href=\"/lw/icn/reality_is_weirdly_normal/\">ordinary</a> subprocess, governed by the same general patterns. It needs to form and privilege physical hypotheses under which it has parts, as well as bridge hypotheses under which those parts correspond in plausible ways to its high-level computational states.</p>\n<p>Cai wouldn't need a <em>complete </em>self-model in order to recognize general facts about its subsystems. Suppose, for instance, that Cai has just one sensor, on its left side, and a motor on its right side. Cai might recognize that the motor and sensor regions of its body correspond to its introspectible decisions and perceptions, respectively.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_2.png?v=1c1f46dd31aab5e5bd893d7d786745ce\" alt=\"\" width=\"523\" height=\"320\" /></p>\n<p align=\"center\"><em>A naturalized agent can recognize that it has physical parts with varying functions. Cai's top and bottom lack sensors and motors altogether, making it clearer that Cai's environment can impact Cai by entirely non-sensory means.</em></p>\n<p>&nbsp;</p>\n<p>We care about Cai's models because we want to use Cai to modify its environment. For example, we may want Cai to convert as much of its environment as possible into grey tiles. Our interest is then in the algorithm that reliably outputs maximally greyifying actions when handed perceptual data.</p>\n<p>If Cai is able to form sophisticated self-models, then Cai can recognize that it's a grey tile maximizer. Since it wants there to be more grey tiles, it also wants to make sure that it continues to exist, provided it believes that it's better than chance at pursuing its goals.</p>\n<p>More specifically, Naturalized Cai can recognize that its actions are some black-box function of its perceptual computations. Since it has a bridge hypothesis linking its perceptions to its middle-left tile, it will then reason that it should <em>preserve </em>its sensory hardware. Cai's self-model tells it that if its sensor fails, then its actions will be based on beliefs that are much less correlated with the environment. And its self-model tells it that if its actions are poorly calibrated, then there will be fewer grey tiles in the universe. Which is bad.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_7.png?v=55945de90d1fa30f2073e062c9de888f\" alt=\"\" width=\"305\" height=\"335\" /></p>\n<p align=\"center\"><em>A naturalistic version of Cai can reason intelligently from the knowledge that its actions (motor output) depend on a specific part of its body that's responsible for perception (environmental input).</em></p>\n<p>&nbsp;</p>\n<p>A physical Cai might need to foresee scenarios like 'an anvil crashes into my head and destroys me', and assign probability mass to them. Bridge hypotheses expressive enough to consider that possibility would not just relate experiences to environmental or hardware states; they would also recognize that the agent's experiences can be absent altogether.</p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_9.png?v=e4ab7391e378b840ed1325c5c24adc9c\" alt=\"\" width=\"711\" height=\"533\" /></p>\n<p align=\"center\"><em>An anvil can destroy Cai's perceptual hardware by crashing into it. A Cartesian might not worry about this eventuality, expecting its experience to persist after its body is smashed. But a naturalized reasoner will form hypotheses like the above, on which its sequence of color experiences suddenly terminates when its sensors are destroyed.</em></p>\n<p>&nbsp;</p>\n<p>This point generalizes to other ways Cai might self-modify, and to other things Cai might alter about itself. For example, Cai might learn that other portions of its brain correspond to its hypotheses and desires.</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_6.png?v=b0acbedcc7136adde91a267db83165a9\" alt=\"\" width=\"512\" height=\"320\" /></p>\n<p align=\"center\"><em>Another very simple model of how different physical structures are associated with different computational patterns.</em></p>\n<p>&nbsp;</p>\n<p>This allows Cai to recognize that its goals depend on the proper functioning of <em>many </em>of its hardware components. If Cai believes that its actions depend on its brain's goal unit's working a specific way, then it will avoid taking pills that foreseeably change its goal unit. If Cai's causal model tells it that agents like it stop exhibiting future-steering behaviors when they self-modify to have mad priors, then it won't self-modify to acquire mad priors. And so on.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_8.png?v=0975b5a26466dcbbe625edf6d8947a03\" alt=\"\" width=\"488\" height=\"328\" /></p>\n<p align=\"center\"><em>If Cai's motor fails, its effect on the world can change as a result. The same is true if its hardware is modified in ways that change its thoughts, or its preferences (i.e., the thing linking its conclusions to its motor).</em></p>\n<p>&nbsp;</p>\n<p>Once Cai recognizes that its brain needs to work in a very specific way for its goals to be achieved, its preferences can take its physical state into account in sensible ways, without our needing to hand-code Cai at the outset to have the right beliefs or preferences over every individual thing that could change in its brain.</p>\n<p>Just the opposite is true for Cartesians. Since they can't form hypotheses like 'my tape heads will stop computing digits if I disassemble them', they can only intelligently navigate such risks if they've been hand-coded in advance to avoid perceptual experiences the programmer thought would correlate with such dangers.</p>\n<p>In other words, even though all of this is still highly informal, there's already some cause to think that a reasoning pattern like Naturalized Cai can generalize in ways that Cartesians can't. The programmers don't need to know&nbsp;<em>everything</em> about Cai's physical state, or anticipate&nbsp;<em>everything</em> about what future changes Cai might undergo, if Cai's epistemology allows it to easily form accurate reductive beliefs and behave accordingly. An agent like this might be adaptive and self-correcting in very novel circumstances, leaving more wiggle room for programmers to make human mistakes.</p>\n<p>&nbsp;</p>\n<h3>Bridging maps of worlds and maps of minds</h3>\n<p>Solomonoff-style dualists have alien blind spots that lead them to neglect the possibility that some hardware state is equivalent to some introspected computation '000110'. TALE-SPIN-like AIs, on the other hand, have blind spots that lead to mistakes like trying to figure out the angular momentum of '000110'.</p>\n<p>A naturalized agent doesn't try to do away<em>&nbsp;</em>with the data/hypothesis type distinction and acquire a typology as simple as TALE-SPIN's. Rather, it tries to tightly interconnect its types using bridges. Naturalizing induction is about combining the dualist's useful&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">map/territory distinction</a>&nbsp;with a more sophisticated metaphysical monism than TALE-SPIN exhibits, resulting in a <em>reductive</em> monist AI.<a style=\"background-color: #ffffff;\" name=\"footnote7back\"></a><sup><a href=\"#footnote7\">7</a></sup></p>\n<p>Alice's simple fixed bridge axiom,&nbsp;{environmental output 0&nbsp;&harr; perceptual input 0, environmental output 1&nbsp;&harr; perceptual input 1}, is inadequate for physically embodied agents. And the problem isn't&nbsp;<em>just&nbsp;</em>that Alice lacks other bridge rules and can't weigh evidence for or against each one. Bridge hypotheses are a step in the right direction, but they need to be diverse enough to express a variety of correlations between the agent's sensory experiences and the physical world, and they need a sensible prior. An agent that only considers bridge hypotheses compatible with the cybernetic agent model will falter whenever it and the environment interact in ways that look nothing like exchanging sensory bits.</p>\n<p>With the help of an inductive algorithm that uses bridge hypotheses to relate sensory data to a continuous physical universe, we can avoid making our AIs Cartesians. This will make their epistemologies much more secure. It will also make it possible for them to want things to be true about the physical universe, not just about the particular sensory experiences they encounter. Actually writing a program that does all this is an OPFAI. Even formalizing how bridge hypotheses ought to work in principle is an OPFAI.</p>\n<p>In my next post, I'll move away from toy models and discuss&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>,&nbsp;Hutter's optimality definition for cybernetic agents. In asking whether the <em>best </em>Cartesian can overcome the difficulties I've described, we'll get a clearer sense of why Solomonoff inductors aren't&nbsp;reflective and reductive enough to predict drastic changes to their sense-input-to-motor-output relation &mdash; and why they <em>can't be </em>that reflective and reductive &mdash; and why this matters.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<h4>Notes</h4>\n<p><sup><a name=\"footnote1\"></a>1</sup> Meehan (1977). Colin Allen first introduced me to this story.&nbsp;<a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Fjd9%2Fbuilding_phenomenological_bridges%2F&amp;v=1&amp;libId=e232dce4-8d1b-4871-9ed3-e80435e6b430&amp;out=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DQbvkja-J9iQC%26pg%3DPA256%26lpg%3DPA256%26dq%3D%2522Henry%2BAnt%2Bwas%2Bthirsty.%2BHe%2522%26source%3Dbl%26ots%3D71hkRLo1T-%26sig%3DBQ8c5_-U_s-oRiDESSeA93mgR0s%26hl%3Den%26sa%3DX%26ei%3DxdabUr-rH8zZoASgxoHgCQ%26ved%3D0CDIQ6AEwAjgK%23v%3Donepage%26q%3D%2522Henry%2520Ant%2520was%2520thirsty.%2520He%2522%26f%3Dfalse&amp;ref=https%3A%2F%2Fwww.google.com%2F&amp;title=Building%20Phenomenological%20Bridges%20-%20Less%20Wrong&amp;txt=TALESPIN&amp;jsonp=vglnk_jsonp_13910468903006\">Dennett</a>&nbsp;discusses it as well.&nbsp;<a href=\"#footnote1back\">\u21a9</a></p>\n<p><sup><a name=\"footnote2\"></a>2</sup>&nbsp;E.g., Durand, Muchnik, Ushakov &amp; Vereshchagin (2004), Epstein &amp; Betke (2011), Legg &amp; Veness (2013), Solomonoff (2011). Hutter (2005) uses the term \"cybernetic agent model\" to emphasize the parallelism between his Turing machine circuit and <a href=\"http://en.wikipedia.org/wiki/Control_theory\">control theory</a>'s <a href=\"http://www.calresco.org/lucas/systems.htm\">cybernetic systems</a>.&nbsp;<a href=\"#footnote2aback\">\u21a9</a>&nbsp;<a href=\"#footnote2bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote3\"></a>3</sup>&nbsp;One simple representation would be: Program Alice to write to her work tape, on round one, 0010 (standing for 'if I output 0, Everett outputs 0; if I output 1, Everett outputs 0'). Ditto for the other three hypotheses, 0111, 0011, and 0110. Then write the hypothesis' probability in binary (initially 25%, represented '11001') to the right of each, and program Alice to edit this number as she receives new evidence. Since the first and third digit stay the same, we can simplify the hypotheses' encoding to 00, 11, 01, 10. Indeed, if the hypotheses remain the same over time there's no reason to visibly distinguish them in the work tape at all, when we can instead just program Alice to use the left-to-right ordering of the four probabilities to distinguish the hypotheses.&nbsp;<a href=\"#footnote3back\">\u21a9</a></p>\n<p><sup><a name=\"footnote4\"></a>4</sup>&nbsp;To the extent our universe&nbsp;<a href=\"/lw/ms/is_reality_ugly/\"><em>perfectly&nbsp;</em>resembles any mathematical structure</a>, it's much more likely to do so&nbsp;<a href=\"/lw/on/reductionism/\">at the level</a>&nbsp;of gluons and mesons than at the level of medium-sized dry goods. The resemblance of apples to natural numbers is much more approximate. Two apples and three apples generally make five apples, but when you start cutting up or pulverizing or genetically altering apples, you may find that other mathematical models do a superior job of predicting the apples' behavior. It seems likely that the only perfectly general and faithful mathematical representation of apples will be some drastically large and unwieldy physics equation.</p>\n<p>Ditto for machines. It's sometimes possible to build a physical machine that closely mimics a given Turing machine &mdash; but only 'closely', as Turing machines have unboundedly large tapes. And although any halting Turing machine can in principle be simulated with a bounded tape (Cockshott &amp; Michaelson (2007)), nearly all Turing machine programs are too large to even be approximated by any physical process.</p>\n<p>All physical machines structurally resemble Turing machines&nbsp;in ways that allow us to draw productive inferences from the one group to the other. See Piccinini's (2011) discussion of the&nbsp;<a href=\"http://www.umsl.edu/~piccininig/CT_Modest_or_Bold.pdf\">physical Church-Turing thesis</a>.&nbsp;But, for all that, the concrete machine and the abstract one remain distinct.&nbsp;<a href=\"#footnote4back\">\u21a9</a></p>\n<p><sup><a name=\"footnote5\"></a>5</sup>&nbsp;Descartes (1641): \"[A]lthough I certainly do possess a body with which I am very closely conjoined; nevertheless, because, on the one hand, I have a clear and distinct idea of myself, in as far as I am only a thinking and unextended thing, and as, on the other hand, I possess a distinct idea of body, in as far as it is only an extended and unthinking thing, it is certain that I (that is, my mind, by which I am what I am) am entirely and truly distinct from my body, and may exist without it.\"</p>\n<p>From this it&rsquo;s clear that Descartes also believed that the mind can exist without the body. This interestingly parallels the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">anvil problem</a>, which I'll discuss more in my next post. However, I don't build immortality into my definition of 'Cartesianism'. Not all agents that act as though there is a Cartesian barrier between their thoughts and the world think that their experiences are future-eternal. I'm taking care not to conflate Cartesianism with the anvil problem because the formalism I'll discuss next time, AIXI, does face both of them. Though the problems are logically distinct, it's true that a naturalized reasoning method would be much less likely to face the anvil problem.&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><sup><a name=\"footnote6\"></a>6</sup>&nbsp;This isn't to say that a Solomonoff inductor would need to be conscious in anything like the way humans are conscious. It can be fruitful to point to similarities between the reasoning patterns of humans and unconscious processes. Indeed, this already happens when we speak of unconscious mental processes within humans.</p>\n<p>Parting ways with Descartes (cf. Kirk (2012)), many present-day dualists would in fact go even further than reductionists in allowing for structural similarities between conscious and unconscious processes, treating all cognitive or functional mental states as (in theory) realizable without consciousness. E.g., Chalmers (1996): \"Although consciousness is a feature of the world that we would not predict from the physical facts, the things we say about consciousness are a garden-variety cognitive phenomenon. Somebody who knew enough about cognitive structure would immediately be able to predict the likelihood of utterances such as 'I feel conscious, in a way that no physical object could be,' or even Descartes's 'Cogito ergo sum.' In principle, some reductive explanation in terms of internal processes should render claims about consciousness no more deeply surprising than any other aspect of behavior.\"&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>\n<p><sup><a name=\"footnote7\"></a>7&nbsp;</sup>And since we happen to live in a world made of physics, the kind of monist we want in practice is a reductive&nbsp;<em>physicalist</em>&nbsp;AI. We want a 'physicalist' as opposed to a reductive monist that thinks everything is made of monads, or abstract objects, or morality fluid, or what-have-you.&nbsp;<a href=\"#footnote7back\">\u21a9</a></p>\n<p>&nbsp;</p>\n<h4>References</h4>\n<p>\u2219 Chalmers (1996). <em>The Conscious Mind: In Search of a Fundamental Theory</em>. Oxford University Press.</p>\n<p>\u2219&nbsp;Cockshott &amp; Michaelson (2007). <a href=\"http://www.dcs.gla.ac.uk/~wpc/reports/wegner25aug.pdf\">Are there new models of computation? Reply to Wegner and Eberbach</a>. <em>The Computer Journal, 50</em>: 232-247.</p>\n<p>\u2219&nbsp;Descartes (1641).&nbsp;<em><a href=\"http://www.wright.edu/~charles.taylor/descartes/mede.html\">Meditations on first philosophy, in which the existence of God and the immortality of the soul are demonstrated</a></em>.</p>\n<p>\u2219 Durand, Muchnik, Ushakov &amp; Vereshchagin (2004). <a href=\"http://link.springer.com/chapter/10.1007/978-3-540-27836-8_40\">Ecological Turing machines</a>. <em>Lecture Notes in Computer Science, 3142</em>: 457-468.</p>\n<p>\u2219&nbsp;Epstein &amp; Betke (2011). <a href=\"http://arxiv.org/abs/1107.0998\">An information-theoretic representation of agent dynamics as set intersections</a>. <em>Lecture Notes in Computer Science, 6830</em>: 72-81.</p>\n<p>\u2219 Hutter (2005). <em><a href=\"http://www.hutter1.net/ai/uaibook.htm\">Universal Artificial Intelligence: Sequence Decisions Based on Algorithmic Probability</a></em>. Springer.</p>\n<p>\u2219 Kirk (2012). <a href=\"http://plato.stanford.edu/archives/sum2012/entries/zombies\">Zombies</a>. In Zalta (ed.),&nbsp;<em>The Stanford Encyclopedia of Philosophy</em>.</p>\n<p>\u2219 Legg &amp; Veness (2013). <a href=\"http://arxiv.org/abs/1109.5951\">An approximation of the Universal Intelligence Measure</a>. <em>Lecture Notes in Computer Science, 7070</em>: 236-249.</p>\n<p>\u2219 Meehan (1977). <a href=\"http://gel.msu.edu/classes/tc848/papers/Meehan.Tale-Spin.pdf\">TALE-SPIN, an interactive program that writes stories</a>. <em>Proceedings of the 5th International Joint Conference on Artificial Intelligence</em>: 91-98.</p>\n<p>\u2219 Piccinini (2011). <a href=\"http://www.umsl.edu/~piccininig/CT_Modest_or_Bold.pdf\">The physical Church-Turing thesis: Modest or bold?</a> <em>British Journal for the Philosophy of Science, 62</em>: 733-769.</p>\n<p>\u2219 Russell &amp; Norvig (2010). <em><a href=\"http://aima.cs.berkeley.edu/\">Artificial Intelligence: A Modern Approach</a></em>. Prentice Hall.</p>\n<p>\u2219&nbsp;Solomonoff (2011). <a href=\"http://world.std.com/~rjs/alpstrong.pdf\">Algorithmic probability&nbsp;&mdash;&nbsp;its discovery&nbsp;&mdash;&nbsp;its properties and application to Strong AI</a>. In Zenil (ed.),&nbsp;<em>Randomness Through Computation: Some Answers, More Questions </em>(pp. 149-157).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "bmfs4jiLaF6HiiYkC": 1, "wMPYFGmhcFg4bSb4Z": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3WuAjWMtxQwTxr2Qn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 77, "extendedScore": null, "score": 0.000213, "legacy": true, "legacyId": "25396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 77, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>: <a href=\"/lw/jd9/building_phenomenological_bridges/\">Building Phenomenological Bridges</a></p>\n<p><strong>Summary</strong>: AI theorists often use models in which agents are crisply separated from their environments. This simplifying assumption can be useful, but it leads to trouble when we build machines that presuppose it. A machine that believes it can only interact with its environment in a narrow, fixed set of ways will not understand the value, or the dangers, of self-modification. By analogy with Descartes' mind/body dualism, I refer to agent/environment dualism as&nbsp;<em>Cartesianism</em>. The&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence#Open_problems\">open problem in Friendly AI</a>&nbsp;(OPFAI) I'm calling&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalized induction</a>&nbsp;is the project of replacing Cartesian approaches to scientific induction with reductive, physicalistic ones.</p>\n<hr>\n<p>&nbsp;</p>\n<p>I'll begin with a story about a storyteller.</p>\n<p>Once upon a time \u2014 specifically, 1976 \u2014&nbsp;there was an AI named TALE-SPIN. This AI told <a href=\"https://en.wikipedia.org/wiki/Interactive_storytelling\">stories</a> by inferring how characters would respond to problems from background knowledge about the characters' traits. One day, TALE-SPIN constructed a most peculiar tale.</p>\n<p style=\"padding-left: 30px;\">Henry Ant was thirsty. He walked over to the river bank where his good friend Bill Bird was sitting. Henry slipped and fell in the river. Gravity drowned.</p>\n<p>Since Henry fell in the river near his friend Bill, TALE-SPIN concluded that Bill rescued Henry. But for Henry to fall in the river, gravity must have pulled Henry. Which means gravity must have been in the river. TALE-SPIN had never been told that gravity knows how to swim; and TALE-SPIN had never been told that gravity has any friends. So gravity drowned.</p>\n<p>TALE-SPIN had previously been programmed to understand involuntary motion in the case of characters being pulled or carried by other characters \u2014 like Bill rescuing Henry. So it was programmed to understand 'character X fell to place Y' as 'gravity moves X to Y', as though gravity were a character in the story.<a name=\"footnote1back\"></a><sup><a href=\"#footnote1\">1</a></sup></p>\n<p>For us, the hypothesis 'gravity drowned' has low prior probability because we know gravity isn't the <em>type </em>of thing that swims or breathes or makes friends. We want agents to seriously consider whether the law of gravity pulls down rocks; we don't want agents to seriously consider whether the law of gravity pulls down the law of electromagnetism. We&nbsp;<a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">may not want</a>&nbsp;an AI to assign&nbsp;<a href=\"/lw/mp/0_and_1_are_not_probabilities/\"><em>zero&nbsp;</em>probability</a>&nbsp;to 'gravity drowned', but we at least want it to neglect the possibility as Ridiculous-By-Default.</p>\n<p>When we introduce deep type distinctions, however, we also introduce new ways our stories can fail.</p>\n<p><a id=\"more\"></a></p>\n<div><br></div>\n<div><br></div>\n<h3 id=\"Hutter_s_cybernetic_agent_model\">Hutter's cybernetic agent model</h3>\n<p>Russell and Norvig's leading AI textbook credits Solomonoff with setting the agenda for the field of AGI: \"AGI looks for a universal algorithm for learning and acting in any environment, and has its roots in the work of Ray Solomonoff[.]\" As an approach to AGI, Solomonoff induction presupposes a model with a strong type distinction between the 'agent' and the 'environment'. To make its intuitive appeal and attendant problems more obvious, I'll sketch out the model.</p>\n<p>A Solomonoff-inspired AI can most easily be represented as a multi-tape&nbsp;<a href=\"http://plato.stanford.edu/entries/turing-machine/\">Turing machine</a>&nbsp;like the one&nbsp;Alex Altair describes in&nbsp;<a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/#all_algorithms\">An Intuitive Explanation of Solomonoff Induction</a>. The machine has:</p>\n<p style=\"padding-left: 30px;\">-&nbsp;<span style=\"text-decoration: underline;\">three tapes</span>, labeled 'input', 'work', and 'output'. Each initially has an infinite strip of 0s written in discrete cells.</p>\n<p style=\"padding-left: 30px;\">- <span style=\"text-decoration: underline;\">one</span><span style=\"text-decoration: underline;\">&nbsp;head per tape</span>, with the input head able to read its cell's digit and move to the right, the output head able to write 0 or 1 to its cell and move to the right, and the work head able to read, write, and move in either direction.</p>\n<p style=\"padding-left: 30px;\">- a&nbsp;<span style=\"text-decoration: underline;\">program</span>, consisting of a finite, fixed set of&nbsp;transition rules.&nbsp;Each rule says when heads read, write, move, or do nothing, and how to transition to another rule.</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_0.png?v=8aed0100dd1d9a71648e37faa55d0865\" alt=\"\" width=\"420\" height=\"444\"></p>\n<p align=\"center\"><em>A three-tape Turing machine.</em></p>\n<p>&nbsp;</p>\n<p>We could imagine two such Turing machines communicating with each other. Call them 'Agent' and 'Environment', or 'Alice' and 'Everett'. Alice and Everett take turns acting. After Everett writes a bit to his output tape, that bit magically appears on Alice's input tape; and likewise, when Alice writes to her output tape, it gets copied to Everett's input tape. AI theorists have used this setup, which <a href=\"http://www.hutter1.net/\">Marcus Hutter</a> calls the <strong>cybernetic agent model</strong>,&nbsp;as an extremely simple representation of an agent that can&nbsp;<strong>perceive</strong>&nbsp;its environment (using the input tape), <strong>think&nbsp;</strong>(using the work tape), and&nbsp;<strong>act</strong>&nbsp;(using the output tape).<a name=\"footnote2aback\"></a><sup><a href=\"#footnote2\">2</a></sup></p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_10.png\" alt=\"\" width=\"660\" height=\"334\"></p>\n<p align=\"center\"><em>A Turing machine model of agent-environment interactions. At first, the machines differ only in their programs. \u2018Alice\u2019 is the agent we want to build, while \u2018Everett\u2019 stands for everything else that\u2019s causally relevant to Alice\u2019s success.</em></p>\n<p>&nbsp;</p>\n<p>We can define Alice and Everett's behavior in terms of any bit-producing Turing machines we'd like, including ones that represent probability distributions and do&nbsp;<a href=\"http://yudkowsky.net/rational/bayes\">Bayesian updating</a>. Alice might, for example, use her work tape to track four distinct possibilities and update probabilities over them:<a name=\"footnote3back\"></a><sup><a href=\"#footnote3\">3</a></sup></p>\n<ul>\n<li>(a) Everett always outputs 0.</li>\n<li>(b) Everett always outputs 1.</li>\n<li>(c) Everett outputs its input.</li>\n<li>(d) Everett outputs the opposite of its input.</li>\n</ul>\n<p>Alice starts with a uniform prior, i.e., 25% probability each. If Alice's first output is 1, and Everett responds with 1, then Alice can store those two facts on her work tape and conditionalize on them both, treating them as though they were certain. This results in 0.5 probability each for (b) and (c), 0 probability for (a) and (d).</p>\n<p>We care about an AI's epistemology only because it informs the AI's behavior \u2014 on this model, its bit output. If Alice outputs whatever bits maximize her expected chance of receiving 1s as input, then <a href=\"/lw/6ha/the_blueminimizing_robot/\">we can say</a> that Alice&nbsp;<strong>prefers</strong>&nbsp;to perceive 1. In the example I just gave, such a preference predicts that Alice will proceed to output 1 forever. Further exploration is unnecessary, since she knows of no other importantly different hypotheses to test.</p>\n<p>Enriching Alice's set of hypotheses for how Everett could act will let Alice win more games against a wider variety of Turing machines. The more programs Alice can pick out and assign a probability to, the more Turing machines Alice will be able to identify and intelligently respond to. If we aren't worried about whether it takes Alice ten minutes or a billion years to compute an update, and Everett will always patiently wait his turn, then we can simply have Alice perform perfect Bayesian updates; if her priors are right, and she translates her beliefs into sensible actions, she'll then be able to&nbsp;<a href=\"/lw/mt/beautiful_probability/\">optimally</a>&nbsp;respond to any environmental Turing machine.</p>\n<p>For AI researchers following Solomonoff's lead, that's the name of the game: Figure out the program that will let Alice behave optimally while communicating with as wide a range of Turing machines as possible, and you've at least solved the&nbsp;<em>theoretical&nbsp;</em>problem of picking out the optimal artificial agent from the space of possible reasoners. The agent/environment model here may look simple, but a number of theorists see it as distilling into its most basic form the task of an AGI.<a name=\"footnote2bback\"></a><sup><a href=\"#footnote2\">2</a></sup></p>\n<p>Yet a Turing machine,&nbsp;<a href=\"/lw/jd9/building_phenomenological_bridges/\">like a cellular automaton</a>, is an&nbsp;<a href=\"https://en.wikipedia.org/wiki/Automata_theory\">abstract machine</a>&nbsp;\u2014 a creature of thought experiments and mathematical proofs. Physical computers can act like abstract computers, in just the same sense that&nbsp;<a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/\">heaps of apples</a>&nbsp;can behave like&nbsp;<a href=\"/lw/f4e/logical_pinpointing/\">the abstract objects we call 'numbers'</a>. But computers and apples are&nbsp;<a href=\"/lw/lc/leaky_generalizations/\">high-level generalizations</a>, imperfectly represented by concise equations.<a name=\"footnote4back\"></a><sup><a href=\"#footnote4\">4</a></sup>&nbsp;When we move from our mental models to trying to build an actual AI, we have to pause and ask how well our formalism captures what's going on in reality.</p>\n<p>&nbsp;</p>\n<h3 id=\"The_problem_with_Alice\">The problem with Alice</h3>\n<p>'Sensory input' or 'data' is what I call the information Alice conditionalizes on; and 'beliefs' or 'hypotheses' is what I call the resultant probability distribution and representation of possibilities (in Alice's program or work tape). This distinction <a href=\"/lw/jiu/can_we_do_without_bridge_hypotheses/\">seems basic to reasoning</a>, so I endorse programming agents to treat them as two clearly distinct types. But in building such agents, we introduce the possibility of&nbsp;<strong>Cartesianism</strong>.</p>\n<p>Ren\u00e9 Descartes held that human minds and brains, although able to causally interact with each other, can each exist in the absence of the other; and, moreover, that the properties of purely material things can never fully explain minds. In his honor, we can call a model or procedure&nbsp;<em>Cartesian</em>&nbsp;if it treats the reasoner as a being separated from the physical universe. Such a being can perceive (and perhaps alter) physical processes, but it can't be identified with any such process.<a name=\"footnote5back\"></a><sup><a href=\"#footnote5\">5</a></sup></p>\n<p>The relevance of Cartesians to AGI work is that we can model them as agents experiencing a strong type distinction between 'mind' and 'matter', and an unshakable belief in the metaphysical independence of those two categories;&nbsp;<em>because&nbsp;</em>they're of such different kinds, they can vary independently. So we end up with AI errors that are the opposite of TALE-SPIN's \u2014 like an induction procedure that distinguishes gravity's type from embodied characters' types&nbsp;<em>so </em>strongly&nbsp;that it cannot hypothesize that, say, particles underlie or mediate both phenomena.</p>\n<p>My claim is that if we plug in 'Alice's sensory data' for 'mind' and 'the stuff Alice hypothesizes as causing the sensory data' for 'matter', then agents that can only model themselves using the cybernetic agent model are Cartesian in the relevant sense.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup></p>\n<p>The model is Cartesian because the agent and its environment&nbsp;<em>can only interact by communicating</em>.&nbsp;That is, their only way of affecting each other is by trading bits printed to tapes.</p>\n<p>If we build an actual AI that believes it's like Alice, it will believe that the environment can't affect it in ways that aren't immediately detectable, can't edit its source code, and can't force it to halt. But that makes the Alice-Everett system almost nothing like a physical agent embedded in a real environment. Under many circumstances, a real AI's environment will alter it directly. E.g., the AI can fall into a volcano. A volcano doesn't harm the agent by feeding unhelpful bits into its environmental sensors. It harms the agent by destroying it.</p>\n<p>A more naturalistic model would say: Alice outputs a bit; Everett reads it; and then Everett does whatever the heck he wants. That might be feeding a new bit into Alice. Or it might be vandalizing Alice's work tape, or smashing Alice flat.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_3.png?v=99c1224a99c75b811f5db5fe92762e63\" alt=\"\" width=\"474\" height=\"422\"></p>\n<p align=\"center\"><em>A robotic Everett tampering with an agent that mistakenly assumes Cartesianism. A real-world agent\u2019s computational states have physical correlates that can be directly edited by the environment. If the agent can't model such scenarios, its reasoning (and resultant decision-making) will suffer.</em></p>\n<p>&nbsp;</p>\n<p>A still more naturalistic approach would be to place Alice&nbsp;<em>inside&nbsp;</em>of Everett, as a subsystem. In the real world, agents are surrounded by their environments. The two form a cohesive whole, bound by the same physical laws, freely interacting and commingling.</p>\n<p>If Alice only worries about whether Everett will output a 0 or 1 to her sensory tape, then no matter how complex an understanding Alice has of Everett's inner workings, Alice will fundamentally misunderstand the situation she's in.&nbsp;Alice won't be able to represent hypotheses about how, for example, a pill might erase her memories or otherwise modify her source code.</p>\n<p>Humans, in contrast, can readily imagine a pill that modifies our memories. It seems childishly easy to hypothesize being changed by avenues other than perceived sensory information. The limitations of the cybernetic agent model aren't immediately obvious, because it isn't easy for us to put ourselves in the shoes of agents with alien blind spots.</p>\n<p>There&nbsp;<em>is&nbsp;</em>an agent-environment distinction, but it's a pragmatic and artificial one. The boundary between the part of the world we call 'agent' and the part we call 'not-agent' (= 'environment') is frequently fuzzy and mutable. If we want to build an agent that's robust across many environments and self-modifications, we can't just design a program that excels at predicting sensory sequences generated by Turing machines. We need an agent that can form accurate beliefs about the actual world it lives in, including accurate beliefs about its own physical underpinnings.</p>\n<p>&nbsp;</p>\n<h3 id=\"From_Cartesianism_to_naturalism\">From Cartesianism to naturalism</h3>\n<p>What would a naturalized self-model, a model of the agent as a process embedded in a lawful universe, look like? As a first attempt, one might point to the pictures of Cai in <a href=\"/lw/jd9/building_phenomenological_bridges/\">Building Phenomenological Bridges</a>.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_5.png?v=a853c43e6b17b53b147daa5b64e58c51\" alt=\"\" width=\"386\" height=\"371\"></p>\n<p align=\"center\"><em>Cai has a simple physical model of itself as a black tile at the center of a cellular automaton grid. Cai's phenomenological bridge hypotheses relate its sensory data to surrounding tiles' states.</em></p>\n<p>&nbsp;</p>\n<p>But this doesn't yet specify a non-Cartesian agent. To treat Cai as a Cartesian, we could view the tiles surrounding Cai as the work tape of Everett, and the dynamics of Cai's environment as Everett's program. (We can also convert Cai's perceptual experiences into a binary sequence on Alice/Cai's input tape, with a translation like 'cyan = 01, magenta = 10, yellow = 11'.)</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_11.png?v=48ddbf4a34f13d8b4f4c1cc413193470\" alt=\"\" width=\"523\" height=\"294\"></p>\n<p align=\"center\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><em>Alice/Cai as a cybernetic agent in a Turing machine circuit.</em></span></p>\n<p>&nbsp;</p>\n<p>The problem isn't that Cai's world is Turing-computable, of course. It's that if Cai's hypotheses are solely about what sorts of perception-correlated patterns of environmental change can occur, then Cai's models will be Cartesian.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jlg_4.png?v=c42f2470b3d8ac21cae40cd545afb023\" alt=\"\" width=\"430\" height=\"258\"></p>\n<p align=\"center\"><em>Cai as a Cartesian treats its sensory experiences as though they exist in a separate world.</em></p>\n<p>&nbsp;</p>\n<p>Cartesian Cai recognizes that its two universes, its sensory experiences and hypothesized environment, can interact. But it thinks they can only do so via a narrow range of stable pathways. No actual agent's mind-matter connections can be that simple and uniform.</p>\n<p>If Cai were a robot in a world resembling its model, it would <em>itself </em>be a complex pattern of tiles. To form accurate predictions, it would need to have self-models and bridge hypotheses that were more sophisticated than any I've considered so far. Humans are the same way: No bridge hypothesis explaining the physical conditions for subjective experience will ever fit on a T-shirt.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_1.png?v=a9eeb910cd4d5c524183df47375183ff\" alt=\"\" width=\"554\" height=\"520\"></p>\n<p align=\"center\"><em>Cai's world divided up into a 9x9 grid. Cai is the central 3x3 grid. Barely visible: Complex computations like Cai's reasoning are possible in this world because they're implemented by even finer tile patterns at smaller scales.</em></p>\n<p>&nbsp;</p>\n<p>Changing Cai's tiles' states \u2014 from black to white, for example \u2014 could have a large impact on its computations, analogous to changing a human brain from solid to gaseous. But if an agent's hypotheses are all shaped like the cybernetic agent model, 'my input/output algorithm is replaced by a dust cloud' won't be in the hypothesis space.</p>\n<p>If you programmed something to thinks like Cartesian Cai, it might decide that its sequence of visual experiences will persist even if the tiles forming its brain completely change state. It wouldn't be able to entertain thoughts like 'if Cai performs self-modification #381, Cai will experience its environment as smells rather than colors' or 'if Cai falls into a volcano, Cai gets destroyed'. No pattern of perceived colors is identical to a perceived smell, or to the absence of perception.</p>\n<p>To form naturalistic self-models and world-models, Cai needs hypotheses that look less like conversations between independent programs, and more like worlds in which it is a fairly <a href=\"/lw/icn/reality_is_weirdly_normal/\">ordinary</a> subprocess, governed by the same general patterns. It needs to form and privilege physical hypotheses under which it has parts, as well as bridge hypotheses under which those parts correspond in plausible ways to its high-level computational states.</p>\n<p>Cai wouldn't need a <em>complete </em>self-model in order to recognize general facts about its subsystems. Suppose, for instance, that Cai has just one sensor, on its left side, and a motor on its right side. Cai might recognize that the motor and sensor regions of its body correspond to its introspectible decisions and perceptions, respectively.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_2.png?v=1c1f46dd31aab5e5bd893d7d786745ce\" alt=\"\" width=\"523\" height=\"320\"></p>\n<p align=\"center\"><em>A naturalized agent can recognize that it has physical parts with varying functions. Cai's top and bottom lack sensors and motors altogether, making it clearer that Cai's environment can impact Cai by entirely non-sensory means.</em></p>\n<p>&nbsp;</p>\n<p>We care about Cai's models because we want to use Cai to modify its environment. For example, we may want Cai to convert as much of its environment as possible into grey tiles. Our interest is then in the algorithm that reliably outputs maximally greyifying actions when handed perceptual data.</p>\n<p>If Cai is able to form sophisticated self-models, then Cai can recognize that it's a grey tile maximizer. Since it wants there to be more grey tiles, it also wants to make sure that it continues to exist, provided it believes that it's better than chance at pursuing its goals.</p>\n<p>More specifically, Naturalized Cai can recognize that its actions are some black-box function of its perceptual computations. Since it has a bridge hypothesis linking its perceptions to its middle-left tile, it will then reason that it should <em>preserve </em>its sensory hardware. Cai's self-model tells it that if its sensor fails, then its actions will be based on beliefs that are much less correlated with the environment. And its self-model tells it that if its actions are poorly calibrated, then there will be fewer grey tiles in the universe. Which is bad.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_7.png?v=55945de90d1fa30f2073e062c9de888f\" alt=\"\" width=\"305\" height=\"335\"></p>\n<p align=\"center\"><em>A naturalistic version of Cai can reason intelligently from the knowledge that its actions (motor output) depend on a specific part of its body that's responsible for perception (environmental input).</em></p>\n<p>&nbsp;</p>\n<p>A physical Cai might need to foresee scenarios like 'an anvil crashes into my head and destroys me', and assign probability mass to them. Bridge hypotheses expressive enough to consider that possibility would not just relate experiences to environmental or hardware states; they would also recognize that the agent's experiences can be absent altogether.</p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_9.png?v=e4ab7391e378b840ed1325c5c24adc9c\" alt=\"\" width=\"711\" height=\"533\"></p>\n<p align=\"center\"><em>An anvil can destroy Cai's perceptual hardware by crashing into it. A Cartesian might not worry about this eventuality, expecting its experience to persist after its body is smashed. But a naturalized reasoner will form hypotheses like the above, on which its sequence of color experiences suddenly terminates when its sensors are destroyed.</em></p>\n<p>&nbsp;</p>\n<p>This point generalizes to other ways Cai might self-modify, and to other things Cai might alter about itself. For example, Cai might learn that other portions of its brain correspond to its hypotheses and desires.</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_6.png?v=b0acbedcc7136adde91a267db83165a9\" alt=\"\" width=\"512\" height=\"320\"></p>\n<p align=\"center\"><em>Another very simple model of how different physical structures are associated with different computational patterns.</em></p>\n<p>&nbsp;</p>\n<p>This allows Cai to recognize that its goals depend on the proper functioning of <em>many </em>of its hardware components. If Cai believes that its actions depend on its brain's goal unit's working a specific way, then it will avoid taking pills that foreseeably change its goal unit. If Cai's causal model tells it that agents like it stop exhibiting future-steering behaviors when they self-modify to have mad priors, then it won't self-modify to acquire mad priors. And so on.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jlg_8.png?v=0975b5a26466dcbbe625edf6d8947a03\" alt=\"\" width=\"488\" height=\"328\"></p>\n<p align=\"center\"><em>If Cai's motor fails, its effect on the world can change as a result. The same is true if its hardware is modified in ways that change its thoughts, or its preferences (i.e., the thing linking its conclusions to its motor).</em></p>\n<p>&nbsp;</p>\n<p>Once Cai recognizes that its brain needs to work in a very specific way for its goals to be achieved, its preferences can take its physical state into account in sensible ways, without our needing to hand-code Cai at the outset to have the right beliefs or preferences over every individual thing that could change in its brain.</p>\n<p>Just the opposite is true for Cartesians. Since they can't form hypotheses like 'my tape heads will stop computing digits if I disassemble them', they can only intelligently navigate such risks if they've been hand-coded in advance to avoid perceptual experiences the programmer thought would correlate with such dangers.</p>\n<p>In other words, even though all of this is still highly informal, there's already some cause to think that a reasoning pattern like Naturalized Cai can generalize in ways that Cartesians can't. The programmers don't need to know&nbsp;<em>everything</em> about Cai's physical state, or anticipate&nbsp;<em>everything</em> about what future changes Cai might undergo, if Cai's epistemology allows it to easily form accurate reductive beliefs and behave accordingly. An agent like this might be adaptive and self-correcting in very novel circumstances, leaving more wiggle room for programmers to make human mistakes.</p>\n<p>&nbsp;</p>\n<h3 id=\"Bridging_maps_of_worlds_and_maps_of_minds\">Bridging maps of worlds and maps of minds</h3>\n<p>Solomonoff-style dualists have alien blind spots that lead them to neglect the possibility that some hardware state is equivalent to some introspected computation '000110'. TALE-SPIN-like AIs, on the other hand, have blind spots that lead to mistakes like trying to figure out the angular momentum of '000110'.</p>\n<p>A naturalized agent doesn't try to do away<em>&nbsp;</em>with the data/hypothesis type distinction and acquire a typology as simple as TALE-SPIN's. Rather, it tries to tightly interconnect its types using bridges. Naturalizing induction is about combining the dualist's useful&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">map/territory distinction</a>&nbsp;with a more sophisticated metaphysical monism than TALE-SPIN exhibits, resulting in a <em>reductive</em> monist AI.<a style=\"background-color: #ffffff;\" name=\"footnote7back\"></a><sup><a href=\"#footnote7\">7</a></sup></p>\n<p>Alice's simple fixed bridge axiom,&nbsp;{environmental output 0&nbsp;\u2194 perceptual input 0, environmental output 1&nbsp;\u2194 perceptual input 1}, is inadequate for physically embodied agents. And the problem isn't&nbsp;<em>just&nbsp;</em>that Alice lacks other bridge rules and can't weigh evidence for or against each one. Bridge hypotheses are a step in the right direction, but they need to be diverse enough to express a variety of correlations between the agent's sensory experiences and the physical world, and they need a sensible prior. An agent that only considers bridge hypotheses compatible with the cybernetic agent model will falter whenever it and the environment interact in ways that look nothing like exchanging sensory bits.</p>\n<p>With the help of an inductive algorithm that uses bridge hypotheses to relate sensory data to a continuous physical universe, we can avoid making our AIs Cartesians. This will make their epistemologies much more secure. It will also make it possible for them to want things to be true about the physical universe, not just about the particular sensory experiences they encounter. Actually writing a program that does all this is an OPFAI. Even formalizing how bridge hypotheses ought to work in principle is an OPFAI.</p>\n<p>In my next post, I'll move away from toy models and discuss&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>,&nbsp;Hutter's optimality definition for cybernetic agents. In asking whether the <em>best </em>Cartesian can overcome the difficulties I've described, we'll get a clearer sense of why Solomonoff inductors aren't&nbsp;reflective and reductive enough to predict drastic changes to their sense-input-to-motor-output relation \u2014 and why they <em>can't be </em>that reflective and reductive \u2014 and why this matters.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<h4 id=\"Notes\">Notes</h4>\n<p><sup><a name=\"footnote1\"></a>1</sup> Meehan (1977). Colin Allen first introduced me to this story.&nbsp;<a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2Fjd9%2Fbuilding_phenomenological_bridges%2F&amp;v=1&amp;libId=e232dce4-8d1b-4871-9ed3-e80435e6b430&amp;out=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DQbvkja-J9iQC%26pg%3DPA256%26lpg%3DPA256%26dq%3D%2522Henry%2BAnt%2Bwas%2Bthirsty.%2BHe%2522%26source%3Dbl%26ots%3D71hkRLo1T-%26sig%3DBQ8c5_-U_s-oRiDESSeA93mgR0s%26hl%3Den%26sa%3DX%26ei%3DxdabUr-rH8zZoASgxoHgCQ%26ved%3D0CDIQ6AEwAjgK%23v%3Donepage%26q%3D%2522Henry%2520Ant%2520was%2520thirsty.%2520He%2522%26f%3Dfalse&amp;ref=https%3A%2F%2Fwww.google.com%2F&amp;title=Building%20Phenomenological%20Bridges%20-%20Less%20Wrong&amp;txt=TALESPIN&amp;jsonp=vglnk_jsonp_13910468903006\">Dennett</a>&nbsp;discusses it as well.&nbsp;<a href=\"#footnote1back\">\u21a9</a></p>\n<p><sup><a name=\"footnote2\"></a>2</sup>&nbsp;E.g., Durand, Muchnik, Ushakov &amp; Vereshchagin (2004), Epstein &amp; Betke (2011), Legg &amp; Veness (2013), Solomonoff (2011). Hutter (2005) uses the term \"cybernetic agent model\" to emphasize the parallelism between his Turing machine circuit and <a href=\"http://en.wikipedia.org/wiki/Control_theory\">control theory</a>'s <a href=\"http://www.calresco.org/lucas/systems.htm\">cybernetic systems</a>.&nbsp;<a href=\"#footnote2aback\">\u21a9</a>&nbsp;<a href=\"#footnote2bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote3\"></a>3</sup>&nbsp;One simple representation would be: Program Alice to write to her work tape, on round one, 0010 (standing for 'if I output 0, Everett outputs 0; if I output 1, Everett outputs 0'). Ditto for the other three hypotheses, 0111, 0011, and 0110. Then write the hypothesis' probability in binary (initially 25%, represented '11001') to the right of each, and program Alice to edit this number as she receives new evidence. Since the first and third digit stay the same, we can simplify the hypotheses' encoding to 00, 11, 01, 10. Indeed, if the hypotheses remain the same over time there's no reason to visibly distinguish them in the work tape at all, when we can instead just program Alice to use the left-to-right ordering of the four probabilities to distinguish the hypotheses.&nbsp;<a href=\"#footnote3back\">\u21a9</a></p>\n<p><sup><a name=\"footnote4\"></a>4</sup>&nbsp;To the extent our universe&nbsp;<a href=\"/lw/ms/is_reality_ugly/\"><em>perfectly&nbsp;</em>resembles any mathematical structure</a>, it's much more likely to do so&nbsp;<a href=\"/lw/on/reductionism/\">at the level</a>&nbsp;of gluons and mesons than at the level of medium-sized dry goods. The resemblance of apples to natural numbers is much more approximate. Two apples and three apples generally make five apples, but when you start cutting up or pulverizing or genetically altering apples, you may find that other mathematical models do a superior job of predicting the apples' behavior. It seems likely that the only perfectly general and faithful mathematical representation of apples will be some drastically large and unwieldy physics equation.</p>\n<p>Ditto for machines. It's sometimes possible to build a physical machine that closely mimics a given Turing machine \u2014 but only 'closely', as Turing machines have unboundedly large tapes. And although any halting Turing machine can in principle be simulated with a bounded tape (Cockshott &amp; Michaelson (2007)), nearly all Turing machine programs are too large to even be approximated by any physical process.</p>\n<p>All physical machines structurally resemble Turing machines&nbsp;in ways that allow us to draw productive inferences from the one group to the other. See Piccinini's (2011) discussion of the&nbsp;<a href=\"http://www.umsl.edu/~piccininig/CT_Modest_or_Bold.pdf\">physical Church-Turing thesis</a>.&nbsp;But, for all that, the concrete machine and the abstract one remain distinct.&nbsp;<a href=\"#footnote4back\">\u21a9</a></p>\n<p><sup><a name=\"footnote5\"></a>5</sup>&nbsp;Descartes (1641): \"[A]lthough I certainly do possess a body with which I am very closely conjoined; nevertheless, because, on the one hand, I have a clear and distinct idea of myself, in as far as I am only a thinking and unextended thing, and as, on the other hand, I possess a distinct idea of body, in as far as it is only an extended and unthinking thing, it is certain that I (that is, my mind, by which I am what I am) am entirely and truly distinct from my body, and may exist without it.\"</p>\n<p>From this it\u2019s clear that Descartes also believed that the mind can exist without the body. This interestingly parallels the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">anvil problem</a>, which I'll discuss more in my next post. However, I don't build immortality into my definition of 'Cartesianism'. Not all agents that act as though there is a Cartesian barrier between their thoughts and the world think that their experiences are future-eternal. I'm taking care not to conflate Cartesianism with the anvil problem because the formalism I'll discuss next time, AIXI, does face both of them. Though the problems are logically distinct, it's true that a naturalized reasoning method would be much less likely to face the anvil problem.&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><sup><a name=\"footnote6\"></a>6</sup>&nbsp;This isn't to say that a Solomonoff inductor would need to be conscious in anything like the way humans are conscious. It can be fruitful to point to similarities between the reasoning patterns of humans and unconscious processes. Indeed, this already happens when we speak of unconscious mental processes within humans.</p>\n<p>Parting ways with Descartes (cf. Kirk (2012)), many present-day dualists would in fact go even further than reductionists in allowing for structural similarities between conscious and unconscious processes, treating all cognitive or functional mental states as (in theory) realizable without consciousness. E.g., Chalmers (1996): \"Although consciousness is a feature of the world that we would not predict from the physical facts, the things we say about consciousness are a garden-variety cognitive phenomenon. Somebody who knew enough about cognitive structure would immediately be able to predict the likelihood of utterances such as 'I feel conscious, in a way that no physical object could be,' or even Descartes's 'Cogito ergo sum.' In principle, some reductive explanation in terms of internal processes should render claims about consciousness no more deeply surprising than any other aspect of behavior.\"&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>\n<p><sup><a name=\"footnote7\"></a>7&nbsp;</sup>And since we happen to live in a world made of physics, the kind of monist we want in practice is a reductive&nbsp;<em>physicalist</em>&nbsp;AI. We want a 'physicalist' as opposed to a reductive monist that thinks everything is made of monads, or abstract objects, or morality fluid, or what-have-you.&nbsp;<a href=\"#footnote7back\">\u21a9</a></p>\n<p>&nbsp;</p>\n<h4 id=\"References\">References</h4>\n<p>\u2219 Chalmers (1996). <em>The Conscious Mind: In Search of a Fundamental Theory</em>. Oxford University Press.</p>\n<p>\u2219&nbsp;Cockshott &amp; Michaelson (2007). <a href=\"http://www.dcs.gla.ac.uk/~wpc/reports/wegner25aug.pdf\">Are there new models of computation? Reply to Wegner and Eberbach</a>. <em>The Computer Journal, 50</em>: 232-247.</p>\n<p>\u2219&nbsp;Descartes (1641).&nbsp;<em><a href=\"http://www.wright.edu/~charles.taylor/descartes/mede.html\">Meditations on first philosophy, in which the existence of God and the immortality of the soul are demonstrated</a></em>.</p>\n<p>\u2219 Durand, Muchnik, Ushakov &amp; Vereshchagin (2004). <a href=\"http://link.springer.com/chapter/10.1007/978-3-540-27836-8_40\">Ecological Turing machines</a>. <em>Lecture Notes in Computer Science, 3142</em>: 457-468.</p>\n<p>\u2219&nbsp;Epstein &amp; Betke (2011). <a href=\"http://arxiv.org/abs/1107.0998\">An information-theoretic representation of agent dynamics as set intersections</a>. <em>Lecture Notes in Computer Science, 6830</em>: 72-81.</p>\n<p>\u2219 Hutter (2005). <em><a href=\"http://www.hutter1.net/ai/uaibook.htm\">Universal Artificial Intelligence: Sequence Decisions Based on Algorithmic Probability</a></em>. Springer.</p>\n<p>\u2219 Kirk (2012). <a href=\"http://plato.stanford.edu/archives/sum2012/entries/zombies\">Zombies</a>. In Zalta (ed.),&nbsp;<em>The Stanford Encyclopedia of Philosophy</em>.</p>\n<p>\u2219 Legg &amp; Veness (2013). <a href=\"http://arxiv.org/abs/1109.5951\">An approximation of the Universal Intelligence Measure</a>. <em>Lecture Notes in Computer Science, 7070</em>: 236-249.</p>\n<p>\u2219 Meehan (1977). <a href=\"http://gel.msu.edu/classes/tc848/papers/Meehan.Tale-Spin.pdf\">TALE-SPIN, an interactive program that writes stories</a>. <em>Proceedings of the 5th International Joint Conference on Artificial Intelligence</em>: 91-98.</p>\n<p>\u2219 Piccinini (2011). <a href=\"http://www.umsl.edu/~piccininig/CT_Modest_or_Bold.pdf\">The physical Church-Turing thesis: Modest or bold?</a> <em>British Journal for the Philosophy of Science, 62</em>: 733-769.</p>\n<p>\u2219 Russell &amp; Norvig (2010). <em><a href=\"http://aima.cs.berkeley.edu/\">Artificial Intelligence: A Modern Approach</a></em>. Prentice Hall.</p>\n<p>\u2219&nbsp;Solomonoff (2011). <a href=\"http://world.std.com/~rjs/alpstrong.pdf\">Algorithmic probability&nbsp;\u2014&nbsp;its discovery&nbsp;\u2014&nbsp;its properties and application to Strong AI</a>. In Zenil (ed.),&nbsp;<em>Randomness Through Computation: Some Answers, More Questions </em>(pp. 149-157).</p>", "sections": [{"title": "Hutter's cybernetic agent model", "anchor": "Hutter_s_cybernetic_agent_model", "level": 1}, {"title": "The problem with Alice", "anchor": "The_problem_with_Alice", "level": 1}, {"title": "From Cartesianism to naturalism", "anchor": "From_Cartesianism_to_naturalism", "level": 1}, {"title": "Bridging maps of worlds and maps of minds", "anchor": "Bridging_maps_of_worlds_and_maps_of_minds", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"title": "References", "anchor": "References", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "62 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ethRJh2E7mSSjzCay", "6FmqiAgS8h4EJm86s", "QGkYCwyC7wTDyt3yT", "hQHuXuRGZxxWXaPgg", "bkSkRwo9SRYxJMiSY", "bTsiPnFndZeqTnWpu", "3FoMuCLqZggTxoC3S", "Tc2H9KbKRjuDJ3WSS", "heJZLrC6EeJaskLbu", "jntmpKiyg6oHFhLcP", "oKiy7YwGToaYXdvnj", "tPqQdLCuxanjhoaNs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T02:40:15.550Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC EA meetup (now with guests!)", "slug": "meetup-washington-dc-ea-meetup-now-with-guests", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cWFCpzEdgiTQRcbXD/meetup-washington-dc-ea-meetup-now-with-guests", "pageUrlRelative": "/posts/cWFCpzEdgiTQRcbXD/meetup-washington-dc-ea-meetup-now-with-guests", "linkUrl": "https://www.lesswrong.com/posts/cWFCpzEdgiTQRcbXD/meetup-washington-dc-ea-meetup-now-with-guests", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20EA%20meetup%20(now%20with%20guests!)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20EA%20meetup%20(now%20with%20guests!)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWFCpzEdgiTQRcbXD%2Fmeetup-washington-dc-ea-meetup-now-with-guests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20EA%20meetup%20(now%20with%20guests!)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWFCpzEdgiTQRcbXD%2Fmeetup-washington-dc-ea-meetup-now-with-guests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWFCpzEdgiTQRcbXD%2Fmeetup-washington-dc-ea-meetup-now-with-guests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wx'>Washington DC EA meetup (now with guests!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 February 2014 03:00:02PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about EA with some people from the DC EA meetup group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wx'>Washington DC EA meetup (now with guests!)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cWFCpzEdgiTQRcbXD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5711472540020053e-06, "legacy": true, "legacyId": "25541", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_EA_meetup__now_with_guests__\">Discussion article for the meetup : <a href=\"/meetups/wx\">Washington DC EA meetup (now with guests!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 February 2014 03:00:02PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about EA with some people from the DC EA meetup group.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_EA_meetup__now_with_guests__1\">Discussion article for the meetup : <a href=\"/meetups/wx\">Washington DC EA meetup (now with guests!)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC EA meetup (now with guests!)", "anchor": "Discussion_article_for_the_meetup___Washington_DC_EA_meetup__now_with_guests__", "level": 1}, {"title": "Discussion article for the meetup : Washington DC EA meetup (now with guests!)", "anchor": "Discussion_article_for_the_meetup___Washington_DC_EA_meetup__now_with_guests__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T03:31:59.291Z", "modifiedAt": null, "url": null, "title": "Open Thread for February 18-24 2014", "slug": "open-thread-for-february-18-24-2014-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rg2Rhy8CLsb5HLT8S/open-thread-for-february-18-24-2014-0", "pageUrlRelative": "/posts/rg2Rhy8CLsb5HLT8S/open-thread-for-february-18-24-2014-0", "linkUrl": "https://www.lesswrong.com/posts/rg2Rhy8CLsb5HLT8S/open-thread-for-february-18-24-2014-0", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20for%20February%2018-24%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20for%20February%2018-24%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frg2Rhy8CLsb5HLT8S%2Fopen-thread-for-february-18-24-2014-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20for%20February%2018-24%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frg2Rhy8CLsb5HLT8S%2Fopen-thread-for-february-18-24-2014-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frg2Rhy8CLsb5HLT8S%2Fopen-thread-for-february-18-24-2014-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rg2Rhy8CLsb5HLT8S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "25542", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T04:00:42.751Z", "modifiedAt": null, "url": null, "title": "Meetup : Second Canberra Meetup - Paranoid Debating", "slug": "meetup-second-canberra-meetup-paranoid-debating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:59.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G2atNzv47k664Lhkp/meetup-second-canberra-meetup-paranoid-debating", "pageUrlRelative": "/posts/G2atNzv47k664Lhkp/meetup-second-canberra-meetup-paranoid-debating", "linkUrl": "https://www.lesswrong.com/posts/G2atNzv47k664Lhkp/meetup-second-canberra-meetup-paranoid-debating", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Second%20Canberra%20Meetup%20-%20Paranoid%20Debating&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Second%20Canberra%20Meetup%20-%20Paranoid%20Debating%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG2atNzv47k664Lhkp%2Fmeetup-second-canberra-meetup-paranoid-debating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Second%20Canberra%20Meetup%20-%20Paranoid%20Debating%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG2atNzv47k664Lhkp%2Fmeetup-second-canberra-meetup-paranoid-debating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG2atNzv47k664Lhkp%2Fmeetup-second-canberra-meetup-paranoid-debating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wy'>Second Canberra Meetup - Paranoid Debating</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 February 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">148 Bunda St, Canberra</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For our second meetup, we will be playing paranoid debating (for details, see <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Paranoid_debating</a>). We will be meeting on Friday the 28th of February at 6:30 pm at Grill'd in Canberra Centre - it is on the ground floor near Supabarn, and offers vegetarian and vegan options.\nIf you are interested in attending regular LW meetups in Canberra, whether you can or can't come to this one, please fill in when you would be regularly available at <a href=\"http://doodle.com/z4se4kpq7kepf737\" rel=\"nofollow\">http://doodle.com/z4se4kpq7kepf737</a> . In addition, we have a FB group where these events will also be announced: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a> . Finally, if you would like to receive email updates, please email me at danielfilan@yahoo.com.au, and I will send you emails about upcoming meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wy'>Second Canberra Meetup - Paranoid Debating</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G2atNzv47k664Lhkp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5712412539198578e-06, "legacy": true, "legacyId": "25544", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Second_Canberra_Meetup___Paranoid_Debating\">Discussion article for the meetup : <a href=\"/meetups/wy\">Second Canberra Meetup - Paranoid Debating</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 February 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">148 Bunda St, Canberra</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For our second meetup, we will be playing paranoid debating (for details, see <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Paranoid_debating</a>). We will be meeting on Friday the 28th of February at 6:30 pm at Grill'd in Canberra Centre - it is on the ground floor near Supabarn, and offers vegetarian and vegan options.\nIf you are interested in attending regular LW meetups in Canberra, whether you can or can't come to this one, please fill in when you would be regularly available at <a href=\"http://doodle.com/z4se4kpq7kepf737\" rel=\"nofollow\">http://doodle.com/z4se4kpq7kepf737</a> . In addition, we have a FB group where these events will also be announced: <a href=\"https://www.facebook.com/groups/lwcanberra/\" rel=\"nofollow\">https://www.facebook.com/groups/lwcanberra/</a> . Finally, if you would like to receive email updates, please email me at danielfilan@yahoo.com.au, and I will send you emails about upcoming meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Second_Canberra_Meetup___Paranoid_Debating1\">Discussion article for the meetup : <a href=\"/meetups/wy\">Second Canberra Meetup - Paranoid Debating</a></h2>", "sections": [{"title": "Discussion article for the meetup : Second Canberra Meetup - Paranoid Debating", "anchor": "Discussion_article_for_the_meetup___Second_Canberra_Meetup___Paranoid_Debating", "level": 1}, {"title": "Discussion article for the meetup : Second Canberra Meetup - Paranoid Debating", "anchor": "Discussion_article_for_the_meetup___Second_Canberra_Meetup___Paranoid_Debating1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T07:09:21.514Z", "modifiedAt": null, "url": null, "title": "Rationality training: academic and clinical perspectives", "slug": "rationality-training-academic-and-clinical-perspectives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:07.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whales", "createdAt": "2014-01-07T06:31:44.661Z", "isAdmin": false, "displayName": "whales"}, "userId": "n7p4iZqwmT3whXoAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vENLgXhMoecM4jmiZ/rationality-training-academic-and-clinical-perspectives", "pageUrlRelative": "/posts/vENLgXhMoecM4jmiZ/rationality-training-academic-and-clinical-perspectives", "linkUrl": "https://www.lesswrong.com/posts/vENLgXhMoecM4jmiZ/rationality-training-academic-and-clinical-perspectives", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20training%3A%20academic%20and%20clinical%20perspectives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20training%3A%20academic%20and%20clinical%20perspectives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvENLgXhMoecM4jmiZ%2Frationality-training-academic-and-clinical-perspectives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20training%3A%20academic%20and%20clinical%20perspectives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvENLgXhMoecM4jmiZ%2Frationality-training-academic-and-clinical-perspectives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvENLgXhMoecM4jmiZ%2Frationality-training-academic-and-clinical-perspectives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1818, "htmlBody": "<p>What can existing research tell us about teaching and learning key rationalist skills? One central idea is the monitoring and adjustment of mental processes, which appears in various forms in the LW community, debiasing research, education research concerning metacognition, and cognitive-behavioral and related therapies. I expect that rationality training can benefit from fully realizing the connections between these fields, which some mainstream research is already beginning to do. In another post, I discuss <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/\">a self-experiment in noticing confusion</a> based on these ideas.</p>\n<p><a id=\"more\"></a></p>\n<h2>Debiasing</h2>\n<p>In <a href=\"/lw/76x/is_rationality_teachable/\">Is Rationality Teachable?</a>, Luke noted some promising successes in teaching statistics and logic, and also in the growing body of research on debiasing. The main lines of research consider debiasing techniques that are socially-administered (incentives, accountability, group decision making) or imposed \"top-down\" as rules (<a href=\"/lw/7ep/practical_debiasing/\">\"consider the opposite\"</a>). While individuals can internalize and use the latter techniques on their own, the top-down and domain-general nature inhibits transfer (their conversion from declarative to tacit knowledge, or migration from System 2 to System 1).<a name=\"footnote1back\"></a><sup><a href=\"#footnote1\">1</a></sup></p>\n<p>A better debiasing technique from the individual perspective would teach someone to notice the feeling of distorted cognition and trigger corrective action: to notice mental flinching, rationalization, or confusion, and then to bring it to full conscious attention and act on it appropriately. I currently see that as the most important component of what Eliezer's getting at with <a href=\"/lw/5kz/the_5second_level/\">The 5-Second Level</a>.<a name=\"footnote2back\"></a><a name=\"footnote3back\"></a><sup><a href=\"#footnote2\">2</a>&nbsp;<a href=\"#footnote3\">3</a></sup></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick (2004)</a> anticipated this idea in his speculation on future directions for debiasing research:</p>\n<blockquote>\n<p>[One direction] comes from the growing focus on how affect, motivation, and self-esteem in\ufb02uence decision making (see Larrick, 1993; Payne &amp; Bettman, Chapter 6, this volume; Rottenstreich &amp; Shu, Chapter 22, this volume). Identifying debiasing techniques for affect-based biases is a promising new area &ndash; What interventions help people cope effectively with emotion endogenous to a decision, such as anticipated regret? Or help them recognize and discount emotion that is extraneous to a decision, such as anger from some unrelated experience? <strong>The answers may bring decision research surprisingly close to clinical psychology, such as techniques used in cognitive-behavioral therapy</strong> [<em>emphasis mine</em>].</p>\n</blockquote>\n<h2>Cognitive Behavioral Therapy</h2>\n<p>How has that speculation fared? It seems that others have made the same connection, but that progress has been limited.&nbsp;<a href=\"http://whaaales.com/LeddyAndersonSchulkin2013.pdf\">Leddy et al. (2013)</a> provide an overview of decision science (DS) and cognitive behavioral therapy (CBT), and finally reflect on the similarities:</p>\n<blockquote>\n<p>The cognitive patterns that DS and CBT have identified and labeled [\"biases\" and \"cognitive distortions,\" usually] are their respective building blocks; however, there are numerous similarities between the patterns each field has identified, despite having arrived at different terminology...</p>\n<p>A difference in the fields of CBT and decision making is in the context, the research and application. CBT is an applied field that incorporates the client's environment. CBT encourages the client to self-monitor, challenge, and modify their thoughts in their own environment, with the goal being for clients to generalize skills learned in the therapy room to real-life contexts. However, a majority of the work in decision science has been conducted in isolation from the context in which real-life decisions take place. While more work is being done in medical, legal, and other fields, there are still limitations to this research because numerous immeasurable variables are difficult to control for in applied research settings (e.g., previous experience, social influences). It is also unclear whether decisions made or skills used in the research setting would be utilized in the real-world.</p>\n<p>Based on the parallels between biases and cognitive distortions, we suggest that CBT practitioners keep in mind various cognitive biases to supplement discussions of cognitive distortions... Conversely, utilizing the techniques of cognitive-behavioral therapy in decision science may be beneficial; the implementation of the cognitive-behavioral techniques of goal setting, cognitive-behavioral assessment, self-monitoring, cognitive restructuring, exposure, behavioral experiments and relapse prevention may improve decision outcome.</p>\n</blockquote>\n<p>The authors then point out research on \"cognitive bias modification,\"<a name=\"footnote4back\"></a><sup><a href=\"#footnote4\">4</a></sup> which looks to be pointed in a good direction concerning the teaching and learning of mental habits to counter cognitive biases, as well as some other work in clinical psychology with a decision-science perspective. \"Self-monitoring\" does come up in the above connection between CBT and decision science, but the authors don't particularly seize on the metacognitive aspect that seems to be the key commonality to me.<a name=\"footnote5back\"></a><sup><a href=\"#footnote5\">5</a></sup></p>\n<p>I should perhaps note that CBT, while vastly better than nothing at all, is <a href=\"http://slatestarcodex.com/2013/09/19/scientific-freud/\">not necessarily the uniquely evidence-based therapy</a> that many of us came to know it as. I mention this because it is at least slight evidence against the mechanism of action lying in patients' successfully learning specific mental skills, and in favor of (among other hypotheses) the effectiveness of simply talking to someone charismatic and empathetic. It could also be that different things work for different people.</p>\n<p>What should we conclude? Perhaps that rationality training could end up looking in some ways more like therapy than like martial arts. It may be worth adapting therapeutic models specifically to debiasing.</p>\n<h2>Metacognition</h2>\n<p>Finally, the education research community seems to have their own body of work on similar concepts under different names. <a href=\"http://whaaales.com/book-review-how-learning-works/#Metacognition\">Metacognition</a>&nbsp;plays a key role in learning, and it can be effectively taught.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup> The term includes a number of important skills and processes, most of which we aren't interested in right now: declarative knowledge about study tactics, \"offline\" planning, retrospective evaluation of performance. These are roughly analogous with \"top-down\" techinques for debiasing.</p>\n<p>But some metacognition research is concerned with real-time or \"online\" self-monitoring&mdash;that is, the \"metacognitive experiences\" associated with ongoing cognitive processing&mdash;very nearly what we're looking for. In general it's difficult to address these processes directly, but they can still be indirectly&nbsp;<a href=\"http://whaaales.com/book-review-how-learning-works/#Mastery\">scaffolded</a>, engaged, and measured. For example, activities that require examining mental models (as opposed to tapping short-term memory alone) have been found effective in promoting calibration of confidence in understanding (\"metacomprehension accuracy,\" which presumably relies on metacognitive experiences). One such activity is generative testing, which includes summarizing learned material after some time has passed. Another is self-explanation/self-questioning during reading, where you progressively construct and test your model by asking yourself how ideas fit with each other.<a name=\"footnote7back\"></a><sup><a href=\"#footnote7\">7</a></sup></p>\n<p><em>Visible Learning: A synthesis of over 800 meta-analyses relating to achievement</em> by John Hattie (2009) ranks teaching strategies that emphasize metacognition very highly. Teaching of self-verbalization/self-questioning and general metacognitive strategies respectively occupy ranks #18 and #13 out of 138 meta-analysis subjects in terms of effect size.<a name=\"footnote8back\"></a><sup><a href=\"#footnote8\">8</a></sup></p>\n<p>One of the meta-analyses discussed finds teaching \"awareness of textual inconsistency\" together with \"the use of self-questioning as both a monitoring and a regulating strategy\" to be particularly effective. \"Reinforcement\" stood out as a particularly effective instructional approach, and combining many instructional strategies generally gave better results.<a name=\"footnote9back\"></a><sup><a href=\"#footnote9\">9</a></sup></p>\n<p>So metacognitive self-monitoring can be usefully taught. In studies these skills are generally scaffolded with prompts and checklists that accompany unrelated learning activities. It's not clear to me how well the skills thus taught can be expected to transfer beyond the immediate learning environment. I've also found little effort to explicitly relate these results to either decision science or therapy. Still, it seems encouraging for someone trying to teach rationality-oriented metacognition.</p>\n<p>My impression is that CFAR's approach is informed to some extent by all of the above research fields, although their current focus is (rightly, I suspect) on the mechanics of personal effectiveness/strategy/goal-directedness/agency.</p>\n<p>&nbsp;</p>\n<p>The above considerations led to my trying a simple self-experiment, in which I counted instances of noticing confusion over the course of a month. I feel it's been successful, and report on it <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/\">here</a>.</p>\n<hr />\n<p><strong>Notes</strong></p>\n<p><a name=\"footnote1\"></a>1.&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick (2004)</a>.&nbsp;<a href=\"#footnote1back\">\u21a9</a></p>\n<p><a name=\"footnote2\"></a>2. See also the CFAR \"Skill of the Week\" contests (<a href=\"/lw/b4f/sotw_check_consequentialism/\">check consequentialism</a>,&nbsp;<a href=\"/lw/bc3/sotw_be_specific/\">be specific</a>,&nbsp;<a href=\"/lw/bnk/sotw_avoid_motivated_cognition/\">avoid motivated cognition</a>), which search for exercises which give participants the opportunity to practice those skills, as well as more examples in&nbsp;<a href=\"/lw/eox/from_first_principles/\">From First Principles</a>,&nbsp;<a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">Value of Information</a>, and&nbsp;the&nbsp;<a href=\"/lw/5kz/the_5second_level/#comments\">comments</a>&nbsp;to The 5-Second Level.&nbsp;<a href=\"#footnote2back\">\u21a9</a></p>\n<p><a name=\"footnote3\"></a>3. Of course, there's also the question of what you actually do after the noticing; this is important if you want to have a concrete skill that you can practice and reinforce&mdash;the routine in cue-routine-reward. I almost want to say you can approximate any automatically-triggered routine to first order as interrupting your train of thought and beginning deliberate reflection on what you noticed. On the other hand, maybe it's better to have the corrective action itself be automatic. A related question comes up in Hertel and Mathews (2011) as a potential research direction for cognitive bias modification: \"In short, an important direction for bias-modification research is the exploration of the extent to which training experiences establish new habits or provide the basis for controlled recollection, as well as the contexts under which automatic or controlled processes dominate in producing good transfer.\"&nbsp;<a href=\"#footnote3back\">\u21a9</a></p>\n<p><a name=\"footnote4\"></a>4. reviewed in&nbsp;<a href=\"http://digitalcommons.trinity.edu/cgi/viewcontent.cgi?article=1013&amp;context=psych_faculty\">Hertel and Mathews (2011)</a>&nbsp;<a href=\"#footnote4back\">\u21a9</a></p>\n<p><a name=\"footnote5\"></a>5. As it turns out, there's also a thing literally called metacognitive therapy (MCT), which emphasizes awareness of and response to the nature of thoughts and beliefs rather than the content of those thoughts and beliefs. It has its own&nbsp;<a href=\"http://www.mct-institute.com/metacognitive-research-and-publications.html\">body of theory and practice</a>, which I haven't particularly looked into. It's nearly all led by the same person, which worries me a little. Elsewhere, some people claim to get introspective and metacognitive boosts from practicing mindfulness meditation. That sounds plausible to me, but I haven't done my homework on the subject, so it gets only a shared footnote. While I'm at it, there's also Eugene Gendlin's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Focusing\">Focusing</a>, which I vaguely recall seeing mentioned in a CFAR context before&mdash;maybe in a rationality diary?&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><a name=\"footnote6\"></a>6. See e.g. <a href=\"http://whaaales.com/Schraw1998.pdf\">Schraw (1998)</a>.&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>\n<p><a name=\"footnote7\"></a>7. Griffin et al. (2013).&nbsp;<a href=\"#footnote7back\">\u21a9</a></p>\n<p><a name=\"footnote8\"></a>8. The effect sizes are d = 0.64 and d = 0.69, synthesized from 3 and 2 meta-analyses, respectively. For comparison, the #1 \"effect size\" is students' self-reported past grades, with d = 1.44. The local favorite of spaced practice is #12 at d = 0.71. The author explains effect size thus: \"An effect size of d = 1.0 indicates an increase of one standard deviation on the outcome&mdash;in this case the outcome is improving school achievement. A one standard deviation increase is typically associated with advancing children's achievement by two to three years, improving the rate of learning by 50%, or a correlation between some variable (e.g., amount of homework) and achievement of approximately r = 0.50.\" Such an effect, he points out, would be blatantly obvious. In the other direction, seemingly nearly every intervention you can try has a positive effect size, with an average of d = 0.40 among those studied.&nbsp;<a href=\"#footnote8back\">\u21a9</a></p>\n<p><a name=\"footnote9\"></a>9. Haller et al. (1988). The authors unfortunately don't define their terminology, but instructional reinforcement in this context means giving praise, gold stars, and so on for the desired behavior.&nbsp;<a href=\"#footnote9back\">\u21a9</a></p>\n<p><strong>References</strong></p>\n<p>Griffin, Wiley, &amp; Salas (2013). Supporting effective self-regulated learning: The critical role of monitoring. <em>International Handbook of Metacognition and Learning Technologies</em>, 19-34.</p>\n<p>Haller, Child, &amp; Walberg (1988). Can comprehension be taught? A quantitative synthesis of \"metacognitive\" studies. <em>Educational Researcher, 17</em>(9)<em>,</em> 5-8.</p>\n<p>Hattie (2009). <em>Visible Learning: A synthesis of over 800 meta-analyses relating to achievement. </em>Routledge.</p>\n<p>Hertel &amp; Mathews (2011). <a href=\"http://digitalcommons.trinity.edu/cgi/viewcontent.cgi?article=1013&amp;context=psych_faculty\">Cognitive bias modification: Past perspectives, current findings, and future applications</a>. <em>Perspectives on Psychological Science, 6, 521-536.</em></p>\n<p>Larrick (2004). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Debiasing</a>. <em>Blackwell Handbook of Judgment and Decision Making</em>,<em>&nbsp;</em>316-337.</p>\n<p>Leddy, Anderson, &amp; Schulkin (2013). <a href=\"http://whaaales.com/LeddyAndersonSchulkin2013.pdf\">Cognitive-behavioral therapy and decision science</a>. <em>New Ideas in Psychology, 31,</em>&nbsp;173-183.</p>\n<p>Schraw (1998). <a href=\"http://whaaales.com/Schraw1998.pdf\">Promoting general metacognitive awareness</a>. <em>Instructional Science, 26, </em>113-125<em>.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6v2FHy8dtyCYg9Kz4": 2, "Ng8Gice9KNkncxqcj": 2, "ux2x9RrJsuykQxT79": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vENLgXhMoecM4jmiZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 29, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "25539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>What can existing research tell us about teaching and learning key rationalist skills? One central idea is the monitoring and adjustment of mental processes, which appears in various forms in the LW community, debiasing research, education research concerning metacognition, and cognitive-behavioral and related therapies. I expect that rationality training can benefit from fully realizing the connections between these fields, which some mainstream research is already beginning to do. In another post, I discuss <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/\">a self-experiment in noticing confusion</a> based on these ideas.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Debiasing\">Debiasing</h2>\n<p>In <a href=\"/lw/76x/is_rationality_teachable/\">Is Rationality Teachable?</a>, Luke noted some promising successes in teaching statistics and logic, and also in the growing body of research on debiasing. The main lines of research consider debiasing techniques that are socially-administered (incentives, accountability, group decision making) or imposed \"top-down\" as rules (<a href=\"/lw/7ep/practical_debiasing/\">\"consider the opposite\"</a>). While individuals can internalize and use the latter techniques on their own, the top-down and domain-general nature inhibits transfer (their conversion from declarative to tacit knowledge, or migration from System 2 to System 1).<a name=\"footnote1back\"></a><sup><a href=\"#footnote1\">1</a></sup></p>\n<p>A better debiasing technique from the individual perspective would teach someone to notice the feeling of distorted cognition and trigger corrective action: to notice mental flinching, rationalization, or confusion, and then to bring it to full conscious attention and act on it appropriately. I currently see that as the most important component of what Eliezer's getting at with <a href=\"/lw/5kz/the_5second_level/\">The 5-Second Level</a>.<a name=\"footnote2back\"></a><a name=\"footnote3back\"></a><sup><a href=\"#footnote2\">2</a>&nbsp;<a href=\"#footnote3\">3</a></sup></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick (2004)</a> anticipated this idea in his speculation on future directions for debiasing research:</p>\n<blockquote>\n<p>[One direction] comes from the growing focus on how affect, motivation, and self-esteem in\ufb02uence decision making (see Larrick, 1993; Payne &amp; Bettman, Chapter 6, this volume; Rottenstreich &amp; Shu, Chapter 22, this volume). Identifying debiasing techniques for affect-based biases is a promising new area \u2013 What interventions help people cope effectively with emotion endogenous to a decision, such as anticipated regret? Or help them recognize and discount emotion that is extraneous to a decision, such as anger from some unrelated experience? <strong>The answers may bring decision research surprisingly close to clinical psychology, such as techniques used in cognitive-behavioral therapy</strong> [<em>emphasis mine</em>].</p>\n</blockquote>\n<h2 id=\"Cognitive_Behavioral_Therapy\">Cognitive Behavioral Therapy</h2>\n<p>How has that speculation fared? It seems that others have made the same connection, but that progress has been limited.&nbsp;<a href=\"http://whaaales.com/LeddyAndersonSchulkin2013.pdf\">Leddy et al. (2013)</a> provide an overview of decision science (DS) and cognitive behavioral therapy (CBT), and finally reflect on the similarities:</p>\n<blockquote>\n<p>The cognitive patterns that DS and CBT have identified and labeled [\"biases\" and \"cognitive distortions,\" usually] are their respective building blocks; however, there are numerous similarities between the patterns each field has identified, despite having arrived at different terminology...</p>\n<p>A difference in the fields of CBT and decision making is in the context, the research and application. CBT is an applied field that incorporates the client's environment. CBT encourages the client to self-monitor, challenge, and modify their thoughts in their own environment, with the goal being for clients to generalize skills learned in the therapy room to real-life contexts. However, a majority of the work in decision science has been conducted in isolation from the context in which real-life decisions take place. While more work is being done in medical, legal, and other fields, there are still limitations to this research because numerous immeasurable variables are difficult to control for in applied research settings (e.g., previous experience, social influences). It is also unclear whether decisions made or skills used in the research setting would be utilized in the real-world.</p>\n<p>Based on the parallels between biases and cognitive distortions, we suggest that CBT practitioners keep in mind various cognitive biases to supplement discussions of cognitive distortions... Conversely, utilizing the techniques of cognitive-behavioral therapy in decision science may be beneficial; the implementation of the cognitive-behavioral techniques of goal setting, cognitive-behavioral assessment, self-monitoring, cognitive restructuring, exposure, behavioral experiments and relapse prevention may improve decision outcome.</p>\n</blockquote>\n<p>The authors then point out research on \"cognitive bias modification,\"<a name=\"footnote4back\"></a><sup><a href=\"#footnote4\">4</a></sup> which looks to be pointed in a good direction concerning the teaching and learning of mental habits to counter cognitive biases, as well as some other work in clinical psychology with a decision-science perspective. \"Self-monitoring\" does come up in the above connection between CBT and decision science, but the authors don't particularly seize on the metacognitive aspect that seems to be the key commonality to me.<a name=\"footnote5back\"></a><sup><a href=\"#footnote5\">5</a></sup></p>\n<p>I should perhaps note that CBT, while vastly better than nothing at all, is <a href=\"http://slatestarcodex.com/2013/09/19/scientific-freud/\">not necessarily the uniquely evidence-based therapy</a> that many of us came to know it as. I mention this because it is at least slight evidence against the mechanism of action lying in patients' successfully learning specific mental skills, and in favor of (among other hypotheses) the effectiveness of simply talking to someone charismatic and empathetic. It could also be that different things work for different people.</p>\n<p>What should we conclude? Perhaps that rationality training could end up looking in some ways more like therapy than like martial arts. It may be worth adapting therapeutic models specifically to debiasing.</p>\n<h2 id=\"Metacognition\">Metacognition</h2>\n<p>Finally, the education research community seems to have their own body of work on similar concepts under different names. <a href=\"http://whaaales.com/book-review-how-learning-works/#Metacognition\">Metacognition</a>&nbsp;plays a key role in learning, and it can be effectively taught.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup> The term includes a number of important skills and processes, most of which we aren't interested in right now: declarative knowledge about study tactics, \"offline\" planning, retrospective evaluation of performance. These are roughly analogous with \"top-down\" techinques for debiasing.</p>\n<p>But some metacognition research is concerned with real-time or \"online\" self-monitoring\u2014that is, the \"metacognitive experiences\" associated with ongoing cognitive processing\u2014very nearly what we're looking for. In general it's difficult to address these processes directly, but they can still be indirectly&nbsp;<a href=\"http://whaaales.com/book-review-how-learning-works/#Mastery\">scaffolded</a>, engaged, and measured. For example, activities that require examining mental models (as opposed to tapping short-term memory alone) have been found effective in promoting calibration of confidence in understanding (\"metacomprehension accuracy,\" which presumably relies on metacognitive experiences). One such activity is generative testing, which includes summarizing learned material after some time has passed. Another is self-explanation/self-questioning during reading, where you progressively construct and test your model by asking yourself how ideas fit with each other.<a name=\"footnote7back\"></a><sup><a href=\"#footnote7\">7</a></sup></p>\n<p><em>Visible Learning: A synthesis of over 800 meta-analyses relating to achievement</em> by John Hattie (2009) ranks teaching strategies that emphasize metacognition very highly. Teaching of self-verbalization/self-questioning and general metacognitive strategies respectively occupy ranks #18 and #13 out of 138 meta-analysis subjects in terms of effect size.<a name=\"footnote8back\"></a><sup><a href=\"#footnote8\">8</a></sup></p>\n<p>One of the meta-analyses discussed finds teaching \"awareness of textual inconsistency\" together with \"the use of self-questioning as both a monitoring and a regulating strategy\" to be particularly effective. \"Reinforcement\" stood out as a particularly effective instructional approach, and combining many instructional strategies generally gave better results.<a name=\"footnote9back\"></a><sup><a href=\"#footnote9\">9</a></sup></p>\n<p>So metacognitive self-monitoring can be usefully taught. In studies these skills are generally scaffolded with prompts and checklists that accompany unrelated learning activities. It's not clear to me how well the skills thus taught can be expected to transfer beyond the immediate learning environment. I've also found little effort to explicitly relate these results to either decision science or therapy. Still, it seems encouraging for someone trying to teach rationality-oriented metacognition.</p>\n<p>My impression is that CFAR's approach is informed to some extent by all of the above research fields, although their current focus is (rightly, I suspect) on the mechanics of personal effectiveness/strategy/goal-directedness/agency.</p>\n<p>&nbsp;</p>\n<p>The above considerations led to my trying a simple self-experiment, in which I counted instances of noticing confusion over the course of a month. I feel it's been successful, and report on it <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/\">here</a>.</p>\n<hr>\n<p><strong id=\"Notes\">Notes</strong></p>\n<p><a name=\"footnote1\"></a>1.&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick (2004)</a>.&nbsp;<a href=\"#footnote1back\">\u21a9</a></p>\n<p><a name=\"footnote2\"></a>2. See also the CFAR \"Skill of the Week\" contests (<a href=\"/lw/b4f/sotw_check_consequentialism/\">check consequentialism</a>,&nbsp;<a href=\"/lw/bc3/sotw_be_specific/\">be specific</a>,&nbsp;<a href=\"/lw/bnk/sotw_avoid_motivated_cognition/\">avoid motivated cognition</a>), which search for exercises which give participants the opportunity to practice those skills, as well as more examples in&nbsp;<a href=\"/lw/eox/from_first_principles/\">From First Principles</a>,&nbsp;<a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">Value of Information</a>, and&nbsp;the&nbsp;<a href=\"/lw/5kz/the_5second_level/#comments\">comments</a>&nbsp;to The 5-Second Level.&nbsp;<a href=\"#footnote2back\">\u21a9</a></p>\n<p><a name=\"footnote3\"></a>3. Of course, there's also the question of what you actually do after the noticing; this is important if you want to have a concrete skill that you can practice and reinforce\u2014the routine in cue-routine-reward. I almost want to say you can approximate any automatically-triggered routine to first order as interrupting your train of thought and beginning deliberate reflection on what you noticed. On the other hand, maybe it's better to have the corrective action itself be automatic. A related question comes up in Hertel and Mathews (2011) as a potential research direction for cognitive bias modification: \"In short, an important direction for bias-modification research is the exploration of the extent to which training experiences establish new habits or provide the basis for controlled recollection, as well as the contexts under which automatic or controlled processes dominate in producing good transfer.\"&nbsp;<a href=\"#footnote3back\">\u21a9</a></p>\n<p><a name=\"footnote4\"></a>4. reviewed in&nbsp;<a href=\"http://digitalcommons.trinity.edu/cgi/viewcontent.cgi?article=1013&amp;context=psych_faculty\">Hertel and Mathews (2011)</a>&nbsp;<a href=\"#footnote4back\">\u21a9</a></p>\n<p><a name=\"footnote5\"></a>5. As it turns out, there's also a thing literally called metacognitive therapy (MCT), which emphasizes awareness of and response to the nature of thoughts and beliefs rather than the content of those thoughts and beliefs. It has its own&nbsp;<a href=\"http://www.mct-institute.com/metacognitive-research-and-publications.html\">body of theory and practice</a>, which I haven't particularly looked into. It's nearly all led by the same person, which worries me a little. Elsewhere, some people claim to get introspective and metacognitive boosts from practicing mindfulness meditation. That sounds plausible to me, but I haven't done my homework on the subject, so it gets only a shared footnote. While I'm at it, there's also Eugene Gendlin's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Focusing\">Focusing</a>, which I vaguely recall seeing mentioned in a CFAR context before\u2014maybe in a rationality diary?&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><a name=\"footnote6\"></a>6. See e.g. <a href=\"http://whaaales.com/Schraw1998.pdf\">Schraw (1998)</a>.&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>\n<p><a name=\"footnote7\"></a>7. Griffin et al. (2013).&nbsp;<a href=\"#footnote7back\">\u21a9</a></p>\n<p><a name=\"footnote8\"></a>8. The effect sizes are d = 0.64 and d = 0.69, synthesized from 3 and 2 meta-analyses, respectively. For comparison, the #1 \"effect size\" is students' self-reported past grades, with d = 1.44. The local favorite of spaced practice is #12 at d = 0.71. The author explains effect size thus: \"An effect size of d = 1.0 indicates an increase of one standard deviation on the outcome\u2014in this case the outcome is improving school achievement. A one standard deviation increase is typically associated with advancing children's achievement by two to three years, improving the rate of learning by 50%, or a correlation between some variable (e.g., amount of homework) and achievement of approximately r = 0.50.\" Such an effect, he points out, would be blatantly obvious. In the other direction, seemingly nearly every intervention you can try has a positive effect size, with an average of d = 0.40 among those studied.&nbsp;<a href=\"#footnote8back\">\u21a9</a></p>\n<p><a name=\"footnote9\"></a>9. Haller et al. (1988). The authors unfortunately don't define their terminology, but instructional reinforcement in this context means giving praise, gold stars, and so on for the desired behavior.&nbsp;<a href=\"#footnote9back\">\u21a9</a></p>\n<p><strong id=\"References\">References</strong></p>\n<p>Griffin, Wiley, &amp; Salas (2013). Supporting effective self-regulated learning: The critical role of monitoring. <em>International Handbook of Metacognition and Learning Technologies</em>, 19-34.</p>\n<p>Haller, Child, &amp; Walberg (1988). Can comprehension be taught? A quantitative synthesis of \"metacognitive\" studies. <em>Educational Researcher, 17</em>(9)<em>,</em> 5-8.</p>\n<p>Hattie (2009). <em>Visible Learning: A synthesis of over 800 meta-analyses relating to achievement. </em>Routledge.</p>\n<p>Hertel &amp; Mathews (2011). <a href=\"http://digitalcommons.trinity.edu/cgi/viewcontent.cgi?article=1013&amp;context=psych_faculty\">Cognitive bias modification: Past perspectives, current findings, and future applications</a>. <em>Perspectives on Psychological Science, 6, 521-536.</em></p>\n<p>Larrick (2004). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Debiasing</a>. <em>Blackwell Handbook of Judgment and Decision Making</em>,<em>&nbsp;</em>316-337.</p>\n<p>Leddy, Anderson, &amp; Schulkin (2013). <a href=\"http://whaaales.com/LeddyAndersonSchulkin2013.pdf\">Cognitive-behavioral therapy and decision science</a>. <em>New Ideas in Psychology, 31,</em>&nbsp;173-183.</p>\n<p>Schraw (1998). <a href=\"http://whaaales.com/Schraw1998.pdf\">Promoting general metacognitive awareness</a>. <em>Instructional Science, 26, </em>113-125<em>.</em></p>", "sections": [{"title": "Debiasing", "anchor": "Debiasing", "level": 1}, {"title": "Cognitive Behavioral Therapy", "anchor": "Cognitive_Behavioral_Therapy", "level": 1}, {"title": "Metacognition", "anchor": "Metacognition", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"title": "References", "anchor": "References", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mjiu8n9qyoqfY7LkF", "H2zKAfiSJR6WJQ8pn", "rrjCeQLopeHXicAZ6", "JcpzFpPBSmzuksmWM", "xypbWhzEEw4ZsRK9i", "NgtYDP3ZtLJaM248W", "thfnus52hgHA32XfQ", "W6oJz6eiQ3q5t84Z3", "xDiqYyqeqPo92PojS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T12:57:24.600Z", "modifiedAt": null, "url": null, "title": "Open Thread for February 18-24 2014", "slug": "open-thread-for-february-18-24-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:06.804Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LQNDcWDxgPezMKtZY/open-thread-for-february-18-24-2014", "pageUrlRelative": "/posts/LQNDcWDxgPezMKtZY/open-thread-for-february-18-24-2014", "linkUrl": "https://www.lesswrong.com/posts/LQNDcWDxgPezMKtZY/open-thread-for-february-18-24-2014", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20for%20February%2018-24%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20for%20February%2018-24%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQNDcWDxgPezMKtZY%2Fopen-thread-for-february-18-24-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20for%20February%2018-24%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQNDcWDxgPezMKtZY%2Fopen-thread-for-february-18-24-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQNDcWDxgPezMKtZY%2Fopen-thread-for-february-18-24-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LQNDcWDxgPezMKtZY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.5718685740883397e-06, "legacy": true, "legacyId": "25543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 459, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T17:27:04.303Z", "modifiedAt": null, "url": null, "title": "[link] Nick Beckstead on improving disaster shelters to increase the chances of recovery from a global catastrophe", "slug": "link-nick-beckstead-on-improving-disaster-shelters-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:48.550Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kp5P5Sjn3nMAEaePQ/link-nick-beckstead-on-improving-disaster-shelters-to", "pageUrlRelative": "/posts/Kp5P5Sjn3nMAEaePQ/link-nick-beckstead-on-improving-disaster-shelters-to", "linkUrl": "https://www.lesswrong.com/posts/Kp5P5Sjn3nMAEaePQ/link-nick-beckstead-on-improving-disaster-shelters-to", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Nick%20Beckstead%20on%20improving%20disaster%20shelters%20to%20increase%20the%20chances%20of%20recovery%20from%20a%20global%20catastrophe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Nick%20Beckstead%20on%20improving%20disaster%20shelters%20to%20increase%20the%20chances%20of%20recovery%20from%20a%20global%20catastrophe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKp5P5Sjn3nMAEaePQ%2Flink-nick-beckstead-on-improving-disaster-shelters-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Nick%20Beckstead%20on%20improving%20disaster%20shelters%20to%20increase%20the%20chances%20of%20recovery%20from%20a%20global%20catastrophe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKp5P5Sjn3nMAEaePQ%2Flink-nick-beckstead-on-improving-disaster-shelters-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKp5P5Sjn3nMAEaePQ%2Flink-nick-beckstead-on-improving-disaster-shelters-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p>Nick Beckstead just published a <a href=\"http://www.effective-altruism.com/improving-disaster-shelters-to-increase-the-chances-of-recovery-from-a-global-catastrophe/\">post</a> on disaster shelters over at the <a href=\"http://effective-altruism.com/\">Effective Altruism Blog</a>. Summary:</p>\n<blockquote>\n<p><strong>What is the problem?</strong> Civilization might not recover from some possible global catastrophes. Conceivably, people with access to disaster shelters or other refuges may be more likely to survive and help civilization recover. However, existing disaster shelters (sometimes built to ensure continuity of government operations and sometimes built to protect individuals), people working on submarines, largely uncontacted peoples, and people living in very remote locations may serve this function to some extent.</p>\n<p><strong>What are the possible interventions?</strong> Other interventions may also increase the chances that humanity would recover from a global catastrophe, but this review focuses on disaster shelters. Proposed methods of improving disaster shelter networks include stocking shelters with appropriately trained people and resources that would enable them to rebuild civilization in case of a near-extinction event, keeping some shelters constantly full of people, increasing food reserves, and building more shelters. A philanthropist could pay to improve existing shelter networks in the above ways, or they could advocate for private shelter builders or governments to make some of the improvements listed above.</p>\n<p><strong>Who else is working on it?</strong> Some governments maintain bunkers in order to maintain continuity of government and/or to protect their citizens. Some individuals purchase and maintain private disaster shelters.</p>\n<p><strong>Questions for further investigation</strong>: With the possible exception of pandemic specifically engineered to kill all humans, I am aware of no scenario in which improved disaster shelters would plausibly enable a small group of people to survive a sudden near-extinction event. In the case of other catastrophes where a much larger number of people would survive, I would guess that improved refuges would play a relatively small role in helping humanity to recover because they would represent a small share of relevant people and resources. Many challenging questions about improving refuges remain, but I would prioritize investigating other issues at this point because refuges seem likely to be of limited value and alternative strategies (such as improving biosecurity and increasing the resilience of industrial and agricultural systems) seem more likely to effectively reduce the global catastrophic risks that improving refuges might plausibly address.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kp5P5Sjn3nMAEaePQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.5721839361640541e-06, "legacy": true, "legacyId": "25548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T18:31:21.749Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Different Reports", "slug": "meetup-moscow-different-reports", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wis6BftCEjoTFwkQJ/meetup-moscow-different-reports", "pageUrlRelative": "/posts/Wis6BftCEjoTFwkQJ/meetup-moscow-different-reports", "linkUrl": "https://www.lesswrong.com/posts/Wis6BftCEjoTFwkQJ/meetup-moscow-different-reports", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Different%20Reports&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Different%20Reports%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWis6BftCEjoTFwkQJ%2Fmeetup-moscow-different-reports%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Different%20Reports%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWis6BftCEjoTFwkQJ%2Fmeetup-moscow-different-reports", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWis6BftCEjoTFwkQJ%2Fmeetup-moscow-different-reports", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/wz'>Moscow, Different Reports</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 February 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to a room inside the building at 16:00. So please do not be late. We will have:</p>\n\n<ul>\n<li><p>Report about genetics and epigenetics.</p></li>\n<li><p>Report about \"Decisive: How to Make Better Choices in Life and Work\" book.</p></li>\n<li><p>Stumbling on happiness for rationalists presentation.</p></li>\n<li><p>Something about Cognitive behavioural therapy, possibly a report.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and through the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/wz'>Moscow, Different Reports</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wis6BftCEjoTFwkQJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5722591389857608e-06, "legacy": true, "legacyId": "25549", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Different_Reports\">Discussion article for the meetup : <a href=\"/meetups/wz\">Moscow, Different Reports</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 February 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to a room inside the building at 16:00. So please do not be late. We will have:</p>\n\n<ul>\n<li><p>Report about genetics and epigenetics.</p></li>\n<li><p>Report about \"Decisive: How to Make Better Choices in Life and Work\" book.</p></li>\n<li><p>Stumbling on happiness for rationalists presentation.</p></li>\n<li><p>Something about Cognitive behavioural therapy, possibly a report.</p></li>\n</ul>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and through the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Different_Reports1\">Discussion article for the meetup : <a href=\"/meetups/wz\">Moscow, Different Reports</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Different Reports", "anchor": "Discussion_article_for_the_meetup___Moscow__Different_Reports", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Different Reports", "anchor": "Discussion_article_for_the_meetup___Moscow__Different_Reports1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T18:48:11.800Z", "modifiedAt": null, "url": null, "title": "Managing your time spent learning", "slug": "managing-your-time-spent-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:58.894Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dpn7BSMzLaeiE386S/managing-your-time-spent-learning", "pageUrlRelative": "/posts/Dpn7BSMzLaeiE386S/managing-your-time-spent-learning", "linkUrl": "https://www.lesswrong.com/posts/Dpn7BSMzLaeiE386S/managing-your-time-spent-learning", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Managing%20your%20time%20spent%20learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AManaging%20your%20time%20spent%20learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpn7BSMzLaeiE386S%2Fmanaging-your-time-spent-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Managing%20your%20time%20spent%20learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpn7BSMzLaeiE386S%2Fmanaging-your-time-spent-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpn7BSMzLaeiE386S%2Fmanaging-your-time-spent-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 742, "htmlBody": "<p><em>This article is written for people who are looking for advice on prioritizing activities, in particular, what to spend time learning.</em></p>\n<p>In thinking about how to budget your time, it's helpful to explicitly prioritize the activities that you engage in in terms of their relative importance, and distinguish between what's important and what you find interesting. Sometimes we exaggerate the usefulness of interesting but only slightly useful activities in their minds, on account of wanting to believe that time spent on them is productive. If you think about how useful an activity is and, how interesting the activity is separately, you're less likely to do this. It's helpful to consider the following four categories of activities:</p>\n<ul>\n<li>Important and interesting:&nbsp;Do, and take your time. Get it right!</li>\n<li>Important and not interesting: Do as much as necessary, and maybe a bit more; look into ways of overcoming procrastination. Also consider ways to make them more interesting.</li>\n<li>Not important and interesting: Do only if you feel like it, don't try to press yourself, and consider substituting with activities that are interesting and important.</li>\n<li>Not important and not interesting:&nbsp;Avoid.</li>\n</ul>\n<p>More below</p>\n<p><a id=\"more\"></a></p>\n<h2>Interesting and important</h2>\n<p>When you find an academic subject interesting, and when it's important (e.g. for your future job, as a prerequisite to courses that you'll take in the future or otherwise related to your future goals), you should delve deeply into it. Gaining deep understanding takes time, and you shouldn't feel as though you're working inefficiently if you find yourself spending a disproportionate amount of time on it.</p>\n<h2>Interesting but not important</h2>\n<p>Intellectually curious people often have intellectual interests that don't advance their career goals. Such interests can absorb a lot of time and hinder one's professional success.</p>\n<p>The question of how to balance these interests with one's career goals is a very personal one.</p>\n<p>In general, if there are two activities that are of comparable interest to you, but of unequal importance, you should choose the more important one.</p>\n<p>If you find that your time is uncomfortably crowded with things that are interesting but not important, you should look for instances where you're exerting willpower on them, and cut back on those, reserving the time that you spend doing things that you find interesting to activities that require relatively little energy, to conserve energy for doing things that it's more difficult to get yourself to do.</p>\n<h2>Important but not interesting</h2>\n<p>Sometimes you have to do things that are uninteresting to achieve your goals. If you have trouble motivating yourself to do these things, you might benefit from our recommendations for overcoming procrastination (forthcoming). Also, consider ways that you might find these specific activities more interesting, by checking out targeted learning recommendations for those activities.</p>\n<h2>Not important and not interesting</h2>\n<p>These activities should be avoided. This point might seem obvious, but despite this, people often do engage in activities that are neither important nor interesting. This most often happens when:</p>\n<ul>\n<li>One hasn't carefully considered the question of whether the activity is important. For example, one might uncritically internalize the view that it's important to learn a language because learning a language said to keep the mind sharp, without considering that there might be other more interesting or important activities that keep the mind sharp to an equal or greater extent, and therefore try to learn a language, even if one doesn't find it interesting. There are benefits to learning a language that one can't get from other activities: the point here is just that keeping one's mind sharp <em>specifically</em> isn't a good reason to learn a language rather than do other things.&nbsp;</li>\n<li>The activity seems interesting at first, and one sets a goal connected with it, but then the activity turns out not to be interesting, and one feels an obligation to continue on account of having already set the goal. For example, one might hear good things about a long novel and set a goal of reading it, find that one doesn't enjoy it, and feel pressure to plow through to the end.&nbsp;</li>\n</ul>\n<p>On the first point, it's important to critically reflect on whether the activities that one is involved in are important. On the second point, all else being equal, fulfilling commitments is good, because it's good for one's self-esteem, but one should still consider whether the cost of fulfilling the commitment is worth it, and also try not to set ambitious goals when the value of fulfilling them is questionable.</p>\n<p><em><a href=\"http://cognitomentoring.quora.com/Prioritizing-activities\">Cross-posted</a> from&nbsp;<a href=\"/cognitomentoring.org\">Cognito Mentoring's</a>&nbsp;Quora blog</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dpn7BSMzLaeiE386S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "25550", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This article is written for people who are looking for advice on prioritizing activities, in particular, what to spend time learning.</em></p>\n<p>In thinking about how to budget your time, it's helpful to explicitly prioritize the activities that you engage in in terms of their relative importance, and distinguish between what's important and what you find interesting. Sometimes we exaggerate the usefulness of interesting but only slightly useful activities in their minds, on account of wanting to believe that time spent on them is productive. If you think about how useful an activity is and, how interesting the activity is separately, you're less likely to do this. It's helpful to consider the following four categories of activities:</p>\n<ul>\n<li>Important and interesting:&nbsp;Do, and take your time. Get it right!</li>\n<li>Important and not interesting: Do as much as necessary, and maybe a bit more; look into ways of overcoming procrastination. Also consider ways to make them more interesting.</li>\n<li>Not important and interesting: Do only if you feel like it, don't try to press yourself, and consider substituting with activities that are interesting and important.</li>\n<li>Not important and not interesting:&nbsp;Avoid.</li>\n</ul>\n<p>More below</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Interesting_and_important\">Interesting and important</h2>\n<p>When you find an academic subject interesting, and when it's important (e.g. for your future job, as a prerequisite to courses that you'll take in the future or otherwise related to your future goals), you should delve deeply into it. Gaining deep understanding takes time, and you shouldn't feel as though you're working inefficiently if you find yourself spending a disproportionate amount of time on it.</p>\n<h2 id=\"Interesting_but_not_important\">Interesting but not important</h2>\n<p>Intellectually curious people often have intellectual interests that don't advance their career goals. Such interests can absorb a lot of time and hinder one's professional success.</p>\n<p>The question of how to balance these interests with one's career goals is a very personal one.</p>\n<p>In general, if there are two activities that are of comparable interest to you, but of unequal importance, you should choose the more important one.</p>\n<p>If you find that your time is uncomfortably crowded with things that are interesting but not important, you should look for instances where you're exerting willpower on them, and cut back on those, reserving the time that you spend doing things that you find interesting to activities that require relatively little energy, to conserve energy for doing things that it's more difficult to get yourself to do.</p>\n<h2 id=\"Important_but_not_interesting\">Important but not interesting</h2>\n<p>Sometimes you have to do things that are uninteresting to achieve your goals. If you have trouble motivating yourself to do these things, you might benefit from our recommendations for overcoming procrastination (forthcoming). Also, consider ways that you might find these specific activities more interesting, by checking out targeted learning recommendations for those activities.</p>\n<h2 id=\"Not_important_and_not_interesting\">Not important and not interesting</h2>\n<p>These activities should be avoided. This point might seem obvious, but despite this, people often do engage in activities that are neither important nor interesting. This most often happens when:</p>\n<ul>\n<li>One hasn't carefully considered the question of whether the activity is important. For example, one might uncritically internalize the view that it's important to learn a language because learning a language said to keep the mind sharp, without considering that there might be other more interesting or important activities that keep the mind sharp to an equal or greater extent, and therefore try to learn a language, even if one doesn't find it interesting. There are benefits to learning a language that one can't get from other activities: the point here is just that keeping one's mind sharp <em>specifically</em> isn't a good reason to learn a language rather than do other things.&nbsp;</li>\n<li>The activity seems interesting at first, and one sets a goal connected with it, but then the activity turns out not to be interesting, and one feels an obligation to continue on account of having already set the goal. For example, one might hear good things about a long novel and set a goal of reading it, find that one doesn't enjoy it, and feel pressure to plow through to the end.&nbsp;</li>\n</ul>\n<p>On the first point, it's important to critically reflect on whether the activities that one is involved in are important. On the second point, all else being equal, fulfilling commitments is good, because it's good for one's self-esteem, but one should still consider whether the cost of fulfilling the commitment is worth it, and also try not to set ambitious goals when the value of fulfilling them is questionable.</p>\n<p><em><a href=\"http://cognitomentoring.quora.com/Prioritizing-activities\">Cross-posted</a> from&nbsp;<a href=\"/cognitomentoring.org\">Cognito Mentoring's</a>&nbsp;Quora blog</em></p>", "sections": [{"title": "Interesting and important", "anchor": "Interesting_and_important", "level": 1}, {"title": "Interesting but not important", "anchor": "Interesting_but_not_important", "level": 1}, {"title": "Important but not interesting", "anchor": "Important_but_not_interesting", "level": 1}, {"title": "Not important and not interesting", "anchor": "Not_important_and_not_interesting", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T20:16:30.017Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: Ask vs. Guess (vs. Tell) Culture", "slug": "meetup-berkeley-ask-vs-guess-vs-tell-culture", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zLSF27x98Sb4rnR73/meetup-berkeley-ask-vs-guess-vs-tell-culture", "pageUrlRelative": "/posts/zLSF27x98Sb4rnR73/meetup-berkeley-ask-vs-guess-vs-tell-culture", "linkUrl": "https://www.lesswrong.com/posts/zLSF27x98Sb4rnR73/meetup-berkeley-ask-vs-guess-vs-tell-culture", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20Ask%20vs.%20Guess%20(vs.%20Tell)%20Culture&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20Ask%20vs.%20Guess%20(vs.%20Tell)%20Culture%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLSF27x98Sb4rnR73%2Fmeetup-berkeley-ask-vs-guess-vs-tell-culture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20Ask%20vs.%20Guess%20(vs.%20Tell)%20Culture%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLSF27x98Sb4rnR73%2Fmeetup-berkeley-ask-vs-guess-vs-tell-culture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLSF27x98Sb4rnR73%2Fmeetup-berkeley-ask-vs-guess-vs-tell-culture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x0'>Berkeley: Ask vs. Guess (vs. Tell) Culture</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all, tonight's meetup will feature the discussion topic of Ask Culture and Guess Culture, which is summarized in the first couple paragraphs of this Less Wrong post:</p>\n\n<p><a href=\"http://lesswrong.com/lw/jis/tell_culture/\" rel=\"nofollow\">http://lesswrong.com/lw/jis/tell_culture/</a></p>\n\n<p>A friend of mine has remarked that this is a topic that causes even professed rationalists to engage in motivated cognition. Are you up to the challenge!?</p>\n\n<p>Please arrive between 7pm and 7:30pm tonight. At 7:30pm we'll review our weekly goals and record goals for the coming week. The discussion of Ask vs. Guess Culture will follow.</p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x0'>Berkeley: Ask vs. Guess (vs. Tell) Culture</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zLSF27x98Sb4rnR73", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5723821354854639e-06, "legacy": true, "legacyId": "25552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Ask_vs__Guess__vs__Tell__Culture\">Discussion article for the meetup : <a href=\"/meetups/x0\">Berkeley: Ask vs. Guess (vs. Tell) Culture</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all, tonight's meetup will feature the discussion topic of Ask Culture and Guess Culture, which is summarized in the first couple paragraphs of this Less Wrong post:</p>\n\n<p><a href=\"http://lesswrong.com/lw/jis/tell_culture/\" rel=\"nofollow\">http://lesswrong.com/lw/jis/tell_culture/</a></p>\n\n<p>A friend of mine has remarked that this is a topic that causes even professed rationalists to engage in motivated cognition. Are you up to the challenge!?</p>\n\n<p>Please arrive between 7pm and 7:30pm tonight. At 7:30pm we'll review our weekly goals and record goals for the coming week. The discussion of Ask vs. Guess Culture will follow.</p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Ask_vs__Guess__vs__Tell__Culture1\">Discussion article for the meetup : <a href=\"/meetups/x0\">Berkeley: Ask vs. Guess (vs. Tell) Culture</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: Ask vs. Guess (vs. Tell) Culture", "anchor": "Discussion_article_for_the_meetup___Berkeley__Ask_vs__Guess__vs__Tell__Culture", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: Ask vs. Guess (vs. Tell) Culture", "anchor": "Discussion_article_for_the_meetup___Berkeley__Ask_vs__Guess__vs__Tell__Culture1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rEBXN3x6kXgD4pLxs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-19T22:30:50.295Z", "modifiedAt": null, "url": null, "title": "Testing my cognition", "slug": "testing-my-cognition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:51.698Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RyanCarey", "createdAt": "2011-04-27T00:19:14.586Z", "isAdmin": false, "displayName": "RyanCarey"}, "userId": "CBkbKSCEzEK2kLQww", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7YnQkANcLfjEs7Sxd/testing-my-cognition", "pageUrlRelative": "/posts/7YnQkANcLfjEs7Sxd/testing-my-cognition", "linkUrl": "https://www.lesswrong.com/posts/7YnQkANcLfjEs7Sxd/testing-my-cognition", "postedAtFormatted": "Wednesday, February 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Testing%20my%20cognition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATesting%20my%20cognition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YnQkANcLfjEs7Sxd%2Ftesting-my-cognition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Testing%20my%20cognition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YnQkANcLfjEs7Sxd%2Ftesting-my-cognition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YnQkANcLfjEs7Sxd%2Ftesting-my-cognition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>Hi all, I'm doing first quantified self experiment. How does this design sound to you?</p>\n<p>&nbsp;</p>\n<p><strong>Metrics</strong></p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>20 min sample of tests from Cambridge brain sciences site: spatial span, double trouble, object reasoning, rotations, hampshire tree task, spatial slider.</p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>10 USMLE Rx questions in 11 mins covering neurology, psychiatry, cognitive sciences and epidemiology, randomised from a pool of 400 questions.</p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>Subjective report of cognitive ability from one to ten.</p>\n<p>These will be taken daily at noon.</p>\n<p>&nbsp;</p>\n<p><strong>Intervention</strong></p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>Take nothing for one week.</p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>Take creatine 5g daily for two weeks&nbsp;</p>\n<p>&bull;<span style=\"white-space: pre;\"> </span>Then take nothing for two weeks</p>\n<p>&nbsp;</p>\n<p>I'm starting with creatine because I'm vegetarian. Then I'll report my findings, re-evaluate value of further experiments and proceed on to some or all of piracetam+choline, Luminosity and dual n-back.</p>\n<p>&nbsp;</p>\n<p>Thoughts on how I can improve this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7YnQkANcLfjEs7Sxd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.5725393169192487e-06, "legacy": true, "legacyId": "25553", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Hi all, I'm doing first quantified self experiment. How does this design sound to you?</p>\n<p>&nbsp;</p>\n<p><strong id=\"Metrics\">Metrics</strong></p>\n<p>\u2022<span style=\"white-space: pre;\"> </span>20 min sample of tests from Cambridge brain sciences site: spatial span, double trouble, object reasoning, rotations, hampshire tree task, spatial slider.</p>\n<p>\u2022<span style=\"white-space: pre;\"> </span>10 USMLE Rx questions in 11 mins covering neurology, psychiatry, cognitive sciences and epidemiology, randomised from a pool of 400 questions.</p>\n<p>\u2022<span style=\"white-space: pre;\"> </span>Subjective report of cognitive ability from one to ten.</p>\n<p>These will be taken daily at noon.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Intervention\">Intervention</strong></p>\n<p>\u2022<span style=\"white-space: pre;\"> </span>Take nothing for one week.</p>\n<p>\u2022<span style=\"white-space: pre;\"> </span>Take creatine 5g daily for two weeks&nbsp;</p>\n<p>\u2022<span style=\"white-space: pre;\"> </span>Then take nothing for two weeks</p>\n<p>&nbsp;</p>\n<p>I'm starting with creatine because I'm vegetarian. Then I'll report my findings, re-evaluate value of further experiments and proceed on to some or all of piracetam+choline, Luminosity and dual n-back.</p>\n<p>&nbsp;</p>\n<p>Thoughts on how I can improve this?</p>", "sections": [{"title": "Metrics", "anchor": "Metrics", "level": 1}, {"title": "Intervention", "anchor": "Intervention", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-20T01:55:12.742Z", "modifiedAt": null, "url": null, "title": "A self-experiment in training \"noticing confusion\"", "slug": "a-self-experiment-in-training-noticing-confusion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.391Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whales", "createdAt": "2014-01-07T06:31:44.661Z", "isAdmin": false, "displayName": "whales"}, "userId": "n7p4iZqwmT3whXoAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mjiu8n9qyoqfY7LkF/a-self-experiment-in-training-noticing-confusion", "pageUrlRelative": "/posts/Mjiu8n9qyoqfY7LkF/a-self-experiment-in-training-noticing-confusion", "linkUrl": "https://www.lesswrong.com/posts/Mjiu8n9qyoqfY7LkF/a-self-experiment-in-training-noticing-confusion", "postedAtFormatted": "Thursday, February 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20self-experiment%20in%20training%20%22noticing%20confusion%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20self-experiment%20in%20training%20%22noticing%20confusion%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjiu8n9qyoqfY7LkF%2Fa-self-experiment-in-training-noticing-confusion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20self-experiment%20in%20training%20%22noticing%20confusion%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjiu8n9qyoqfY7LkF%2Fa-self-experiment-in-training-noticing-confusion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjiu8n9qyoqfY7LkF%2Fa-self-experiment-in-training-noticing-confusion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2140, "htmlBody": "<p>I <a href=\"/lw/jpf/rationality_training_academic_and_clinical/\">previously discussed</a> the potential relevance of therapeutic and instructional models of metacognitive training to LW-style rationality skills. As an attempted concrete realization of what this connection could look like, I ran a self-experiment in which I counted instances of noticing confusion. Below I elaborate on the motivation and design of the experiment, then discuss some quantitative results and qualitative reflections.</p>\n<p><a id=\"more\"></a></p>\n<h2>Background</h2>\n<p>Self-monitoring as a treatment vehicle in cognitive-behavioral and related therapies can take many forms. In one (to my secondhand understanding), the patient is coached in noticing a physical or mental behavior by identifying examples of the behavior and heuristics for when to watch for it, and by examining the feeling of the behavior itself. This is accompanied by practice of coping strategies. The patient is instructed to count the occurrences of that behavior on zir own. This is ideally done with a \"wrist counter,\" which is always available, can be incremented with the press of a single button, and gives both tactile and visual feedback on being pressed.</p>\n<p>The patient might, for example, count instances of acting on zir own initiative, or of having positive thoughts about zirself. In this case, tying the thought to the specific physical action of pressing the button, as well as watching a \"score\" go up, helps with the reward circuit for both noticing the thought and the content of the thought.&nbsp;</p>\n<p>The patient could also count negative thoughts, engaging in bad habits, inappropriate \"should\" statements or other \"cognitive distortions.\" At first, so I'm told, the count will go up, as you get better at noticing; then (optimistically) back down over a few weeks, as your symptoms diminish. In this case, it's important not to focus on the fact you're doing something \"bad.\" Instead, try to reward noticing and dispelling the bad thing, or at least to reward noticing that you're focusing on the bad thing rather than rewarding the noticing. (If all else fails, reward noticing that you're focusing on failing to reward noticing that you're focusing on the bad thing rather than rewarding the noticing. That should definitely do it, right?)</p>\n<p>This seems doubly useful: not only are you practicing and rewarding the noticing skill, but in tying it to a physical action, you necessarily bring the noticed behavior to your conscious attention, so that you can deal with it deliberately. If you noticed yourself dismissing a compliment, you'd take that opportunity to point out to yourself that the dismissal is mostly evidence of your mental state, and only weakly of the compliment's validity; you'd try to take the compliment at face value.</p>\n<h2>Design</h2>\n<p>I chose to implement a version of this for a personal version of <a href=\"/lw/if/your_strength_as_a_rationalist/\">noticing confusion</a>. (I also considered noticing a mental flinch, noticing motivated reasoning, flagging beliefs for review, activating curiosity, welcoming bad news, being specific, and noticing others' nonspecificity/asking for examples. I decided to go for now with what would be most personally useful and the most frequent.) I'm using <a href=\"https://play.google.com/store/apps/details?id=de.bigbyte.tools.simplecounterwidget&amp;hl=en\">this counter widget</a> on my phone's home screen. It's two button presses away at any time, and it shows me a nice big number. I also see it whenever I use my phone, which is good for scaffolding but bad for transfer&mdash;on one hand, I get reminders to pay attention to my mental processes, so I'm more likely to be able to practice the noticing skill; on the other, I might be inhibiting my learning to apply the skill without reminders. Since I could just keep using the counter if it helped, I didn't worry too much about this.</p>\n<p>The details of the fuzzy introspective rules for whether I get to count something as noticing confusion probably don't matter so much, but the basic idea is this: If I notice an unresolved tension or conflict between things I believe, then I count it. I don't count the related and also-crucial noticing that I simply don't understand something&mdash;I have to identify a conflict. (I see \"notice when I don't understand something\" as Level 0 of this skill. It's also particularly easy to practice: just read something on an unfamiliar subject, and draw a question mark next to any specific thing you don't understand. Ideally, revisit those marks later. Get in the habit of doing this for everything you read.) I don't count confusions in retrospect&mdash;if I've already resolved a confusion by the time I bring it to conscious awareness and can press the button, then I don't count it. That was a personally controversial call, but there's another sense in which \"noticing and resolving confusion\" is simply a mode of thought that operates semi- or sub-consciously. I didn't want to get bogged down in counting those, and this seemed like a simple rule to split the cases.</p>\n<p>Thus, some non-examples (still worth noticing in their own right, and often leading to pinpointing a confusion):</p>\n<ul>\n<li>I don't understand that.</li>\n<li>That doesn't seem right.</li>\n<li>That's surprising. Wait, is it? I'm not really sure what I expected, now that I think about it.</li>\n</ul>\n<p>And some examples of thoughts that I would count (by the way, these mental processes, like most, are mostly nonverbal for me, so don't take this literally; I'm noticing a feeling like tension in the connections between concepts):</p>\n<ul>\n<li>X conflicts with my understanding of Y because Z.</li>\n<li>Why does that apply in case A but not case B?</li>\n<li>I expected the graph to look like J, but it looks like L.</li>\n<li>I don't think the software usually gives me that message. [Hint: IT DOESN'T. DO NOT PROCEED.]</li>\n</ul>\n<p>Before I began, I guessed that I encountered this kind of confusion several times a day, mostly in seminars, papers, textbooks, debugging, simulated data, and experimental data. I suspected that I already consciously notice many of them, but not all, and that increasing the catch rate would markedly improve how much understanding I got out of the above activities and perhaps prevent some expensive mistakes.</p>\n<p>I attempted to keep my confusion-inducing workload constant by working the same number of hours every day. I also distributed my reading of textbooks/papers and my talk attendance to give roughly constant combined time each day, although I'm not sure that those activities had a particularly different density of confusion from my ordinary work. I typically took a couple days a week off of cognitively demanding work, and this pattern is visible in the data, at least at first.</p>\n<p>The night before starting the experiment, I ran myself through a couple-hour training exercise on a meaty-looking paper, expressly to pay attention to conflicts in my growing understanding of the result as well as tensions between the content of the paper and my background knowledge, following recommendations of <a href=\"/lw/jpf/rationality_training_academic_and_clinical/\">instructional research on metacognition</a>. This was already pretty satisfying and left me feeling good about my self-experiment. The challenge would be to see whether I could improve at spotting and pinning down my nagging doubts, and whether I could take this watchfulness beyond the more-studied domain of self-monitoring while reading. Both of these things seemed to happen.</p>\n<h2>Results</h2>\n<p>The quantitative results are promising, but not especially informative. There's only so much I can say with a month's worth of data points in such a non-rigorous self-experiment. As it turned out, my guess of \"several times a day\" was pretty good&mdash;for a good day, full of demanding work, which was what came to mind when guessing. In truth, there's a lot more variation between days, which didn't disappear as I got better at pressing the button: there's a standard deviation of 2.85 counts for week 1, 2.81 counts for week 5, and 2.81 counts for all days.</p>\n<p>Here's what the data looks like, with a moving weekly average (thus accounting for the weekend effect) and moving weekly 1&sigma; bounds (e.g. &plusmn; 2.85/&radic;7 for the first week):</p>\n<p><img src=\"http://images.lesswrong.com/t3_jpu_1.png?v=00228faa1ac81c275bab7d7aa1a6a8f6\" alt=\"\" width=\"360\" height=\"255\" /></p>\n<p>By week 3, the weekly count has gone up by a standard deviation, and it stays there or higher for weeks 4 and 5. Again, I don't want to lean too hard on these numbers&mdash;I wasn't rigorously consistent about the amount and nature of my daily work or the rules for counting. Weeks 1 and 2 might have been bad weeks, so that the increase doesn't represent a real improvement; there's also room for my desire to have a better-looking LW post to have increased the counts. And there's a little ambiguity about what I'm measuring: perhaps the increase in counting comes only from remembering to press the button, and there are plenty of other times when I notice confusions and consciously address them without identifying them as button-pressing candidates. My guess is that this isn't the case&mdash;the increase seemed to come in the form of things I barely didn't miss.</p>\n<p><del>If I na&iuml;vely say that Week 1 establishes a true distribution for averaged weekly counts, then being more than 1&sigma; above the mean for three weeks would have a probability of about p = (0.16)<sup>3</sup> = 0.0041 if that true count distribution remained constant. I'm not going to do any more sophisticated analysis than that, since I don't think the data really supports it.</del>&nbsp;See <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/alfd\">this detailed comment</a> by VincentYu. There's also a barely-significant&nbsp;relationship with the previous night's sleep duration (<em>p&nbsp;</em>= 0.043, +1 count per hour of sleep). If I adjust for this, the appearance of improvement still holds:</p>\n<p><img src=\"http://images.lesswrong.com/t3_jpu_0.png?v=827819dd79da22d86d02da2adb9cab05\" alt=\"\" width=\"360\" height=\"255\" /></p>\n<p>So sleep perhaps accounts for a small amount of random variation, and not the overall shift.</p>\n<p>Finally, some qualitative reflections:</p>\n<ul>\n<li>I feel like I gained more solid understanding of things and solved a lot of problems faster as a direct consequence of focusing on my feelings of confusion. Given that the counts went up, I suspect that things were understood and problems solved that wouldn't have been at all had I not been doing this.</li>\n<li>I occasionally found myself mentally searching for potential contradictions when I encountered new information. This is called either \"cheating\" or \"<a href=\"http://xkcd.com/810/\">mission accomplished</a>.\"</li>\n<li>You might have noticed that I didn't say anything about what to do after bringing confusion to conscious attention. It turns out that curiosity hijacks my brain once I pinpoint an apparent contradiction, far more so than when I simply notice that I don't understand something. I do what I can to encourage that process.</li>\n<li>I'm underconfident in the significance of my confusions. When I have a vague sense that something's wrong, I'm often tempted to dismiss it as a weird fact about my brain, an uninteresting exception to a weak generalization, or something that would be resolved if I just did the math. But never in the course of this experiment did I count something that turned out to be unimportant.</li>\n<li>At first, I didn't seem to exercise this skill on days where I wasn't doing cognitively demanding work, or when most of my work was not in an academic context (typically weekends). Over time, I began doing so more, although still less than on demanding academic days. This shows up in the disappearance of weekend dips in the data with time, and I think it's a good sign concerning transfer.</li>\n<li>A few weeks in, I began spontaneously recalling past instances of confusion, apparently on the strength of their connections to the feeling of being confused. Some of these I'd never resolved&mdash;I remembered a professor telling me years ago that the <a href=\"http://en.wikipedia.org/wiki/Galaxy_filament\">filamentary organization of galaxies</a> had never been observed. That had sounded obviously wrong to me, but I'd just shrugged and moved on. The contradiction lay dormant in my mind until a week ago, when I took a minute to figure out that she was almost definitely talking about direct observation of intergalactic filaments. Depending on what counts (intergalactic? intracluster? visible/dark matter?), that didn't happen until <a href=\"http://www.nature.com/news/dark-matter-s-tendrils-revealed-1.10951\">2012</a>&nbsp;or (provisionally)&nbsp;<a href=\"http://www.nature.com/news/light-from-ancient-quasar-reveals-intergalactic-web-1.14550\">very recently</a>. (That's entirely irrelevant to my current work, but I thought it was interesting.)</li>\n<li>I had one lucid dream during the past 5 weeks, and it explicitly began with noticing confusion in this sense. But that's not very meaningful, since I ordinarily expect around one lucid dream in 8 weeks. It's just as plausible to me that I began lucid-dreaming and then my brain made the connection to this experiment.</li>\n</ul>\n<h2>Conclusion</h2>\n<p>The quantitative results are promising, but for me, the qualitative lessons are more important&mdash;particularly my underconfidence and the possibility of using contradiction to <a href=\"/lw/aa7/get_curious/\">fuel curiosity</a>. I'll keep counting confusions like this for a while, but I'm <a href=\"/lw/21l/lampshading/\">not going to worry much</a> about experimental validity. Similarly, it doesn't matter a whole lot to me whether the apparent gains rely on using the counter, since it costs me basically nothing to continue using it. I suppose that one could look into that by taking a break from counting and resuming it after a few months, but that's honestly not my priority.</p>\n<p>This is a really easy thing to try, and I'd like to encourage others to build on the simple attempt I've presented here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5gcpKG2XEAZGj5DEf": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mjiu8n9qyoqfY7LkF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 63, "extendedScore": null, "score": 0.000164, "legacy": true, "legacyId": "25554", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I <a href=\"/lw/jpf/rationality_training_academic_and_clinical/\">previously discussed</a> the potential relevance of therapeutic and instructional models of metacognitive training to LW-style rationality skills. As an attempted concrete realization of what this connection could look like, I ran a self-experiment in which I counted instances of noticing confusion. Below I elaborate on the motivation and design of the experiment, then discuss some quantitative results and qualitative reflections.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Background\">Background</h2>\n<p>Self-monitoring as a treatment vehicle in cognitive-behavioral and related therapies can take many forms. In one (to my secondhand understanding), the patient is coached in noticing a physical or mental behavior by identifying examples of the behavior and heuristics for when to watch for it, and by examining the feeling of the behavior itself. This is accompanied by practice of coping strategies. The patient is instructed to count the occurrences of that behavior on zir own. This is ideally done with a \"wrist counter,\" which is always available, can be incremented with the press of a single button, and gives both tactile and visual feedback on being pressed.</p>\n<p>The patient might, for example, count instances of acting on zir own initiative, or of having positive thoughts about zirself. In this case, tying the thought to the specific physical action of pressing the button, as well as watching a \"score\" go up, helps with the reward circuit for both noticing the thought and the content of the thought.&nbsp;</p>\n<p>The patient could also count negative thoughts, engaging in bad habits, inappropriate \"should\" statements or other \"cognitive distortions.\" At first, so I'm told, the count will go up, as you get better at noticing; then (optimistically) back down over a few weeks, as your symptoms diminish. In this case, it's important not to focus on the fact you're doing something \"bad.\" Instead, try to reward noticing and dispelling the bad thing, or at least to reward noticing that you're focusing on the bad thing rather than rewarding the noticing. (If all else fails, reward noticing that you're focusing on failing to reward noticing that you're focusing on the bad thing rather than rewarding the noticing. That should definitely do it, right?)</p>\n<p>This seems doubly useful: not only are you practicing and rewarding the noticing skill, but in tying it to a physical action, you necessarily bring the noticed behavior to your conscious attention, so that you can deal with it deliberately. If you noticed yourself dismissing a compliment, you'd take that opportunity to point out to yourself that the dismissal is mostly evidence of your mental state, and only weakly of the compliment's validity; you'd try to take the compliment at face value.</p>\n<h2 id=\"Design\">Design</h2>\n<p>I chose to implement a version of this for a personal version of <a href=\"/lw/if/your_strength_as_a_rationalist/\">noticing confusion</a>. (I also considered noticing a mental flinch, noticing motivated reasoning, flagging beliefs for review, activating curiosity, welcoming bad news, being specific, and noticing others' nonspecificity/asking for examples. I decided to go for now with what would be most personally useful and the most frequent.) I'm using <a href=\"https://play.google.com/store/apps/details?id=de.bigbyte.tools.simplecounterwidget&amp;hl=en\">this counter widget</a> on my phone's home screen. It's two button presses away at any time, and it shows me a nice big number. I also see it whenever I use my phone, which is good for scaffolding but bad for transfer\u2014on one hand, I get reminders to pay attention to my mental processes, so I'm more likely to be able to practice the noticing skill; on the other, I might be inhibiting my learning to apply the skill without reminders. Since I could just keep using the counter if it helped, I didn't worry too much about this.</p>\n<p>The details of the fuzzy introspective rules for whether I get to count something as noticing confusion probably don't matter so much, but the basic idea is this: If I notice an unresolved tension or conflict between things I believe, then I count it. I don't count the related and also-crucial noticing that I simply don't understand something\u2014I have to identify a conflict. (I see \"notice when I don't understand something\" as Level 0 of this skill. It's also particularly easy to practice: just read something on an unfamiliar subject, and draw a question mark next to any specific thing you don't understand. Ideally, revisit those marks later. Get in the habit of doing this for everything you read.) I don't count confusions in retrospect\u2014if I've already resolved a confusion by the time I bring it to conscious awareness and can press the button, then I don't count it. That was a personally controversial call, but there's another sense in which \"noticing and resolving confusion\" is simply a mode of thought that operates semi- or sub-consciously. I didn't want to get bogged down in counting those, and this seemed like a simple rule to split the cases.</p>\n<p>Thus, some non-examples (still worth noticing in their own right, and often leading to pinpointing a confusion):</p>\n<ul>\n<li>I don't understand that.</li>\n<li>That doesn't seem right.</li>\n<li>That's surprising. Wait, is it? I'm not really sure what I expected, now that I think about it.</li>\n</ul>\n<p>And some examples of thoughts that I would count (by the way, these mental processes, like most, are mostly nonverbal for me, so don't take this literally; I'm noticing a feeling like tension in the connections between concepts):</p>\n<ul>\n<li>X conflicts with my understanding of Y because Z.</li>\n<li>Why does that apply in case A but not case B?</li>\n<li>I expected the graph to look like J, but it looks like L.</li>\n<li>I don't think the software usually gives me that message. [Hint: IT DOESN'T. DO NOT PROCEED.]</li>\n</ul>\n<p>Before I began, I guessed that I encountered this kind of confusion several times a day, mostly in seminars, papers, textbooks, debugging, simulated data, and experimental data. I suspected that I already consciously notice many of them, but not all, and that increasing the catch rate would markedly improve how much understanding I got out of the above activities and perhaps prevent some expensive mistakes.</p>\n<p>I attempted to keep my confusion-inducing workload constant by working the same number of hours every day. I also distributed my reading of textbooks/papers and my talk attendance to give roughly constant combined time each day, although I'm not sure that those activities had a particularly different density of confusion from my ordinary work. I typically took a couple days a week off of cognitively demanding work, and this pattern is visible in the data, at least at first.</p>\n<p>The night before starting the experiment, I ran myself through a couple-hour training exercise on a meaty-looking paper, expressly to pay attention to conflicts in my growing understanding of the result as well as tensions between the content of the paper and my background knowledge, following recommendations of <a href=\"/lw/jpf/rationality_training_academic_and_clinical/\">instructional research on metacognition</a>. This was already pretty satisfying and left me feeling good about my self-experiment. The challenge would be to see whether I could improve at spotting and pinning down my nagging doubts, and whether I could take this watchfulness beyond the more-studied domain of self-monitoring while reading. Both of these things seemed to happen.</p>\n<h2 id=\"Results\">Results</h2>\n<p>The quantitative results are promising, but not especially informative. There's only so much I can say with a month's worth of data points in such a non-rigorous self-experiment. As it turned out, my guess of \"several times a day\" was pretty good\u2014for a good day, full of demanding work, which was what came to mind when guessing. In truth, there's a lot more variation between days, which didn't disappear as I got better at pressing the button: there's a standard deviation of 2.85 counts for week 1, 2.81 counts for week 5, and 2.81 counts for all days.</p>\n<p>Here's what the data looks like, with a moving weekly average (thus accounting for the weekend effect) and moving weekly 1\u03c3 bounds (e.g. \u00b1 2.85/\u221a7 for the first week):</p>\n<p><img src=\"http://images.lesswrong.com/t3_jpu_1.png?v=00228faa1ac81c275bab7d7aa1a6a8f6\" alt=\"\" width=\"360\" height=\"255\"></p>\n<p>By week 3, the weekly count has gone up by a standard deviation, and it stays there or higher for weeks 4 and 5. Again, I don't want to lean too hard on these numbers\u2014I wasn't rigorously consistent about the amount and nature of my daily work or the rules for counting. Weeks 1 and 2 might have been bad weeks, so that the increase doesn't represent a real improvement; there's also room for my desire to have a better-looking LW post to have increased the counts. And there's a little ambiguity about what I'm measuring: perhaps the increase in counting comes only from remembering to press the button, and there are plenty of other times when I notice confusions and consciously address them without identifying them as button-pressing candidates. My guess is that this isn't the case\u2014the increase seemed to come in the form of things I barely didn't miss.</p>\n<p><del>If I na\u00efvely say that Week 1 establishes a true distribution for averaged weekly counts, then being more than 1\u03c3 above the mean for three weeks would have a probability of about p = (0.16)<sup>3</sup> = 0.0041 if that true count distribution remained constant. I'm not going to do any more sophisticated analysis than that, since I don't think the data really supports it.</del>&nbsp;See <a href=\"/lw/jpu/a_selfexperiment_in_training_noticing_confusion/alfd\">this detailed comment</a> by VincentYu. There's also a barely-significant&nbsp;relationship with the previous night's sleep duration (<em>p&nbsp;</em>= 0.043, +1 count per hour of sleep). If I adjust for this, the appearance of improvement still holds:</p>\n<p><img src=\"http://images.lesswrong.com/t3_jpu_0.png?v=827819dd79da22d86d02da2adb9cab05\" alt=\"\" width=\"360\" height=\"255\"></p>\n<p>So sleep perhaps accounts for a small amount of random variation, and not the overall shift.</p>\n<p>Finally, some qualitative reflections:</p>\n<ul>\n<li>I feel like I gained more solid understanding of things and solved a lot of problems faster as a direct consequence of focusing on my feelings of confusion. Given that the counts went up, I suspect that things were understood and problems solved that wouldn't have been at all had I not been doing this.</li>\n<li>I occasionally found myself mentally searching for potential contradictions when I encountered new information. This is called either \"cheating\" or \"<a href=\"http://xkcd.com/810/\">mission accomplished</a>.\"</li>\n<li>You might have noticed that I didn't say anything about what to do after bringing confusion to conscious attention. It turns out that curiosity hijacks my brain once I pinpoint an apparent contradiction, far more so than when I simply notice that I don't understand something. I do what I can to encourage that process.</li>\n<li>I'm underconfident in the significance of my confusions. When I have a vague sense that something's wrong, I'm often tempted to dismiss it as a weird fact about my brain, an uninteresting exception to a weak generalization, or something that would be resolved if I just did the math. But never in the course of this experiment did I count something that turned out to be unimportant.</li>\n<li>At first, I didn't seem to exercise this skill on days where I wasn't doing cognitively demanding work, or when most of my work was not in an academic context (typically weekends). Over time, I began doing so more, although still less than on demanding academic days. This shows up in the disappearance of weekend dips in the data with time, and I think it's a good sign concerning transfer.</li>\n<li>A few weeks in, I began spontaneously recalling past instances of confusion, apparently on the strength of their connections to the feeling of being confused. Some of these I'd never resolved\u2014I remembered a professor telling me years ago that the <a href=\"http://en.wikipedia.org/wiki/Galaxy_filament\">filamentary organization of galaxies</a> had never been observed. That had sounded obviously wrong to me, but I'd just shrugged and moved on. The contradiction lay dormant in my mind until a week ago, when I took a minute to figure out that she was almost definitely talking about direct observation of intergalactic filaments. Depending on what counts (intergalactic? intracluster? visible/dark matter?), that didn't happen until <a href=\"http://www.nature.com/news/dark-matter-s-tendrils-revealed-1.10951\">2012</a>&nbsp;or (provisionally)&nbsp;<a href=\"http://www.nature.com/news/light-from-ancient-quasar-reveals-intergalactic-web-1.14550\">very recently</a>. (That's entirely irrelevant to my current work, but I thought it was interesting.)</li>\n<li>I had one lucid dream during the past 5 weeks, and it explicitly began with noticing confusion in this sense. But that's not very meaningful, since I ordinarily expect around one lucid dream in 8 weeks. It's just as plausible to me that I began lucid-dreaming and then my brain made the connection to this experiment.</li>\n</ul>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>The quantitative results are promising, but for me, the qualitative lessons are more important\u2014particularly my underconfidence and the possibility of using contradiction to <a href=\"/lw/aa7/get_curious/\">fuel curiosity</a>. I'll keep counting confusions like this for a while, but I'm <a href=\"/lw/21l/lampshading/\">not going to worry much</a> about experimental validity. Similarly, it doesn't matter a whole lot to me whether the apparent gains rely on using the counter, since it costs me basically nothing to continue using it. I suppose that one could look into that by taking a break from counting and resuming it after a few months, but that's honestly not my priority.</p>\n<p>This is a really easy thing to try, and I'd like to encourage others to build on the simple attempt I've presented here.</p>", "sections": [{"title": "Background", "anchor": "Background", "level": 1}, {"title": "Design", "anchor": "Design", "level": 1}, {"title": "Results", "anchor": "Results", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vENLgXhMoecM4jmiZ", "5JDkW4MYXit2CquLs", "bGtdeqbgTzuLvZ5zn", "goCfoiQkniQwPryki"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-20T03:23:31.605Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Bridging laws", "slug": "meetup-urbana-champaign-bridging-laws", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ejMWnaJW9q6ioGZBJ/meetup-urbana-champaign-bridging-laws", "pageUrlRelative": "/posts/ejMWnaJW9q6ioGZBJ/meetup-urbana-champaign-bridging-laws", "linkUrl": "https://www.lesswrong.com/posts/ejMWnaJW9q6ioGZBJ/meetup-urbana-champaign-bridging-laws", "postedAtFormatted": "Thursday, February 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Bridging%20laws&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Bridging%20laws%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FejMWnaJW9q6ioGZBJ%2Fmeetup-urbana-champaign-bridging-laws%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Bridging%20laws%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FejMWnaJW9q6ioGZBJ%2Fmeetup-urbana-champaign-bridging-laws", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FejMWnaJW9q6ioGZBJ%2Fmeetup-urbana-champaign-bridging-laws", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/x1\">Urbana-Champaign: Bridging laws</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">23 February 2014 02:00:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">412 W. Elm St, Urbana, IL</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><a href=\"/user/RobbBB/submitted/\">RobBB</a>&nbsp;has made some recent posts about how an agent's representation of the world can interact with its perceptions and plans. That seems like a good topic, so let's discuss it.</p>\n<p>Bonus question: If an agent represents its utility as a function over states of the world including itself, can it still wirehead?</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/x1\">Urbana-Champaign: Bridging laws</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ejMWnaJW9q6ioGZBJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.5728818717900804e-06, "legacy": true, "legacyId": "25557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Bridging_laws\">Discussion article for the meetup : <a href=\"/meetups/x1\">Urbana-Champaign: Bridging laws</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">23 February 2014 02:00:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">412 W. Elm St, Urbana, IL</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><a href=\"/user/RobbBB/submitted/\">RobBB</a>&nbsp;has made some recent posts about how an agent's representation of the world can interact with its perceptions and plans. That seems like a good topic, so let's discuss it.</p>\n<p>Bonus question: If an agent represents its utility as a function over states of the world including itself, can it still wirehead?</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Bridging_laws1\">Discussion article for the meetup : <a href=\"/meetups/x1\">Urbana-Champaign: Bridging laws</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Bridging laws", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Bridging_laws", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Bridging laws", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Bridging_laws1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-20T09:40:56.517Z", "modifiedAt": null, "url": null, "title": "[link] The ethics of genetically enhanced monkey slaves", "slug": "link-the-ethics-of-genetically-enhanced-monkey-slaves", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:40.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GFCjxF3kqmk3cZD5T/link-the-ethics-of-genetically-enhanced-monkey-slaves", "pageUrlRelative": "/posts/GFCjxF3kqmk3cZD5T/link-the-ethics-of-genetically-enhanced-monkey-slaves", "linkUrl": "https://www.lesswrong.com/posts/GFCjxF3kqmk3cZD5T/link-the-ethics-of-genetically-enhanced-monkey-slaves", "postedAtFormatted": "Thursday, February 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20The%20ethics%20of%20genetically%20enhanced%20monkey%20slaves&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20The%20ethics%20of%20genetically%20enhanced%20monkey%20slaves%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFCjxF3kqmk3cZD5T%2Flink-the-ethics-of-genetically-enhanced-monkey-slaves%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20The%20ethics%20of%20genetically%20enhanced%20monkey%20slaves%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFCjxF3kqmk3cZD5T%2Flink-the-ethics-of-genetically-enhanced-monkey-slaves", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFCjxF3kqmk3cZD5T%2Flink-the-ethics-of-genetically-enhanced-monkey-slaves", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>Interesting <a href=\"http://blog.ted.com/2014/02/19/the-ethics-of-genetically-enhanced-monkey-slaves/\">interview</a> with Julian Savulescu (professor of practical ethics at the University of Oxford) on \"the ethics of the biological enhancement of the human race\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GFCjxF3kqmk3cZD5T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "25559", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T08:05:38.146Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014Practical Taoism", "slug": "meetup-west-la-practical-taoism", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qQy6cRWmqQatstSb5/meetup-west-la-practical-taoism", "pageUrlRelative": "/posts/qQy6cRWmqQatstSb5/meetup-west-la-practical-taoism", "linkUrl": "https://www.lesswrong.com/posts/qQy6cRWmqQatstSb5/meetup-west-la-practical-taoism", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94Practical%20Taoism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94Practical%20Taoism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqQy6cRWmqQatstSb5%2Fmeetup-west-la-practical-taoism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94Practical%20Taoism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqQy6cRWmqQatstSb5%2Fmeetup-west-la-practical-taoism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqQy6cRWmqQatstSb5%2Fmeetup-west-la-practical-taoism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 332, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x2'>West LA\u2014Practical Taoism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into the Del Taco. I will bring a Rubik's Cube and put it on a table, in case you require visual confirmation that You Are In The Right Place before asking anyone whether you're in the right place.</p>\n\n<p><strong>Parking</strong> is, well, I don't actually know. There's a sign that says the local lot allows only 45 minutes, but that may or may not be enforced. Others have reported that it's easy to find less time-unlimited parking nearby, although not as guaranteed.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>Look at your Internet argument. Now look at <a href=\"http://ow.ly/nt1B3\" rel=\"nofollow\">http://ow.ly/nt1B3</a> Now look at your Internet argument. Your Internet argument is now stupid. Actually, your internet argument was already stupid.\u2014<a href=\"https://twitter.com/St_Rev/status/362272952940642305\" rel=\"nofollow\">St. Rev</a></p>\n</blockquote>\n\n<p>If you have political opinions, you are a fool. If you think your political opinions matter, you are doubly the fool. If you think it's important that other people share your political opinions: fool. If you actually spend a significant portion of your time trying to get people to have the same political opinions as you, you cannot be redeemed. These are the facts that we must face, because you and I are but human and do not understand society. Metapolitical opinions, of course, are completely rational and not at all subject to these considerations.</p>\n\n<p>We will be discussing Michael Huemer's paper <a href=\"http://studiahumana.com/pliki/wydania/In%20Praise%20of%20Passivity.pdf\" rel=\"nofollow\">In Praise of Passivity</a>, of which section 4 is practical advice given its conclusions. Hence practical Taoism: useful inaction.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killer</a></li>\n<li><a href=\"http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/\">A Fable of Science and Politics</a></li>\n<li><a href=\"http://www.overcomingbias.com/2013/11/the-coalition-politics-hypothesis.html\">The Coalition Politics Hypothesis</a></li>\n<li><a href=\"http://www.overcomingbias.com/2014/02/rah-local-politics.html\">Rah Local Politics</a></li>\n<li><a href=\"http://studiahumana.com/pliki/wydania/In%20Praise%20of%20Passivity.pdf\" rel=\"nofollow\">In Praise of Passivity</a>, by Michael Huemer</li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</em> However, I would very much like it if someone, anyone, anyone at all, were to read the material beforehand.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x2'>West LA\u2014Practical Taoism</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qQy6cRWmqQatstSb5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.5749000593308028e-06, "legacy": true, "legacyId": "25570", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Practical_Taoism\">Discussion article for the meetup : <a href=\"/meetups/x2\">West LA\u2014Practical Taoism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 February 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into the Del Taco. I will bring a Rubik's Cube and put it on a table, in case you require visual confirmation that You Are In The Right Place before asking anyone whether you're in the right place.</p>\n\n<p><strong>Parking</strong> is, well, I don't actually know. There's a sign that says the local lot allows only 45 minutes, but that may or may not be enforced. Others have reported that it's easy to find less time-unlimited parking nearby, although not as guaranteed.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>Look at your Internet argument. Now look at <a href=\"http://ow.ly/nt1B3\" rel=\"nofollow\">http://ow.ly/nt1B3</a> Now look at your Internet argument. Your Internet argument is now stupid. Actually, your internet argument was already stupid.\u2014<a href=\"https://twitter.com/St_Rev/status/362272952940642305\" rel=\"nofollow\">St. Rev</a></p>\n</blockquote>\n\n<p>If you have political opinions, you are a fool. If you think your political opinions matter, you are doubly the fool. If you think it's important that other people share your political opinions: fool. If you actually spend a significant portion of your time trying to get people to have the same political opinions as you, you cannot be redeemed. These are the facts that we must face, because you and I are but human and do not understand society. Metapolitical opinions, of course, are completely rational and not at all subject to these considerations.</p>\n\n<p>We will be discussing Michael Huemer's paper <a href=\"http://studiahumana.com/pliki/wydania/In%20Praise%20of%20Passivity.pdf\" rel=\"nofollow\">In Praise of Passivity</a>, of which section 4 is practical advice given its conclusions. Hence practical Taoism: useful inaction.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killer</a></li>\n<li><a href=\"http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/\">A Fable of Science and Politics</a></li>\n<li><a href=\"http://www.overcomingbias.com/2013/11/the-coalition-politics-hypothesis.html\">The Coalition Politics Hypothesis</a></li>\n<li><a href=\"http://www.overcomingbias.com/2014/02/rah-local-politics.html\">Rah Local Politics</a></li>\n<li><a href=\"http://studiahumana.com/pliki/wydania/In%20Praise%20of%20Passivity.pdf\" rel=\"nofollow\">In Praise of Passivity</a>, by Michael Huemer</li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</em> However, I would very much like it if someone, anyone, anyone at all, were to read the material beforehand.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Practical_Taoism1\">Discussion article for the meetup : <a href=\"/meetups/x2\">West LA\u2014Practical Taoism</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014Practical Taoism", "anchor": "Discussion_article_for_the_meetup___West_LA_Practical_Taoism", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014Practical Taoism", "anchor": "Discussion_article_for_the_meetup___West_LA_Practical_Taoism1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "6hfGNLf4Hg5DXqJCF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T11:50:45.044Z", "modifiedAt": null, "url": null, "title": "Jobs and internships available at the Centre for Effective Altruism: new 'EA outreach' roles added", "slug": "jobs-and-internships-available-at-the-centre-for-effective-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:48.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tog", "createdAt": "2011-10-04T12:54:07.164Z", "isAdmin": false, "displayName": "tog"}, "userId": "b4f6teTtsKfegjTaH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/argjqCrM9v58ij9Ht/jobs-and-internships-available-at-the-centre-for-effective-0", "pageUrlRelative": "/posts/argjqCrM9v58ij9Ht/jobs-and-internships-available-at-the-centre-for-effective-0", "linkUrl": "https://www.lesswrong.com/posts/argjqCrM9v58ij9Ht/jobs-and-internships-available-at-the-centre-for-effective-0", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Jobs%20and%20internships%20available%20at%20the%20Centre%20for%20Effective%20Altruism%3A%20new%20'EA%20outreach'%20roles%20added&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJobs%20and%20internships%20available%20at%20the%20Centre%20for%20Effective%20Altruism%3A%20new%20'EA%20outreach'%20roles%20added%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FargjqCrM9v58ij9Ht%2Fjobs-and-internships-available-at-the-centre-for-effective-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Jobs%20and%20internships%20available%20at%20the%20Centre%20for%20Effective%20Altruism%3A%20new%20'EA%20outreach'%20roles%20added%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FargjqCrM9v58ij9Ht%2Fjobs-and-internships-available-at-the-centre-for-effective-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FargjqCrM9v58ij9Ht%2Fjobs-and-internships-available-at-the-centre-for-effective-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>I recently posted on LessWrong main about the <a href=\"/lw/jmx/jobs_and_internships_available_at_the_centre_for/\">jobs and internships currently available at the Centre for Effective Altruism</a>. (As I mentioned, effective altruism in general and CEA in particular have been discussed many times on LessWrong, so these opportunities might be of interest to some readers!) We're just starting a new project to do effective altruism outreach and 'marketing' (much like what Peter Hurford discusses in <a href=\"http://www.everydayutilitarian.com/essays/could-ea-community-building-be-an-easy-and-underfunded-win/\">this post</a>), so have added some new roles in this to the recruitment round; there's a full description of them <a href=\"http://www.effective-altruism.com/jobs-available-at-ceas-new-ea-outreach-project/\">here</a>. If you're interested, <a href=\"http://home.centreforeffectivealtruism.org/careers\">apply</a> by 5pm GMT on February 28th, and if you know anyone who might be, do pass it along!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "argjqCrM9v58ij9Ht", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.575164216438972e-06, "legacy": true, "legacyId": "25571", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pJK8ptvcCzc83jsE3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T12:23:35.889Z", "modifiedAt": null, "url": null, "title": "Intelligence Metrics with Naturalized Induction using UDT", "slug": "intelligence-metrics-with-naturalized-induction-using-udt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:07.699Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Squark", "createdAt": "2013-02-04T19:29:04.489Z", "isAdmin": false, "displayName": "Squark"}, "userId": "k4QpNYXcigqfG85t6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2ZGCq44Wv8p4btyL7/intelligence-metrics-with-naturalized-induction-using-udt", "pageUrlRelative": "/posts/2ZGCq44Wv8p4btyL7/intelligence-metrics-with-naturalized-induction-using-udt", "linkUrl": "https://www.lesswrong.com/posts/2ZGCq44Wv8p4btyL7/intelligence-metrics-with-naturalized-induction-using-udt", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Metrics%20with%20Naturalized%20Induction%20using%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Metrics%20with%20Naturalized%20Induction%20using%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZGCq44Wv8p4btyL7%2Fintelligence-metrics-with-naturalized-induction-using-udt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Metrics%20with%20Naturalized%20Induction%20using%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZGCq44Wv8p4btyL7%2Fintelligence-metrics-with-naturalized-induction-using-udt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZGCq44Wv8p4btyL7%2Fintelligence-metrics-with-naturalized-induction-using-udt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2210, "htmlBody": "<p>Followup to: <a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">Intelligence Metrics and Decision Theory</a><br />Related to:&nbsp;<a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse: Reductionism as Engineering Problem</a></p>\n<p>A central problem in AGI is giving a formal definition of intelligence. <a href=\"http://www.hutter1.net/ai/aixigentle.pdf\">Marcus Hutter</a> has proposed <a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>&nbsp;as a model of perfectly intelligent agent. <a href=\"http://arxiv.org/abs/0712.3329\">Legg and Hutter</a>&nbsp;have defined a quantitative measure of intelligence applicable to any suitable formalized agent such that AIXI is the agent with maximal intelligence according to this measure.</p>\n<p>Legg-Hutter intelligence suffers from a number of problems I have <a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">previously</a> discussed, the most important being:</p>\n<ul>\n<li>The formalism is inherently Cartesian. Solving this problem is known as <a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalized induction</a>&nbsp;and it is discussed in detail <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">here</a>.</li>\n<li>The utility function Legg &amp; Hutter use is a formalization of reinforcement learning, while we would like to consider agents with arbitrary preferences. Moreover, a real AGI designed with reinforcement learning would tend to wrestle control of the reinforcement signal from the operators (<em>there must be a classic reference on this but I can't find it. Help?</em>). It is straightword to tweak to formalism to allow for any utility function which depends on the agent's sensations and actions, however we would like to be able to use any ontology for defining it.</li>\n</ul>\n<div><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf\">Orseau and Ring</a>&nbsp;proposed a non-Cartesian intelligence metric however their formalism appears to be <em>too</em>&nbsp;general, in particular there is no <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff induction</a> or any analogue thereof, instead a completely general probability measure is used.</div>\n<div><br /></div>\n<div><a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">My attempt</a> at defining a non-Cartesian intelligence metric ran into problems of decision-theoretic flavor. The way I tried to used <a href=\"http://wiki.lesswrong.com/wiki/UDT\">UDT</a>&nbsp;seems unsatisfactory, and later I tried a <a href=\"/lw/h93/metatickle_intelligence_metrics_and_friendly/\">different approach</a> related to metatickle EDT.&nbsp;</div>\n<div><br /></div>\n<div>In this post, I claim to accomplish the following:</div>\n<div>\n<ul>\n<li>Define a formalism for logical uncertainty. <em>When I started writing this I thought this formalism might be novel but now I see it is essentially the same as that of <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">Benja</a>.</em></li>\n<li>Use this formalism to define a non-constructive formalization of UDT. By \"non-constructive\" I mean something that assigns values to actions rather than a specific algorithm like <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">here</a>.</li>\n<li>Apply the formalization of UDT to my quasi-Solomonoff framework to yield an intelligence metric.</li>\n<li>Slightly modify my original definition of the quasi-Solomonoff measure so that the confidence of the innate model becomes a continuous rather than discrete parameter. This leads to an interesting conjecture.</li>\n<li>Propose a \"preference agnostic\" variant as an alternative to Legg &amp; Hutter's reinforcement learning.</li>\n<li>Discuss certain anthropic and decision-theoretic aspects.</li>\n</ul>\n<h1>Logical Uncertainty</h1>\n<p><em>The formalism introduced here was originally proposed by </em><em><a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">Benja</a>.</em></p>\n<p>Fix a formal system <strong>F</strong>. We want to be able to assign probabilities to statements <strong>s</strong>&nbsp;in <strong>F</strong>, taking into account limited computing resources. Fix <strong>D</strong>&nbsp;a natural number related to the amount of computing resources that I call \"depth of analysis\".</p>\n<p>Define P<sub>0</sub>(<strong>s</strong>) := 1/2 for all <strong>s</strong>&nbsp;to be our initial prior, i.e. each statement's truth value is decided by a fair coin toss. Now define <br />P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) := P<sub>0</sub>(<strong>s</strong> | there are no contradictions of length &lt;= <strong>D</strong>).</p>\n<p>Consider <strong>X</strong> to be a number in [0, 1] given by a definition in <strong>F</strong>. Then <strong>d</strong><sub>k</sub>(<strong>X</strong>) := \"The k-th digit of the binary expansion of&nbsp;<strong>X</strong> is 1\"&nbsp;is a statement in <strong>F</strong>. We define E<sub><strong>D</strong></sub>(<strong>X</strong>) :=&nbsp;&Sigma;<sub>k</sub> 2<sup>-k</sup> P<sub style=\"font-weight: bold;\">D</sub>(<strong>d</strong><sub>k</sub>(<strong>X</strong>)).</p>\n<h2>Remarks</h2>\n<ul>\n<li>Clearly if <strong>s</strong> is provable in&nbsp;<strong>F</strong>&nbsp;then for&nbsp;<strong>D</strong>&nbsp;&gt;&gt; 0,&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) = 1. Similarly if \"not <strong>s</strong>\" is provable in&nbsp;<strong>F</strong>&nbsp;then for&nbsp;<strong>D</strong>&nbsp;&gt;&gt; 0,&nbsp;<br />P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) = 0.</li>\n<li>If each digit of&nbsp;<strong>X</strong>&nbsp;is decidable in <strong>F</strong>&nbsp;then lim<sub><strong>D&nbsp;</strong>-&gt; inf</sub> E<sub><strong>D</strong></sub>(<strong>X</strong>) exists and equals the value of <strong>X</strong> according to <strong>F</strong>.</li>\n<li>For <strong>s</strong> of length &gt; <strong>D</strong>,&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) = 1/2 since no contradiction of length &lt;= <strong>D</strong> can involve <strong>s</strong>.</li>\n<li>It is an interesting <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/7agz\">question</a>&nbsp;whether&nbsp;lim<sub><strong>D&nbsp;</strong>-&gt; inf</sub>&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) exists for any <strong>s</strong>. It seems false that this limit always exists <em>and</em>&nbsp;equals 0 or 1, i.e. this formalism is <em>not</em>&nbsp;a loophole in Goedel incompleteness. To see this consider statements that require a high (arithmetical hierarchy) order halting oracle to decide.</li>\n<li>In computational terms, <strong>D</strong> corresponds to non-deterministic spatial complexity. It is spatial since we assign truth values simultaneously to all statements so in any given contradiction it is enough to retain the \"thickest\" step. It is non-deterministic since it's enough for a contradiction to exists, we don't have an actual computation which produces it. I suspect this can be made more formal using the <a href=\"https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence\">Curry-Howard isomorphism</a>, unfortunately I don't understand the latter yet.</li>\n</ul>\n<h1>Non-Constructive UDT</h1>\n<p>Consider <strong>A</strong> a decision algorithm for optimizing utility&nbsp;<strong>U</strong>, producing an output (\"decision\") which is an element of&nbsp;<strong>C</strong>. Here&nbsp;<strong>U</strong>&nbsp;is just a constant defined in <strong>F</strong>. We define the <strong>U</strong>-value of <strong>c</strong>&nbsp;in <strong>C</strong> for <strong>A</strong>&nbsp;at depth of analysis <strong>D</strong>&nbsp;to be <br />V<sub><strong>D</strong></sub>(<strong>c</strong>, <strong>A</strong>; <strong>U</strong>) := E<sub><strong>D</strong></sub>(<strong>U</strong> | \"<strong>A</strong> produces <strong>c</strong>\" is true). It is only well defined as long as \"<strong>A</strong>&nbsp;doesn't&nbsp;produce&nbsp;<strong>c</strong>\" cannot be proved at depth of analysis <strong>D</strong>&nbsp;i.e. P<sub style=\"font-weight: bold;\">D</sub>(\"<strong>A</strong>&nbsp;produces&nbsp;<strong>c</strong>\") &gt; 0. We define the <em>absolute</em>&nbsp;<strong>U</strong>-value of&nbsp;<strong>c</strong>&nbsp;for&nbsp;<strong>A</strong>&nbsp;to be <br />V(<strong>c</strong>,&nbsp;<strong>A</strong>;&nbsp;<strong>U</strong>) := E<sub><strong>D</strong>(<strong>c</strong>, <strong>A</strong>)</sub>(<strong>U</strong>&nbsp;| \"<strong>A</strong>&nbsp;produces&nbsp;<strong>c</strong>\" is true) where <strong>D</strong>(<strong>c</strong>, <strong>A</strong>) := max {<strong>D</strong> |&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(\"<strong>A</strong>&nbsp;produces&nbsp;<strong>c</strong>\") &gt; 0}. Of course&nbsp;<strong>D</strong>(<strong>c</strong>,&nbsp;<strong>A</strong>) can be infinite in which case E<span style=\"font-size: 11.199999809265137px;\"><sub>inf</sub></span>(...) is understood to mean lim<sub><strong>D</strong> -&gt; inf</sub> E<sub><strong>D</strong></sub>(...).</p>\n<p>For example V(<strong>c</strong>,&nbsp;<strong>A</strong>;&nbsp;<strong>U</strong>) yields the natural values for <strong>A</strong> an <a href=\"http://wiki.lesswrong.com/wiki/Ambient_decision_theory\">ambient control algorithm</a>&nbsp;applied to e.g. a simple model of Newcomb's problem. &nbsp;To see this note that given <strong>A</strong>'s output the value of&nbsp;<strong>U </strong>can be determined at low depths of analysis whereas the output of <strong>A</strong> requires a very high depth of analysis to determine.</p>\n<h1>Naturalized Induction</h1>\n<p>Our starting point is the \"innate model\" <strong>N</strong>: a certain a priori model of the universe including the agent <strong>G</strong>. This model encodes the universe as a sequence of natural numbers <strong>Y</strong> = (<strong>y</strong><sub>k</sub>) which obeys either specific deterministic or non-deterministic dynamics or at least some constraints on the possible histories. It may or may not include information on the initial conditions. For example, <strong>N</strong> can describe the universe as a universal Turing machine <strong>M</strong> (representing <strong>G</strong>) with special \"sensory\" registers <strong>e</strong>. <strong>N</strong> constraints the dynamics to be compatible with the rules of the Turing machine but leaves unspecified the behavior of <strong>e</strong>. Alternatively, <strong>N</strong> can contain in addition to <strong>M</strong> a non-trivial model of the environment. Or <strong>N</strong> can be a cellular automaton with the agent corresponding to a certain collection of cells.</p>\n<p>However, <strong>G</strong>'s confidence in <strong>N</strong> is limited: otherwise it wouldn't need induction. We cannot start with 0 confidence: it's impossible to program a machine if you don't have even a guess of how it works. Instead we introduce a positive real number&nbsp;<strong>t</strong> which represents the timescale over which <strong>N</strong> is expected to hold. We then assign to each hypothesis <strong>H</strong> about&nbsp;<strong>Y</strong>&nbsp;(you can think about them as programs which compute&nbsp;<strong>y</strong><sub>k</sub> given <strong>y</strong><sub>j</sub> for j &lt; k; more on that later) the weight QS(<strong>H</strong>) := 2<sup>-L(<strong>H</strong>)&nbsp;</sup>(1 - e<sup>-t(<strong>H</strong>)/<strong>t</strong></sup>). Here L(<strong>H</strong>) is the length of <strong>H</strong>'s encoding in bits and t(<strong>H</strong>) is the time during which <strong>H</strong> remains compatible with <strong>N</strong>. This is defined for <strong>N</strong> of deterministic / constraint type but can be generalized to stochastic <strong>N</strong>.&nbsp;</p>\n<p>The weights QS(<strong>H</strong>) define a probability measure on the space of hypotheses which induces a probability measure on the space of histories <strong>Y</strong>. Thus we get an alternative to Solomonoff induction which allows for <strong>G</strong> to be a mechanistic part of the universe, at the price of introducing <strong>N</strong> and <strong>t</strong>.&nbsp;</p>\n<h2>Remarks</h2>\n<ul>\n<li>Note that time is discrete in this formalism but <strong>t</strong> is continuous.</li>\n<li>Since we're later going to use logical uncertainties wrt the formal system <strong>F</strong>, it is tempting to construct the hypothesis space out of predicates in <strong>F</strong> rather than programs.</li>\n</ul>\n<h1>Intelligence Metric</h1>\n<p>To assign intelligence to agents we need to add two ingredients:</p>\n<ul>\n<li>The decoding <strong>Q</strong>: {<strong>Y</strong>} -&gt; {bit-string}&nbsp;of the agent <strong>G</strong> from the universe <strong>Y</strong>. For example <strong>Q</strong> can read off the program loaded into <strong>M</strong> at time k=0.</li>\n<li>A utility function <strong>U</strong>: {<strong>Y</strong>} -&gt; [0, 1] representing <strong>G</strong>'s preferences. <strong>U</strong> has to be given by a definition in <strong>F</strong>. Note that <strong>N</strong> provides the ontology wrt which <strong>U</strong> is defined.</li>\n</ul>\n<div>It seems tempting to define the intelligence to be E<sub>QS</sub>(<strong>U</strong> | <strong>Q</strong>), the conditional expectation value of <strong>U</strong> for a given value of <strong>Q</strong> in the quasi-Solomonoff measure. However, this is wrong for roughly the same reasons <a href=\"http://wiki.lesswrong.com/wiki/Evidential_Decision_Theory\">EDT</a> is wrong (see <a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">previous post</a> for details).</div>\n<p>Instead, we define I(<strong>Q</strong><sub>0</sub>) := E<sub>QS</sub>(E<sub>max</sub>(<strong>U</strong>(<strong>Y</strong>(<strong>H</strong>))&nbsp;| \"<strong>Q</strong>(<strong>Y</strong>(<strong>H</strong>)) = <strong>Q</strong><sub>0</sub>\" is true)). Here the subscript max stands for maximal depth of analysis, as in the construction of absolute UDT value above.&nbsp;</p>\n<h2>Remarks</h2>\n<ul>\n<li>IMO the correct way to look at this is intelligence metric = value of decision for the decision problem \"what should I program into my robot?\". If <strong>N</strong> is a highly detailed model including \"me\" (the programmer of the AI), this literally becomes the case. However for theoretical analysis it is likely to be more convenient to work with simple <strong>N </strong>(also conceptually it leaves room for a \"purist\" notion of agent's intelligence, decoupled from the fine details of its creator). \n<ul>\n<li>As opposed to usual UDT, the algorithm (<strong>H</strong>) making the decision (<strong>Q</strong>) is not known with certainty. I think this represents a real uncertainty that has to be taken into account in decision problems in general: the decision-maker doesn't know her own algorithm. Since this \"introspective uncertainty\" is highly correlated with \"indexical\" uncertainty (uncertainty about the universe), it prevents us from absorbing the later into the utility function as proposed by <a href=\"/lw/jn2/preferences_without_existence/\">Coscott</a>.&nbsp;</li>\n</ul>\n</li>\n<li>For high values of <strong>t</strong>, <strong>G</strong> can improve its understanding of the universe by bootstrapping the knowledge it already has. This is not possible for low values of <strong>t</strong>. In other words, if I cannot trust my mind at all, I cannot deduce anything. This leads me to an interesting conjecture: There is a a critical value <strong>t</strong>*&nbsp;of <strong>t</strong> from which this bootstrapping becomes possible (the positive feedback look of knowledge becomes critical). I(<strong>Q</strong>) is non-smooth at&nbsp;<strong>t</strong>*&nbsp;(phase transition).</li>\n<li>If we wish to understand intelligence, it might be beneficial to decouple it from the choice of preferences. To achieve this we can introduce the preference formula as an unknown parameter in <strong>N</strong>. For example, if <strong>G</strong> is realized by a machine <strong>M</strong>, we can connect <strong>M</strong> to a data storage <strong>E</strong> whose content is left undetermined by <strong>N</strong>. We can then define <strong>U</strong> to be defined by the formula encoded in <strong>E </strong>at time k=0. This leads to I(<strong>Q</strong>) being a sort of \"general-purpose\" intelligence while avoiding the problems associated with reinforcement learning.</li>\n<li>As opposed to Legg-Hutter intelligence, there appears to be no simple explicit description for <strong>Q</strong>*&nbsp;maximizing I(<strong>Q</strong>) (e.g. among all programs of given length). This is not surprising, since computational cost considerations come into play. In this framework it appears to be inherently impossible to decouple the computational cost considerations: <strong>G</strong>'s computations have to be realized mechanistically and therefore cannot be free of time cost and side-effects.</li>\n<li>Ceteris paribus, <strong>Q</strong>* deals efficiently with problems like <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a>. The \"ceteris paribus\" conditional is necessary here since because of cost and side-effects of computations it is difficult to make absolute claims. However, it doesn't deal efficiently with counterfactual mugging in which <strong>G</strong> doesn't exist in the \"other universe\". This is because the ontology used for defining <strong>U</strong> (which is given by <strong>N</strong>) assumes <strong>G</strong> <em>does </em>exist. At least this is the case for simple ontologies like described above: possibly we can construct <strong>N</strong> in which <strong>G</strong> might or might not exist. Also, if <strong>G</strong> uses a <em>quantum</em> ontology (i.e. <strong>N</strong> describes the universe in terms of a wavefunction and <strong>U</strong> computes the quantum expectation value of an operator) then it <em>does</em>&nbsp;take into account other Everett universes in which <strong>G</strong> doesn't exist.</li>\n<li>For many choices of&nbsp;<strong>N </strong>(for example if the <strong>G</strong> is realized by a machine <strong>M</strong>), QS-induction assigns well-defined probabilities to <a href=\"/lw/19d/the_anthropic_trilemma/\">subjective expectations</a>, contrary to what is expected from UDT. However: \n<ul>\n<li>This is not the case for all <strong>N</strong>. In particular, if <strong>N</strong> admits destruction of <strong>M</strong> then <strong>M</strong>'s sensations after the point of destruction are not well-defined. Indeed, we better allow for destruction of <strong>M</strong> if we want <strong>G</strong>'s preferences to behave properly in such an event. That is, if we <em>don't</em>&nbsp;allow it we get a \"weak anvil problem\" in the sense that <strong>G</strong>&nbsp;experiences an <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">ontological crisis</a> when discovering its own mortality and the outcome of this crisis is not obvious. Note though that it is not the same as the original (\"strong\") anvil problem, for example <strong>G</strong> might come to the conclusion the dynamics of \"<strong>M</strong>'s ghost\" will be some sort of random.</li>\n<li>These probabilities probably depend significantly on <strong>N</strong> and don't amount to an elegant universal law for solving the anthropic trilemma.</li>\n<li>Indeed this framework is not completely \"updateless\", it is \"partially updated\" by the introduction of <strong>N</strong> and <strong>t</strong>. This suggests we might want the updates to be minimal in some sense, in particular <strong>t</strong> should be&nbsp;<strong>t</strong>*.</li>\n</ul>\n</li>\n<li>The framework suggests there is no conceptual problem with cosmologies in which <a href=\"https://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brains</a> are abundant.&nbsp;<strong>Q</strong>* wouldn't think it is a Boltzmann brain since the long address of Boltzmann brains within the universe makes the respective hypotheses complex thus suppressing them, even disregarding the suppression associated with <strong>N</strong>. I doubt this argument is original but I feel the framework validates it to some extent. \n<ul>\n</ul>\n</li>\n</ul>\n<p>&nbsp;</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2ZGCq44Wv8p4btyL7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 23, "extendedScore": null, "score": 1.5752027671889097e-06, "legacy": true, "legacyId": "25569", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Followup to: <a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">Intelligence Metrics and Decision Theory</a><br>Related to:&nbsp;<a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse: Reductionism as Engineering Problem</a></p>\n<p>A central problem in AGI is giving a formal definition of intelligence. <a href=\"http://www.hutter1.net/ai/aixigentle.pdf\">Marcus Hutter</a> has proposed <a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>&nbsp;as a model of perfectly intelligent agent. <a href=\"http://arxiv.org/abs/0712.3329\">Legg and Hutter</a>&nbsp;have defined a quantitative measure of intelligence applicable to any suitable formalized agent such that AIXI is the agent with maximal intelligence according to this measure.</p>\n<p>Legg-Hutter intelligence suffers from a number of problems I have <a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">previously</a> discussed, the most important being:</p>\n<ul>\n<li>The formalism is inherently Cartesian. Solving this problem is known as <a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalized induction</a>&nbsp;and it is discussed in detail <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">here</a>.</li>\n<li>The utility function Legg &amp; Hutter use is a formalization of reinforcement learning, while we would like to consider agents with arbitrary preferences. Moreover, a real AGI designed with reinforcement learning would tend to wrestle control of the reinforcement signal from the operators (<em>there must be a classic reference on this but I can't find it. Help?</em>). It is straightword to tweak to formalism to allow for any utility function which depends on the agent's sensations and actions, however we would like to be able to use any ontology for defining it.</li>\n</ul>\n<div><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" href=\"http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf\">Orseau and Ring</a>&nbsp;proposed a non-Cartesian intelligence metric however their formalism appears to be <em>too</em>&nbsp;general, in particular there is no <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff induction</a> or any analogue thereof, instead a completely general probability measure is used.</div>\n<div><br></div>\n<div><a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">My attempt</a> at defining a non-Cartesian intelligence metric ran into problems of decision-theoretic flavor. The way I tried to used <a href=\"http://wiki.lesswrong.com/wiki/UDT\">UDT</a>&nbsp;seems unsatisfactory, and later I tried a <a href=\"/lw/h93/metatickle_intelligence_metrics_and_friendly/\">different approach</a> related to metatickle EDT.&nbsp;</div>\n<div><br></div>\n<div>In this post, I claim to accomplish the following:</div>\n<div>\n<ul>\n<li>Define a formalism for logical uncertainty. <em>When I started writing this I thought this formalism might be novel but now I see it is essentially the same as that of <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">Benja</a>.</em></li>\n<li>Use this formalism to define a non-constructive formalization of UDT. By \"non-constructive\" I mean something that assigns values to actions rather than a specific algorithm like <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">here</a>.</li>\n<li>Apply the formalization of UDT to my quasi-Solomonoff framework to yield an intelligence metric.</li>\n<li>Slightly modify my original definition of the quasi-Solomonoff measure so that the confidence of the innate model becomes a continuous rather than discrete parameter. This leads to an interesting conjecture.</li>\n<li>Propose a \"preference agnostic\" variant as an alternative to Legg &amp; Hutter's reinforcement learning.</li>\n<li>Discuss certain anthropic and decision-theoretic aspects.</li>\n</ul>\n<h1 id=\"Logical_Uncertainty\">Logical Uncertainty</h1>\n<p><em>The formalism introduced here was originally proposed by </em><em><a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">Benja</a>.</em></p>\n<p>Fix a formal system <strong>F</strong>. We want to be able to assign probabilities to statements <strong>s</strong>&nbsp;in <strong>F</strong>, taking into account limited computing resources. Fix <strong>D</strong>&nbsp;a natural number related to the amount of computing resources that I call \"depth of analysis\".</p>\n<p>Define P<sub>0</sub>(<strong>s</strong>) := 1/2 for all <strong>s</strong>&nbsp;to be our initial prior, i.e. each statement's truth value is decided by a fair coin toss. Now define <br>P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) := P<sub>0</sub>(<strong>s</strong> | there are no contradictions of length &lt;= <strong>D</strong>).</p>\n<p>Consider <strong>X</strong> to be a number in [0, 1] given by a definition in <strong>F</strong>. Then <strong>d</strong><sub>k</sub>(<strong>X</strong>) := \"The k-th digit of the binary expansion of&nbsp;<strong>X</strong> is 1\"&nbsp;is a statement in <strong>F</strong>. We define E<sub><strong>D</strong></sub>(<strong>X</strong>) :=&nbsp;\u03a3<sub>k</sub> 2<sup>-k</sup> P<sub style=\"font-weight: bold;\">D</sub>(<strong>d</strong><sub>k</sub>(<strong>X</strong>)).</p>\n<h2 id=\"Remarks\">Remarks</h2>\n<ul>\n<li>Clearly if <strong>s</strong> is provable in&nbsp;<strong>F</strong>&nbsp;then for&nbsp;<strong>D</strong>&nbsp;&gt;&gt; 0,&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) = 1. Similarly if \"not <strong>s</strong>\" is provable in&nbsp;<strong>F</strong>&nbsp;then for&nbsp;<strong>D</strong>&nbsp;&gt;&gt; 0,&nbsp;<br>P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) = 0.</li>\n<li>If each digit of&nbsp;<strong>X</strong>&nbsp;is decidable in <strong>F</strong>&nbsp;then lim<sub><strong>D&nbsp;</strong>-&gt; inf</sub> E<sub><strong>D</strong></sub>(<strong>X</strong>) exists and equals the value of <strong>X</strong> according to <strong>F</strong>.</li>\n<li>For <strong>s</strong> of length &gt; <strong>D</strong>,&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) = 1/2 since no contradiction of length &lt;= <strong>D</strong> can involve <strong>s</strong>.</li>\n<li>It is an interesting <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/7agz\">question</a>&nbsp;whether&nbsp;lim<sub><strong>D&nbsp;</strong>-&gt; inf</sub>&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(<strong>s</strong>) exists for any <strong>s</strong>. It seems false that this limit always exists <em>and</em>&nbsp;equals 0 or 1, i.e. this formalism is <em>not</em>&nbsp;a loophole in Goedel incompleteness. To see this consider statements that require a high (arithmetical hierarchy) order halting oracle to decide.</li>\n<li>In computational terms, <strong>D</strong> corresponds to non-deterministic spatial complexity. It is spatial since we assign truth values simultaneously to all statements so in any given contradiction it is enough to retain the \"thickest\" step. It is non-deterministic since it's enough for a contradiction to exists, we don't have an actual computation which produces it. I suspect this can be made more formal using the <a href=\"https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence\">Curry-Howard isomorphism</a>, unfortunately I don't understand the latter yet.</li>\n</ul>\n<h1 id=\"Non_Constructive_UDT\">Non-Constructive UDT</h1>\n<p>Consider <strong>A</strong> a decision algorithm for optimizing utility&nbsp;<strong>U</strong>, producing an output (\"decision\") which is an element of&nbsp;<strong>C</strong>. Here&nbsp;<strong>U</strong>&nbsp;is just a constant defined in <strong>F</strong>. We define the <strong>U</strong>-value of <strong>c</strong>&nbsp;in <strong>C</strong> for <strong>A</strong>&nbsp;at depth of analysis <strong>D</strong>&nbsp;to be <br>V<sub><strong>D</strong></sub>(<strong>c</strong>, <strong>A</strong>; <strong>U</strong>) := E<sub><strong>D</strong></sub>(<strong>U</strong> | \"<strong>A</strong> produces <strong>c</strong>\" is true). It is only well defined as long as \"<strong>A</strong>&nbsp;doesn't&nbsp;produce&nbsp;<strong>c</strong>\" cannot be proved at depth of analysis <strong>D</strong>&nbsp;i.e. P<sub style=\"font-weight: bold;\">D</sub>(\"<strong>A</strong>&nbsp;produces&nbsp;<strong>c</strong>\") &gt; 0. We define the <em>absolute</em>&nbsp;<strong>U</strong>-value of&nbsp;<strong>c</strong>&nbsp;for&nbsp;<strong>A</strong>&nbsp;to be <br>V(<strong>c</strong>,&nbsp;<strong>A</strong>;&nbsp;<strong>U</strong>) := E<sub><strong>D</strong>(<strong>c</strong>, <strong>A</strong>)</sub>(<strong>U</strong>&nbsp;| \"<strong>A</strong>&nbsp;produces&nbsp;<strong>c</strong>\" is true) where <strong>D</strong>(<strong>c</strong>, <strong>A</strong>) := max {<strong>D</strong> |&nbsp;P<sub style=\"font-weight: bold;\">D</sub>(\"<strong>A</strong>&nbsp;produces&nbsp;<strong>c</strong>\") &gt; 0}. Of course&nbsp;<strong>D</strong>(<strong>c</strong>,&nbsp;<strong>A</strong>) can be infinite in which case E<span style=\"font-size: 11.199999809265137px;\"><sub>inf</sub></span>(...) is understood to mean lim<sub><strong>D</strong> -&gt; inf</sub> E<sub><strong>D</strong></sub>(...).</p>\n<p>For example V(<strong>c</strong>,&nbsp;<strong>A</strong>;&nbsp;<strong>U</strong>) yields the natural values for <strong>A</strong> an <a href=\"http://wiki.lesswrong.com/wiki/Ambient_decision_theory\">ambient control algorithm</a>&nbsp;applied to e.g. a simple model of Newcomb's problem. &nbsp;To see this note that given <strong>A</strong>'s output the value of&nbsp;<strong>U </strong>can be determined at low depths of analysis whereas the output of <strong>A</strong> requires a very high depth of analysis to determine.</p>\n<h1 id=\"Naturalized_Induction\">Naturalized Induction</h1>\n<p>Our starting point is the \"innate model\" <strong>N</strong>: a certain a priori model of the universe including the agent <strong>G</strong>. This model encodes the universe as a sequence of natural numbers <strong>Y</strong> = (<strong>y</strong><sub>k</sub>) which obeys either specific deterministic or non-deterministic dynamics or at least some constraints on the possible histories. It may or may not include information on the initial conditions. For example, <strong>N</strong> can describe the universe as a universal Turing machine <strong>M</strong> (representing <strong>G</strong>) with special \"sensory\" registers <strong>e</strong>. <strong>N</strong> constraints the dynamics to be compatible with the rules of the Turing machine but leaves unspecified the behavior of <strong>e</strong>. Alternatively, <strong>N</strong> can contain in addition to <strong>M</strong> a non-trivial model of the environment. Or <strong>N</strong> can be a cellular automaton with the agent corresponding to a certain collection of cells.</p>\n<p>However, <strong>G</strong>'s confidence in <strong>N</strong> is limited: otherwise it wouldn't need induction. We cannot start with 0 confidence: it's impossible to program a machine if you don't have even a guess of how it works. Instead we introduce a positive real number&nbsp;<strong>t</strong> which represents the timescale over which <strong>N</strong> is expected to hold. We then assign to each hypothesis <strong>H</strong> about&nbsp;<strong>Y</strong>&nbsp;(you can think about them as programs which compute&nbsp;<strong>y</strong><sub>k</sub> given <strong>y</strong><sub>j</sub> for j &lt; k; more on that later) the weight QS(<strong>H</strong>) := 2<sup>-L(<strong>H</strong>)&nbsp;</sup>(1 - e<sup>-t(<strong>H</strong>)/<strong>t</strong></sup>). Here L(<strong>H</strong>) is the length of <strong>H</strong>'s encoding in bits and t(<strong>H</strong>) is the time during which <strong>H</strong> remains compatible with <strong>N</strong>. This is defined for <strong>N</strong> of deterministic / constraint type but can be generalized to stochastic <strong>N</strong>.&nbsp;</p>\n<p>The weights QS(<strong>H</strong>) define a probability measure on the space of hypotheses which induces a probability measure on the space of histories <strong>Y</strong>. Thus we get an alternative to Solomonoff induction which allows for <strong>G</strong> to be a mechanistic part of the universe, at the price of introducing <strong>N</strong> and <strong>t</strong>.&nbsp;</p>\n<h2 id=\"Remarks1\">Remarks</h2>\n<ul>\n<li>Note that time is discrete in this formalism but <strong>t</strong> is continuous.</li>\n<li>Since we're later going to use logical uncertainties wrt the formal system <strong>F</strong>, it is tempting to construct the hypothesis space out of predicates in <strong>F</strong> rather than programs.</li>\n</ul>\n<h1 id=\"Intelligence_Metric\">Intelligence Metric</h1>\n<p>To assign intelligence to agents we need to add two ingredients:</p>\n<ul>\n<li>The decoding <strong>Q</strong>: {<strong>Y</strong>} -&gt; {bit-string}&nbsp;of the agent <strong>G</strong> from the universe <strong>Y</strong>. For example <strong>Q</strong> can read off the program loaded into <strong>M</strong> at time k=0.</li>\n<li>A utility function <strong>U</strong>: {<strong>Y</strong>} -&gt; [0, 1] representing <strong>G</strong>'s preferences. <strong>U</strong> has to be given by a definition in <strong>F</strong>. Note that <strong>N</strong> provides the ontology wrt which <strong>U</strong> is defined.</li>\n</ul>\n<div>It seems tempting to define the intelligence to be E<sub>QS</sub>(<strong>U</strong> | <strong>Q</strong>), the conditional expectation value of <strong>U</strong> for a given value of <strong>Q</strong> in the quasi-Solomonoff measure. However, this is wrong for roughly the same reasons <a href=\"http://wiki.lesswrong.com/wiki/Evidential_Decision_Theory\">EDT</a> is wrong (see <a href=\"/lw/h4x/intelligence_metrics_and_decision_theories/\">previous post</a> for details).</div>\n<p>Instead, we define I(<strong>Q</strong><sub>0</sub>) := E<sub>QS</sub>(E<sub>max</sub>(<strong>U</strong>(<strong>Y</strong>(<strong>H</strong>))&nbsp;| \"<strong>Q</strong>(<strong>Y</strong>(<strong>H</strong>)) = <strong>Q</strong><sub>0</sub>\" is true)). Here the subscript max stands for maximal depth of analysis, as in the construction of absolute UDT value above.&nbsp;</p>\n<h2 id=\"Remarks2\">Remarks</h2>\n<ul>\n<li>IMO the correct way to look at this is intelligence metric = value of decision for the decision problem \"what should I program into my robot?\". If <strong>N</strong> is a highly detailed model including \"me\" (the programmer of the AI), this literally becomes the case. However for theoretical analysis it is likely to be more convenient to work with simple <strong>N </strong>(also conceptually it leaves room for a \"purist\" notion of agent's intelligence, decoupled from the fine details of its creator). \n<ul>\n<li>As opposed to usual UDT, the algorithm (<strong>H</strong>) making the decision (<strong>Q</strong>) is not known with certainty. I think this represents a real uncertainty that has to be taken into account in decision problems in general: the decision-maker doesn't know her own algorithm. Since this \"introspective uncertainty\" is highly correlated with \"indexical\" uncertainty (uncertainty about the universe), it prevents us from absorbing the later into the utility function as proposed by <a href=\"/lw/jn2/preferences_without_existence/\">Coscott</a>.&nbsp;</li>\n</ul>\n</li>\n<li>For high values of <strong>t</strong>, <strong>G</strong> can improve its understanding of the universe by bootstrapping the knowledge it already has. This is not possible for low values of <strong>t</strong>. In other words, if I cannot trust my mind at all, I cannot deduce anything. This leads me to an interesting conjecture: There is a a critical value <strong>t</strong>*&nbsp;of <strong>t</strong> from which this bootstrapping becomes possible (the positive feedback look of knowledge becomes critical). I(<strong>Q</strong>) is non-smooth at&nbsp;<strong>t</strong>*&nbsp;(phase transition).</li>\n<li>If we wish to understand intelligence, it might be beneficial to decouple it from the choice of preferences. To achieve this we can introduce the preference formula as an unknown parameter in <strong>N</strong>. For example, if <strong>G</strong> is realized by a machine <strong>M</strong>, we can connect <strong>M</strong> to a data storage <strong>E</strong> whose content is left undetermined by <strong>N</strong>. We can then define <strong>U</strong> to be defined by the formula encoded in <strong>E </strong>at time k=0. This leads to I(<strong>Q</strong>) being a sort of \"general-purpose\" intelligence while avoiding the problems associated with reinforcement learning.</li>\n<li>As opposed to Legg-Hutter intelligence, there appears to be no simple explicit description for <strong>Q</strong>*&nbsp;maximizing I(<strong>Q</strong>) (e.g. among all programs of given length). This is not surprising, since computational cost considerations come into play. In this framework it appears to be inherently impossible to decouple the computational cost considerations: <strong>G</strong>'s computations have to be realized mechanistically and therefore cannot be free of time cost and side-effects.</li>\n<li>Ceteris paribus, <strong>Q</strong>* deals efficiently with problems like <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a>. The \"ceteris paribus\" conditional is necessary here since because of cost and side-effects of computations it is difficult to make absolute claims. However, it doesn't deal efficiently with counterfactual mugging in which <strong>G</strong> doesn't exist in the \"other universe\". This is because the ontology used for defining <strong>U</strong> (which is given by <strong>N</strong>) assumes <strong>G</strong> <em>does </em>exist. At least this is the case for simple ontologies like described above: possibly we can construct <strong>N</strong> in which <strong>G</strong> might or might not exist. Also, if <strong>G</strong> uses a <em>quantum</em> ontology (i.e. <strong>N</strong> describes the universe in terms of a wavefunction and <strong>U</strong> computes the quantum expectation value of an operator) then it <em>does</em>&nbsp;take into account other Everett universes in which <strong>G</strong> doesn't exist.</li>\n<li>For many choices of&nbsp;<strong>N </strong>(for example if the <strong>G</strong> is realized by a machine <strong>M</strong>), QS-induction assigns well-defined probabilities to <a href=\"/lw/19d/the_anthropic_trilemma/\">subjective expectations</a>, contrary to what is expected from UDT. However: \n<ul>\n<li>This is not the case for all <strong>N</strong>. In particular, if <strong>N</strong> admits destruction of <strong>M</strong> then <strong>M</strong>'s sensations after the point of destruction are not well-defined. Indeed, we better allow for destruction of <strong>M</strong> if we want <strong>G</strong>'s preferences to behave properly in such an event. That is, if we <em>don't</em>&nbsp;allow it we get a \"weak anvil problem\" in the sense that <strong>G</strong>&nbsp;experiences an <a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\">ontological crisis</a> when discovering its own mortality and the outcome of this crisis is not obvious. Note though that it is not the same as the original (\"strong\") anvil problem, for example <strong>G</strong> might come to the conclusion the dynamics of \"<strong>M</strong>'s ghost\" will be some sort of random.</li>\n<li>These probabilities probably depend significantly on <strong>N</strong> and don't amount to an elegant universal law for solving the anthropic trilemma.</li>\n<li>Indeed this framework is not completely \"updateless\", it is \"partially updated\" by the introduction of <strong>N</strong> and <strong>t</strong>. This suggests we might want the updates to be minimal in some sense, in particular <strong>t</strong> should be&nbsp;<strong>t</strong>*.</li>\n</ul>\n</li>\n<li>The framework suggests there is no conceptual problem with cosmologies in which <a href=\"https://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brains</a> are abundant.&nbsp;<strong>Q</strong>* wouldn't think it is a Boltzmann brain since the long address of Boltzmann brains within the universe makes the respective hypotheses complex thus suppressing them, even disregarding the suppression associated with <strong>N</strong>. I doubt this argument is original but I feel the framework validates it to some extent. \n<ul>\n</ul>\n</li>\n</ul>\n<p>&nbsp;</p>\n</div>", "sections": [{"title": "Logical Uncertainty", "anchor": "Logical_Uncertainty", "level": 1}, {"title": "Remarks", "anchor": "Remarks", "level": 2}, {"title": "Non-Constructive UDT", "anchor": "Non_Constructive_UDT", "level": 1}, {"title": "Naturalized Induction", "anchor": "Naturalized_Induction", "level": 1}, {"title": "Remarks", "anchor": "Remarks1", "level": 2}, {"title": "Intelligence Metric", "anchor": "Intelligence_Metric", "level": 1}, {"title": "Remarks", "anchor": "Remarks2", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vPtMSvnF8B5hM5LdL", "3WuAjWMtxQwTxr2Qn", "cL3ATdvGZ5kTy8AB4", "PgKADaJE4ERjtMtP9", "NvwJMQvfu9hbBdG6d", "y7jZ9BLEeuNTzgAE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T16:28:42.944Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Yale", "slug": "new-lw-meetup-yale", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:38.301Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S2xbQSYvTjyQZQJyT/new-lw-meetup-yale", "pageUrlRelative": "/posts/S2xbQSYvTjyQZQJyT/new-lw-meetup-yale", "linkUrl": "https://www.lesswrong.com/posts/S2xbQSYvTjyQZQJyT/new-lw-meetup-yale", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Yale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Yale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS2xbQSYvTjyQZQJyT%2Fnew-lw-meetup-yale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Yale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS2xbQSYvTjyQZQJyT%2Fnew-lw-meetup-yale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS2xbQSYvTjyQZQJyT%2Fnew-lw-meetup-yale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 535, "htmlBody": "<p><strong>This summary was posted to LW main on February 14th. The following week's summary is <a href=\"/lw/jqc/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/wn\">Yale: Initial Meetup:&nbsp;<span class=\"date\">16 February 2014 02:00PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/wq\">Bratislava Meetup X.:&nbsp;<span class=\"date\">24 February 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/wp\">Hamburg - about Procrastination:&nbsp;<span class=\"date\">21 February 2014 </span>07:00PM+1:00 (MET)</a></li>\n<li><a href=\"/meetups/wr\">Montreal Less Wrong - Easy Lifehacks:&nbsp;<span class=\"date\">19 February 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/wg\">Princeton NJ Meetup:&nbsp;<span class=\"date\">22 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/wo\">Saint Petersburg sunday meetup:&nbsp;<span class=\"date\">16 February 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/w4\">Sydney Meetup: February:&nbsp;<span class=\"date\">26 February 2014 06:30PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">15 February 2020 01:30PM</span></a></li>\n<li><a href=\"/meetups/w6\">[Berlin] Community Weekend in Berlin:&nbsp;<span class=\"date\">11 April 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/wm\">London VOI meetup 16/2, plus socials 9/2 and 23/2:&nbsp;<span class=\"date\">23 February 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/w9\">Vienna:&nbsp;<span class=\"date\">15 February 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S2xbQSYvTjyQZQJyT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5754904976297221e-06, "legacy": true, "legacyId": "25502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CQNvSuAqvwGKwzmRp", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T16:43:47.648Z", "modifiedAt": null, "url": null, "title": "Meetup: Philadelphia, February 23: Debugging in Meatspace", "slug": "meetup-philadelphia-february-23-debugging-in-meatspace", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8PyQh9XN7WyjjAup5/meetup-philadelphia-february-23-debugging-in-meatspace", "pageUrlRelative": "/posts/8PyQh9XN7WyjjAup5/meetup-philadelphia-february-23-debugging-in-meatspace", "linkUrl": "https://www.lesswrong.com/posts/8PyQh9XN7WyjjAup5/meetup-philadelphia-february-23-debugging-in-meatspace", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Philadelphia%2C%20February%2023%3A%20Debugging%20in%20Meatspace&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Philadelphia%2C%20February%2023%3A%20Debugging%20in%20Meatspace%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8PyQh9XN7WyjjAup5%2Fmeetup-philadelphia-february-23-debugging-in-meatspace%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Philadelphia%2C%20February%2023%3A%20Debugging%20in%20Meatspace%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8PyQh9XN7WyjjAup5%2Fmeetup-philadelphia-february-23-debugging-in-meatspace", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8PyQh9XN7WyjjAup5%2Fmeetup-philadelphia-february-23-debugging-in-meatspace", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\">1:00 PM, February 23, Nam Phuong (Vietnamese restaurant),&nbsp;</span><span style=\"font-size: 13px; text-align: justify; color: #222222; font-family: arial, sans-serif; line-height: 16px;\">&nbsp;</span><span style=\"font-size: 13px; text-align: justify; color: #222222; font-family: arial, sans-serif; line-height: 16px;\">1100 Washington Ave, Philadelphia, PA 19147 (west side of street, just south of Washington)</span></p>\n<p><a href=\"http://melancholia.typepad.com/melancholia/2009/07/do-easy.html&lt;/a&gt;\">The Discipline of DE</a> by William Burroughs.</p>\n<blockquote>DE is a way of doing. It is a way of doing everything you do. DE simply means doing whatever you do in the easiest most relaxed way you can manage which is also the quickest and most efficient way, as you will find as you advance in DE.  You can start right now tidying up your flat, moving furniture or books, washing dishes, making tea, sorting papers. Consider the weight of objects: exactly how much force is needed to get the object from here to there? Consider its shape and texture and function. Where exactly does it belong? Use just the amount of force necessary to get the object from here to there. Don't fumble, jerk, grab an object. Drop cool possessive fingers onto it like a gentle old cop making a soft arrest.</blockquote>\n<p><a href=\"http://nancylebov.livejournal.com/604306.html\">Some related links</a>.</p>\n<p>&nbsp;</p>\n<p>If you want to be on a mailing list for the meetup, here's the <a href=\"https://groups.google.com/forum/#!forum/lesswrong-philadelphia\">google group</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8PyQh9XN7WyjjAup5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5755082003574569e-06, "legacy": true, "legacyId": "25573", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T19:02:31.946Z", "modifiedAt": null, "url": null, "title": "LINK-How we make our depression worse", "slug": "link-how-we-make-our-depression-worse", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.868Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lom6AQiBFSCd4Jx3m/link-how-we-make-our-depression-worse", "pageUrlRelative": "/posts/Lom6AQiBFSCd4Jx3m/link-how-we-make-our-depression-worse", "linkUrl": "https://www.lesswrong.com/posts/Lom6AQiBFSCd4Jx3m/link-how-we-make-our-depression-worse", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK-How%20we%20make%20our%20depression%20worse&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK-How%20we%20make%20our%20depression%20worse%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLom6AQiBFSCd4Jx3m%2Flink-how-we-make-our-depression-worse%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK-How%20we%20make%20our%20depression%20worse%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLom6AQiBFSCd4Jx3m%2Flink-how-we-make-our-depression-worse", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLom6AQiBFSCd4Jx3m%2Flink-how-we-make-our-depression-worse", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>http://www.alternet.org/print/books/youre-making-your-depression-worse-self-help-bringing-us-down</p>\n<p>TL;DR:</p>\n<blockquote>\n<p>only a human being can feel bad about feeling bad</p>\n</blockquote>\n<p>results in a positive feedback loop pushing people into depression.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lom6AQiBFSCd4Jx3m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 14, "extendedScore": null, "score": 1.5756711022343075e-06, "legacy": true, "legacyId": "25575", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T20:55:43.256Z", "modifiedAt": null, "url": null, "title": "RESCHEDULED: NYC Rationality Megameetup and Unconference. NOW: 4/5 - 4/6", "slug": "rescheduled-nyc-rationality-megameetup-and-unconference-now", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KenChen", "createdAt": "2011-02-16T18:02:23.420Z", "isAdmin": false, "displayName": "KenChen"}, "userId": "Tay9Y5o7ehACBHeqc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vzPDzFh2SRH4QnJ63/rescheduled-nyc-rationality-megameetup-and-unconference-now", "pageUrlRelative": "/posts/vzPDzFh2SRH4QnJ63/rescheduled-nyc-rationality-megameetup-and-unconference-now", "linkUrl": "https://www.lesswrong.com/posts/vzPDzFh2SRH4QnJ63/rescheduled-nyc-rationality-megameetup-and-unconference-now", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20RESCHEDULED%3A%20NYC%20Rationality%20Megameetup%20and%20Unconference.%20NOW%3A%204%2F5%20-%204%2F6&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARESCHEDULED%3A%20NYC%20Rationality%20Megameetup%20and%20Unconference.%20NOW%3A%204%2F5%20-%204%2F6%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvzPDzFh2SRH4QnJ63%2Frescheduled-nyc-rationality-megameetup-and-unconference-now%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=RESCHEDULED%3A%20NYC%20Rationality%20Megameetup%20and%20Unconference.%20NOW%3A%204%2F5%20-%204%2F6%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvzPDzFh2SRH4QnJ63%2Frescheduled-nyc-rationality-megameetup-and-unconference-now", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvzPDzFh2SRH4QnJ63%2Frescheduled-nyc-rationality-megameetup-and-unconference-now", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 18px; text-align: justify;\">We are rescheduling the megameetup, to accommodate more members of the NYC community, and to be closer to the date of the next </span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 18px; text-align: justify;\" href=\"http://rationality.org/workshops/\">CFAR NY workshop</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 18px; text-align: justify;\">.</span></p>\n<div id=\"entry_t3_jp6\" class=\"content clear\" style=\"color: #000000; font-family: Arial, Helvetica, sans-serif; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 18px; orphans: auto; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: #ffffff;\">\n<div class=\"md\" style=\"font-size: small;\">\n<div>\n<p class=\"p1\" style=\"margin: 0px 0px 1em;\">Reposting the description with updated details here:</p>\n<p class=\"p1\" style=\"margin: 0px 0px 1em; padding-left: 30px;\">On the weekend of April 5-6, the NYC community will be hosting a megameetup and rationality unconference. Everyone who can make the trip is strongly encouraged to come. There will be presentations, interesting discussions, and cake.</p>\n<p class=\"p1\" style=\"margin: 0px 0px 1em; padding-left: 30px;\">This will be an <a href=\"http://en.wikipedia.org/wiki/Unconference\">unconference</a>, and as such will be a highly participatory event. There won't be scheduled presentations -- participants will sign up on the day of the event (15-minutes per presentation) in order to present.</p>\n<p class=\"p1\" style=\"margin: 0px 0px 1em; padding-left: 30px;\">If you'd like to help out, or if you need crash space, please send me a PM.</p>\n<p class=\"p1\" style=\"margin: 0px 0px 1em;\">If you already commented on whether you are coming, please post again on the meetup page here: http://lesswrong.com/meetups/x3</p>\n<p class=\"p1\" style=\"margin: 0px 0px 1em;\"><strong style=\"font-weight: bold;\">Location:</strong></p>\n<address><span style=\"font-style: normal;\">Highgarden<br />851 Park Place<br />Brooklyn, NY 11216</span></address><address><span style=\"font-style: normal;\"><br /></span></address></div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vzPDzFh2SRH4QnJ63", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5758040264695644e-06, "legacy": true, "legacyId": "25577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-21T21:04:58.409Z", "modifiedAt": null, "url": null, "title": "Meetup : NYC Rationality Megameetup and Unconference: April 5-6", "slug": "meetup-nyc-rationality-megameetup-and-unconference-april-5-6", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:33.598Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KenChen", "createdAt": "2011-02-16T18:02:23.420Z", "isAdmin": false, "displayName": "KenChen"}, "userId": "Tay9Y5o7ehACBHeqc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N24g3vWYgAyyDBYaz/meetup-nyc-rationality-megameetup-and-unconference-april-5-6", "pageUrlRelative": "/posts/N24g3vWYgAyyDBYaz/meetup-nyc-rationality-megameetup-and-unconference-april-5-6", "linkUrl": "https://www.lesswrong.com/posts/N24g3vWYgAyyDBYaz/meetup-nyc-rationality-megameetup-and-unconference-april-5-6", "postedAtFormatted": "Friday, February 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20NYC%20Rationality%20Megameetup%20and%20Unconference%3A%20April%205-6&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20NYC%20Rationality%20Megameetup%20and%20Unconference%3A%20April%205-6%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN24g3vWYgAyyDBYaz%2Fmeetup-nyc-rationality-megameetup-and-unconference-april-5-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20NYC%20Rationality%20Megameetup%20and%20Unconference%3A%20April%205-6%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN24g3vWYgAyyDBYaz%2Fmeetup-nyc-rationality-megameetup-and-unconference-april-5-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN24g3vWYgAyyDBYaz%2Fmeetup-nyc-rationality-megameetup-and-unconference-april-5-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 417, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x3'>NYC Rationality Megameetup and Unconference: April 5-6</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 April 2014 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Highgarden, 851 Park Place, Brooklyn, NY 11216</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>Saturday</strong></p>\n\n<p>3:30pm Opening ceremony</p>\n\n<p>4:00pm \"Legal Concerns Surrounding Startups\" by Dave Carlson</p>\n\n<p>6:30pm Dinner ordering</p>\n\n<p>7:00pm \"Goal Factoring Workshop\" by Ben Landau-Taylor</p>\n\n<p>9:00pm Stupid questions live thread</p>\n\n<p>10:00pm Uncake</p>\n\n<p>10:30pm Open discussion</p>\n\n<hr />\n\n<p><strong>Sunday</strong></p>\n\n<p>9:00am Breakfast served</p>\n\n<p>10:00am Unconference</p>\n\n<p>1:30pm Lunch break</p>\n\n<p>2:30pm Startup idea discussion</p>\n\n<p>5:00pm Closing ceremony</p>\n\n<hr />\n\n<p>Attendees are encouraged to arrive early Saturday afternoon to have time to settle in.</p>\n\n<p><strong>\"Legal Concerns Surrounding Startups\" by Dave Carlson</strong></p>\n\n<p>Attorney David Carlson will discuss general legal issues that arise when forming a start-up, including:\n - Structuring the start-up.\n - Precautions when seeking VC funding.\n - Common traps for the unwary when dealing with legal documents.</p>\n\n<p>The presentation will be very informal; the goal is to focus on whichever topics the audience is most interested in. The discussion may not be recorded.</p>\n\n<p>Disclaimer (because of course there is a disclaimer):</p>\n\n<p>The presentation will, necessarily, focus on <em>general</em> legal issues rather than on anyone's <em>specific</em> legal problems. Accordingly, the presentation will not constitute \"legal advice\" for specific situations, which depend on the evaluation of precise factual circumstances.</p>\n\n<p><strong>Goal Factoring Workshop by Ben Landau-Taylor</strong></p>\n\n<p>Do you ever find yourself saying, \u201cUnfortunately, I have to\u2026 X?\u201d Goal factoring teaches you to systematically break down everything obvious and non-obvious you\u2019re accomplishing, and ask about ways you could achieve those factors separately and more effectively \u2013 a new perspective on everything from reading habits to email etiquette to outings with friends.</p>\n\n<p><strong>Stupid Rationality Questions</strong></p>\n\n<p>On Saturday night, we'll answer all your Stupid Rationality Questions! Ever have questions that you were too embarrassed to ask? Submit them now using the link below, and we'll discuss those in person.</p>\n\n<p>Submit your stupid rationality questions anonymously:\n<a href=\"https://docs.google.com/forms/d/1FDTZwQIRLdNa_neTsdgExVi5cTICLrqvhmAxLCE1Thw/viewform\" rel=\"nofollow\">https://docs.google.com/forms/d/1FDTZwQIRLdNa_neTsdgExVi5cTICLrqvhmAxLCE1Thw/viewform</a></p>\n\n<p><strong>Unconference</strong></p>\n\n<p>On Sunday, we'll be holding an unconference, starting at 10:00 AM. This will be a highly participatory event, and participants are invited to give 15-minute talks on whatever they find interesting. Some topics people are preparing include:</p>\n\n<p>\u2022 Running a kickstarter</p>\n\n<p>\u2022 Talent coefficients and the growth mindset</p>\n\n<p>\u2022 Current landscape of the Effective Altruism community</p>\n\n<p><strong>Meta</strong></p>\n\n<p>Facebook event: <a href=\"https://www.facebook.com/events/723051941048754\" rel=\"nofollow\">https://www.facebook.com/events/723051941048754</a></p>\n\n<p>Please donate! Donations will be used to fund this event and future events at Highgarden: 1FtWbh8GyAcDewSy32geCBgsFoHEN7UoZ</p>\n\n<p>If you think you might come, please leave a comment and a confidence estimate. For example, if you would bring two guests and are 75% certain you will come, your comment might contain \"me +2, 75%.\" If you'd like to help out, or if you need crash space, please send me a PM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x3'>NYC Rationality Megameetup and Unconference: April 5-6</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N24g3vWYgAyyDBYaz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5758148931523983e-06, "legacy": true, "legacyId": "25578", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___NYC_Rationality_Megameetup_and_Unconference__April_5_6\">Discussion article for the meetup : <a href=\"/meetups/x3\">NYC Rationality Megameetup and Unconference: April 5-6</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 April 2014 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Highgarden, 851 Park Place, Brooklyn, NY 11216</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong id=\"Saturday\">Saturday</strong></p>\n\n<p>3:30pm Opening ceremony</p>\n\n<p>4:00pm \"Legal Concerns Surrounding Startups\" by Dave Carlson</p>\n\n<p>6:30pm Dinner ordering</p>\n\n<p>7:00pm \"Goal Factoring Workshop\" by Ben Landau-Taylor</p>\n\n<p>9:00pm Stupid questions live thread</p>\n\n<p>10:00pm Uncake</p>\n\n<p>10:30pm Open discussion</p>\n\n<hr>\n\n<p><strong id=\"Sunday\">Sunday</strong></p>\n\n<p>9:00am Breakfast served</p>\n\n<p>10:00am Unconference</p>\n\n<p>1:30pm Lunch break</p>\n\n<p>2:30pm Startup idea discussion</p>\n\n<p>5:00pm Closing ceremony</p>\n\n<hr>\n\n<p>Attendees are encouraged to arrive early Saturday afternoon to have time to settle in.</p>\n\n<p><strong id=\"_Legal_Concerns_Surrounding_Startups__by_Dave_Carlson\">\"Legal Concerns Surrounding Startups\" by Dave Carlson</strong></p>\n\n<p>Attorney David Carlson will discuss general legal issues that arise when forming a start-up, including:\n - Structuring the start-up.\n - Precautions when seeking VC funding.\n - Common traps for the unwary when dealing with legal documents.</p>\n\n<p>The presentation will be very informal; the goal is to focus on whichever topics the audience is most interested in. The discussion may not be recorded.</p>\n\n<p>Disclaimer (because of course there is a disclaimer):</p>\n\n<p>The presentation will, necessarily, focus on <em>general</em> legal issues rather than on anyone's <em>specific</em> legal problems. Accordingly, the presentation will not constitute \"legal advice\" for specific situations, which depend on the evaluation of precise factual circumstances.</p>\n\n<p><strong id=\"Goal_Factoring_Workshop_by_Ben_Landau_Taylor\">Goal Factoring Workshop by Ben Landau-Taylor</strong></p>\n\n<p>Do you ever find yourself saying, \u201cUnfortunately, I have to\u2026 X?\u201d Goal factoring teaches you to systematically break down everything obvious and non-obvious you\u2019re accomplishing, and ask about ways you could achieve those factors separately and more effectively \u2013 a new perspective on everything from reading habits to email etiquette to outings with friends.</p>\n\n<p><strong id=\"Stupid_Rationality_Questions\">Stupid Rationality Questions</strong></p>\n\n<p>On Saturday night, we'll answer all your Stupid Rationality Questions! Ever have questions that you were too embarrassed to ask? Submit them now using the link below, and we'll discuss those in person.</p>\n\n<p>Submit your stupid rationality questions anonymously:\n<a href=\"https://docs.google.com/forms/d/1FDTZwQIRLdNa_neTsdgExVi5cTICLrqvhmAxLCE1Thw/viewform\" rel=\"nofollow\">https://docs.google.com/forms/d/1FDTZwQIRLdNa_neTsdgExVi5cTICLrqvhmAxLCE1Thw/viewform</a></p>\n\n<p><strong id=\"Unconference\">Unconference</strong></p>\n\n<p>On Sunday, we'll be holding an unconference, starting at 10:00 AM. This will be a highly participatory event, and participants are invited to give 15-minute talks on whatever they find interesting. Some topics people are preparing include:</p>\n\n<p>\u2022 Running a kickstarter</p>\n\n<p>\u2022 Talent coefficients and the growth mindset</p>\n\n<p>\u2022 Current landscape of the Effective Altruism community</p>\n\n<p><strong id=\"Meta\">Meta</strong></p>\n\n<p>Facebook event: <a href=\"https://www.facebook.com/events/723051941048754\" rel=\"nofollow\">https://www.facebook.com/events/723051941048754</a></p>\n\n<p>Please donate! Donations will be used to fund this event and future events at Highgarden: 1FtWbh8GyAcDewSy32geCBgsFoHEN7UoZ</p>\n\n<p>If you think you might come, please leave a comment and a confidence estimate. For example, if you would bring two guests and are 75% certain you will come, your comment might contain \"me +2, 75%.\" If you'd like to help out, or if you need crash space, please send me a PM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___NYC_Rationality_Megameetup_and_Unconference__April_5_61\">Discussion article for the meetup : <a href=\"/meetups/x3\">NYC Rationality Megameetup and Unconference: April 5-6</a></h2>", "sections": [{"title": "Discussion article for the meetup : NYC Rationality Megameetup and Unconference: April 5-6", "anchor": "Discussion_article_for_the_meetup___NYC_Rationality_Megameetup_and_Unconference__April_5_6", "level": 1}, {"title": "Saturday", "anchor": "Saturday", "level": 2}, {"title": "Sunday", "anchor": "Sunday", "level": 2}, {"title": "\"Legal Concerns Surrounding Startups\" by Dave Carlson", "anchor": "_Legal_Concerns_Surrounding_Startups__by_Dave_Carlson", "level": 2}, {"title": "Goal Factoring Workshop by Ben Landau-Taylor", "anchor": "Goal_Factoring_Workshop_by_Ben_Landau_Taylor", "level": 2}, {"title": "Stupid Rationality Questions", "anchor": "Stupid_Rationality_Questions", "level": 2}, {"title": "Unconference", "anchor": "Unconference", "level": 2}, {"title": "Meta", "anchor": "Meta", "level": 2}, {"title": "Discussion article for the meetup : NYC Rationality Megameetup and Unconference: April 5-6", "anchor": "Discussion_article_for_the_meetup___NYC_Rationality_Megameetup_and_Unconference__April_5_61", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-22T05:16:30.782Z", "modifiedAt": null, "url": null, "title": "17 Rules to Make a Definition that Avoids the 37 Ways of Words Being Wrong", "slug": "17-rules-to-make-a-definition-that-avoids-the-37-ways-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:02.893Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mathnerd314", "createdAt": "2011-12-03T00:02:40.811Z", "isAdmin": false, "displayName": "mathnerd314"}, "userId": "wJRwChs8xYjdJPGiv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ya378WMGFjz7vKCRr/17-rules-to-make-a-definition-that-avoids-the-37-ways-of", "pageUrlRelative": "/posts/Ya378WMGFjz7vKCRr/17-rules-to-make-a-definition-that-avoids-the-37-ways-of", "linkUrl": "https://www.lesswrong.com/posts/Ya378WMGFjz7vKCRr/17-rules-to-make-a-definition-that-avoids-the-37-ways-of", "postedAtFormatted": "Saturday, February 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2017%20Rules%20to%20Make%20a%20Definition%20that%20Avoids%20the%2037%20Ways%20of%20Words%20Being%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A17%20Rules%20to%20Make%20a%20Definition%20that%20Avoids%20the%2037%20Ways%20of%20Words%20Being%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYa378WMGFjz7vKCRr%2F17-rules-to-make-a-definition-that-avoids-the-37-ways-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=17%20Rules%20to%20Make%20a%20Definition%20that%20Avoids%20the%2037%20Ways%20of%20Words%20Being%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYa378WMGFjz7vKCRr%2F17-rules-to-make-a-definition-that-avoids-the-37-ways-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYa378WMGFjz7vKCRr%2F17-rules-to-make-a-definition-that-avoids-the-37-ways-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 324, "htmlBody": "<p>Eliezer's <a title=\"Ch. 50 Author's Notes\" href=\"https://www.evernote.com/pub/adelenedawner/Eliezer#b=90390ce2-1356-4522-959e-a300957704c5&amp;st=p&amp;n=2c519b03-7a75-41cc-87aa-abac27d3bcef\">writing style</a> of A-&gt;B, then A, then B, though generally clear, results in a large amount of redundancy.</p>\n<p>In this post, I have attempted to reduce the number of rules needed to remember by half. The numbers are the rules from the <a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">original post</a>.</p>\n<p>So, without further ado, a good definition for a word:</p>\n<ol>\n<li>can be shown to be wrong<sup>37</sup> and is not the final<sup>13</sup> authority<sup>18 19</sup></li>\n<li>has strong justifications<sup>33</sup> for the word's existence<sup>32</sup> and its particular definition,<sup>20</sup> which leave no room for an argument<sup>17 22</sup></li>\n<li>agrees with conventional usage<sup>4</sup></li>\n<li>explains what context the word depends on<sup>36</sup></li>\n<li>limits its scope to avoid overlap with other meanings<sup>25</sup></li>\n<li>does not assume that definitions are the best way of giving words semantics<sup>12</sup></li>\n<li>directs a complex mental paintbrush<sup>35</sup> to paint detailed pictures of the thing you're trying to think about<sup>23</sup></li>\n<li>is a brain inference aid<sup>13</sup> that refers to and instructs one on how to find a specific/unique<sup>24</sup> similarity cluster<sup>21</sup> that is apparent from empirical experience<sup>28 29 30</sup>, the cluster's size being inversely proportional to the word's length<sup>31</sup></li>\n<li>is not a binary category<sup>9 11</sup> and cannot be used for deductive inference<sup>27</sup></li>\n<li>requires observing only<sup>14</sup> a few<sup>3</sup> real-world<sup>1</sup> properties that can be easily<sup>5</sup> verified<sup>2</sup> and are less abstract<sup>6</sup> than the word being defined (in particular, the definition cannot be circular<sup>16</sup>)</li>\n<li>is not just a list of random properties<sup>10 21</sup></li>\n<li>contains no negated properties<sup>10 33</sup></li>\n<li>specifies exhaustively all of the correct connotations of the word<sup>25 26</sup></li>\n<li>makes the properties of a random object satisfying the definition be nearly independent<sup>34</sup></li>\n<li>has examples<sup>6</sup> which satisfy the definition, including the original example(s) that motivated the definition being given<sup>15</sup> and typical/conventional examples<sup>7</sup></li>\n<li>tells you which examples are more typical or less typical<sup>9</sup></li>\n<li>captures enough characteristics of the examples to identify non-members<sup>8</sup></li>\n</ol>\n<p>And there you go. 17 rules, follow them all and you can't use words wrongly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ya378WMGFjz7vKCRr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 21, "extendedScore": null, "score": 1.5763923757214942e-06, "legacy": true, "legacyId": "25585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FaJaCgqBKphrDzDSj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-22T06:59:16.874Z", "modifiedAt": null, "url": null, "title": "Is love a good idea?", "slug": "is-love-a-good-idea", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:02.705Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sHPN3RYiFgWkd7Gs7/is-love-a-good-idea", "pageUrlRelative": "/posts/sHPN3RYiFgWkd7Gs7/is-love-a-good-idea", "linkUrl": "https://www.lesswrong.com/posts/sHPN3RYiFgWkd7Gs7/is-love-a-good-idea", "postedAtFormatted": "Saturday, February 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20love%20a%20good%20idea%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20love%20a%20good%20idea%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsHPN3RYiFgWkd7Gs7%2Fis-love-a-good-idea%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20love%20a%20good%20idea%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsHPN3RYiFgWkd7Gs7%2Fis-love-a-good-idea", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsHPN3RYiFgWkd7Gs7%2Fis-love-a-good-idea", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 414, "htmlBody": "<p>I've searched around on LW for this question, and haven't seen it brought up. Which surprises me, because I think it's an important question.</p>\n<p>I'm honestly not sure what I think. One one hand, love clearly leads to an element of happiness when done properly. This seems to be inescapable, probably because it's encoded in our DNA or something. But on the other hand, there's two things that really make me question whether or not love is a good idea.</p>\n<p>1) I have a <em>very</em> reductionist viewpoint, on everything. So I always ask myself, \"What am I really trying to optimize here, and what is the best way to optimize it?\". When I <a href=\"https://medium.com/i-m-h-o/cbb6e2bdf8ed\">think about it</a>, I come to the conclusion that I'm always trying to optimize my happiness. The answer to the question of, \"why does this matter?\" is always, \"because it makes me happy\". So then, the idea of love bothers me, because you sort of throw rational thinking out the window, stop asking why something actually matters, and just decide that this significant other <em>intrinsically matters</em> to you. I question whether this type of thinking is optimal, and personally, whether or not I'm even capable of it.</p>\n<p>2) It seems so obsessive, and I question whether or not it makes sense to obsess so much over one thing. This <a href=\"http://www.economist.com/node/2424049\">article</a> actually explores the brain chemicals involved in love, and suggests that the chemicals are similar to those that appear in OCD.</p>\n<p>Finally, there's the issue of permanence. Not all love is intended to be permanent, but a lot of the time it is. How can you commit to something so permanently? This makes me think of the mind projection fallacy. Perhaps people commit it with love. They think that the object of their desire is intrinsically desirable, when in fact it is the properties of this object that make it desirable. These properties are far from permanent (I'd go as far as to say that they're volatile, at least if you take the long view). So how does it make sense to commit to something so permanently?</p>\n<p>So my take is that there is probably a form of love that is rational to take. Something along the lines of enjoying each others company, and caring for one another and stuff, but not being blindly committed to one another, and being honest about the fact that you wouldn't do anything for one another, and will in fact probably grow apart at some point.&nbsp;</p>\n<p>What do you guys think?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sHPN3RYiFgWkd7Gs7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 3, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "25586", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-22T16:20:30.798Z", "modifiedAt": null, "url": null, "title": "LessWrong Hamburg Second Meetup Notes: In need of Structure", "slug": "lesswrong-hamburg-second-meetup-notes-in-need-of-structure", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:58.529Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d2f6Eggpj3edzKefe/lesswrong-hamburg-second-meetup-notes-in-need-of-structure", "pageUrlRelative": "/posts/d2f6Eggpj3edzKefe/lesswrong-hamburg-second-meetup-notes-in-need-of-structure", "linkUrl": "https://www.lesswrong.com/posts/d2f6Eggpj3edzKefe/lesswrong-hamburg-second-meetup-notes-in-need-of-structure", "postedAtFormatted": "Saturday, February 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Hamburg%20Second%20Meetup%20Notes%3A%20In%20need%20of%20Structure&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Hamburg%20Second%20Meetup%20Notes%3A%20In%20need%20of%20Structure%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd2f6Eggpj3edzKefe%2Flesswrong-hamburg-second-meetup-notes-in-need-of-structure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Hamburg%20Second%20Meetup%20Notes%3A%20In%20need%20of%20Structure%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd2f6Eggpj3edzKefe%2Flesswrong-hamburg-second-meetup-notes-in-need-of-structure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd2f6Eggpj3edzKefe%2Flesswrong-hamburg-second-meetup-notes-in-need-of-structure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p><strong>Review of our second&nbsp;<a href=\"/lw/jnk/meetup_lesswrong_hamburg_about_procrastination/\">Meetup : LessWrong Hamburg - about Procrastination</a></strong></p>\n<p>When I arrived late there already was a discussion about the benefits and content of LessWrong. I didn't take a clear organizers role and instead let the meetup mostly run itself. This worked OK but also led to the planned topic procrastication falling off the table until very late when it was discovered that there was actually more interest in it than everybody seemed to have assumed.</p>\n<p>There were phases where the discussion was dominated by everyday topics and not focussed. This was partly because some participants knew each other well and played topics back and forth. This kept the others out. I could have moderated this but wasn't clearly aware of it until it was explicitly and friendly made a topic.</p>\n<p>Despite the unstructured format we got the following positive results:</p>\n<p>\n<ul>\n<li>I had brought <a href=\"http://www.shabanali.com/upload/paulekman1.pdf\"><em>Emotions Revealed</em> by Ekman</a> (and other books) and left it lying on the table. Page turning led to the faces test and we took the test and discovered which emotions we could read or differentiate best and least. See also <a href=\"http://wiki.lesswrong.com/wiki/Emotion\">Emotion in the LW Wiki</a>.</li>\n<li>A diabetes <a href=\"http://en.wikipedia.org/wiki/Blood_glucose_monitoring\">glucose test</a> was demonstrated (curiosity overcame fear of being needled) and one actually had an unexpectedly elevated reading.</li>\n<li>When discussing how to stop smoking we turned up one option that convinced one participant: When relocating next month he will try to find a non-smoker&nbsp;flat-sharing community.</li>\n<li>We noticed that our communication cultures differed and talked about <a href=\"/lw/jis/tell_culture/\">guess and ask cultures</a> and how we could improve&nbsp;(I saw that this was a topic at the recent&nbsp;<a href=\"/lw/jps/meetup_berkeley_ask_vs_guess_vs_tell_culture/\">Berkeley Meetup</a>).</li>\n<li>We noticed that that we had difficulty finding structure and made an explicit agenda for the next meetup.</li>\n</ul>\n</p>\n<p>We planned the next meetup and chose a moderator to keep us more focussed.</p>\n<p>Lessons:</p>\n<ul>\n<li>Suitable books laying about can direct discussion to productive topics.</li>\n<li>Lively discussions about off-topics eat time and can keep participants out - but also provide casual athmosphere.</li>\n<li>Missing meetup structure can cause dissatisfaction with content&nbsp;uncertainness about direction.</li>\n</ul>\n<p>(none of these surprising)</p>\n<p>We also played a game (Set), had some fun, took photos and planned a next meetup.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d2f6Eggpj3edzKefe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 1.5771730677469943e-06, "legacy": true, "legacyId": "25587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CwfvDnZYjsvMKjmhD", "rEBXN3x6kXgD4pLxs", "zLSF27x98Sb4rnR73"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-22T16:57:52.048Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Hamburg - Structure", "slug": "meetup-lesswrong-hamburg-structure", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pgyE458aEgAztninW/meetup-lesswrong-hamburg-structure", "pageUrlRelative": "/posts/pgyE458aEgAztninW/meetup-lesswrong-hamburg-structure", "linkUrl": "https://www.lesswrong.com/posts/pgyE458aEgAztninW/meetup-lesswrong-hamburg-structure", "postedAtFormatted": "Saturday, February 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Hamburg%20-%20Structure&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Hamburg%20-%20Structure%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpgyE458aEgAztninW%2Fmeetup-lesswrong-hamburg-structure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Hamburg%20-%20Structure%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpgyE458aEgAztninW%2Fmeetup-lesswrong-hamburg-structure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpgyE458aEgAztninW%2Fmeetup-lesswrong-hamburg-structure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x4'>LessWrong Hamburg - Structure</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 March 2014 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Fehnweg 4, 22415 Hamburg, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be at another location again: Fehnweg 4, 22415 Hamburg in the northern part of Hamburg reachable from the center in about 30mins by public transportation.</p>\n\n<p>As we didn't really <a href=\"http://lesswrong.com/r/discussion/lw/jqr/lesswrong_hamburg_second_meetup_notes_in_need_of/\">get around to procrastination last time</a> we commited to have and follow an agenda this time:</p>\n\n<ul>\n<li>Open beginning until we are complete</li>\n<li>Introduction and agenda presented by moderator</li>\n<li>Short presentation about procrastination and dealing with it (G.Z.).</li>\n<li>Break for discussion and games</li>\n<li>Possibly we will have a short presentation about <a href=\"http://en.wikipedia.org/wiki/Theme-centered_interaction\" rel=\"nofollow\">theme-centered interaction</a> to improve our communication culture (S.Z.)</li>\n<li>Break for discussion and games</li>\n<li>A short list of LessWrong topics that might be interesting for the 'newbies'.</li>\n<li>Time for discussing these LW topics</li>\n<li>Planning of next meetup</li>\n<li>Free form meetup</li>\n</ul>\n\n<p>The meetup is at my home and I will provide for food and drisks but you may bring sweets.</p>\n\n<p>The Meetup is open-ended (again) which judging from the last times means significantly past midnight.</p>\n\n<p>The mailing list: <a href=\"https://groups.google.com/forum/#!forum/lesswrong-hamburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/lesswrong-hamburg</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x4'>LessWrong Hamburg - Structure</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pgyE458aEgAztninW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5772170068405019e-06, "legacy": true, "legacyId": "25588", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Hamburg___Structure\">Discussion article for the meetup : <a href=\"/meetups/x4\">LessWrong Hamburg - Structure</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 March 2014 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Fehnweg 4, 22415 Hamburg, Germany</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be at another location again: Fehnweg 4, 22415 Hamburg in the northern part of Hamburg reachable from the center in about 30mins by public transportation.</p>\n\n<p>As we didn't really <a href=\"http://lesswrong.com/r/discussion/lw/jqr/lesswrong_hamburg_second_meetup_notes_in_need_of/\">get around to procrastination last time</a> we commited to have and follow an agenda this time:</p>\n\n<ul>\n<li>Open beginning until we are complete</li>\n<li>Introduction and agenda presented by moderator</li>\n<li>Short presentation about procrastination and dealing with it (G.Z.).</li>\n<li>Break for discussion and games</li>\n<li>Possibly we will have a short presentation about <a href=\"http://en.wikipedia.org/wiki/Theme-centered_interaction\" rel=\"nofollow\">theme-centered interaction</a> to improve our communication culture (S.Z.)</li>\n<li>Break for discussion and games</li>\n<li>A short list of LessWrong topics that might be interesting for the 'newbies'.</li>\n<li>Time for discussing these LW topics</li>\n<li>Planning of next meetup</li>\n<li>Free form meetup</li>\n</ul>\n\n<p>The meetup is at my home and I will provide for food and drisks but you may bring sweets.</p>\n\n<p>The Meetup is open-ended (again) which judging from the last times means significantly past midnight.</p>\n\n<p>The mailing list: <a href=\"https://groups.google.com/forum/#!forum/lesswrong-hamburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/lesswrong-hamburg</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Hamburg___Structure1\">Discussion article for the meetup : <a href=\"/meetups/x4\">LessWrong Hamburg - Structure</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Hamburg - Structure", "anchor": "Discussion_article_for_the_meetup___LessWrong_Hamburg___Structure", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Hamburg - Structure", "anchor": "Discussion_article_for_the_meetup___LessWrong_Hamburg___Structure1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d2f6Eggpj3edzKefe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-22T18:18:10.823Z", "modifiedAt": null, "url": null, "title": "Meetup : Philadelphia-- Debugging in Meatspace", "slug": "meetup-philadelphia-debugging-in-meatspace", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BkXv785PuCsvwZPLa/meetup-philadelphia-debugging-in-meatspace", "pageUrlRelative": "/posts/BkXv785PuCsvwZPLa/meetup-philadelphia-debugging-in-meatspace", "linkUrl": "https://www.lesswrong.com/posts/BkXv785PuCsvwZPLa/meetup-philadelphia-debugging-in-meatspace", "postedAtFormatted": "Saturday, February 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Philadelphia--%20Debugging%20in%20Meatspace&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Philadelphia--%20Debugging%20in%20Meatspace%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkXv785PuCsvwZPLa%2Fmeetup-philadelphia-debugging-in-meatspace%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Philadelphia--%20Debugging%20in%20Meatspace%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkXv785PuCsvwZPLa%2Fmeetup-philadelphia-debugging-in-meatspace", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkXv785PuCsvwZPLa%2Fmeetup-philadelphia-debugging-in-meatspace", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<h2><span style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px; font-weight: normal;\">The discussion prompt William Burroughs's \"The discipline of DE (do easy)\"&nbsp;</span><br style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px; font-weight: normal;\" /><a style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; text-decoration: none; color: #6611cc; cursor: pointer; font-family: Arial, Helvetica, sans-serif; font-size: 13px; font-weight: normal;\" href=\"http://melancholia.typepad.com/melancholia/2009/07/do-easy.html\" target=\"_blank\">http://melancholia.typepad.com/melancholia/2009/07/do-easy.html</a><span style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px; font-weight: normal;\">&nbsp;</span></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">23 February 2014 01:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Nam Phuong, 11th &amp; Broad St., Philadelphia, PA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Meets every two weeks. Forum/mailing list: groups.google.com/forum/#!forum/lesswrong-philadelphia</p>\n</div>\n</div>\n<!-- .content -->\n<h2><br /></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BkXv785PuCsvwZPLa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.5773114849863888e-06, "legacy": true, "legacyId": "25589", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-23T00:04:54.267Z", "modifiedAt": null, "url": null, "title": "Meetup : Salt Lake City UT \u2014 Open Possibilities and Improv Skills", "slug": "meetup-salt-lake-city-ut-open-possibilities-and-improv", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:59.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FsJFZhJmLieh6AzPr/meetup-salt-lake-city-ut-open-possibilities-and-improv", "pageUrlRelative": "/posts/FsJFZhJmLieh6AzPr/meetup-salt-lake-city-ut-open-possibilities-and-improv", "linkUrl": "https://www.lesswrong.com/posts/FsJFZhJmLieh6AzPr/meetup-salt-lake-city-ut-open-possibilities-and-improv", "postedAtFormatted": "Sunday, February 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Salt%20Lake%20City%20UT%20%E2%80%94%20Open%20Possibilities%20and%20Improv%20Skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Salt%20Lake%20City%20UT%20%E2%80%94%20Open%20Possibilities%20and%20Improv%20Skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFsJFZhJmLieh6AzPr%2Fmeetup-salt-lake-city-ut-open-possibilities-and-improv%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Salt%20Lake%20City%20UT%20%E2%80%94%20Open%20Possibilities%20and%20Improv%20Skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFsJFZhJmLieh6AzPr%2Fmeetup-salt-lake-city-ut-open-possibilities-and-improv", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFsJFZhJmLieh6AzPr%2Fmeetup-salt-lake-city-ut-open-possibilities-and-improv", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 377, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x6'>Salt Lake City UT \u2014 Open Possibilities and Improv Skills</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 March 2014 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Kafeneios Coffeehouse, 258 W 3300 S, Salt Lake City UT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Less Wrong Meetup</p>\n\n<p>\"Why should I come to this meetup?\" you may ask.</p>\n\n<p>Well, maybe you should and maybe you shouldn't. It all depends. Do you want to:</p>\n\n<ul>\n<li>Learn and try the classic exercise of thinking on your feet in a casual, low-risk environment.</li>\n<li>Have a blast keeping an interesting story going with off-the-wall ideas.</li>\n<li>Explore the possible connection between the skills you can pick up with improv and the skills useful for meta-success, e.g. Accepting the situation at hand, Being Specific, Building on opportunities.</li>\n</ul>\n\n<p>And as always, Do you feel like:</p>\n\n<ul>\n<li>Getting specific feedback and positive reinforcement on valuable habits of good thinking and success.</li>\n<li>Receiving empathy and support for struggles with akrasia and intellectual integrity.. and maybe brag a little after you've won the battle.</li>\n<li>Hanging out with just generally awesome people? (We certainly do. Which is why you should definitely come!)</li>\n</ul>\n\n<p>Any of that interest you? All you have to do is:</p>\n\n<ul>\n<li>Show up at Kafeneios Coffeehouse on March 8th @ 3pm\n    * (258 W 3300 S, Salt Lake City)</li>\n<li>Find us near the center of the first big room. I'll be the one with the whiteboard.</li>\n<li>Bring your observations on what you've been up to since last meetup, if you've got any to share.</li>\n<li>Leave your politics, or other ideas that you identify with or are emotionally invested in, at the door. </li>\n</ul>\n\n<p>If you're the kind of person that likes to be ahead of the game, check out these links:\n<a href=\"http://www.dangoldstein.com/howtoimprovise.html\" rel=\"nofollow\">http://www.dangoldstein.com/howtoimprovise.html</a>\n<a href=\"http://improvencyclopedia.org/references//David_Alger%60s_First_10_Rules_of_Improv.html\" rel=\"nofollow\">http://improvencyclopedia.org/references//David_Alger%60s_First_10_Rules_of_Improv.html</a></p>\n\n<p>Thank you for your time, and thank you for being part of the most fun, engaging, and all-around rewarding social group I know.</p>\n\n<p>If you have any questions or if there is anything at all bothering you, please don't hesitate to message me. And please err on the side of asking; it costs little to address your concerns if they're trivial and costs a lot to ignore them if they're not.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x6'>Salt Lake City UT \u2014 Open Possibilities and Improv Skills</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FsJFZhJmLieh6AzPr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 1, "extendedScore": null, "score": 1.577719477773426e-06, "legacy": true, "legacyId": "25590", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_UT___Open_Possibilities_and_Improv_Skills\">Discussion article for the meetup : <a href=\"/meetups/x6\">Salt Lake City UT \u2014 Open Possibilities and Improv Skills</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 March 2014 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Kafeneios Coffeehouse, 258 W 3300 S, Salt Lake City UT</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Less Wrong Meetup</p>\n\n<p>\"Why should I come to this meetup?\" you may ask.</p>\n\n<p>Well, maybe you should and maybe you shouldn't. It all depends. Do you want to:</p>\n\n<ul>\n<li>Learn and try the classic exercise of thinking on your feet in a casual, low-risk environment.</li>\n<li>Have a blast keeping an interesting story going with off-the-wall ideas.</li>\n<li>Explore the possible connection between the skills you can pick up with improv and the skills useful for meta-success, e.g. Accepting the situation at hand, Being Specific, Building on opportunities.</li>\n</ul>\n\n<p>And as always, Do you feel like:</p>\n\n<ul>\n<li>Getting specific feedback and positive reinforcement on valuable habits of good thinking and success.</li>\n<li>Receiving empathy and support for struggles with akrasia and intellectual integrity.. and maybe brag a little after you've won the battle.</li>\n<li>Hanging out with just generally awesome people? (We certainly do. Which is why you should definitely come!)</li>\n</ul>\n\n<p>Any of that interest you? All you have to do is:</p>\n\n<ul>\n<li>Show up at Kafeneios Coffeehouse on March 8th @ 3pm\n    * (258 W 3300 S, Salt Lake City)</li>\n<li>Find us near the center of the first big room. I'll be the one with the whiteboard.</li>\n<li>Bring your observations on what you've been up to since last meetup, if you've got any to share.</li>\n<li>Leave your politics, or other ideas that you identify with or are emotionally invested in, at the door. </li>\n</ul>\n\n<p>If you're the kind of person that likes to be ahead of the game, check out these links:\n<a href=\"http://www.dangoldstein.com/howtoimprovise.html\" rel=\"nofollow\">http://www.dangoldstein.com/howtoimprovise.html</a>\n<a href=\"http://improvencyclopedia.org/references//David_Alger%60s_First_10_Rules_of_Improv.html\" rel=\"nofollow\">http://improvencyclopedia.org/references//David_Alger%60s_First_10_Rules_of_Improv.html</a></p>\n\n<p>Thank you for your time, and thank you for being part of the most fun, engaging, and all-around rewarding social group I know.</p>\n\n<p>If you have any questions or if there is anything at all bothering you, please don't hesitate to message me. And please err on the side of asking; it costs little to address your concerns if they're trivial and costs a lot to ignore them if they're not.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_UT___Open_Possibilities_and_Improv_Skills1\">Discussion article for the meetup : <a href=\"/meetups/x6\">Salt Lake City UT \u2014 Open Possibilities and Improv Skills</a></h2>", "sections": [{"title": "Discussion article for the meetup : Salt Lake City UT \u2014 Open Possibilities and Improv Skills", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_UT___Open_Possibilities_and_Improv_Skills", "level": 1}, {"title": "Discussion article for the meetup : Salt Lake City UT \u2014 Open Possibilities and Improv Skills", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_UT___Open_Possibilities_and_Improv_Skills1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-23T22:24:18.000Z", "modifiedAt": null, "url": null, "title": "In Favor of Niceness, Community, and Civilization", "slug": "in-favor-of-niceness-community-and-civilization", "viewCount": null, "lastCommentedAt": "2021-08-21T22:06:03.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rkpDX7j7va6c8Q7cZ/in-favor-of-niceness-community-and-civilization", "pageUrlRelative": "/posts/rkpDX7j7va6c8Q7cZ/in-favor-of-niceness-community-and-civilization", "linkUrl": "https://www.lesswrong.com/posts/rkpDX7j7va6c8Q7cZ/in-favor-of-niceness-community-and-civilization", "postedAtFormatted": "Sunday, February 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Favor%20of%20Niceness%2C%20Community%2C%20and%20Civilization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Favor%20of%20Niceness%2C%20Community%2C%20and%20Civilization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkpDX7j7va6c8Q7cZ%2Fin-favor-of-niceness-community-and-civilization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Favor%20of%20Niceness%2C%20Community%2C%20and%20Civilization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkpDX7j7va6c8Q7cZ%2Fin-favor-of-niceness-community-and-civilization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkpDX7j7va6c8Q7cZ%2Fin-favor-of-niceness-community-and-civilization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6105, "htmlBody": "<p><i>[Content warning: Discussion of social justice, discussion of violence, spoilers for Jacqueline Carey books.]</p>\n<p>[Edit 10/25: This post was inspired by a debate with a friend of a friend on Facebook who has since become somewhat famous. I&#8217;ve renamed him here to &#8220;Andrew Cord&#8221; to protect his identity.]</i></p>\n<p><b>I.</b></p>\n<p>Andrew Cord <A HREF=\"http://www.patheos.com/blogs/hallq/2014/02/on-some-criticism-of-lesswrong/\">criticizes me</A> for my bold and controversial suggestion that maybe people should try to tell slightly fewer blatant hurtful lies:</p>\n<blockquote><p>I just find it kind of darkly amusing and sad that the \u201crationalist community\u201d loves \u201crationality is winning\u201d so much as a tagline and yet are clearly not winning. And then complain about losing rather than changing their tactics to match those of people who are winning.</p>\n<p>Which is probably because if you *really* want to be the kind of person who wins you have to actually care about winning something, which means you have to have politics, which means you have to embrace \u201cpolitics the mindkiller\u201d and \u201cpolitics is war and arguments are soldiers\u201d, and Scott would clearly rather spend the rest of his life losing than do this.</p>\n<p>That post [<A HREF=\"http://slatestarcodex.com/2014/02/17/lies-damned-lies-and-social-media-part-5-of-%e2%88%9e/\">the one debunking false rape statistics</A>] is exactly my problem with Scott. He seems to honestly think that it\u2019s a worthwhile use of his time, energy and mental effort to download evil people\u2019s evil worldviews into his mind and try to analytically debate them with statistics and cost-benefit analyses.</p>\n<p>He gets *mad* at people whom he detachedly intellectually agrees with but who are willing to back up their beliefs with war and fire rather than pussyfooting around with debate-team nonsense.</p>\n<p>It honestly makes me kind of sick. It is exactly the kind of thing that \u201csocial justice\u201d activists like me *intend* to attack and \u201ctrigger\u201d when we use \u201ctriggery\u201d catchphrases about the mewling pusillanimity of privileged white allies.</p></blockquote>\n<p>In other words, if a fight is important to you, fight nasty. If that means lying, lie. If that means insults, insult. If that means silencing people, silence.</p>\n<p>It always makes me happy when my ideological opponents come out and say eloquently and openly what I&#8217;ve always secretly suspected them of believing.</p>\n<p>My natural instinct is to give some of the reasons why I think Andrew is wrong, starting with the history of the &#8220;noble lie&#8221; concept and moving on to some examples of why it didn&#8217;t work very well, and why it might not be expected not to work so well in the future.</p>\n<p>But in a way, that would be assuming the conclusion. I wouldn&#8217;t be showing respect for Andrew&#8217;s arguments. I wouldn&#8217;t be going halfway to meet them on their own terms.</p>\n<p>The respectful way to rebut Andrew&#8217;s argument would be to spread malicious lies about Andrew to a couple of media outlets, fan the flames, and wait for them to destroy his reputation.  Then if the stress ends up bursting an aneurysm in his brain, I can dance on his grave, singing: </p>\n<blockquote><p>\u266a \u266c I won this debate in a very effective manner. Now you can&#8217;t argue in favor of nasty debate tactics any more \u266c \u266a</p></blockquote>\n<p>I&#8217;m not going to do that, but if I <i>did</i> it&#8217;s unclear to me how Andrew could object. I mean, he thinks that sexism is detrimental to society, so spreading lies and destroying people is justified in order to stop it. I think that discourse based on mud-slinging and falsehoods is detrimental to society. Therefore&#8230;</p>\n<p><b>II.</b></p>\n<p>But really, all this talk of lying and spreading rumors about people is &#8211; what was Andrew&#8217;s terminology &#8211; &#8220;pussyfooting around with debate-team nonsense&#8221;. You know who got things done? The IRA. They didn&#8217;t agree with the British occupation of Northern Ireland and they weren&#8217;t afraid to let people know in that very special way only a nail-bomb shoved through your window at night can.</p>\n<p>Why not assassinate prominent racist and sexist politicians and intellectuals? I won&#8217;t name names since that would be crossing a line, but I&#8217;m sure you can generate several of them who are sufficiently successful and charismatic that, if knocked off, there would not be an equally competent racist or sexist immediately available to replace them, and it would thus be a serious setback for the racism/sexism movement.</p>\n<p>Other people can appeal to &#8220;the social contract&#8221; or &#8220;the general civilizational rule not to use violence&#8221;, but not Andrew:</p>\n<blockquote><p>I think that whether or not I use certain weapons has zero impact on whether or not those weapons are used against me, and people who think they do are either appealing to a kind of vague Kantian morality that I think is invalid or a specific kind of \u201chonor among foes\u201d that I think does not exist.</p></blockquote>\n<p>And don&#8217;t give me that nonsense about the police. I&#8217;m sure a smart person like you can think of clever exciting new ways to commit the perfect murder. Unless you do not believe there will <i>ever</i> be an opportunity to defect unpunished, you need this sort of social contract to take you at least some of the way.</p>\n<p>He continues:</p>\n<blockquote><p>When Scott calls rhetorical tactics he dislikes \u201cbullets\u201d and denigrates them it actually hilariously plays right into this point&#8230;to be \u201cpro-bullet\u201d or \u201canti-bullet\u201d is ridiculous. Bullets, as you say, are neutral. I am in favor of my side using bullets as best they can to destroy the enemy\u2019s ability to use bullets.</p>\n<p>In a war, a real war, a war for survival, you use all the weapons in your arsenal because you assume the enemy will use all the weapons in theirs. Because you understand that it IS a war.</p></blockquote>\n<p>There are a lot of things I am tempted to say to this.</p>\n<p>Like &#8220;And that is why the United States immediately nukes every country it goes to war with.&#8221;</p>\n<p>Or &#8220;And that is why the Geneva Convention was so obviously impossible that no one even bothered to attend the conference&#8221;.</p>\n<p>Or &#8220;And that is why, <A HREF=\"http://slatestarcodex.com/2013/05/22/apart-from-better-sanitation-and-medicine-and-education-and-irrigation-and-public-health-and-roads-and-public-order-what-has-modernity-done-for-us/\">to this very day</A>, we solve every international disagreement through total war.&#8221;</p>\n<p>Or &#8220;And that is why Martin Luther King was immediately reduced to a nonentity, and we remember the Weathermen as the sole people responsible for the success of the civil rights movement&#8221;</p>\n<p>But I think what I am <i>actually</i> going to say is that, for the love of God, if you like bullets so much, stop using them as a metaphor for &#8216;spreading false statistics&#8217; and go buy a gun.</p>\n<p><b>III.</b></p>\n<p>So let&#8217;s derive why violence is not in fact The One True Best Way To Solve All Our Problems. You can get most of this from <a href=\"http://smile.amazon.com/gp/product/1619491702/ref=as_li_tl?ie=UTF8&#038;camp=1789&#038;creative=390957&#038;creativeASIN=1619491702&#038;linkCode=as2&#038;tag=slastacod-20&#038;linkId=AYGYZKSORVSJ52VW\">Hobbes</a><img src=\"http://ir-na.amazon-adsystem.com/e/ir?t=slastacod-20&#038;l=as2&#038;o=1&#038;a=1619491702\" width=\"1\" height=\"1\" border=\"0\" alt=\"\" style=\"border:none !important; margin:0px !important;\" />, but this blog post will be shorter.</p>\n<p>Suppose I am a radical Catholic who believes all Protestants deserve to die, and therefore go around killing Protestants. So far, so good.</p>\n<p>Unfortunately, there might be some radical Protestants around who believe all Catholics deserve to die. If there weren&#8217;t before, there probably are now. So they go around killing Catholics, we&#8217;re both unhappy and/or dead, our economy tanks, hundreds of innocent people end up as collateral damage, and our country goes down the toilet.</p>\n<p>So we make an agreement: I won&#8217;t kill any more Catholics, you don&#8217;t kill any more Protestants. The specific Irish example was called the Good Friday Agreement and the general case is called &#8220;civilization&#8221;.</p>\n<p>So then I try to destroy the hated Protestants using the government. I go around trying to pass laws banning Protestant worship and preventing people from condemning Catholicism.</p>\n<p>Unfortunately, maybe the next government in power is a Protestant government, and they pass laws banning Catholic worship and preventing people from condemning Protestantism. No one can securely practice their own religion, no one can learn about other religions, people are constantly plotting civil war, academic freedom is severely curtailed, and once again the country goes down the toilet.</p>\n<p>So again we make an agreement. I won&#8217;t use the apparatus of government against Protestantism, you don&#8217;t use the apparatus of government against Catholicism. The specific American example is the First Amendment and the general case is called &#8220;liberalism&#8221;, or to be dramatic about it, &#8220;civilization 2.0&#8221;</p>\n<p>Every case in which both sides agree to lay down their weapons and be nice to each other has corresponded to spectacular gains by both sides and a new era of human flourishing.</p>\n<p>&#8220;Wait a second, no!&#8221; someone yells. &#8220;I see where you&#8217;re going with this. You&#8217;re going to say that agreeing not to spread malicious lies about each other would also be a civilized and beneficial system. Like maybe the Protestants could stop saying that the Catholics worshipped the Devil, and the Catholics could stop saying the Protestants hate the Virgin Mary, and they could both relax the whole thing about the Jews baking the blood of Christian children into their matzah.</p>\n<p>&#8220;But your two examples were about contracts written on paper and enforced by the government. So maybe a &#8216;no malicious lies&#8217; amendment to the Constitution would work if it were enforceable, <i>which it isn&#8217;t</i>, but just <i>asking</i> people to stop spreading malicious lies is doomed from the start. The Jews will no doubt spread lies against <i>us</i>, so if we stop spreading lies about them, all we&#8217;re doing is abandoning an effective weapon against a religion I personally know to be heathenish! Rationalists should win, so put the blood libel on the front page of every newspaper!&#8221;</p>\n<p>Or, as Andrew puts it:</p>\n<blockquote><p>Whether or not I use certain weapons has zero impact on whether or not those weapons are used against me, and people who think they do are either appealing to a kind of vague Kantian morality that I think is invalid or a specific kind of \u201chonor among foes\u201d that I think does not exist.</p></blockquote>\n<p>So let&#8217;s talk about how beneficial game-theoretic equilibria can come to exist even in the absence of centralized enforcers. I know of two main ways: reciprocal communitarianism, and divine grace.</p>\n<p>Reciprocal communitarianism is probably how altruism evolved. Some mammal started running <A HREF=\"https://en.wikipedia.org/wiki/Tit_for_tat\">TIT-FOR-TAT</A>, the program where you cooperate with anyone whom you expect to cooperate with you. Gradually you form a successful community of cooperators. The defectors either join your community and agree to play by your rules or get outcompeted.</p>\n<p>Divine grace is more complicated. I was tempted to call it &#8220;spontaneous order&#8221; until I remembered the rationalist proverb that if you don&#8217;t understand something, you need to call it by a term that reminds you that don&#8217;t understand it or else you&#8217;ll think you&#8217;ve explained it when you&#8217;ve just named it. </p>\n<p>But consider the following: I am a pro-choice atheist. When I lived in Ireland, one of my friends was a pro-life Christian. I thought she was responsible for the unnecessary suffering of millions of women. She thought I was responsible for killing millions of babies. And yet she invited me over to her house for dinner without poisoning the food. And I ate it, and thanked her, and sent her a nice card, without smashing all her china.</p>\n<p>Please try not to be insufficiently surprised by this. Every time <A HREF=\"http://slatestarcodex.com/2013/03/17/not-just-a-mere-political-issue/\">a Republican and a Democrat break bread together with good will</A>, it is a miracle. It is an equilibrium as beneficial as civilization or liberalism, which developed in the total absence of any central enforcing authority. </p>\n<p>When you look for these equilibria, there are lots and lots. Andrew says there is no &#8220;honor among foes&#8221;, but if you read the <i>Iliad</i> or any other account of ancient warfare, there is practically nothing <i>but</i> honor among foes, and it wasn&#8217;t generated by some sort of Homeric version of the Geneva Convention, it just sort of happened. During World War I, the English and Germans spontaneously got out of their trenches and celebrated Christmas together with each other, and on the sidelines Andrew was shouting &#8220;No! Stop celebrating Christmas! Quick, shoot them before they shoot you!&#8221; but they didn&#8217;t listen.</p>\n<p>All I will say in way of explaining these miraculous equilibria is that they seem to have something to do with inheriting a cultural norm and not screwing it up. Punishing the occasional defector seems to be a big part of not screwing it up. How exactly that cultural norm came to be is less clear to me, but it might have something to do with the reasons why <A HREF=\"http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/\">an entire civilization&#8217;s bureaucrats may suddenly turn 100% honest at the same time</A>. I&#8217;m pretty sure I&#8217;m supposed to say the words <A HREF=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</A> around this point too, and perhaps bring up the kind of Platonic contract <A HREF=\"http://slatestarcodex.com/2013/04/08/whose-utilitarianism/\">that I have written about previously</A>.</p>\n<p>I think most of our useful social norms exist through a combination of divine grace and reciprocal communitarianism. To some degree they arise spontaneously and are preserved by the honor system. To another degree, they are stronger or weaker in different groups, and the groups that enforce them are so much more pleasant than the groups that don&#8217;t that people are willing to go along.</p>\n<p>The norm against malicious lies follows this pattern. Politicians lie, but not <i>too much</i>. Take the top story on Politifact Fact Check today. Some Republican claimed his supposedly-maverick Democratic opponent actually voted with Obama&#8217;s economic policies <A HREF=\"http://www.politifact.com/truth-o-meter/statements/2014/feb/21/dan-sullivan/begich-supports-97-percent-obamas-economic-policy-/\">97 percent of the time</A>. Fact Check explains that the statistic used was actually for <i>all</i> votes, not just economic votes, and that members of Congress typically have to have >90% agreement with their president because of the way partisan politics work. So it&#8217;s a lie, and is properly listed as one. But it&#8217;s a lie based on slightly misinterpreting a real statistic. He didn&#8217;t just totally make up a number. He didn&#8217;t even just make up something else, like &#8220;My opponent personally helped design most of Obama&#8217;s legislation&#8221;.</p>\n<p>Even the guy in the fake rape statistics post lied less than he <i>possibly could have</i>. He got his fake numbers by conflating rapes per sex act with rapes per lifetime, and it&#8217;s really hard for me to imagine someone doing that by anything resembling accident. But he couldn&#8217;t bring himself to go the extra step and just totally make up numbers with no grounding whatsoever. And part of me wonders: why not? If you&#8217;re going to use numbers you know are false to destroy people, why is it better to derive the numbers through a formula you know is incorrect, than to just skip the math and make the numbers up in the first place? &#8220;The FBI has determined that no false rape claims have ever been submitted, my source is an obscure report they published, when your local library doesn&#8217;t have it you will just accept that libraries can&#8217;t have all books, and suspect nothing.&#8221; </p>\n<p>This would have been a <i>more believable</i> claim than the one he made. Because he showed his work, it was easy for me to debunk it. If he had just said it was in some obscure report, I wouldn&#8217;t have gone through the trouble. So why did he go the harder route?</p>\n<p>People <i>know</i> lying is wrong. They know if they lied they would be punished. More <s>spontaneous social order</s> miraculous divine grace. And so they want to hedge their bets, be able to say &#8220;Well, I didn&#8217;t exactly <i>lie</i>, per se.&#8221;</p>\n<p>And this is good! We <i>want</i> to make it politically unacceptable to have people say that Jews bake the blood of Christian children into their matzah. Now we build on that success. We start hounding around the edges of currently acceptable lies. &#8220;Okay, you didn&#8217;t <i>literally</i> make up your statistics, but you still lied, and you still should be cast out from the community of people who have reasonable discussions and never trusted by anyone again.&#8221;</p>\n<p>It might not totally succeed in making a new norm against this kind of thing. But at least it will prevent other people from seeing their success, taking heart, and having the number of lies which are socially acceptable gradually <i>advance</i>.</p>\n<p>So much for protecting what we have been given by divine grace. But there is also reciprocal communitarianism to think of.</p>\n<p>I seek out people who signal that they want to discuss things honestly and rationally. Then I try to discuss things honestly and rationally with those people. I try to concentrate as much of my social interaction there as possible.</p>\n<p>So far this project is going pretty well. My friends are nice, my romantic relationships are low-drama, my debates are productive and I am learning so, so much. </p>\n<p>And people think &#8220;Hm, I could hang out at 4Chan and be called a &#8216;fag&#8217;. Or I could hang out at Slate Star Codex and discuss things rationally and learn a lot. And if I want to be allowed in, all I have to do is not be an intellectually dishonest jerk.&#8221; </p>\n<p>And so our community grows. And all over the world, the mysterious divine forces favoring honest and kind equilibria gain a little bit more power over the mysterious divine forces favoring lying and malicious equilibria.</p>\n<p>Andrew thinks I am trying to fight all the evils of the world, and doing so in a stupid way. But sometimes I just want to cultivate my garden.</p>\n<p><b>IV.</b></p>\n<p>Andrew goes on to complain:</p>\n<blockquote><p>Scott&#8230;seems to [dispassionately debate] evil people\u2019s evil worldviews &#8230;with statistics and cost-benefit analyses.</p>\n<p>He gets <i>mad</i> at people whom he detachedly intellectually agrees with but who are willing to back up their beliefs with war and fire rather than pussyfooting around with debate-team nonsense.</p></blockquote>\n<p>I accept this criticism as an accurate description of what I do.</p>\n<p>Compare to the following two critiques: &#8220;The Catholic Church wastes so much energy getting upset about heretics who believe <i>mostly</i> the same things as they do, when there are literally <i>millions</i> of Hindus over in India who don&#8217;t believe in Catholicism <i>at all</i>! What dumb priorities!&#8221;</p>\n<p>Or &#8220;How could Joseph McCarthy get angry about a couple of people who <i>might</i> have been Communists in the US movie industry, when over in Moscow there were <i>thousands</i> of people who were openly <i>super</i> Communist <i>all the time</i>?&#8221;</p>\n<p>There might be foot-long giant centipedes in the Amazon, but I am a lot more worried about boll weevils in my walled garden.</p>\n<p>Creationists lie. Homeopaths lie. Anti-vaxxers lie. This is part of the Great Circle of Life. It is not necessary to call out every lie by a creationist, because the sort of person who is still listening to creationists is not the sort of person who is likely to be moved by call-outs. There is a role for <i>organized</i> action against creationists, like preventing them from getting their opinions taught in schools, but the marginal blog post &#8220;debunking&#8221; a creationist on something is a waste of time. Everybody who wants to discuss things rationally has already formed a walled garden and locked the creationists outside of it.</p>\n<p>Anti-Semites fight nasty. The Ku Klux Klan fights nasty. Neo-Nazis fight nasty. We dismiss them with equanimity, in accordance with the ancient proverb: &#8220;Haters gonna hate&#8221;. There is a role for <i>organized</i> opposition to these groups, like making sure they can&#8217;t actually terrorize anyone, but the marginal blog post condemning Nazism is a waste of time. Everybody who wants to discuss things charitably and compassionately has already formed a walled garden and locked the Nazis outside of it.</p>\n<p>People who want to discuss things rationally and charitably have not yet looked over the false rape statistics article and decided to lock Charles Clymer out of their walled garden.</p>\n<p>He is not a heathen, he is a heretic. He is not a foreigner, he is a traitor. He comes in talking all liberalism and statistics, and then he betrays the signals he has just sent. He is not just some guy who defects in the Prisoner&#8217;s Dilemma. He is the guy who defects while wearing the <A HREF=\"http://www.redbubble.com/people/lalaithion/works/10939446-i-cooperate-in-the-prisoners-dilemma?p=t-shirt\">&#8220;I COOPERATE IN PRISONERS DILEMMAS&#8221; t-shirt</A>.</p>\n<p>What really, <i>really</i> bothered me wasn&#8217;t Clymer at all: it was that <i>rationalists</i> were taking him seriously. Smart people, kind people! I even said so in my article. Boll weevils in our beautiful walled garden!</p>\n<p>Why am I always harping on feminism? I feel like we&#8217;ve got a good thing going, we&#8217;ve ratified our Platonic contract to be intellectually honest and charitable to each other, we are going about perma-cooperating in the Prisoner&#8217;s Dilemma and reaping gains from trade.</p>\n<p>And then someone says &#8220;Except that of course regardless of all that I reserve the right to still use lies and insults and harassment and <A HREF=\"http://lesswrong.com/lw/uy/dark_side_epistemology/\">dark epistemology</A> to spread feminism&#8221;. Sometimes they do this explicitly, like Andrew did. Other times they use a more nuanced argument like &#8220;Surely you didn&#8217;t think the same rules against lies and insults and harassment should apply to oppressed and privileged people, did you?&#8221; And other times they don&#8217;t say anything, but just show their true colors by reblogging an awful article with false statistics.</p>\n<p>(and still other times they don&#8217;t do any of this and they are wonderful people whom I am glad to know)</p>\n<p>But then someone else says &#8220;Well, if they get their exception, I deserve my exception,&#8221; and then someone else says &#8220;Well, if those two get exceptions, I&#8217;m out&#8221;, and <i>you have no idea how difficult it is to successfully renegotiate the terms of a timeless Platonic contract that doesn&#8217;t literally exist</i>.</p>\n<p>No! I am Exception Nazi! NO EXCEPTION FOR YOU! Civilization didn&#8217;t conquer the world by forbidding you to murder your enemies <i>unless</i> they are actually unrighteous in which case go ahead and kill them all. Liberals didn&#8217;t give their lives in the battle against tyranny to end discrimination against all religions <i>except</i> Jansenism because seriously fuck Jansenists. Here we have built our <A HREF=\"http://lesswrong.com/lw/ase/schelling_fences_on_slippery_slopes/\">Schelling fence</A> and here we are defending it to the bitter end.</p>\n<p><b>V.</b></p>\n<p>Contrary to how it may appear, I am not trying to doom feminism.</p>\n<p>Feminists like to mock the naivete of anyone who says that classical liberalism would suffice to satisfy feminist demands. And true, you cannot simply assume Adam Smith and derive Andrea Dworkin. Not being an asshole to women and not writing laws declaring them officially inferior are both good starts, but it not enough if there&#8217;s still cultural baggage and entrenched gender norms.</p>\n<p>But here I am, defending this principle &#8211; kind of a supercharged version of liberalism &#8211; of &#8220;It is not okay to use lies, insults, and harassment against people, even if it would help you enforce your preferred social norms.&#8221;</p>\n<p>And I notice that this gets us a heck of a lot closer to feminism than Andrew&#8217;s principle of &#8220;Go ahead and use lies, insults, and harassment if they are effective ways to enforce your preferred social norms.&#8221;</p>\n<p>Feminists are very concerned about slut-shaming, where people harass women who have too much premarital sex. They point out that this is very hurtful to women, that men might underestimate the amount of hurt it causes women, and that the standard-classical-liberal solution of removing relevant government oppression does nothing. All excellent points.</p>\n<p>But one assumes the harassers think that women having premarital sex is detrimental to society. So they apply their general principle: &#8220;I should use lies, insults, and harassment to enforce my preferred social norms.&#8221;</p>\n<p>But this is the principle Andrew is asserting, against myself and liberalism.</p>\n<p>Feminists think that women should be free from fear of rape, and that, if raped, no one should be able to excuse themselves with &#8220;well, she was asking for it&#8221;.</p>\n<p>But this is the same anti-violence principle as saying that the IRA shouldn&#8217;t throw nail-bombs through people&#8217;s windows or that, nail bombs having been thrown, the IRA can&#8217;t use as an excuse &#8220;Yeah, well, they were complicit with the evil British occupation, they deserved it.&#8221; Again, I feel like I&#8217;m defending this principle a whole lot more strongly and consistently than Andrew is.</p>\n<p>Feminists are, shall we say, divided about transgender people, but let&#8217;s allow that the correct solution is to respect their rights. </p>\n<p>When I was young and stupid, I <A HREF=\"http://slatestarcodex.com/2013/02/18/typical-mind-and-gender-identity/\">used to believe</A> that transgender was really, really dumb. That they were looking for attention or making it up or something along those lines. </p>\n<p>Luckily, since I was a classical liberal, my reaction to this mistake was &#8211; to not bother them, and to get very very angry at people who did bother them. I <A HREF=\"http://slatestarcodex.com/2013/12/28/a-comment-i-posted-on-what-would-jt-do/\">got upset with</A> people trying to fire Phil Robertson for being homophobic even though homophobia is stupid. You better bet I also got upset with people trying to fire transgender people back when I thought transgender was stupid.</p>\n<p>And then I grew older and wiser and learned &#8211; hey, transgender isn&#8217;t stupid at all, they have very important reasons for what they do and go through and I was atrociously wrong. And I said a mea culpa. </p>\n<p>But it could have been worse. I didn&#8217;t like transgender people, and so I <i>left them alone while still standing up for their rights</i>. My epistemic structure <i>failed gracefully</i>. For anyone who&#8217;s not <A HREF=\"http://en.wikipedia.org/wiki/Overconfidence_bias\">overconfident</A>, and so who expects massive epistemic failure on a variety of important issues all the time, graceful failure modes are a <i>really important feature</i> for an epistemic structure to have.</p>\n<p>God only knows what Andrew would have done, if through bad luck he had accidentally gotten it into his head that transgender people are bad. From his own words, we know he wouldn&#8217;t be &#8220;pussyfooting around with debate-team nonsense&#8221;.</p>\n<p>I admit there are many feminist principles that cannot be derived from, or are even opposed to my own liberal principles. For example, some feminists have suggested that pornography be banned because it increases the likelihood of violence against women. Others suggest that research into gender differences should be banned, or at least we should stigmatize and harass the researchers, because any discoveries made might lend aid and comfort to sexists.</p>\n<p>To the first, I would point out that there is now strong evidence</A> that pornography, especially violent objectifying pornography, <A HREF=\"http://slatestarcodex.com/2013/06/22/social-psychology-is-a-flamethrower/\">very significantly decreases violence against women</A>. I would ask them whether they&#8217;re happy that we did the nice liberal thing and waited until all the evidence came in so we could discuss it rationally, rather than immediately moving to harass and silence anyone taking the pro-pornography side.</p>\n<p>And to the second, well, we have a genuine disagreement. But I wonder whether they would prefer to discuss that disagreement reasonably, or whether we should both try to harass and destroy the other until one or both of us are too damaged to continue the struggle.</p>\n<p>And if feminists agree to have that reasonable discussion, but lose, I would tell them that they get a consolation prize. Having joined liberal society, they can be sure that no matter what those researchers find, I and all of their new liberal-society buddies will fight tooth and nail against anyone who uses any tiny differences those researchers find to challenge the central liberal belief that everyone of every gender has basic human dignity. Any victory for me is going to be a victory for feminists as well; maybe not a perfect victory, but a heck of a lot better than what they have right now.</p>\n<p><b>VI.</b></p>\n<p>I am not trying to fight all the evils of the world. I am just trying to cultivate my garden.</p>\n<p>And you argue: &#8220;But isn&#8217;t that selfish and oppressive and privileged? Isn&#8217;t that confining everyone outside of your walled garden to racism and sexism and nastiness?</p>\n<p>But there is a famous comic which demonstrates <A HREF=\"http://24.media.tumblr.com/tumblr_md4wxk9vp11rc6co7o1_1280.jpg\">what can happen to certain walled gardens</A>.</p>\n<p>Why yes, it does sound like I&#8217;m making the unshakeable assumption that liberalism always wins, doesn&#8217;t it? That people who voluntarily relinquish certain forms of barbarism will be able to gradually expand their territory against the hordes outside, instead of immediately being conquered by their less scrupulous neighbors? And it looks like Andrew isn&#8217;t going to let that assumption pass.</p>\n<p>He writes: </p>\n<blockquote><p>The *whole history* of why the institutional Left in our society is a party of toothless, spineless, gutless losers and they\u2019ve spent two generations doing nothing but lose.</p></blockquote>\n<p>One is reminded of the old joke about the Nazi papers. The rabbi catches an old Jewish man reading the Nazi newspaper and demands to know how he could look at such garbage. The man answers &#8220;When I read our Jewish newpapers, the news is so depressing &#8211; oppression, death, genocide! But here, everything is great! We control the banks, we control the media. Why, just yesterday they said we had a plan to kick the Gentiles out of Germany entirely!&#8221;</p>\n<p>And I have two thoughts about this.</p>\n<p>First, it argues that &#8220;Evil people are doing evil things, so we are justified in using any weapons we want to stop them, no matter how nasty&#8221; suffers from a certain flaw. Everyone believes their enemies are evil people doing evil things. If you&#8217;re a Nazi, you are just defending yourself, in a very proportionate manner, against the Vast Jewish Conspiracy To Destroy All Germans.</p>\n<p>But second, before taking Andrew&#8217;s words for how disastrously liberalism is doing, we should check the newspapers put out by liberalism&#8217;s enemies. Here&#8217;s Mencius Moldbug:</p>\n<blockquote><p>Cthulhu may swim slowly. But he only swims left. Isn&#8217;t that interesting?</p>\n<p>In each of the following conflicts in Anglo-American history, you see a victory of left over right: the English Civil War, the so-called &#8220;Glorious Revolution,&#8221; the American Revolution, the American Civil War, World War I, and World War II. Clearly, if you want to be on the winning team, you want to start on the left side of the field.</p>\n<p>Where is the John Birch Society, now? What about the NAACP? Cthulhu swims left, and left, and left. There are a few brief periods of true reaction in American history &#8211; the post-Reconstruction era or Redemption, the Return to Normalcy of Harding, and a couple of others. But they are unusual and feeble compared to the great leftward shift. McCarthyism is especially noticeable as such. And you&#8217;ll note that McCarthy didn&#8217;t exactly win.</p>\n<p>In the history of American democracy, if you take the mainstream political position (Overton Window, if you care) at time T1, and place it on the map at a later time T2, T1 is always way to the right, near the fringe or outside it. So, for instance, if you take the average segregationist voter of 1963 and let him vote in the 2008 election, he will be way out on the wacky right wing. Cthulhu has passed him by.</p></blockquote>\n<p>I&#8217;ve got to say Mencius makes a much more convincing argument than Andrew does.</p>\n<p>Robert Frost says &#8220;A liberal is a man too broad-minded to take his own side in a quarrel&#8221;. Ha ha ha.</p>\n<p>And yet, outside of Saudi Arabia you&#8217;ll have a hard time finding a country that doesn&#8217;t at least pay lip service to liberal ideas. Stranger still, many of those then go on to <i>actually implement them</i>, either voluntarily or after succumbing to strange pressures they don&#8217;t understand. In particular, the history of the past few hundred years in the United States has been a history of decreasing censorship and increasing tolerance. </p>\n<p>Contra the Reactionaries, feminism isn&#8217;t an exception to that, it&#8217;s a casualty of it. 1970s feminists were saying that all women need to rise up and smash the patriarchy, possibly with literal smashing-implements. 2010s feminists are saying that if some women want to be housewives, that&#8217;s great and their own choice because in a liberal society everyone should be free to pursue their own self-actualization.</p>\n<p>And that has <i>corresponded to</i> spectacular successes of the specific causes liberals like to push, like feminism, civil rights, gay marriage, et cetera, et cetera, et cetera.</p>\n<p>A liberal is a man too broad-minded to take his own side in a quarrel. And yet when liberals enter quarrels, they always win. Isn&#8217;t that interesting?</p>\n<p><b>VII.</b></p>\n<p>Andrew thinks that liberals who voluntarily relinquish any form of fighting back are just ignoring perfectly effective weapons. I&#8217;ll provide the quote:</p>\n<blockquote><p>In a war, a real war, a war for survival, you use all the weapons in your arsenal because you assume the enemy will use all the weapons in theirs. Because you understand that it IS a war&#8230; Any energy spent mentally debating how, in a perfect world run by a Lawful Neutral Cosmic Arbiter that will never exist, we could settle wars without bullets is energy you could better spend down at the range improving your marksmanship&#8230; I am amazed that the \u201crationalist community\u201d finds it to still be so opaque.</p></blockquote>\n<p>Let me name some other people who mysteriously managed to miss this perfectly obvious point.</p>\n<p>The early Christian Church had the slogan &#8220;resist not evil&#8221; (Matthew 5:39), and indeed, their idea of Burning The Fucking System To The Ground was to go unprotestingly to martyrdom while publicly forgiving their executioners. They were up against the Roman Empire, possibly the most effective military machine in history, ruled by some of the cruelest men who have ever lived. By Andrew&#8217;s reckoning, this should have been the biggest smackdown in the entire history of smackdowns. </p>\n<p>And it kind of was. Just not the way most people expected.</p>\n<p>Mahatma Gandhi said &#8220;Non-violence is the greatest force at the disposal of mankind. It is mightier than the mightiest weapon of destruction devised by the ingenuity of man.&#8221; Another guy who fought one of the largest empires ever to exist and won resoundingly. And he was pretty insistent on truth too: &#8220;Non-violence and truth are inseparable and presuppose one another.&#8221;</p>\n<p>Also skilled at missing the obvious: Martin Luther King. Desmond Tutu. Aung San Suu Kyi. Nelson Mandela was smart and effective at the beginning of his career, but fell into a pattern of missing the obvious when he was older. Maybe it was Alzheimers.</p>\n<p>Of course, there are counterexamples. Jews who nonviolently resisted the Nazis didn&#8217;t have a very good track record. You need a certain pre-existing level of civilization for liberalism to be a good idea, and a certain pre-existing level of liberalism for supercharged liberalism where you don&#8217;t spread malicious lies and harass other people to be a good idea. You need to have pre-existing community norms in place before trying to summon mysterious beneficial equilibria.</p>\n<p>So perhaps I am being too harsh on Andrew, to contrast him with Aung San Suu Kyi and her ilk. After all, all Aung San Suu Kyi had to do was fight the Burmese junta, a cabal of incredibly brutal military dictators who killed several thousand people, tortured anyone who protested against them, and sent eight hundred thousand people they just didn&#8217;t like to forced labor camps. Andrew has to deal with <i>people on Facebook who aren&#8217;t as feminist as he is</i>. Clearly this requires much stronger measures!</p>\n<p><b>VIII.</b></p>\n<p>Liberalism does not conquer by fire and sword. Liberalism conquers by communities of people who agree to play by the rules, slowly growing until eventually an equilibrium is disturbed. Its battle cry is not &#8220;Death to the unbelievers!&#8221; but &#8220;If you&#8217;re nice, you can join our cuddle pile!&#8221;</p>\n<p>But some people, through lack of imagination, fail to find this battle cry sufficiently fear-inspiring. </p>\n<p>I hate to invoke fictional evidence, especially since perhaps Andrew&#8217;s strongest point is that the real world doesn&#8217;t work like fiction. But these people need to read Jacqueline Carey&#8217;s <a href=\"https://www.amazon.com/Kushiels-Avatar-Legacy-Jacqueline-Carey/dp/0765347539/ref=as_li_ss_tl?_encoding=UTF8&#038;redirect=true&#038;ref_=as_li_tl&#038;linkCode=ll1&#038;tag=slatestarcode-20&#038;linkId=f61fdbf4491eba98c440ba7b494b01a6\"><i>Kushiel&#8217;s Avatar</i></a>.</p>\n<p>Elua is the god of kindness and flowers and free love. All the other gods are gods of blood and fire, and Elua is just like &#8220;Love as thou wilt&#8221; and &#8220;All knowlege is worth having&#8221;. He is the patron deity of exactly the kind of sickeningly sweet namby-pamby charitable liberalism that Andrew is complaining about.</p>\n<p>And there is a certain commonality to a lot of the Kushiel books, where some tyrant or sorcerer thinks that a god of flowers and free love will be a pushover, and starts harassing his followers. And the only Eluite who shows up to stop him is Ph\u00e8dre n\u00f3 Delaunay, and the tyrant thinks &#8220;Ha! A woman, who doesn&#8217;t even know how to fight, doesn&#8217;t have any magic! What a wuss!&#8221;</p>\n<p>But here is an important rule about dealing with fantasy book characters.</p>\n<p>If you ever piss off Sauron, you should probably find the Ring of Power and take it to Mount Doom.</p>\n<p>If you ever get piss off Voldemort, you should probably start looking for Horcruxes.</p>\n<p>If you ever piss off Ph\u00e8dre n\u00f3 Delaunay, <i>run and never stop running</i>.</p>\n<p>Elua is the god of flowers and free love and he is terrifying. If you oppose him, there will not be enough left of you to bury, and it will not matter because there will not be enough left of your city to bury you in.</p>\n<p>And Jacqueline Carey and Mencius Moldbug are both wiser than Andrew Cord.</p>\n<p>Carey portrays liberalism as Elua, a terrifying unspeakable Elder God who is fundamentally good.</p>\n<p>Moldbug portrays liberalism as Cthulhu, a terrifying unspeakable Elder God who is fundamentally evil.</p>\n<p>But Andrew? He <i>doesn&#8217;t even seem to realize liberalism is a terrifying unspeakable Elder God at all</i>. It&#8217;s like, <i>what?</i></p>\n<p>Andrew is the poor shmuck who is sitting there saying &#8220;Ha ha, a god who doesn&#8217;t even control any hell-monsters or command his worshippers to become killing machines. What a weakling! This is going to be so easy!&#8221;</p>\n<p>And you want to scream: &#8220;THERE IS ONLY ONE WAY THIS CAN POSSIBLY END AND IT INVOLVES YOU BEING EATEN BY YOUR OWN LEGIONS OF DEMONAICALLY CONTROLLED ANTS&#8221;</p>\n<p>(uh, spoilers)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ai87fPyyT6mWb4YkT": 4, "izp6eeJJEg9v5zcur": 18, "ogWsaHQKwa6ddidRC": 4, "Ng8Gice9KNkncxqcj": 2, "gHCNhqxuJq2bZ2akb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rkpDX7j7va6c8Q7cZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 70, "extendedScore": null, "score": 0.000183, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "xmDeR64CivZiTAcLx", "canonicalCollectionSlug": "codex", "canonicalBookId": "kcCvSNNZd8pfQvf9E", "canonicalNextPostSlug": "guided-by-the-beauty-of-our-weapons", "canonicalPrevPostSlug": "repost-the-demiurge-s-older-brother", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><i>[Content warning: Discussion of social justice, discussion of violence, spoilers for Jacqueline Carey books.]</i></p><i>\n</i><p><i>[Edit 10/25: This post was inspired by a debate with a friend of a friend on Facebook who has since become somewhat famous. I\u2019ve renamed him here to \u201cAndrew Cord\u201d to protect his identity.]</i></p>\n<p><b id=\"I_\">I.</b></p>\n<p>Andrew Cord <a href=\"http://www.patheos.com/blogs/hallq/2014/02/on-some-criticism-of-lesswrong/\">criticizes me</a> for my bold and controversial suggestion that maybe people should try to tell slightly fewer blatant hurtful lies:</p>\n<blockquote><p>I just find it kind of darkly amusing and sad that the \u201crationalist community\u201d loves \u201crationality is winning\u201d so much as a tagline and yet are clearly not winning. And then complain about losing rather than changing their tactics to match those of people who are winning.</p>\n<p>Which is probably because if you *really* want to be the kind of person who wins you have to actually care about winning something, which means you have to have politics, which means you have to embrace \u201cpolitics the mindkiller\u201d and \u201cpolitics is war and arguments are soldiers\u201d, and Scott would clearly rather spend the rest of his life losing than do this.</p>\n<p>That post [<a href=\"http://slatestarcodex.com/2014/02/17/lies-damned-lies-and-social-media-part-5-of-%e2%88%9e/\">the one debunking false rape statistics</a>] is exactly my problem with Scott. He seems to honestly think that it\u2019s a worthwhile use of his time, energy and mental effort to download evil people\u2019s evil worldviews into his mind and try to analytically debate them with statistics and cost-benefit analyses.</p>\n<p>He gets *mad* at people whom he detachedly intellectually agrees with but who are willing to back up their beliefs with war and fire rather than pussyfooting around with debate-team nonsense.</p>\n<p>It honestly makes me kind of sick. It is exactly the kind of thing that \u201csocial justice\u201d activists like me *intend* to attack and \u201ctrigger\u201d when we use \u201ctriggery\u201d catchphrases about the mewling pusillanimity of privileged white allies.</p></blockquote>\n<p>In other words, if a fight is important to you, fight nasty. If that means lying, lie. If that means insults, insult. If that means silencing people, silence.</p>\n<p>It always makes me happy when my ideological opponents come out and say eloquently and openly what I\u2019ve always secretly suspected them of believing.</p>\n<p>My natural instinct is to give some of the reasons why I think Andrew is wrong, starting with the history of the \u201cnoble lie\u201d concept and moving on to some examples of why it didn\u2019t work very well, and why it might not be expected not to work so well in the future.</p>\n<p>But in a way, that would be assuming the conclusion. I wouldn\u2019t be showing respect for Andrew\u2019s arguments. I wouldn\u2019t be going halfway to meet them on their own terms.</p>\n<p>The respectful way to rebut Andrew\u2019s argument would be to spread malicious lies about Andrew to a couple of media outlets, fan the flames, and wait for them to destroy his reputation.  Then if the stress ends up bursting an aneurysm in his brain, I can dance on his grave, singing: </p>\n<blockquote><p>\u266a \u266c I won this debate in a very effective manner. Now you can\u2019t argue in favor of nasty debate tactics any more \u266c \u266a</p></blockquote>\n<p>I\u2019m not going to do that, but if I <i>did</i> it\u2019s unclear to me how Andrew could object. I mean, he thinks that sexism is detrimental to society, so spreading lies and destroying people is justified in order to stop it. I think that discourse based on mud-slinging and falsehoods is detrimental to society. Therefore\u2026</p>\n<p><b id=\"II_\">II.</b></p>\n<p>But really, all this talk of lying and spreading rumors about people is \u2013 what was Andrew\u2019s terminology \u2013 \u201cpussyfooting around with debate-team nonsense\u201d. You know who got things done? The IRA. They didn\u2019t agree with the British occupation of Northern Ireland and they weren\u2019t afraid to let people know in that very special way only a nail-bomb shoved through your window at night can.</p>\n<p>Why not assassinate prominent racist and sexist politicians and intellectuals? I won\u2019t name names since that would be crossing a line, but I\u2019m sure you can generate several of them who are sufficiently successful and charismatic that, if knocked off, there would not be an equally competent racist or sexist immediately available to replace them, and it would thus be a serious setback for the racism/sexism movement.</p>\n<p>Other people can appeal to \u201cthe social contract\u201d or \u201cthe general civilizational rule not to use violence\u201d, but not Andrew:</p>\n<blockquote><p>I think that whether or not I use certain weapons has zero impact on whether or not those weapons are used against me, and people who think they do are either appealing to a kind of vague Kantian morality that I think is invalid or a specific kind of \u201chonor among foes\u201d that I think does not exist.</p></blockquote>\n<p>And don\u2019t give me that nonsense about the police. I\u2019m sure a smart person like you can think of clever exciting new ways to commit the perfect murder. Unless you do not believe there will <i>ever</i> be an opportunity to defect unpunished, you need this sort of social contract to take you at least some of the way.</p>\n<p>He continues:</p>\n<blockquote><p>When Scott calls rhetorical tactics he dislikes \u201cbullets\u201d and denigrates them it actually hilariously plays right into this point\u2026to be \u201cpro-bullet\u201d or \u201canti-bullet\u201d is ridiculous. Bullets, as you say, are neutral. I am in favor of my side using bullets as best they can to destroy the enemy\u2019s ability to use bullets.</p>\n<p>In a war, a real war, a war for survival, you use all the weapons in your arsenal because you assume the enemy will use all the weapons in theirs. Because you understand that it IS a war.</p></blockquote>\n<p>There are a lot of things I am tempted to say to this.</p>\n<p>Like \u201cAnd that is why the United States immediately nukes every country it goes to war with.\u201d</p>\n<p>Or \u201cAnd that is why the Geneva Convention was so obviously impossible that no one even bothered to attend the conference\u201d.</p>\n<p>Or \u201cAnd that is why, <a href=\"http://slatestarcodex.com/2013/05/22/apart-from-better-sanitation-and-medicine-and-education-and-irrigation-and-public-health-and-roads-and-public-order-what-has-modernity-done-for-us/\">to this very day</a>, we solve every international disagreement through total war.\u201d</p>\n<p>Or \u201cAnd that is why Martin Luther King was immediately reduced to a nonentity, and we remember the Weathermen as the sole people responsible for the success of the civil rights movement\u201d</p>\n<p>But I think what I am <i>actually</i> going to say is that, for the love of God, if you like bullets so much, stop using them as a metaphor for \u2018spreading false statistics\u2019 and go buy a gun.</p>\n<p><b id=\"III_\">III.</b></p>\n<p>So let\u2019s derive why violence is not in fact The One True Best Way To Solve All Our Problems. You can get most of this from <a href=\"http://smile.amazon.com/gp/product/1619491702/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=1619491702&amp;linkCode=as2&amp;tag=slastacod-20&amp;linkId=AYGYZKSORVSJ52VW\">Hobbes</a><img src=\"http://ir-na.amazon-adsystem.com/e/ir?t=slastacod-20&amp;l=as2&amp;o=1&amp;a=1619491702\" width=\"1\" height=\"1\" border=\"0\" alt=\"\" style=\"border:none !important; margin:0px !important;\">, but this blog post will be shorter.</p>\n<p>Suppose I am a radical Catholic who believes all Protestants deserve to die, and therefore go around killing Protestants. So far, so good.</p>\n<p>Unfortunately, there might be some radical Protestants around who believe all Catholics deserve to die. If there weren\u2019t before, there probably are now. So they go around killing Catholics, we\u2019re both unhappy and/or dead, our economy tanks, hundreds of innocent people end up as collateral damage, and our country goes down the toilet.</p>\n<p>So we make an agreement: I won\u2019t kill any more Catholics, you don\u2019t kill any more Protestants. The specific Irish example was called the Good Friday Agreement and the general case is called \u201ccivilization\u201d.</p>\n<p>So then I try to destroy the hated Protestants using the government. I go around trying to pass laws banning Protestant worship and preventing people from condemning Catholicism.</p>\n<p>Unfortunately, maybe the next government in power is a Protestant government, and they pass laws banning Catholic worship and preventing people from condemning Protestantism. No one can securely practice their own religion, no one can learn about other religions, people are constantly plotting civil war, academic freedom is severely curtailed, and once again the country goes down the toilet.</p>\n<p>So again we make an agreement. I won\u2019t use the apparatus of government against Protestantism, you don\u2019t use the apparatus of government against Catholicism. The specific American example is the First Amendment and the general case is called \u201cliberalism\u201d, or to be dramatic about it, \u201ccivilization 2.0\u201d</p>\n<p>Every case in which both sides agree to lay down their weapons and be nice to each other has corresponded to spectacular gains by both sides and a new era of human flourishing.</p>\n<p>\u201cWait a second, no!\u201d someone yells. \u201cI see where you\u2019re going with this. You\u2019re going to say that agreeing not to spread malicious lies about each other would also be a civilized and beneficial system. Like maybe the Protestants could stop saying that the Catholics worshipped the Devil, and the Catholics could stop saying the Protestants hate the Virgin Mary, and they could both relax the whole thing about the Jews baking the blood of Christian children into their matzah.</p>\n<p>\u201cBut your two examples were about contracts written on paper and enforced by the government. So maybe a \u2018no malicious lies\u2019 amendment to the Constitution would work if it were enforceable, <i>which it isn\u2019t</i>, but just <i>asking</i> people to stop spreading malicious lies is doomed from the start. The Jews will no doubt spread lies against <i>us</i>, so if we stop spreading lies about them, all we\u2019re doing is abandoning an effective weapon against a religion I personally know to be heathenish! Rationalists should win, so put the blood libel on the front page of every newspaper!\u201d</p>\n<p>Or, as Andrew puts it:</p>\n<blockquote><p>Whether or not I use certain weapons has zero impact on whether or not those weapons are used against me, and people who think they do are either appealing to a kind of vague Kantian morality that I think is invalid or a specific kind of \u201chonor among foes\u201d that I think does not exist.</p></blockquote>\n<p>So let\u2019s talk about how beneficial game-theoretic equilibria can come to exist even in the absence of centralized enforcers. I know of two main ways: reciprocal communitarianism, and divine grace.</p>\n<p>Reciprocal communitarianism is probably how altruism evolved. Some mammal started running <a href=\"https://en.wikipedia.org/wiki/Tit_for_tat\">TIT-FOR-TAT</a>, the program where you cooperate with anyone whom you expect to cooperate with you. Gradually you form a successful community of cooperators. The defectors either join your community and agree to play by your rules or get outcompeted.</p>\n<p>Divine grace is more complicated. I was tempted to call it \u201cspontaneous order\u201d until I remembered the rationalist proverb that if you don\u2019t understand something, you need to call it by a term that reminds you that don\u2019t understand it or else you\u2019ll think you\u2019ve explained it when you\u2019ve just named it. </p>\n<p>But consider the following: I am a pro-choice atheist. When I lived in Ireland, one of my friends was a pro-life Christian. I thought she was responsible for the unnecessary suffering of millions of women. She thought I was responsible for killing millions of babies. And yet she invited me over to her house for dinner without poisoning the food. And I ate it, and thanked her, and sent her a nice card, without smashing all her china.</p>\n<p>Please try not to be insufficiently surprised by this. Every time <a href=\"http://slatestarcodex.com/2013/03/17/not-just-a-mere-political-issue/\">a Republican and a Democrat break bread together with good will</a>, it is a miracle. It is an equilibrium as beneficial as civilization or liberalism, which developed in the total absence of any central enforcing authority. </p>\n<p>When you look for these equilibria, there are lots and lots. Andrew says there is no \u201chonor among foes\u201d, but if you read the <i>Iliad</i> or any other account of ancient warfare, there is practically nothing <i>but</i> honor among foes, and it wasn\u2019t generated by some sort of Homeric version of the Geneva Convention, it just sort of happened. During World War I, the English and Germans spontaneously got out of their trenches and celebrated Christmas together with each other, and on the sidelines Andrew was shouting \u201cNo! Stop celebrating Christmas! Quick, shoot them before they shoot you!\u201d but they didn\u2019t listen.</p>\n<p>All I will say in way of explaining these miraculous equilibria is that they seem to have something to do with inheriting a cultural norm and not screwing it up. Punishing the occasional defector seems to be a big part of not screwing it up. How exactly that cultural norm came to be is less clear to me, but it might have something to do with the reasons why <a href=\"http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/\">an entire civilization\u2019s bureaucrats may suddenly turn 100% honest at the same time</a>. I\u2019m pretty sure I\u2019m supposed to say the words <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</a> around this point too, and perhaps bring up the kind of Platonic contract <a href=\"http://slatestarcodex.com/2013/04/08/whose-utilitarianism/\">that I have written about previously</a>.</p>\n<p>I think most of our useful social norms exist through a combination of divine grace and reciprocal communitarianism. To some degree they arise spontaneously and are preserved by the honor system. To another degree, they are stronger or weaker in different groups, and the groups that enforce them are so much more pleasant than the groups that don\u2019t that people are willing to go along.</p>\n<p>The norm against malicious lies follows this pattern. Politicians lie, but not <i>too much</i>. Take the top story on Politifact Fact Check today. Some Republican claimed his supposedly-maverick Democratic opponent actually voted with Obama\u2019s economic policies <a href=\"http://www.politifact.com/truth-o-meter/statements/2014/feb/21/dan-sullivan/begich-supports-97-percent-obamas-economic-policy-/\">97 percent of the time</a>. Fact Check explains that the statistic used was actually for <i>all</i> votes, not just economic votes, and that members of Congress typically have to have &gt;90% agreement with their president because of the way partisan politics work. So it\u2019s a lie, and is properly listed as one. But it\u2019s a lie based on slightly misinterpreting a real statistic. He didn\u2019t just totally make up a number. He didn\u2019t even just make up something else, like \u201cMy opponent personally helped design most of Obama\u2019s legislation\u201d.</p>\n<p>Even the guy in the fake rape statistics post lied less than he <i>possibly could have</i>. He got his fake numbers by conflating rapes per sex act with rapes per lifetime, and it\u2019s really hard for me to imagine someone doing that by anything resembling accident. But he couldn\u2019t bring himself to go the extra step and just totally make up numbers with no grounding whatsoever. And part of me wonders: why not? If you\u2019re going to use numbers you know are false to destroy people, why is it better to derive the numbers through a formula you know is incorrect, than to just skip the math and make the numbers up in the first place? \u201cThe FBI has determined that no false rape claims have ever been submitted, my source is an obscure report they published, when your local library doesn\u2019t have it you will just accept that libraries can\u2019t have all books, and suspect nothing.\u201d </p>\n<p>This would have been a <i>more believable</i> claim than the one he made. Because he showed his work, it was easy for me to debunk it. If he had just said it was in some obscure report, I wouldn\u2019t have gone through the trouble. So why did he go the harder route?</p>\n<p>People <i>know</i> lying is wrong. They know if they lied they would be punished. More <s>spontaneous social order</s> miraculous divine grace. And so they want to hedge their bets, be able to say \u201cWell, I didn\u2019t exactly <i>lie</i>, per se.\u201d</p>\n<p>And this is good! We <i>want</i> to make it politically unacceptable to have people say that Jews bake the blood of Christian children into their matzah. Now we build on that success. We start hounding around the edges of currently acceptable lies. \u201cOkay, you didn\u2019t <i>literally</i> make up your statistics, but you still lied, and you still should be cast out from the community of people who have reasonable discussions and never trusted by anyone again.\u201d</p>\n<p>It might not totally succeed in making a new norm against this kind of thing. But at least it will prevent other people from seeing their success, taking heart, and having the number of lies which are socially acceptable gradually <i>advance</i>.</p>\n<p>So much for protecting what we have been given by divine grace. But there is also reciprocal communitarianism to think of.</p>\n<p>I seek out people who signal that they want to discuss things honestly and rationally. Then I try to discuss things honestly and rationally with those people. I try to concentrate as much of my social interaction there as possible.</p>\n<p>So far this project is going pretty well. My friends are nice, my romantic relationships are low-drama, my debates are productive and I am learning so, so much. </p>\n<p>And people think \u201cHm, I could hang out at 4Chan and be called a \u2018fag\u2019. Or I could hang out at Slate Star Codex and discuss things rationally and learn a lot. And if I want to be allowed in, all I have to do is not be an intellectually dishonest jerk.\u201d </p>\n<p>And so our community grows. And all over the world, the mysterious divine forces favoring honest and kind equilibria gain a little bit more power over the mysterious divine forces favoring lying and malicious equilibria.</p>\n<p>Andrew thinks I am trying to fight all the evils of the world, and doing so in a stupid way. But sometimes I just want to cultivate my garden.</p>\n<p><b id=\"IV_\">IV.</b></p>\n<p>Andrew goes on to complain:</p>\n<blockquote><p>Scott\u2026seems to [dispassionately debate] evil people\u2019s evil worldviews \u2026with statistics and cost-benefit analyses.</p>\n<p>He gets <i>mad</i> at people whom he detachedly intellectually agrees with but who are willing to back up their beliefs with war and fire rather than pussyfooting around with debate-team nonsense.</p></blockquote>\n<p>I accept this criticism as an accurate description of what I do.</p>\n<p>Compare to the following two critiques: \u201cThe Catholic Church wastes so much energy getting upset about heretics who believe <i>mostly</i> the same things as they do, when there are literally <i>millions</i> of Hindus over in India who don\u2019t believe in Catholicism <i>at all</i>! What dumb priorities!\u201d</p>\n<p>Or \u201cHow could Joseph McCarthy get angry about a couple of people who <i>might</i> have been Communists in the US movie industry, when over in Moscow there were <i>thousands</i> of people who were openly <i>super</i> Communist <i>all the time</i>?\u201d</p>\n<p>There might be foot-long giant centipedes in the Amazon, but I am a lot more worried about boll weevils in my walled garden.</p>\n<p>Creationists lie. Homeopaths lie. Anti-vaxxers lie. This is part of the Great Circle of Life. It is not necessary to call out every lie by a creationist, because the sort of person who is still listening to creationists is not the sort of person who is likely to be moved by call-outs. There is a role for <i>organized</i> action against creationists, like preventing them from getting their opinions taught in schools, but the marginal blog post \u201cdebunking\u201d a creationist on something is a waste of time. Everybody who wants to discuss things rationally has already formed a walled garden and locked the creationists outside of it.</p>\n<p>Anti-Semites fight nasty. The Ku Klux Klan fights nasty. Neo-Nazis fight nasty. We dismiss them with equanimity, in accordance with the ancient proverb: \u201cHaters gonna hate\u201d. There is a role for <i>organized</i> opposition to these groups, like making sure they can\u2019t actually terrorize anyone, but the marginal blog post condemning Nazism is a waste of time. Everybody who wants to discuss things charitably and compassionately has already formed a walled garden and locked the Nazis outside of it.</p>\n<p>People who want to discuss things rationally and charitably have not yet looked over the false rape statistics article and decided to lock Charles Clymer out of their walled garden.</p>\n<p>He is not a heathen, he is a heretic. He is not a foreigner, he is a traitor. He comes in talking all liberalism and statistics, and then he betrays the signals he has just sent. He is not just some guy who defects in the Prisoner\u2019s Dilemma. He is the guy who defects while wearing the <a href=\"http://www.redbubble.com/people/lalaithion/works/10939446-i-cooperate-in-the-prisoners-dilemma?p=t-shirt\">\u201cI COOPERATE IN PRISONERS DILEMMAS\u201d t-shirt</a>.</p>\n<p>What really, <i>really</i> bothered me wasn\u2019t Clymer at all: it was that <i>rationalists</i> were taking him seriously. Smart people, kind people! I even said so in my article. Boll weevils in our beautiful walled garden!</p>\n<p>Why am I always harping on feminism? I feel like we\u2019ve got a good thing going, we\u2019ve ratified our Platonic contract to be intellectually honest and charitable to each other, we are going about perma-cooperating in the Prisoner\u2019s Dilemma and reaping gains from trade.</p>\n<p>And then someone says \u201cExcept that of course regardless of all that I reserve the right to still use lies and insults and harassment and <a href=\"http://lesswrong.com/lw/uy/dark_side_epistemology/\">dark epistemology</a> to spread feminism\u201d. Sometimes they do this explicitly, like Andrew did. Other times they use a more nuanced argument like \u201cSurely you didn\u2019t think the same rules against lies and insults and harassment should apply to oppressed and privileged people, did you?\u201d And other times they don\u2019t say anything, but just show their true colors by reblogging an awful article with false statistics.</p>\n<p>(and still other times they don\u2019t do any of this and they are wonderful people whom I am glad to know)</p>\n<p>But then someone else says \u201cWell, if they get their exception, I deserve my exception,\u201d and then someone else says \u201cWell, if those two get exceptions, I\u2019m out\u201d, and <i>you have no idea how difficult it is to successfully renegotiate the terms of a timeless Platonic contract that doesn\u2019t literally exist</i>.</p>\n<p>No! I am Exception Nazi! NO EXCEPTION FOR YOU! Civilization didn\u2019t conquer the world by forbidding you to murder your enemies <i>unless</i> they are actually unrighteous in which case go ahead and kill them all. Liberals didn\u2019t give their lives in the battle against tyranny to end discrimination against all religions <i>except</i> Jansenism because seriously fuck Jansenists. Here we have built our <a href=\"http://lesswrong.com/lw/ase/schelling_fences_on_slippery_slopes/\">Schelling fence</a> and here we are defending it to the bitter end.</p>\n<p><b id=\"V_\">V.</b></p>\n<p>Contrary to how it may appear, I am not trying to doom feminism.</p>\n<p>Feminists like to mock the naivete of anyone who says that classical liberalism would suffice to satisfy feminist demands. And true, you cannot simply assume Adam Smith and derive Andrea Dworkin. Not being an asshole to women and not writing laws declaring them officially inferior are both good starts, but it not enough if there\u2019s still cultural baggage and entrenched gender norms.</p>\n<p>But here I am, defending this principle \u2013 kind of a supercharged version of liberalism \u2013 of \u201cIt is not okay to use lies, insults, and harassment against people, even if it would help you enforce your preferred social norms.\u201d</p>\n<p>And I notice that this gets us a heck of a lot closer to feminism than Andrew\u2019s principle of \u201cGo ahead and use lies, insults, and harassment if they are effective ways to enforce your preferred social norms.\u201d</p>\n<p>Feminists are very concerned about slut-shaming, where people harass women who have too much premarital sex. They point out that this is very hurtful to women, that men might underestimate the amount of hurt it causes women, and that the standard-classical-liberal solution of removing relevant government oppression does nothing. All excellent points.</p>\n<p>But one assumes the harassers think that women having premarital sex is detrimental to society. So they apply their general principle: \u201cI should use lies, insults, and harassment to enforce my preferred social norms.\u201d</p>\n<p>But this is the principle Andrew is asserting, against myself and liberalism.</p>\n<p>Feminists think that women should be free from fear of rape, and that, if raped, no one should be able to excuse themselves with \u201cwell, she was asking for it\u201d.</p>\n<p>But this is the same anti-violence principle as saying that the IRA shouldn\u2019t throw nail-bombs through people\u2019s windows or that, nail bombs having been thrown, the IRA can\u2019t use as an excuse \u201cYeah, well, they were complicit with the evil British occupation, they deserved it.\u201d Again, I feel like I\u2019m defending this principle a whole lot more strongly and consistently than Andrew is.</p>\n<p>Feminists are, shall we say, divided about transgender people, but let\u2019s allow that the correct solution is to respect their rights. </p>\n<p>When I was young and stupid, I <a href=\"http://slatestarcodex.com/2013/02/18/typical-mind-and-gender-identity/\">used to believe</a> that transgender was really, really dumb. That they were looking for attention or making it up or something along those lines. </p>\n<p>Luckily, since I was a classical liberal, my reaction to this mistake was \u2013 to not bother them, and to get very very angry at people who did bother them. I <a href=\"http://slatestarcodex.com/2013/12/28/a-comment-i-posted-on-what-would-jt-do/\">got upset with</a> people trying to fire Phil Robertson for being homophobic even though homophobia is stupid. You better bet I also got upset with people trying to fire transgender people back when I thought transgender was stupid.</p>\n<p>And then I grew older and wiser and learned \u2013 hey, transgender isn\u2019t stupid at all, they have very important reasons for what they do and go through and I was atrociously wrong. And I said a mea culpa. </p>\n<p>But it could have been worse. I didn\u2019t like transgender people, and so I <i>left them alone while still standing up for their rights</i>. My epistemic structure <i>failed gracefully</i>. For anyone who\u2019s not <a href=\"http://en.wikipedia.org/wiki/Overconfidence_bias\">overconfident</a>, and so who expects massive epistemic failure on a variety of important issues all the time, graceful failure modes are a <i>really important feature</i> for an epistemic structure to have.</p>\n<p>God only knows what Andrew would have done, if through bad luck he had accidentally gotten it into his head that transgender people are bad. From his own words, we know he wouldn\u2019t be \u201cpussyfooting around with debate-team nonsense\u201d.</p>\n<p>I admit there are many feminist principles that cannot be derived from, or are even opposed to my own liberal principles. For example, some feminists have suggested that pornography be banned because it increases the likelihood of violence against women. Others suggest that research into gender differences should be banned, or at least we should stigmatize and harass the researchers, because any discoveries made might lend aid and comfort to sexists.</p>\n<p>To the first, I would point out that there is now strong evidence that pornography, especially violent objectifying pornography, <a href=\"http://slatestarcodex.com/2013/06/22/social-psychology-is-a-flamethrower/\">very significantly decreases violence against women</a>. I would ask them whether they\u2019re happy that we did the nice liberal thing and waited until all the evidence came in so we could discuss it rationally, rather than immediately moving to harass and silence anyone taking the pro-pornography side.</p>\n<p>And to the second, well, we have a genuine disagreement. But I wonder whether they would prefer to discuss that disagreement reasonably, or whether we should both try to harass and destroy the other until one or both of us are too damaged to continue the struggle.</p>\n<p>And if feminists agree to have that reasonable discussion, but lose, I would tell them that they get a consolation prize. Having joined liberal society, they can be sure that no matter what those researchers find, I and all of their new liberal-society buddies will fight tooth and nail against anyone who uses any tiny differences those researchers find to challenge the central liberal belief that everyone of every gender has basic human dignity. Any victory for me is going to be a victory for feminists as well; maybe not a perfect victory, but a heck of a lot better than what they have right now.</p>\n<p><b id=\"VI_\">VI.</b></p>\n<p>I am not trying to fight all the evils of the world. I am just trying to cultivate my garden.</p>\n<p>And you argue: \u201cBut isn\u2019t that selfish and oppressive and privileged? Isn\u2019t that confining everyone outside of your walled garden to racism and sexism and nastiness?</p>\n<p>But there is a famous comic which demonstrates <a href=\"http://24.media.tumblr.com/tumblr_md4wxk9vp11rc6co7o1_1280.jpg\">what can happen to certain walled gardens</a>.</p>\n<p>Why yes, it does sound like I\u2019m making the unshakeable assumption that liberalism always wins, doesn\u2019t it? That people who voluntarily relinquish certain forms of barbarism will be able to gradually expand their territory against the hordes outside, instead of immediately being conquered by their less scrupulous neighbors? And it looks like Andrew isn\u2019t going to let that assumption pass.</p>\n<p>He writes: </p>\n<blockquote><p>The *whole history* of why the institutional Left in our society is a party of toothless, spineless, gutless losers and they\u2019ve spent two generations doing nothing but lose.</p></blockquote>\n<p>One is reminded of the old joke about the Nazi papers. The rabbi catches an old Jewish man reading the Nazi newspaper and demands to know how he could look at such garbage. The man answers \u201cWhen I read our Jewish newpapers, the news is so depressing \u2013 oppression, death, genocide! But here, everything is great! We control the banks, we control the media. Why, just yesterday they said we had a plan to kick the Gentiles out of Germany entirely!\u201d</p>\n<p>And I have two thoughts about this.</p>\n<p>First, it argues that \u201cEvil people are doing evil things, so we are justified in using any weapons we want to stop them, no matter how nasty\u201d suffers from a certain flaw. Everyone believes their enemies are evil people doing evil things. If you\u2019re a Nazi, you are just defending yourself, in a very proportionate manner, against the Vast Jewish Conspiracy To Destroy All Germans.</p>\n<p>But second, before taking Andrew\u2019s words for how disastrously liberalism is doing, we should check the newspapers put out by liberalism\u2019s enemies. Here\u2019s Mencius Moldbug:</p>\n<blockquote><p>Cthulhu may swim slowly. But he only swims left. Isn\u2019t that interesting?</p>\n<p>In each of the following conflicts in Anglo-American history, you see a victory of left over right: the English Civil War, the so-called \u201cGlorious Revolution,\u201d the American Revolution, the American Civil War, World War I, and World War II. Clearly, if you want to be on the winning team, you want to start on the left side of the field.</p>\n<p>Where is the John Birch Society, now? What about the NAACP? Cthulhu swims left, and left, and left. There are a few brief periods of true reaction in American history \u2013 the post-Reconstruction era or Redemption, the Return to Normalcy of Harding, and a couple of others. But they are unusual and feeble compared to the great leftward shift. McCarthyism is especially noticeable as such. And you\u2019ll note that McCarthy didn\u2019t exactly win.</p>\n<p>In the history of American democracy, if you take the mainstream political position (Overton Window, if you care) at time T1, and place it on the map at a later time T2, T1 is always way to the right, near the fringe or outside it. So, for instance, if you take the average segregationist voter of 1963 and let him vote in the 2008 election, he will be way out on the wacky right wing. Cthulhu has passed him by.</p></blockquote>\n<p>I\u2019ve got to say Mencius makes a much more convincing argument than Andrew does.</p>\n<p>Robert Frost says \u201cA liberal is a man too broad-minded to take his own side in a quarrel\u201d. Ha ha ha.</p>\n<p>And yet, outside of Saudi Arabia you\u2019ll have a hard time finding a country that doesn\u2019t at least pay lip service to liberal ideas. Stranger still, many of those then go on to <i>actually implement them</i>, either voluntarily or after succumbing to strange pressures they don\u2019t understand. In particular, the history of the past few hundred years in the United States has been a history of decreasing censorship and increasing tolerance. </p>\n<p>Contra the Reactionaries, feminism isn\u2019t an exception to that, it\u2019s a casualty of it. 1970s feminists were saying that all women need to rise up and smash the patriarchy, possibly with literal smashing-implements. 2010s feminists are saying that if some women want to be housewives, that\u2019s great and their own choice because in a liberal society everyone should be free to pursue their own self-actualization.</p>\n<p>And that has <i>corresponded to</i> spectacular successes of the specific causes liberals like to push, like feminism, civil rights, gay marriage, et cetera, et cetera, et cetera.</p>\n<p>A liberal is a man too broad-minded to take his own side in a quarrel. And yet when liberals enter quarrels, they always win. Isn\u2019t that interesting?</p>\n<p><b id=\"VII_\">VII.</b></p>\n<p>Andrew thinks that liberals who voluntarily relinquish any form of fighting back are just ignoring perfectly effective weapons. I\u2019ll provide the quote:</p>\n<blockquote><p>In a war, a real war, a war for survival, you use all the weapons in your arsenal because you assume the enemy will use all the weapons in theirs. Because you understand that it IS a war\u2026 Any energy spent mentally debating how, in a perfect world run by a Lawful Neutral Cosmic Arbiter that will never exist, we could settle wars without bullets is energy you could better spend down at the range improving your marksmanship\u2026 I am amazed that the \u201crationalist community\u201d finds it to still be so opaque.</p></blockquote>\n<p>Let me name some other people who mysteriously managed to miss this perfectly obvious point.</p>\n<p>The early Christian Church had the slogan \u201cresist not evil\u201d (Matthew 5:39), and indeed, their idea of Burning The Fucking System To The Ground was to go unprotestingly to martyrdom while publicly forgiving their executioners. They were up against the Roman Empire, possibly the most effective military machine in history, ruled by some of the cruelest men who have ever lived. By Andrew\u2019s reckoning, this should have been the biggest smackdown in the entire history of smackdowns. </p>\n<p>And it kind of was. Just not the way most people expected.</p>\n<p>Mahatma Gandhi said \u201cNon-violence is the greatest force at the disposal of mankind. It is mightier than the mightiest weapon of destruction devised by the ingenuity of man.\u201d Another guy who fought one of the largest empires ever to exist and won resoundingly. And he was pretty insistent on truth too: \u201cNon-violence and truth are inseparable and presuppose one another.\u201d</p>\n<p>Also skilled at missing the obvious: Martin Luther King. Desmond Tutu. Aung San Suu Kyi. Nelson Mandela was smart and effective at the beginning of his career, but fell into a pattern of missing the obvious when he was older. Maybe it was Alzheimers.</p>\n<p>Of course, there are counterexamples. Jews who nonviolently resisted the Nazis didn\u2019t have a very good track record. You need a certain pre-existing level of civilization for liberalism to be a good idea, and a certain pre-existing level of liberalism for supercharged liberalism where you don\u2019t spread malicious lies and harass other people to be a good idea. You need to have pre-existing community norms in place before trying to summon mysterious beneficial equilibria.</p>\n<p>So perhaps I am being too harsh on Andrew, to contrast him with Aung San Suu Kyi and her ilk. After all, all Aung San Suu Kyi had to do was fight the Burmese junta, a cabal of incredibly brutal military dictators who killed several thousand people, tortured anyone who protested against them, and sent eight hundred thousand people they just didn\u2019t like to forced labor camps. Andrew has to deal with <i>people on Facebook who aren\u2019t as feminist as he is</i>. Clearly this requires much stronger measures!</p>\n<p><b id=\"VIII_\">VIII.</b></p>\n<p>Liberalism does not conquer by fire and sword. Liberalism conquers by communities of people who agree to play by the rules, slowly growing until eventually an equilibrium is disturbed. Its battle cry is not \u201cDeath to the unbelievers!\u201d but \u201cIf you\u2019re nice, you can join our cuddle pile!\u201d</p>\n<p>But some people, through lack of imagination, fail to find this battle cry sufficiently fear-inspiring. </p>\n<p>I hate to invoke fictional evidence, especially since perhaps Andrew\u2019s strongest point is that the real world doesn\u2019t work like fiction. But these people need to read Jacqueline Carey\u2019s <a href=\"https://www.amazon.com/Kushiels-Avatar-Legacy-Jacqueline-Carey/dp/0765347539/ref=as_li_ss_tl?_encoding=UTF8&amp;redirect=true&amp;ref_=as_li_tl&amp;linkCode=ll1&amp;tag=slatestarcode-20&amp;linkId=f61fdbf4491eba98c440ba7b494b01a6\"><i>Kushiel\u2019s Avatar</i></a>.</p>\n<p>Elua is the god of kindness and flowers and free love. All the other gods are gods of blood and fire, and Elua is just like \u201cLove as thou wilt\u201d and \u201cAll knowlege is worth having\u201d. He is the patron deity of exactly the kind of sickeningly sweet namby-pamby charitable liberalism that Andrew is complaining about.</p>\n<p>And there is a certain commonality to a lot of the Kushiel books, where some tyrant or sorcerer thinks that a god of flowers and free love will be a pushover, and starts harassing his followers. And the only Eluite who shows up to stop him is Ph\u00e8dre n\u00f3 Delaunay, and the tyrant thinks \u201cHa! A woman, who doesn\u2019t even know how to fight, doesn\u2019t have any magic! What a wuss!\u201d</p>\n<p>But here is an important rule about dealing with fantasy book characters.</p>\n<p>If you ever piss off Sauron, you should probably find the Ring of Power and take it to Mount Doom.</p>\n<p>If you ever get piss off Voldemort, you should probably start looking for Horcruxes.</p>\n<p>If you ever piss off Ph\u00e8dre n\u00f3 Delaunay, <i>run and never stop running</i>.</p>\n<p>Elua is the god of flowers and free love and he is terrifying. If you oppose him, there will not be enough left of you to bury, and it will not matter because there will not be enough left of your city to bury you in.</p>\n<p>And Jacqueline Carey and Mencius Moldbug are both wiser than Andrew Cord.</p>\n<p>Carey portrays liberalism as Elua, a terrifying unspeakable Elder God who is fundamentally good.</p>\n<p>Moldbug portrays liberalism as Cthulhu, a terrifying unspeakable Elder God who is fundamentally evil.</p>\n<p>But Andrew? He <i>doesn\u2019t even seem to realize liberalism is a terrifying unspeakable Elder God at all</i>. It\u2019s like, <i>what?</i></p>\n<p>Andrew is the poor shmuck who is sitting there saying \u201cHa ha, a god who doesn\u2019t even control any hell-monsters or command his worshippers to become killing machines. What a weakling! This is going to be so easy!\u201d</p>\n<p>And you want to scream: \u201cTHERE IS ONLY ONE WAY THIS CAN POSSIBLY END AND IT INVOLVES YOU BEING EATEN BY YOUR OWN LEGIONS OF DEMONAICALLY CONTROLLED ANTS\u201d</p>\n<p>(uh, spoilers)</p>", "sections": [{"title": "I.", "anchor": "I_", "level": 1}, {"title": "II.", "anchor": "II_", "level": 1}, {"title": "III.", "anchor": "III_", "level": 1}, {"title": "IV.", "anchor": "IV_", "level": 1}, {"title": "V.", "anchor": "V_", "level": 1}, {"title": "VI.", "anchor": "VI_", "level": 1}, {"title": "VII.", "anchor": "VII_", "level": 1}, {"title": "VIII.", "anchor": "VIII_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XTWkjCJScy2GFAgDt", "Kbm6QnJv9dgWsPHQP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-23T22:35:28.134Z", "modifiedAt": null, "url": null, "title": "Beware garblejargon", "slug": "beware-garblejargon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:58.715Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KnaveOfAllTrades", "createdAt": "2012-07-20T02:08:23.538Z", "isAdmin": false, "displayName": "KnaveOfAllTrades"}, "userId": "FuACexYrBpyrMmz5C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rkWQgxMWMpKqhFrJz/beware-garblejargon", "pageUrlRelative": "/posts/rkWQgxMWMpKqhFrJz/beware-garblejargon", "linkUrl": "https://www.lesswrong.com/posts/rkWQgxMWMpKqhFrJz/beware-garblejargon", "postedAtFormatted": "Sunday, February 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beware%20garblejargon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeware%20garblejargon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkWQgxMWMpKqhFrJz%2Fbeware-garblejargon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beware%20garblejargon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkWQgxMWMpKqhFrJz%2Fbeware-garblejargon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkWQgxMWMpKqhFrJz%2Fbeware-garblejargon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p><a href=\"/user/BrienneStrohl\">Brienne</a> on Facebook:</p>\n<p><em> </em></p>\n<p><em>\"There seems to be a whole class of words that have some sort of anti-reasoning field. Everything's going fine, and then one of these wibbly wobbly distortion devices pops up and wrecks your whole field for a decade or three.</em></p>\n<p><em> </em></p>\n<p><em> </em></p>\n<p><em>Properties of garblejargon:</em></p>\n<p><em> </em></p>\n<p><em>(1) Early on, you gain status by using it.<br /> (2) Later, you lose status by not using it.<br /> (3) It reliably causes the user to confuse the map with the territory.<br /> (4) It feels really satisfying for most users.<br /> (5) When people attempt to explain the same thing without using the garblejargon, they either become overtly incoherent, or give accounts that directly contradict most similar attempts by others.</em></p>\n<p><em> </em></p>\n<p><em>Other examples of garblejargon: <a href=\"/lw/iv/the_futility_of_emergence/\">emergence</a>, <a href=\"http://en.wikipedia.org/wiki/Truthmaker\">truth-maker</a>, <a href=\"http://en.wikipedia.org/wiki/Universal_%28metaphysics%29\">universals</a> (maybe).\"</em></p>\n<p><em> </em></p>\n<p><a href=\"https://www.facebook.com/strohl89/posts/10152237491864598\">(Slightly edited for Less Wrong.)</a></p>\n<p>I'd note that 'accounts that directly contradict most similar attempts by others' might often agree with the original attempts in obvious literal denotation but differ significantly in subtle connotation or the cognitive processes they cue, such that the difference in <a href=\"/lw/4h/when_truth_isnt_enough/\"><em>sense</em></a> amounts to a huge difference in the ways of thinking promoted by the attempt to explain vs. by the original usage.</p>\n<p>What are other examples of garblejargon?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rkWQgxMWMpKqhFrJz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -2, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "25591", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8QzZKw9WHRxjR4948", "9hR2RmpJmxT8dyPo4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-23T23:50:38.330Z", "modifiedAt": null, "url": null, "title": "SUDT: A toy decision theory for updateless anthropics", "slug": "sudt-a-toy-decision-theory-for-updateless-anthropics", "viewCount": null, "lastCommentedAt": "2018-08-01T11:36:49.105Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NaZPjaLPCGZWdTyrL/sudt-a-toy-decision-theory-for-updateless-anthropics", "pageUrlRelative": "/posts/NaZPjaLPCGZWdTyrL/sudt-a-toy-decision-theory-for-updateless-anthropics", "linkUrl": "https://www.lesswrong.com/posts/NaZPjaLPCGZWdTyrL/sudt-a-toy-decision-theory-for-updateless-anthropics", "postedAtFormatted": "Sunday, February 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SUDT%3A%20A%20toy%20decision%20theory%20for%20updateless%20anthropics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASUDT%3A%20A%20toy%20decision%20theory%20for%20updateless%20anthropics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaZPjaLPCGZWdTyrL%2Fsudt-a-toy-decision-theory-for-updateless-anthropics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SUDT%3A%20A%20toy%20decision%20theory%20for%20updateless%20anthropics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaZPjaLPCGZWdTyrL%2Fsudt-a-toy-decision-theory-for-updateless-anthropics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaZPjaLPCGZWdTyrL%2Fsudt-a-toy-decision-theory-for-updateless-anthropics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2541, "htmlBody": "<p>The best approach I know for thinking about anthropic problems is Wei Dai's <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a> (UDT). We aren't yet able to solve all problems that we'd like to&mdash;for example, when it comes to game theory, the only games we have any idea how to solve are very symmetric ones&mdash;but for many anthropic problems, UDT gives the obviously correct solution. However, UDT is somewhat underspecified, and <a href=\"http://wiki.lesswrong.com/wiki/Ambient_decision_theory\">cousin_it's concrete models of UDT</a> based on formal logic are rather heavyweight if all you want is to figure out the solution to a simple anthropic problem.</p>\n<p>In this post, I introduce a toy decision theory, <em>Simple Updateless Decision Theory</em> or SUDT, which is most definitely not a replacement for UDT but makes it easy to formally model and solve the kind of anthropic problems that we usually apply UDT to. (And, of course, it gives the same solutions as UDT.) I'll illustrate this with a few examples.</p>\n<p>This post is a bit boring, because all it does is to take a bit of math that we already implicitly use all the time when we apply updateless reasoning to anthropic problems, and spells it out in excruciating detail. If you're already well-versed in that sort of thing, you're not going to learn much from this post. The reason I'm posting it anyway is that there are things I want to say about updateless anthropics, with a bit of simple math here and there, and while the math may be intuitive, the best thing I can point to in terms of details are the posts on UDT, which contain lots of irrelevant complications. So the main purpose of this post is to save people from having to reverse-engineer the simple math of SUDT from the more complex / less well-specified math of UDT.</p>\n<p>(I'll also argue that Psy-Kosh's non-anthropic problem is a type of counterfactual mugging, I'll use the concept of <a href=\"/lw/jkm/lzombies_lzombies/\">l-zombies</a> to explain why UDT's response to this problem is correct, and I'll explain why this argument still works if there aren't any l-zombies.)</p>\n<p align=\"center\">*</p>\n<p>I'll introduce SUDT by way of a first example: <strong>the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a></strong>. In my preferred version, Omega appears to you and tells you that it has thrown a very biased coin, which had only a 1/1000 chance of landing heads; however, in this case, the coin has in fact fallen heads, which is why Omega is talking to you. It asks you to choose between two options, (H) and (T). If you choose (H), Omega will create a Friendly AI; if you choose (T), it will destroy the world. However, there is a catch: Before throwing the coin, Omega made a prediction about which of these options you would choose if the coin came up heads (and it was able to make a highly confident prediction). If the coin had come up tails, Omega would have destroyed the world if it's predicted that you'd choose (H), and it would have created a Friendly AI if it's predicted (T). (Incidentally, if it hadn't been able to make a confident prediction, it would just have destroyed the world outright.)</p>\n<table border=\"1\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>Coin falls <strong>heads</strong> (chance = 1/1000)</td>\n<td>Coin falls <strong>tails</strong> (chance = 999/1000)</td>\n</tr>\n<tr>\n<td>You choose <strong>(H)</strong> if coin falls heads</td>\n<td align=\"center\">Positive intelligence explosion<br /></td>\n<td align=\"center\">Humanity wiped out</td>\n</tr>\n<tr>\n<td>You choose <strong>(T)</strong> if coin falls heads</td>\n<td align=\"center\">Humanity wiped out</td>\n<td align=\"center\">Positive intelligence explosion<br /></td>\n</tr>\n</tbody>\n</table>\n<p>In this example, we are considering two possible worlds: <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctext%7Bheads%7D\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctext%7Btails%7D\" alt=\"\" />. We write <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5COmega\" alt=\"\" /> (no pun intended) for the set of all possible worlds; thus, in this case, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5COmega%20=%20%5C%7B%5Ctext%7Bheads%7D,%5Ctext%7Btails%7D%5C%7D\" alt=\"\" />. We also have a probability distribution over <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5COmega\" alt=\"\" />, which we call <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BP%7D\" alt=\"\" />. In our example, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctextstyle%5Cmathbb%7BP%7D%28%5Ctext%7Bheads%7D%29%20=%20%5Cfrac%7B1%7D%7B1000%7D\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctextstyle%5Cmathbb%7BP%7D%28%5Ctext%7Btails%7D%29%20=%20%5Cfrac%7B999%7D%7B1000%7D\" alt=\"\" />.</p>\n<p>In the counterfactual mugging, there is only one situation you might find yourself in in which you need to make a decision, namely when Omega tells you that the coin has fallen heads. In general, we write <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BI%7D\" alt=\"\" /> for the set of all possible situations in which you might need to make a decision; the <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BI%7D\" alt=\"\" /> stands for the <em>information</em> available to you, including both sensory input and your memories. In our case, we'll write <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BI%7D%20=%20%5C%7B*%5C%7D\" alt=\"\" />, where <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=*\" alt=\"\" /> is the single situation where you need to make a decision.</p>\n<p>For every <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i%5Cin%5Cmathcal%7BI%7D\" alt=\"\" />, we write <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BA%7D%28i%29\" alt=\"\" /> for the set of possible <em>actions</em> you can take if you find yourself in situation <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i\" alt=\"\" />. In our case,<img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BA%7D%28*%29%20=%20%5C%7B%28%5Cmathrm%20H%29,%28%5Cmathrm%20T%29%5C%7D\" alt=\"\" />. A <em>policy</em> (or \"plan\") is a function <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%28i%29\" alt=\"\" /> that associates to every situation <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i%5Cin%5Cmathcal%7BI%7D\" alt=\"\" /> an action <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%28i%29%5Cin%5Cmathcal%7BA%7D%28i%29\" alt=\"\" /> to take in this situation. We write <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5CPi\" alt=\"\" /> for the set of all policies. In our case, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5CPi%20=%20%5C%7B%5Cpi_H,%5Cpi_T%5C%7D\" alt=\"\" />, where <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi_H%28*%29%20=%20%28%5Cmathrm%20H%29\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi_T%28*%29%20=%20%28%5Cmathrm%20T%29\" alt=\"\" />.</p>\n<p>Next, there is a set of <em>outcomes</em>, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BO%7D\" alt=\"\" />, which specify all the features of what happens in the world that make a difference to our final goals, and the <em>outcome function</em> <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%20:%20%5COmega%5Ctimes%5CPi%5Cto%5Cmathcal%7BO%7D\" alt=\"\" />, which for every possible world <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Comega%5Cin%5COmega\" alt=\"\" /> and every policy <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%5Cin%5CPi\" alt=\"\" /> specifies the outcome <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%5Comega,%5Cpi%29%5Cin%5Cmathcal%7BO%7D\" alt=\"\" /> that results from executing <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi\" alt=\"\" /> in the world <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Comega\" alt=\"\" />. In our case, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BO%7D%20=%20%5C%7B%28%2B%29,%28-%29%5C%7D\" alt=\"\" /> (standing for FAI and DOOM), and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%5Ctext%7Bheads%7D,%5Cpi_H%29%20=%20o%28%5Ctext%7Btails%7D,%5Cpi_T%29%20=%20%28%2B%29\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%5Ctext%7Bheads%7D,%5Cpi_T%29%20=%20o%28%5Ctext%7Btails%7D,%5Cpi_H%29%20=%20%28-%29\" alt=\"\" />.</p>\n<p>Finally, we have a <em>utility function</em> <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%20:%20%5Cmathcal%7BO%7D%5Cto%5Cmathbb%7BR%7D\" alt=\"\" />. In our case, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%28%28-%29%29%20=%200\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%28%28%2B%29%29%20=%201\" alt=\"\" />. (The exact numbers don't really matter, as long as <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%28%28%2B%29%29%20%3E%20u%28%28-%29%29\" alt=\"\" />, because utility functions don't change their meaning under affine transformations, i.e. when you add a constant to all utilities or multiply all utilities by a positive number.)</p>\n<p>Thus, <strong>an SUDT decision problem consists of the following ingredients:</strong> The sets <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5COmega\" alt=\"\" />, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BI%7D\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BO%7D\" alt=\"\" /> of possible worlds, situations you need to make a decision in, and outcomes; for every <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i%5Cin%5Cmathcal%7BI%7D\" alt=\"\" />, the set <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BA%7D%28i%29\" alt=\"\" /> of possible actions in that situation; the probability distribution <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BP%7D\" alt=\"\" />; and the outcome and utility functions <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%20:%20%5COmega%5Ctimes%5CPi%5Cto%5Cmathcal%7BO%7D\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%20:%20%5Cmathcal%7BO%7D%5Cto%5Cmathbb%7BR%7D\" alt=\"\" />. SUDT then says that you should choose a policy <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%5Cin%5CPi\" alt=\"\" /> that maximizes the expected utility <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BE%7D%5Bu%28o%28%5Cboldsymbol%7B%5Comega%7D,%5Cpi%29%29%5D\" alt=\"\" />, where <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BE%7D\" alt=\"\" /> is the expectation with respect to <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BP%7D\" alt=\"\" />, and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cboldsymbol%7B%5Comega%7D%5Cin%5COmega\" alt=\"\" /> is the true world.</p>\n<p>In our case, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BE%7D%5Bu%28o%28%5Cboldsymbol%7B%5Comega%7D,%5Cpi%29%29%5D\" alt=\"\" /> is just the probability of the good outcome <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%28%2B%29\" alt=\"\" />, according to the (prior) distribution <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BP%7D\" alt=\"\" />. For <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%20=%20%5Cpi_H\" alt=\"\" />, that probability is 1/1000; for <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%20=%20%5Cpi_T\" alt=\"\" />, it is 999/1000. Thus, SUDT (like UDT) recommends choosing (T).</p>\n<p>If you set up the problem in SUDT like that, it's kind of hidden why you could possibly think that's <em>not</em> the right thing to do, since we aren't distinguishing situations <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i%5Cin%5Cmathcal%7BI%7D\" alt=\"\" /> that are \"actually experienced\" in a particular possible world <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Comega%5Cin%5COmega\" alt=\"\" />; there's nothing in the formalism that reflects the fact that Omega never asks us for our choice if the coin comes up tails. In <a href=\"/lw/jkm/lzombies_lzombies/\">my post on l-zombies</a>, I've argued that this makes sense because even if there's no version of you that actually consciously experiences being in the heads world, this version still exists as a Turing machine and the choices that it makes influence what happens in the real world. If all mathematically possible experiences exist, so that there <em>aren't</em> any l-zombies, but some experiences are \"experienced more\" (have more \"magical reality fluid\") than others, the argument is even clearer&mdash;even if there's some anthropic sense in which, upon being told that the coin fell heads, you can conclude that you should assign a high probability of being in the heads world, <em>the same version of you still exists in the tails world</em>, and its choices influence what happens there. And if everything is experienced to the same degree (no magical reality fluid), the argument is clearer still.</p>\n<p align=\"center\">*</p>\n<p>From Vladimir Nesov's counterfactual mugging, let's move on to what I'd like to call <strong>Psy-Kosh's probably counterfactual mugging</strong>, better known as <a href=\"/lw/3dy/solve_psykoshs_nonanthropic_problem/\"><strong>Psy-Kosh's non-anthropic problem</strong></a>. This time, you're not alone: Omega gathers you together with 999,999 other advanced rationalists, all well-versed in anthropic reasoning and SUDT. It places each of you in a separate room. Then, as before, it throws a very biased coin, which has only a 1/1000 chance of landing heads. If the coin <em>does</em> land heads, then Omega asks all of you to choose between two options, (H) and (T). If the coin falls <em>tails</em>, on the other hand, Omega chooses one of you at random and asks that person to choose between (H) and (T). If the coin lands heads and you all choose (H), Omega will create a Friendly AI; same if the coin lands tails, and the person who's asked chooses (T); else, Omega will destroy the world.</p>\n<table border=\"1\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>Coin falls <strong>heads</strong> (chance = 1/1000)</td>\n<td>Coin falls <strong>tails</strong> (chance = 999/1000)</td>\n</tr>\n<tr>\n<td>Everyone chooses <strong>(H)</strong> if asked</td>\n<td align=\"center\">Positive intelligence explosion<br /></td>\n<td align=\"center\">Humanity wiped out</td>\n</tr>\n<tr>\n<td>Everyone chooses <strong>(T)</strong> if asked<br /></td>\n<td align=\"center\">Humanity wiped out</td>\n<td align=\"center\">Positive intelligence explosion</td>\n</tr>\n<tr>\n<td>Different people choose differently<br /></td>\n<td align=\"center\">Humanity wiped out</td>\n<td align=\"center\">(Depends on who is asked)<br /></td>\n</tr>\n</tbody>\n</table>\n<p>We'll assume that all of you prefer a positive FOOM over a gloomy DOOM, which means that all of you have the same values as far as the outcomes of this little dilemma are concerned: <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BO%7D%20=%20%5C%7B%28%2B%29,%28-%29%5C%7D\" alt=\"\" />, as before, and all of you have the same utility function, given by <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%28%28-%29%29%20=%200\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%28%28%2B%29%29%20=%201\" alt=\"\" />. As long as that's the case, we can apply SUDT to find a sensible policy for everybody to follow (though when there is more than one optimal policy, and the different people involved can't talk to each other, it may not be clear how one of the policies should be chosen).</p>\n<p>This time, we have a million different people, who can in principle each make an independent decision about what to answer if Omega asks them the question. Thus, we have <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BI%7D%20=%20%5C%7B1,\\dotsc,10^6%5C%7D\" alt=\"\" />. Each of these people can choose between (H) and (T), so <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BA%7D%28i%29%20=%20%5C%7B%28%5Cmathrm%20H%29,%28%5Cmathrm%20T%29%5C%7D\" alt=\"\" /> for every person <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i%5Cin%5Cmathcal%7BI%7D\" alt=\"\" />, and a policy <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi\" alt=\"\" /> is a function that returns either (H) or (T) for every <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i%5Cin%5Cmathcal%7BI%7D\" alt=\"\" />. Obviously, we're particularly interested in the policies <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi_H\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi_T\" alt=\"\" /> satisfying <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi_H%28i%29%20=%20%28%5Cmathrm%20H%29\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi_T%28i%29%20=%20%28%5Cmathrm%20T%29\" alt=\"\" /> for all <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=i\" alt=\"\" />.</p>\n<p>The possible worlds are <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5COmega%20=%20%5C%7B%5Ctext%7Bheads%7D,(%5Ctext%7Btails%7D,1),\\dotsc,(%5Ctext%7Btails%7D,10^6)%5C%7D\" alt=\"\" />, and their probabilities are <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctextstyle%5Cmathbb%7BP%7D%28%5Ctext%7Bheads%7D%29%20=%20%5Cfrac%7B1%7D%7B1000%7D\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctextstyle%5Cmathbb%7BP%7D%28(%5Ctext%7Btails%7D,i)%29%20=%20%5Cfrac%7B999%7D%7B1000%7D%5Ccdot10^%7B-6%7D\" alt=\"\" />. The outcome function is as follows: <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%5Ctext%7Bheads%7D,%5Cpi_H%29%20=%20%28%2B%29\" alt=\"\" />, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%5Ctext%7Bheads%7D,%5Cpi%29%20=%20%28-%29\" alt=\"\" /> for <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=\\pi\\neq\\pi_H\" alt=\"\" />, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o((\\mathrm{tails},i),\\pi)=(%2B)\" alt=\"\" /> if <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=\\pi(i)=(\\mathrm{T})\" alt=\"\" />, and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%28%5Cmathrm%7Btails%7D,i%29,%5Cpi%29=%28-%29\" alt=\"\" /> otherwise.</p>\n<p>What does SUDT recommend? As in the counterfactual mugging, <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathbb%7BE%7D%5Bu%28o%28%5Cboldsymbol%7B%5Comega%7D,%5Cpi%29%29%5D\" alt=\"\" /> is the probability of the good outcome <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%28%2B%29\" alt=\"\" />, under policy <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi\" alt=\"\" />. For <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%20=%20%5Cpi_H\" alt=\"\" />, the good outcome can only happen if the coin falls heads: in other words, with probability <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cfrac%7B1%7D%7B1000%7D\" alt=\"\" />. If <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%20\\neq%20%5Cpi_H\" alt=\"\" />, then the good outcome can <em>not</em> happen if the coin falls heads, because in that case everybody gets asked, and at least one person chooses (T). Thus, in this case, the good outcome will happen only if the coin comes up tails and the randomly chosen person answers (T); this probability is <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cfrac%7B999%7D%7B1000%7D\\cdot n\\cdot10^{-6}\" alt=\"\" />, where <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=n\" alt=\"\" /> is the number of people answering (T). Clearly, this is maximized for <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cpi%20=%20%5Cpi_T\" alt=\"\" />, where <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=n=10^6\" alt=\"\" />; moreover, in this case we get the probability <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cfrac%7B999%7D%7B1000%7D&gt;%5Cfrac%7B1%7D%7B1000%7D\" alt=\"\" />, which is better than for <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%20%5Cpi_H\" alt=\"\" />, so SUDT recommends the plan <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%20%5Cpi_T\" alt=\"\" />.</p>\n<p>Again, when you set up the problem in SUDT, it's not even obvious why anyone might think this <em>wasn't</em> the correct answer. The reason is that if Omega asks you, and you update on the fact that you've been asked, then after updating, you are quite certain that the coin has landed <em>heads</em>: yes, your prior probability was only 1/1000, but if the coin has landed tails, the chances that <em>you</em> would be asked was only one in a million, so the posterior odds are about 1000:1 in favor of heads. So, you might reason, it would be best if everybody chose (H); and moreover, all the people in the other rooms will reason the same way as you, so if you choose (H), they will as well, and this maximizes the probability that humanity survives. This relies on the fact that the others will choose the same way as you, but since you're all good rationalists using the same decision theory, that's going to be the case.</p>\n<p>But in the worlds where the coin comes up tails, and Omega chooses someone else than you, the version of you that gets asked for its decision still \"exists\"... as an l-zombie. You might think that what this version of you does or doesn't do doesn't influence what happens in the real world; but if we accept the argument from the previous paragraph that your decisions are \"linked\" to those of the other people in the experiment, then they're <em>still</em> linked if the version of you making the decision is an l-zombie: If we see you as a Turing machine making a decision, that Turing machine should reason, \"If the coin came up tails and someone else was chosen, then I'm an l-zombie, but the person who is actually chosen will reason exactly the same way I'm doing now, and will come to the same decision; hence, my decision influences what happens in the real world even in this case, and I can't do an update and just ignore those possible worlds.\"</p>\n<p>I call this the \"probably counterfactual mugging\" because in the counterfactual mugging, you are making your choice because of its benefits in a possible world that is <em>ruled out</em> by your observations, while in the probably counterfactual mugging, you're making it because of its benefits in a set of possible worlds that is made <em>very improbable</em> by your observations (because <em>most</em> of the worlds in this set are ruled out). As with the counterfactual mugging, this argument is just all the stronger if there are no l-zombies because all mathematically possible experiences are in fact experienced.</p>\n<p align=\"center\">*</p>\n<p>As a final example, let's look at what I'd like to call <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\"><strong>Eliezer's anthropic mugging</strong></a>: the anthropic problem that inspired Psy-Kosh's non-anthropic one. This time, you're alone again, except that there's many of you: Omega is creating a million copies of you. It flips its usual very biased coin, and if that coin falls heads, it places all of you in exactly identical green rooms. If the coin falls tails, it places <em>one</em> of you in a green room, and all the others in red rooms. It then asks all copies in green rooms to choose between (H) and (T); if your choice agrees with the coin, FOOM, else DOOM.</p>\n<table border=\"1\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>Coin falls <strong>heads</strong> (chance = 1/1000)</td>\n<td>Coin falls <strong>tails</strong> (chance = 999/1000)</td>\n</tr>\n<tr>\n<td>Green roomers choose <strong>(H)</strong></td>\n<td align=\"center\">Positive intelligence explosion<br /></td>\n<td align=\"center\">Humanity wiped out</td>\n</tr>\n<tr>\n<td>Green roomers choose <strong>(T)</strong></td>\n<td align=\"center\">Humanity wiped out</td>\n<td align=\"center\">Positive intelligence explosion<br /></td>\n</tr>\n</tbody>\n</table>\n<p>Our possible worlds are back to being <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5COmega%20=%20%5C%7B%5Ctext%7Bheads%7D,%5Ctext%7Btails%7D%5C%7D\" alt=\"\" />, with probabilities <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctextstyle%5Cmathbb%7BP%7D%28%5Ctext%7Bheads%7D%29%20=%20%5Cfrac%7B1%7D%7B1000%7D\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Ctextstyle%5Cmathbb%7BP%7D%28%5Ctext%7Btails%7D%29%20=%20%5Cfrac%7B999%7D%7B1000%7D\" alt=\"\" />. We are also back to being able to make a choice in only one particular situation, namely when you're a copy in a green room: <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BI%7D%20=%20%5C%7B*%5C%7D\" alt=\"\" />. Actions are <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BA%7D%28*%29%20=%20%5C%7B%28%5Cmathrm%20H%29,%28%5Cmathrm%20T%29%5C%7D\" alt=\"\" />, outcomes <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=%5Cmathcal%7BO%7D%20=%20%5C%7B%28%2B%29,%28-%29%5C%7D\" alt=\"\" />, utilities <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%28%28-%29%29%20=%200\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=u%28%28%2B%29%29%20=%201\" alt=\"\" />, and the outcome function is given by <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%5Ctext%7Bheads%7D,%5Cpi_H%29%20=%20o%28%5Ctext%7Btails%7D,%5Cpi_T%29%20=%20%28%2B%29\" alt=\"\" /> and <img src=\"http://s0.wp.com/latex.php?bg=ffffff&amp;fg=000000&amp;s=0&amp;latex=o%28%5Ctext%7Bheads%7D,%5Cpi_T%29%20=%20o%28%5Ctext%7Btails%7D,%5Cpi_H%29%20=%20%28-%29\" alt=\"\" />. In other words, from SUDT's perspective, this is <em>exactly identical</em> to the situation with the counterfactual mugging, and thus the solution is the same: Once more, SUDT recommends choosing (T).</p>\n<p>On the other hand, the reason why someone might think that (H) could be the right answer is closer to that for Psy-Kosh's probably counterfactual mugging: After waking up in a green room, what should be your posterior probability that the coin has fallen heads? Updateful anthropic reasoning says that you should be quite sure that it has fallen heads. If you plug those probabilities into an expected utility calculation, it comes out as in Psy-Kosh's case, heavily favoring (H).</p>\n<p>But even if these are good probabilities to assign epistemically (to satisfy your curiosity about what the world probably looks like), in light of the arguments from the counterfactual and the probably counterfactual muggings (where updating <em>definitely</em> is the right thing to do epistemically, but plugging these probabilities into the expected utility calculation gives the wrong result), it doesn't seem strange to me to come to the conclusion that choosing (T) is correct in Eliezer's anthropic mugging as well.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NaZPjaLPCGZWdTyrL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 27, "extendedScore": null, "score": 1.5793991142172314e-06, "legacy": true, "legacyId": "25551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": true, "version": "1.0.0", "pingbacks": {"Posts": ["7nAxgQYGYrEY5ZCAD", "YZzoWGCJsoRBBbmQg", "ZTEkZNLrmycNuCNYq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-24T04:13:10.620Z", "modifiedAt": null, "url": null, "title": "LINK: In favor of niceness, community, and civilisation", "slug": "link-in-favor-of-niceness-community-and-civilisation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:08.506Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d83JhJYFFcidmcipN/link-in-favor-of-niceness-community-and-civilisation", "pageUrlRelative": "/posts/d83JhJYFFcidmcipN/link-in-favor-of-niceness-community-and-civilisation", "linkUrl": "https://www.lesswrong.com/posts/d83JhJYFFcidmcipN/link-in-favor-of-niceness-community-and-civilisation", "postedAtFormatted": "Monday, February 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20In%20favor%20of%20niceness%2C%20community%2C%20and%20civilisation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20In%20favor%20of%20niceness%2C%20community%2C%20and%20civilisation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd83JhJYFFcidmcipN%2Flink-in-favor-of-niceness-community-and-civilisation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20In%20favor%20of%20niceness%2C%20community%2C%20and%20civilisation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd83JhJYFFcidmcipN%2Flink-in-favor-of-niceness-community-and-civilisation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd83JhJYFFcidmcipN%2Flink-in-favor-of-niceness-community-and-civilisation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>Scott, known on LessWrong as Yvain, recently wrote a post complaining about <a href=\"http://slatestarcodex.com/2014/02/17/lies-damned-lies-and-social-media-part-5-of-%E2%88%9E/\">an inaccurate rape statistic</a>.</p>\n<p>Arthur Chu, who is notable for winning money on Jeopardy recently, argued against Scott's stance that we should be honest in arguments in a comment thread on Jeff Kaufman's Facebook profile, which can be read <a href=\"http://www.jefftk.com/p/are-feminist-blog-stats-atypically-bad#fb-649351137992_976222\">here</a>.</p>\n<p>Scott just responded <a href=\"http://slatestarcodex.com/2014/02/23/in-favor-of-niceness-community-and-civilization/\">here</a>, with a number of points relevant to the topic of rationalist communities.</p>\n<p>I am interested in what LW thinks of this.</p>\n<p>Obviously, at some point being polite in our arguments is silly. I'd be interested in people's opinions of how dire the real world consequences have to be before it's worthwhile debating dishonestly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NDjABjx6DRfSKcqED": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d83JhJYFFcidmcipN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 42, "extendedScore": null, "score": 1.5797087513321858e-06, "legacy": true, "legacyId": "25596", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 137, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-24T13:04:51.261Z", "modifiedAt": null, "url": null, "title": "What are some related communities online?", "slug": "what-are-some-related-communities-online", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:08.953Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "casebash", "createdAt": "2012-11-04T06:10:08.650Z", "isAdmin": false, "displayName": "casebash"}, "userId": "MrwJ5w7siWBQ4bMiE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FNr2mNzdDwgKkFpzS/what-are-some-related-communities-online", "pageUrlRelative": "/posts/FNr2mNzdDwgKkFpzS/what-are-some-related-communities-online", "linkUrl": "https://www.lesswrong.com/posts/FNr2mNzdDwgKkFpzS/what-are-some-related-communities-online", "postedAtFormatted": "Monday, February 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20some%20related%20communities%20online%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20some%20related%20communities%20online%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNr2mNzdDwgKkFpzS%2Fwhat-are-some-related-communities-online%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20some%20related%20communities%20online%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNr2mNzdDwgKkFpzS%2Fwhat-are-some-related-communities-online", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFNr2mNzdDwgKkFpzS%2Fwhat-are-some-related-communities-online", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p>I think that it might be useful to create a list of related communities online that people might want to check out. Suggestions much appreciated.</p>\n<p><strong>Directly Related to rationalism</strong></p>\n<p><a href=\"http://skeptics.stackexchange.com/\">Skeptics Stack Exchange</a> - useful for confirming factual claims you are skeptical of. Requires specific, answerable questions</p>\n<p><a href=\"http://boards.straightdope.com/sdmb/\">Straight dope</a> - similar to Skeptics, but less strict on the questions accepted</p>\n<p><a href=\"http://reddit.com/r/changemyview\">Change my view</a> - place to get your view challenged</p>\n<p><a href=\"http://cogsci.stackexchange.com/\">Cognitive Science</a>&nbsp;- you are expected to do your reading first, but useful if your want to lean about what the research says</p>\n<p><a href=\"http://www.reddit.com/r/philosophy\">Philosophy Reddit</a></p>\n<p><a href=\"http://rationalwiki.org/wiki/Main_Page\">Rational Wiki</a>: I only just learned about the existence of this site, so I don't know how good it is yet</p>\n<p><strong>Secondary</strong>:</p>\n<p>Stacks:&nbsp;<a href=\"http://stackoverflow.com/\">StackOverflow</a>, <a href=\"http://cs.stackexchange.com/\">Computer Science StackExchange</a>, <a href=\"http://cstheory.stackexchange.com/\">Theoretical computer science</a>, <a href=\"http://mathoverflow.net/\">MathOverflow</a>, <a href=\"http://math.stackexchange.com/\">Math StackExchange</a>, <a href=\"http://stats.stackexchange.com/\">Cross Validated</a>, <a href=\"http://physics.stackexchange.com/\">Physics Stackexchange</a></p>\n<p><a href=\"http://www.reddit.com/r/askscience\">Ask Science</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FNr2mNzdDwgKkFpzS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "25598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-24T13:43:20.443Z", "modifiedAt": null, "url": null, "title": "How to teach to magical thinkers?", "slug": "how-to-teach-to-magical-thinkers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:03.727Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SdnqovootBygJPk6P/how-to-teach-to-magical-thinkers", "pageUrlRelative": "/posts/SdnqovootBygJPk6P/how-to-teach-to-magical-thinkers", "linkUrl": "https://www.lesswrong.com/posts/SdnqovootBygJPk6P/how-to-teach-to-magical-thinkers", "postedAtFormatted": "Monday, February 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20teach%20to%20magical%20thinkers%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20teach%20to%20magical%20thinkers%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdnqovootBygJPk6P%2Fhow-to-teach-to-magical-thinkers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20teach%20to%20magical%20thinkers%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdnqovootBygJPk6P%2Fhow-to-teach-to-magical-thinkers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdnqovootBygJPk6P%2Fhow-to-teach-to-magical-thinkers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>I'm afraid I haven't properly designed the Muggles Studies <a href=\"/lw/irh/a_muggle_studies_course/\">course</a> I introduced at my local Harry Potter fan club. Last Sunday we finally had our second class (after wasted months of insistence and delays), and I introduced some very basic descriptions of common biases, while of course emphasizing the <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">need</a> to detect them in ourselves before trying to detect them in other people. At some point, which I didn't completely notice, the discussion changed from an explanation of the attribution bias into a series of multicultural examples in favor of moral relativity. I honestly don't know how that happened, but as more and more attendants voiced their comments, I started to fear someone would irreversibly damage the lessons I was trying to teach. They basically stopped short of calling the scientific method a cultural construct, at which point I'm sure I would have snapped. I don't know what to make of this. Some part of me tries to encourage me and make me put more effort into showing these people the need for more reductionism in their worldview, but another part of me just wants to give them up as hopeless postmodernists. What should I do?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SdnqovootBygJPk6P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "25599", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZhWGp2Wax5ZSHabPY", "AdYdLP2sRqPMoe8fb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-24T17:20:49.248Z", "modifiedAt": null, "url": null, "title": "A medium for more rational discussion", "slug": "a-medium-for-more-rational-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:01.146Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ptXKHqSxsgjmvAfz2/a-medium-for-more-rational-discussion", "pageUrlRelative": "/posts/ptXKHqSxsgjmvAfz2/a-medium-for-more-rational-discussion", "linkUrl": "https://www.lesswrong.com/posts/ptXKHqSxsgjmvAfz2/a-medium-for-more-rational-discussion", "postedAtFormatted": "Monday, February 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20medium%20for%20more%20rational%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20medium%20for%20more%20rational%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptXKHqSxsgjmvAfz2%2Fa-medium-for-more-rational-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20medium%20for%20more%20rational%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptXKHqSxsgjmvAfz2%2Fa-medium-for-more-rational-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptXKHqSxsgjmvAfz2%2Fa-medium-for-more-rational-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p>It would be cool if online discussions allowed you to 1) declare your claims, 2) declare how your claims depend on each other (ie. make a dependency tree), 3) discuss the claims, and 4) update the status of the claim by saying whether or not you agree with it, and using something like the <a href=\"/r/discussion/lw/iv0/creating_a_text_shorthand_for_uncertainty/\">text shorthand for uncertainty</a> to say how confident you are in your agreement/disagreement.</p>\n<p>I think that mapping out these things visually would allow for more productive conversation. And it would also allow newcomers to the discussion to quickly and easily get up to date, rather than having to sift through tons of comments. On this note, there should also probably be something like an&nbsp;<a href=\"http://www.quora.com/Quora-Answer-Wikis-Feature/What-is-an-Answer-Wiki-on-Quora\">answer wiki</a> for each claim to summarize the arguments and say what the consensus is.</p>\n<p>I get the feeling that it should be flexible though. That probably means that it should be accompanied by the normal commenting system. Sometimes you don't actually know what your claims are, but need to \"talk it out\" in order to figure out what they are. Sometimes you don't really know how they depend on each other. And sometimes you have something tangential to say (on that note, there should probably be an area for tangential comments, or at least a way to flag them as tangential).</p>\n<p>As far who would be interested in this, obviously this Less Wrong community would be interested, and I think that there are definitely some other online communities that would (Hacker News, some subreddits...).</p>\n<p>Also, this may be speculating, but I would hope that it would develop a reputation for the most effective way to have a productive discussion. So much so that people would start saying, \"go outline your argument on [name]\". Maybe there'd even be pressure for politicians to do this. If so, then I think this could put pressure on society to be more rational.</p>\n<p>What do you guys think?</p>\n<p>&nbsp;</p>\n<p>EDIT: If anyone is actually interested in building this, you definitely have my permission (don't worry about \"stealing the idea\"). I want to build it, but 1) I don't think I'm a good enough programmer yet, and 2) I'm busy with my startup.</p>\n<p>EDIT: Another idea: if you think that a statement commits an established fallacy, then you should be able to flag it (like <a href=\"https://d262ilb51hltx0.cloudfront.net/max/1180/0*ZbM17O_TeidBxbVM.gif\">this</a>). And if enough other people agree, then the statement is underlined or highlighted or something. The advantage to this is that it makes the discussion less \"bulky\". A simple version of this would be flagging things as <a href=\"http://www.paulgraham.com/disagree.html\">less than DH6</a>. But there are obviously a bunch of other things worth flagging that Eliezer has talked about in the sequences that are pretty non-controversial.</p>\n<p>EDIT: Here is a <a href=\"https://www.dropbox.com/s/wisoy2og4rb4zps/rationalmedium.pdf\">rough mockup</a> of how it would look. Notes:&nbsp;</p>\n<p>- The claims should show how many votes of agreement/disagreement they got. Probably using&nbsp;<a href=\"/r/discussion/lw/iv0/creating_a_text_shorthand_for_uncertainty/\">text shorthand for uncertainty</a>.</p>\n<p>- The claims should be colored green if there is a lot of agreement, and red if there is a lot of disagreement.</p>\n<p>- See edit above. Commenting in the discussion should be like&nbsp;<a href=\"https://d262ilb51hltx0.cloudfront.net/max/1180/0*ZbM17O_TeidBxbVM.gif\">this</a>. And you should be able to flag statements as fallacious in a similar way. If there is enough agreement about the flag, the statement should be underlined in red or something.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ptXKHqSxsgjmvAfz2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 13, "extendedScore": null, "score": 1.580638342253663e-06, "legacy": true, "legacyId": "25600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jsfSXH8mGrLy9pPqr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-24T22:20:24.948Z", "modifiedAt": null, "url": null, "title": "Interest in a Christchurch (New Zealand) Meetup", "slug": "interest-in-a-christchurch-new-zealand-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:39.922Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "free_rip", "createdAt": "2010-04-04T09:56:57.893Z", "isAdmin": false, "displayName": "free_rip"}, "userId": "q8zJHtRt24KhQ6fSs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DiPXSYeuphffBT5te/interest-in-a-christchurch-new-zealand-meetup", "pageUrlRelative": "/posts/DiPXSYeuphffBT5te/interest-in-a-christchurch-new-zealand-meetup", "linkUrl": "https://www.lesswrong.com/posts/DiPXSYeuphffBT5te/interest-in-a-christchurch-new-zealand-meetup", "postedAtFormatted": "Monday, February 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interest%20in%20a%20Christchurch%20(New%20Zealand)%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterest%20in%20a%20Christchurch%20(New%20Zealand)%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDiPXSYeuphffBT5te%2Finterest-in-a-christchurch-new-zealand-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interest%20in%20a%20Christchurch%20(New%20Zealand)%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDiPXSYeuphffBT5te%2Finterest-in-a-christchurch-new-zealand-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDiPXSYeuphffBT5te%2Finterest-in-a-christchurch-new-zealand-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<p>I'm considering starting a Christchurch LessWrong Meetup and would like to get a measure of interest in the area. Including me, there are already three people interested, so you're sure to meet someone new! Please comment if you'd be at all interested.</p>\n<p>&nbsp;</p>\n<p>I'd also like to find out if Sunday afternoon/evening is a good time for you. Considering Chch is pretty small, I'd like to find a time everyone can make it.</p>\n<p>&nbsp;</p>\n<p>Location would likely be James Hight Library at Canterbury University (there are bookable private discussion rooms, nearby food places, and anyone can access it).</p>\n<p>&nbsp;</p>\n<p>Topics up for grabs too, leave a suggestion if you'd like.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DiPXSYeuphffBT5te", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5809921836111517e-06, "legacy": true, "legacyId": "25601", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-25T00:56:34.368Z", "modifiedAt": null, "url": null, "title": "[LINK] Sean Carrol's reflections on his debate with WL Craig on \"God and Cosmology\"", "slug": "link-sean-carrol-s-reflections-on-his-debate-with-wl-craig", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:02.439Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gemsMw4acvMWrg6wW/link-sean-carrol-s-reflections-on-his-debate-with-wl-craig", "pageUrlRelative": "/posts/gemsMw4acvMWrg6wW/link-sean-carrol-s-reflections-on-his-debate-with-wl-craig", "linkUrl": "https://www.lesswrong.com/posts/gemsMw4acvMWrg6wW/link-sean-carrol-s-reflections-on-his-debate-with-wl-craig", "postedAtFormatted": "Tuesday, February 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Sean%20Carrol's%20reflections%20on%20his%20debate%20with%20WL%20Craig%20on%20%22God%20and%20Cosmology%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Sean%20Carrol's%20reflections%20on%20his%20debate%20with%20WL%20Craig%20on%20%22God%20and%20Cosmology%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgemsMw4acvMWrg6wW%2Flink-sean-carrol-s-reflections-on-his-debate-with-wl-craig%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Sean%20Carrol's%20reflections%20on%20his%20debate%20with%20WL%20Craig%20on%20%22God%20and%20Cosmology%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgemsMw4acvMWrg6wW%2Flink-sean-carrol-s-reflections-on-his-debate-with-wl-craig", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgemsMw4acvMWrg6wW%2Flink-sean-carrol-s-reflections-on-his-debate-with-wl-craig", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 812, "htmlBody": "<p>I previously <a href=\"/lw/jez/open_thread_for_january_17_2014/a9wz\">mentioned</a> this debate a month ago and predicted that Sean Carroll is unlikely to do very well. The debate happened last Friday and Sean posted his <a href=\"http://www.preposterousuniverse.com/blog/2014/02/24/post-debate-reflections/\">post-debate reflections</a> on his popular blog (the full video will be posted soon). Some excerpts:</p>\n<p style=\"padding-left: 30px;\">I think it went well, although I can easily think of several ways I could have done better. On the substance, my major points were that the demand for &ldquo;causes&rdquo; and &ldquo;explanations&rdquo; is completely inappropriate for modern fundamental physics/cosmology, and that theism is not taken seriously in professional cosmological circles because it is hopelessly ill-defined (no matter what happens in the universe, you can argue that God would have wanted it that way). He defended two of his favorite arguments, the &ldquo;cosmological argument&rdquo; and the fine-tuning argument; no real surprises there. In terms of style, from my perspective things got a bit frustrating, because the following pattern repeated multiple times: Craig would make an argument, I would reply, and Craig would just repeat the original argument.</p>\n<p style=\"padding-left: 30px;\">The cosmological argument has two premises: (1) If the universe had a beginning, it has a transcendent cause; and (2) The universe had a beginning. [...]&nbsp;My attitude toward the above two premises is that (2) is completely uncertain, while the &ldquo;obvious&rdquo; one (1) is flat-out false. Or not even false, as I put it, because <strong>the notion of a &ldquo;cause&rdquo; isn&rsquo;t part of an appropriate vocabulary to use for discussing fundamental physics</strong>. [Emphasis mine]</p>\n<p style=\"padding-left: 30px;\">The Aristotelian analysis of causes is outdated when it comes to modern fundamental physics; what matters is whether you can find a formal mathematical model that accounts for the data.&nbsp;</p>\n<p>Sean goes over a couple of mistakes he thinks he made in the debate, basically being blindsided by WLC bringing up obscure papers and misinterpreting them to suit his argument.&nbsp;</p>\n<p>Sean's reflections are very detailed and worth reading, though I found them hard to summarize. It looks like WLC did his homework better than SC, but it's hard to tell whether it mattered until the video is made public and various interested parties gave their feedback. Another couple of quotes, with my emphasis:</p>\n<p style=\"padding-left: 30px;\">For my closing statement, I couldn&rsquo;t think of many responses to Craig&rsquo;s closing statement that wouldn&rsquo;t have simply be me reiterating points from my first two speeches. So I took the opportunity to pull back a little and look at the bigger picture. Namely: <strong>we&rsquo;re talking about &ldquo;God and Cosmology,&rdquo; but nobody really becomes a believer in God because it provides the best cosmology. They become theists for other reasons, and the cosmology comes later.</strong> That&rsquo;s because religion is enormously more than theism. Most people become religious for other (non-epistemic) reasons: it provides meaning and purpose, or a sense of community, or a way to be in contact with something transcendent, or simply because it&rsquo;s an important part of their culture. The problem is that theism, while not identical to religion, forms its basis, at least in most Western religions. So &mdash; maybe, I suggested, tentatively &mdash; that could change. I give theists a hard time for not accepting the implications of modern science, but I am also happy to give naturalists a hard time when they don&rsquo;t appreciate the enormous task we face in answering all of the questions that we used to think were answered by God. [...]</p>\n<p style=\"padding-left: 30px;\">To me, Craig&rsquo;s best moment of the weekend came at the very end, as part of the summary panel discussion. Earlier in the day, Tim Maudlin (who gave an great pro-naturalism talk, explaining that God&rsquo;s existence wouldn&rsquo;t have any moral consequences even if it were true) had grumped a little bit about the format. His point was that formal point-counterpoint debates aren&rsquo;t really the way philosophy is done, which would be closer to a Socratic discussion where issues can be clarified and extended more efficiently. And I agree with that, as far as it goes. But Craig had a robust response, which I also agree with: yes, a debate like this isn&rsquo;t how philosophy is done, but there are things worth doing other than philosophy, or even teaching philosophy. He said, candidly, that the <strong>advantage of the debate format is that it brings out audiences, who find a bit of give-and-take more exciting than a lecture or series of lectures</strong>. It&rsquo;s hard to teach subtle and tricky concepts in such a format, but that&rsquo;s always a hard thing to do;<strong> the point is that if you get the audience there in the first place, a good debater can at least plant a few new ideas in their heads, and hopefully inspire them to take the initiative and learn more on their own.</strong></p>\n<p>Sean concurs: \"If we think we have good ideas, we should do everything we can to bring them to as many people as possible.\"</p>\n<p>I hope Luke or someone else will find time to watch the video once posted and give their impressions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gemsMw4acvMWrg6wW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "25603", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-25T04:57:43.117Z", "modifiedAt": null, "url": null, "title": "Open Thread February 25 - March 3", "slug": "open-thread-february-25-march-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:07.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8hgksGoy97gEPeA4m/open-thread-february-25-march-3", "pageUrlRelative": "/posts/8hgksGoy97gEPeA4m/open-thread-february-25-march-3", "linkUrl": "https://www.lesswrong.com/posts/8hgksGoy97gEPeA4m/open-thread-february-25-march-3", "postedAtFormatted": "Tuesday, February 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20February%2025%20-%20March%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20February%2025%20-%20March%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8hgksGoy97gEPeA4m%2Fopen-thread-february-25-march-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20February%2025%20-%20March%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8hgksGoy97gEPeA4m%2Fopen-thread-february-25-march-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8hgksGoy97gEPeA4m%2Fopen-thread-february-25-march-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8hgksGoy97gEPeA4m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 1.5814616404698094e-06, "legacy": true, "legacyId": "25604", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 356, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-25T10:43:12.746Z", "modifiedAt": null, "url": null, "title": "Single player extensive-form games as a model of UDT", "slug": "single-player-extensive-form-games-as-a-model-of-udt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:09.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W4sDWwGZ4puRBXMEZ/single-player-extensive-form-games-as-a-model-of-udt", "pageUrlRelative": "/posts/W4sDWwGZ4puRBXMEZ/single-player-extensive-form-games-as-a-model-of-udt", "linkUrl": "https://www.lesswrong.com/posts/W4sDWwGZ4puRBXMEZ/single-player-extensive-form-games-as-a-model-of-udt", "postedAtFormatted": "Tuesday, February 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Single%20player%20extensive-form%20games%20as%20a%20model%20of%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingle%20player%20extensive-form%20games%20as%20a%20model%20of%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW4sDWwGZ4puRBXMEZ%2Fsingle-player-extensive-form-games-as-a-model-of-udt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Single%20player%20extensive-form%20games%20as%20a%20model%20of%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW4sDWwGZ4puRBXMEZ%2Fsingle-player-extensive-form-games-as-a-model-of-udt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW4sDWwGZ4puRBXMEZ%2Fsingle-player-extensive-form-games-as-a-model-of-udt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 691, "htmlBody": "<p>This post was inspired by Benja&#x27;s <a href=\"https://www.lesswrong.com/r/discussion/lw/jpr/sudt_a_toy_decision_theory_for_updateless/\">SUDT</a> post. I&#x27;m going to describe another simplified model of UDT which is equivalent to Benja&#x27;s proposal, and is based on standard game theory concepts as described in <a href=\"http://en.wikipedia.org/wiki/Extensive-form_game\">this Wikipedia article</a>.</p><p>First let&#x27;s define what is a &quot;single player extensive-form game with chance moves and imperfect information&quot;:</p><ol><li>A &quot;single player extensive-form game&quot; is a tree of nodes. Each leaf node is a utility value. A play of the game starts at the root and ends at some leaf node.</li><li>Some non-leaf nodes are &quot;chance nodes&quot;, with probabilities assigned to branches going out of that node. All other non-leaf nodes are &quot;decision nodes&quot;, where the player can choose which branch to take. (Thanks to badger for helping me fix an error in this part!)</li><li>&quot;Imperfect information&quot; means the decision nodes are grouped into &quot;information sets&quot;. The player doesn&#x27;t know which node they&#x27;re currently at, only which information set it belongs to.</li><li>&quot;Imperfect recall&quot; is a special case of imperfect information, where knowing the current information set doesn&#x27;t even allow the player to figure out which information sets were previously visited, like in the <a href=\"https://www.lesswrong.com/lw/182/the_absentminded_driver/\">Absent-Minded Driver</a> problem.</li><li>We will assume that the player can use &quot;behavioral strategies&quot;, where the player can make a random choice at each node independently, rather than &quot;mixed strategies&quot;, which randomize over the set of pure strategies for the entire game. See <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.4679&rep=rep1&type=pdf\">Piccione and Rubinstein&#x27;s paper</a> for more on this difference. (Thanks to Coscott for pointing out that assumption!)</li><li>The behavioral strategy with the highest expected utility will be taken as the solution of the game.</li></ol><p>Now let&#x27;s try using that to solve some UDT problems:</p><p><a href=\"https://www.lesswrong.com/lw/182/the_absentminded_driver/\">Absent-Minded Driver</a> is the simplest case, since it&#x27;s already discussed in the literature as a game of the above form. It&#x27;s strange that not everyone agrees that the best strategy is indeed the best, but let&#x27;s skip that and move on.</p><p><a href=\"https://www.lesswrong.com/lw/3dy/solve_psykoshs_nonanthropic_problem/\">Psy-Kosh&#x27;s non-anthropic problem</a> is more tricky, because it has multiple players. We will model it as a single-player game anyway, putting the decision nodes of the different players in sequence and grouping them together into information sets in the natural way. The resulting game tree is complicated, but the solution is the same as UDT&#x27;s. As a bonus, we see that our model does not need any kind of anthropic probabilities, because it doesn&#x27;t specify or use the probabilities of individual nodes within an information set.</p><p><a href=\"https://www.lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">Wei Dai&#x27;s coordination problem</a> is similar to the previous one, but with multiple players choosing different actions based on different information. If we use the same trick of folding all players into one, and group the decision nodes into information sets in the natural way, we get the right solution again. It&#x27;s nice to see that our model automatically solves problems that require Wei&#x27;s &quot;explicit optimization of global strategy&quot;.</p><p><a href=\"https://www.lesswrong.com/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a> is even more tricky, because writing it as an extensive-form game must include a decision node for Omega&#x27;s simulation of the player. Some people are okay with that, and our model gives the right solution. But others feel that it leads to confusing questions about the nature of observation. For example, what if Omega used a logical coin, and the player could actually check which way the coin came up by doing a long calculation? Paying up is probably the right decision, but our model here doesn&#x27;t have enough detail.</p><p>Finally, <a href=\"https://www.lesswrong.com/lw/5rq/example_decision_theory_problem_agent_simulates/\">Agent Simulates Predictor</a> is the kind of problem that cannot be captured by our model at all, because logical uncertainty is the whole point of ASP.</p><p>It&#x27;s instructive to see the difference between the kind of UDT problems that fit our model and those that require something more. Also it would be easy to implement the model as a computer program, and solve some UDT problems automatically. (Though the exercise wouldn&#x27;t have much scientific value, because extensive-form games are a well known idea.) In this way it&#x27;s a little similar to Patrick&#x27;s work on <a href=\"https://www.lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/\">modal agents</a>, which made certain problems solvable on the computer by using modal logic instead of enumerating proofs. Now I wonder if other problems that involve logical uncertainty could also be solved by some simplified model?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W4sDWwGZ4puRBXMEZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 25, "extendedScore": null, "score": 8.6e-05, "legacy": true, "legacyId": "25605", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": true, "version": "1.0.0", "pingbacks": {"Posts": ["NaZPjaLPCGZWdTyrL", "GfHdNfqxe3cSCfpHL", "YZzoWGCJsoRBBbmQg", "g8xh9R7RaNitKtkaa", "mg6jDEuQEjBGtibX7", "q9DbfYfFzkotno9hG", "iQWk5jYeDg5ACCmpx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-25T14:11:30.234Z", "modifiedAt": null, "url": null, "title": "The innovation tree, overshadowed in the innovation forest", "slug": "the-innovation-tree-overshadowed-in-the-innovation-forest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:35.434Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9FfaJLdLDbDZJZcxG/the-innovation-tree-overshadowed-in-the-innovation-forest", "pageUrlRelative": "/posts/9FfaJLdLDbDZJZcxG/the-innovation-tree-overshadowed-in-the-innovation-forest", "linkUrl": "https://www.lesswrong.com/posts/9FfaJLdLDbDZJZcxG/the-innovation-tree-overshadowed-in-the-innovation-forest", "postedAtFormatted": "Tuesday, February 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20innovation%20tree%2C%20overshadowed%20in%20the%20innovation%20forest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20innovation%20tree%2C%20overshadowed%20in%20the%20innovation%20forest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FfaJLdLDbDZJZcxG%2Fthe-innovation-tree-overshadowed-in-the-innovation-forest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20innovation%20tree%2C%20overshadowed%20in%20the%20innovation%20forest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FfaJLdLDbDZJZcxG%2Fthe-innovation-tree-overshadowed-in-the-innovation-forest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FfaJLdLDbDZJZcxG%2Fthe-innovation-tree-overshadowed-in-the-innovation-forest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 695, "htmlBody": "<p><em><a href=\"http://blog.practicalethics.ox.ac.uk/2014/02/the-innovation-tree-overshadowed-in-the-innovation-forest/\">Cross-posted</a>&nbsp;at Practical Ethics.</em></p>\n<p>Many have pronounced that the era of innovation dead, peace be to its soul. From Tyler Cowen's <a href=\"http://www.amazon.co.uk/The-Great-Stagnation-Low-Hanging-Eventually-ebook/dp/B004H0M8QS\">decree</a> that we've picked all the low hanging fruit of innovation, through Robert Gordon's <a href=\"http://www.ted.com/talks/robert_gordon_the_death_of_innovation_the_end_of_growth.html\">idea</a> that further innovation growth is threatened by \"six headwinds\", to Gary Karparov's and Peter Thiel's <a href=\"http://www.oxfordmartin.ox.ac.uk/event/1403\">theory</a> that risk aversion has stifled innovation, there is no lack of predictions about the end of discovery.</p>\n<p>I don't propose to address the issue with something as practical and useful as actual data. Instead, staying true to my philosophical environment, I propose a thought experiment that hopefully may shed some light. The core idea is that we might be underestimating the impact of innovation because we have so much of it.</p>\n<p>Imagine that technological innovation had for some reason stopped around the 1945 - with one exception: the CD and CD player/burner. Fast forwards a few decades, and visualise society. We can imagine a society completely dominated by the CD. We'd have all the usual uses for the CD - music, songs and similar - of course, but also much more.<a id=\"more\"></a></p>\n<p>Without mass television, the CD and radio would become the premier sources of entertainment for everyone. Newspapers would experiment with bundling CDs with their subscriptions, giving high-quality live sound recordings from major events. The walkman culture and all the transformations it brought would instead have been based around the portable CD player. Bosses at large companies would probably get into the habit of recording motivational messages and sending them to all employees. Syndicated columnists would record themselves on CDs to distribute to the fast-paced workers who didn't have time to be at a radio at a certain time, or who were snobish about high quality audio. Underground movements would stay in contact with smuggled CDs (as would various youth cultures) and families would stay in touch with mailed CD messages. Various corporations would experiments with ways of sending data via CD, maybe by connecting them up to some sort of typing machine. CD-geek movements would learn to interpret the data just by listening, without needing a translation machine. Coded messages would be sent on CDs, and vast amounts of government secrets and mundane data would be stored on it. Many other ideas would no doubt spring up to use this ubiquitous technology - CD contracts, anyone?</p>\n<p>Any social transformations of society would be enabled by the CD, or certainly blamed on it (\"the anti-Barzil war riots of the 1960s were all the fault of permissive parents and those silver disks!\"). Jobs would be lost and new ones created and columnists would enthuse or warn about the technology at length. In 2000, it would be unanimously voted the single most transformative technology of the 20th century. And then when the first proper computers began to be developed in 2001, they would be based initially on CD technology...</p>\n<p>Such a transformative impact for what in our world is a rather mundane and obsolete technology! What's different about our world that relegates the CD to such a low status? Well, we have more innovations. We have colour television, walkmans, VCRs, computers, ipods, mobile phones and many other technologies that overlap with the CD and can take over many of its roles. As a result we judge the CD to be a medium technology not because it isn't so innovative, <em>but because we have so many other innovations</em>. Thus the more innovations we have, the less transformative each one is.</p>\n<p>To really drive this point home, we could go back to our hypothetical CD-only world and remove older innovations like the radio and the telephone. Then the CD would take on more roles and be seen as more more innovative and more transformative than before. But again, we haven't improved the CD in any way to get this change of perspective: we've just chopped down the other innovation trees to allow us to see how innovative the CD truly is. And the CD was chosen almost at random to illustrate the point; a similar picture would emerge if any other technology had been chosen as the \"only big innovation since 1945\".&nbsp;They are all very innovative, but they get lost in so many other innovations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9FfaJLdLDbDZJZcxG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 1.5821164102076356e-06, "legacy": true, "legacyId": "25494", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-25T14:58:16.582Z", "modifiedAt": null, "url": null, "title": "Innovation's low-hanging fruits: on the demand or supply sides?", "slug": "innovation-s-low-hanging-fruits-on-the-demand-or-supply", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:01.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uNWAwwZiwDQTSv2YD/innovation-s-low-hanging-fruits-on-the-demand-or-supply", "pageUrlRelative": "/posts/uNWAwwZiwDQTSv2YD/innovation-s-low-hanging-fruits-on-the-demand-or-supply", "linkUrl": "https://www.lesswrong.com/posts/uNWAwwZiwDQTSv2YD/innovation-s-low-hanging-fruits-on-the-demand-or-supply", "postedAtFormatted": "Tuesday, February 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Innovation's%20low-hanging%20fruits%3A%20on%20the%20demand%20or%20supply%20sides%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInnovation's%20low-hanging%20fruits%3A%20on%20the%20demand%20or%20supply%20sides%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuNWAwwZiwDQTSv2YD%2Finnovation-s-low-hanging-fruits-on-the-demand-or-supply%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Innovation's%20low-hanging%20fruits%3A%20on%20the%20demand%20or%20supply%20sides%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuNWAwwZiwDQTSv2YD%2Finnovation-s-low-hanging-fruits-on-the-demand-or-supply", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuNWAwwZiwDQTSv2YD%2Finnovation-s-low-hanging-fruits-on-the-demand-or-supply", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 466, "htmlBody": "<p><em><a href=\"http://blog.practicalethics.ox.ac.uk/2014/02/innovations-low-hanging-fruits-on-the-demand-or-supply-sides/\">Cross-posted</a> at Practical Ethics.</em></p>\n<p>This is an addendum to a&nbsp;<a href=\"/r/discussion/lw/jo6/the_innovation_tree_overshadowed_in_the/\">previous post</a>, which argued that we may be underestimating the impact of innovation because we have so much of it. I noted that we underestimated the innovative aspect of the CD because many other technologies partially overlapped with it, such as television, radio, cinema, ipod, walkman, landline phone, mobile phone, laptop, VCR and Tivo's. Without these overlapping technologies, we could see the CD's true potential and estimate it higher as an innovation. Many different technologies could substitute for each other.</p>\n<p>But this argument brings out a salient point: if so many innovations overlap or potentially overlap, then there must be many more innovations that purposes for innovations. Tyler Cowen <a href=\"Tyler Cowen http://techcrunch.com/2011/11/15/keen-on-tyler-cowen-why-the-internet-isnt-as-innovative-as-a-flushing-toilet-tctv/\">made the interesting point</a> that the internet isn't as innovative as the flushing toilet (or indeed the television). He certainly has a point here: imagine society without toilets or youtube, which would be most tolerable (or most survivable)?<a id=\"more\"></a></p>\n<p>But the flush toilet can only be invented once. We might have access to <a href=\"http://simpsons.wikia.com/wiki/Thirty_Minutes_Over_Tokyo\">talking super toilets with multi-coloured fountains</a>&nbsp;- but all the bells and whistles are less useful that the original flushing toilet aspect. That's because flush toilets responded effectively to a real human need: how to dispose of human waste in urban areas. Once that problem is solved, further innovation is mainly wasted.</p>\n<p>This suggests that while we may indeed be <a href=\"http://www.amazon.co.uk/The-Great-Stagnation-Low-Hanging-Eventually-ebook/dp/B004H0M8QS\">plucking the innovation low-hanging fruits</a>, it might not be because we lack a supply of innovation, but because we're exhausting the easy demand for innovation. What current needs do we have that we're waiting for innovation to solve? What's problems are we facing that are as important as removing human waste from urban areas?</p>\n<p>There seem to be very few. Maybe solving death and disease: and we can make a very strong case that medical innovation <a href=\"http://www.nature.com/nrd/journal/v8/n12/full/nrd2961.html\">is indeed slowing</a>. Poverty is another one; but it's not like we know of a specific technological innovation that would solve poverty, if only someone would develop it. We might want easy access to space, or effective alternative energies: but the way people and governments spend their money confirms that this is not a top priority for many.&nbsp;Even if we had teleporters, would future Tyler Cowens be writing that they're not as innovative as the car - and would they be correct, in that a teleporter is just a more efficient way of solving a problem that cars and airplanes had already partially solved?</p>\n<p>In summary, outside of the medical field, I don't see any conceivable realistic technological innovation that would be as transformative as the flush toilet, vaccinations, birth control, telephones, cars and airplanes. We might have exhausted the low-hanging fruits in our desires.</p>\n<p>EDIT: some have suggested \"<a href=\"http://www.nanowerk.com/spotlight/spotid=30839.php\">high-throughput atomically precise manufacturing</a>\" as a general solution to material poverty, which would be an interesting counterexample.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uNWAwwZiwDQTSv2YD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "25606", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9FfaJLdLDbDZJZcxG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-25T15:50:35.350Z", "modifiedAt": null, "url": null, "title": "\"Smarter than us\" is out!", "slug": "smarter-than-us-is-out", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:08.603Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pam5oJXECo8ka6ikA/smarter-than-us-is-out", "pageUrlRelative": "/posts/Pam5oJXECo8ka6ikA/smarter-than-us-is-out", "linkUrl": "https://www.lesswrong.com/posts/Pam5oJXECo8ka6ikA/smarter-than-us-is-out", "postedAtFormatted": "Tuesday, February 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Smarter%20than%20us%22%20is%20out!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Smarter%20than%20us%22%20is%20out!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPam5oJXECo8ka6ikA%2Fsmarter-than-us-is-out%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Smarter%20than%20us%22%20is%20out!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPam5oJXECo8ka6ikA%2Fsmarter-than-us-is-out", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPam5oJXECo8ka6ikA%2Fsmarter-than-us-is-out", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 340, "htmlBody": "<p>We're pleased to announce the release of \"Smarter Than Us: The Rise of Machine Intelligence\", commissioned by MIRI and written by Oxford University&rsquo;s Stuart Armstrong, and available in&nbsp;<a href=\"http://intelligence.org/smarter-than-us/\">EPUB, MOBI, PDF</a>, and from the&nbsp;<a href=\"http://www.amazon.com/gp/product/B00IB4N4KU/ref=as_li_ss_tl?tag=lesswrong-20\">Amazon</a>&nbsp;and&nbsp;<a href=\"https://itunes.apple.com/us/book/smarter-than-us/id816744180\">Apple</a>&nbsp;ebook stores.</p>\n<p style=\"padding-left: 30px;\"><img style=\"float:right;\" src=\"http://images.lesswrong.com/t3_jrb_0.png?v=c1161819d2def5870c2a150f0349c3ca\" alt=\"\" width=\"300\" height=\"436\" />What happens when machines become smarter than humans? Forget lumbering Terminators. The power of an artificial intelligence (AI) comes from its intelligence, not physical strength and laser guns. Humans steer the future not because we&rsquo;re the strongest or the fastest but because we&rsquo;re the smartest. When machines become smarter than humans, we&rsquo;ll be handing them the steering wheel. What promises&mdash;and perils&mdash;will these powerful machines present? This new book navigates these questions with clarity and wit.</p>\n<p style=\"padding-left: 30px;\">Can we instruct AIs to steer the future as we desire? What goals should we program into them? It turns out this question is difficult to answer! Philosophers have tried for thousands of years to define an ideal world, but there remains no consensus. The prospect of goal-driven, smarter-than-human AI gives moral philosophy a new urgency. The future could be filled with joy, art, compassion, and beings living worthwhile and wonderful lives&mdash;but only if we&rsquo;re able to precisely define what a &ldquo;good&rdquo; world is, and skilled enough to describe it perfectly to a computer program.</p>\n<p style=\"padding-left: 30px;\">AIs, like computers, will do what we say&mdash;which is not necessarily what we mean. Such precision requires encoding the entire system of human values for an AI: explaining them to a mind that is alien to us, defining every ambiguous term, clarifying every edge case. Moreover, our values are fragile: in some cases, if we mis-define a single piece of the puzzle&mdash;say, consciousness&mdash;we end up with roughly 0% of the value we intended to reap, instead of 99% of the value.</p>\n<p style=\"padding-left: 30px;\">Though an understanding of the problem is only beginning to spread, researchers from fields ranging from philosophy to computer science to economics are working together to conceive and test solutions. Are we up to the challenge?</p>\n<p>Special thanks to all those at the FHI, MIRI and Less Wrong who helped with this work, and those who <a href=\"/lw/j4j/please_vote_for_a_title_for_an_upcoming_book/\">voted on the name</a>!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1, "jZF2jwLnPKBv6m3Ag": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pam5oJXECo8ka6ikA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 41, "extendedScore": null, "score": 1.5822336144971116e-06, "legacy": true, "legacyId": "25607", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R2ecEunTrLxrxMjDp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-25T18:21:42.780Z", "modifiedAt": null, "url": null, "title": "Is IQ what we actually need to know?", "slug": "is-iq-what-we-actually-need-to-know", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:33.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zc9kwpM5uMH7HMeve/is-iq-what-we-actually-need-to-know", "pageUrlRelative": "/posts/Zc9kwpM5uMH7HMeve/is-iq-what-we-actually-need-to-know", "linkUrl": "https://www.lesswrong.com/posts/Zc9kwpM5uMH7HMeve/is-iq-what-we-actually-need-to-know", "postedAtFormatted": "Tuesday, February 25th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20IQ%20what%20we%20actually%20need%20to%20know%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20IQ%20what%20we%20actually%20need%20to%20know%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZc9kwpM5uMH7HMeve%2Fis-iq-what-we-actually-need-to-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20IQ%20what%20we%20actually%20need%20to%20know%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZc9kwpM5uMH7HMeve%2Fis-iq-what-we-actually-need-to-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZc9kwpM5uMH7HMeve%2Fis-iq-what-we-actually-need-to-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<p>I've never heard of anyone saying \"I thought that person was really intelligent, but they turned out not to be\", and when there are scandals about people with fake credentials, they don't seem to come from people with fake credentials making mistakes-- instead, someone checks the history.</p>\n<p>It seems to me that you can find out a lot about people's intelligence by talking with them a little, though I've underestimated people who were bright enough but didn't present as intellectual.</p>\n<p>The real problems are with identifying conscientiousness, benevolence, and loyalty-- that's where the unpleasant surprises show up.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4cKQgA4S7xfNeeWXg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zc9kwpM5uMH7HMeve", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 0, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "25608", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-26T06:00:25.556Z", "modifiedAt": null, "url": null, "title": "Rational Evangelism", "slug": "rational-evangelism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:09.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aarongertler", "createdAt": "2017-06-17T00:54:32.818Z", "isAdmin": false, "displayName": "aarongertler"}, "userId": "8mnQfaaqwi2mevNtq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QiiPX2nj7aSTCpXuT/rational-evangelism", "pageUrlRelative": "/posts/QiiPX2nj7aSTCpXuT/rational-evangelism", "linkUrl": "https://www.lesswrong.com/posts/QiiPX2nj7aSTCpXuT/rational-evangelism", "postedAtFormatted": "Wednesday, February 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Evangelism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Evangelism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQiiPX2nj7aSTCpXuT%2Frational-evangelism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Evangelism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQiiPX2nj7aSTCpXuT%2Frational-evangelism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQiiPX2nj7aSTCpXuT%2Frational-evangelism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 757, "htmlBody": "<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">Not \"rationality evangelism\", which&nbsp;<a href=\"http://rationality.org/groups/\"><span style=\"color: blue;\">CFAR</span></a>&nbsp;is&nbsp;doing already if I understand their mission. \"Rational evangelism\", which is what CFAR would do if they were Catholic missionaries.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">If you&nbsp;believe in Hell, as many people very truly do, it is hard for Hell not to seem like the world's most important problem. </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">To some extent, proselytizing religions treat Hell with respect--they spend billions of dollars trying to save sinners, and the most devout often spend their lives preaching the Gospel (insert non-Christian variant).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">But is Hell given <em>enough </em>respect? Every group meets with mixed success in solving its problems, but the problem of eternal suffering leaves little room for \"mixed success\". E</span><span style=\"font-size: 10pt;\">ven the most powerful religions are <a href=\"http://en.wikipedia.org/wiki/Priest_shortage_in_the_Roman_Catholic_Church\">stuck in patterns</a> that make the work of salvation very difficult indeed. And some seem willing to&nbsp;reduce their&nbsp;evangelism* for reasons that aren't especially convincing in the face of \"nonbelievers are quite possibly going to burn, or at least be outside the presence of God, forever\".</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">What if you were a rationalist who viewed Hell like&nbsp;certain Less Wrongers view the Singularity? (This belief would be hard to reconcile with rationalism generally, but for the sake of argument...) How would you tackle the problem of eternal suffering with the same passion we spend on probability theory and friendly AI?</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">I wrote a long thought experiment to better define the problem, involving a religion called \"Normomism\", but it was awkward. There are plenty of real religions whose members&nbsp;believe in Hell, or at least in a Heaven that many people aren't going to (also a terrible loss).&nbsp;Some have a stated mission of saving as many people as possible from a bad afterlife.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">So where are they falling short?&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">If you were the Pope, or the Caliph, or the&nbsp;supreme dictator&nbsp;of&nbsp;some smaller religion, what tactics would you use to&nbsp;convince more people to&nbsp;do and believe exactly the things that&nbsp;would save&nbsp;them--whether that's faith or good works? Why haven't these tactics been tried already? Is there really much room for improvement?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><strong><span style=\"font-size: 14pt;\">Spreading the Word</span></strong></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">This post isn't a dig at believers, though it does seem like many people don't act on their sincere belief in an eternal afterlife. (I don't mind when people try to convert me--at least they care!)</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">My main point: It's worth considering that people who believe in Very Bad Future Outcomes have been working to prevent those outcomes for thousands of years, and have stumbled upon formidable techniques for doing so.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">I've thought for a while about rational evangelism, and it's surprisingly hard to come up with ways that people like&nbsp;<a href=\"http://christianity.about.com/od/christiancelebrities/p/rickwarrenbio.htm\"><span style=\"color: blue;\">Rick Warren</span></a>&nbsp;and&nbsp;<a href=\"http://wpfj.org/goingtoheaven.htm\"><span style=\"color: blue;\">Jerry Lovett</span></a>&nbsp;could improve their methodology. (Read Lovett's \"contact me\" paragraph for the part that really impressed me.)</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">We speak often of borrowing from religion, but these conversations mostly touch on social bonding, rather than what it means to spread ideas so important that the fate of the human race depends on them. (<a href=\"/lw/1e/raising_the_sanity_waterline/\"><span style=\"color: blue;\">\"Raising the Sanity Waterline\"</span></a>&nbsp;is a great start, but those ideas haven't been the focus of many recent posts.)</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">I'm not saying this is a perfect comparison. The rationalist war for the future won't be fought one soul at a time, and we won't save anyone with a deathbed confession.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">But cryogenic freezing does exist. And on a more collective level, convincing the right people that the far future matters could be a coup on the level of&nbsp;<a href=\"http://www.thenagain.info/webchron/easteurope/ConstantineConverts.html\"><span style=\"color: blue;\">Constantine's conversion</span></a>.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">CFAR is doing good things in the direction of rationality evangelism. How can the rest of us do more?&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><strong><span style=\"font-size: 14pt;\">Living Like We Mean It</span></strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">This movement is going places. But I fear we may spend too much time (at least proportionally) arguing amongst ourselves, when bringing others into the fold is a key piece of the puzzle.&nbsp;And if we&rsquo;d like to expand the flock (or, more appropriately, the herd of cats), what can we learn from history&rsquo;s most persuasive organizations?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">I often pass up my chance to talk to people about something as simple as Givewell, let alone existential risk, and it's been a long time since I last name-dropped a Less Wrong technique. I don't think I'm alone in this.**&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">I've met plenty of Christians who exude the same optimism and conviviality as a Rick Warren or a&nbsp;<a href=\"http://simpsons.wikia.com/wiki/Ned_Flanders\"><span style=\"color: blue;\">Ned Flanders</span></a>. These kinds of people are a major boon for the Christian religion. Even if most of us are introverts, what's stopping us from teaching ourselves to live the same way?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">Still, I'm new here, and I could be wrong. What do you think? &nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10.0pt; mso-bidi-font-size: 11.0pt; line-height: 107%;\">* Text editor's giving me&nbsp;some trouble, but&nbsp;the link is here: <a href=\"http://www.relevantmagazine.com/god/practical-faith/evangelism-interfaith-world\">http://www.relevantmagazine.com/god/practical-faith/evangelism-interfaith-world</a>&nbsp;</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">** Peter Boghossian's&nbsp;<a href=\"http://www.amazon.com/Manual-Creating-Atheists-Peter-Boghossian/dp/1939578094/ref=la_B00CXT0V0K_1_1?s=books&amp;ie=UTF8&amp;qid=1393387606&amp;sr=1-1\"><span style=\"color: blue;\">Manual for Creating Atheists</span></a>&nbsp;has lots to say about using rationality techniques in the course of daily life, and is well worth reading, though the author can be an asshole sometimes.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QiiPX2nj7aSTCpXuT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 12, "extendedScore": null, "score": 1.583239486611943e-06, "legacy": true, "legacyId": "25613", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">Not \"rationality evangelism\", which&nbsp;<a href=\"http://rationality.org/groups/\"><span style=\"color: blue;\">CFAR</span></a>&nbsp;is&nbsp;doing already if I understand their mission. \"Rational evangelism\", which is what CFAR would do if they were Catholic missionaries.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">If you&nbsp;believe in Hell, as many people very truly do, it is hard for Hell not to seem like the world's most important problem. </span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">To some extent, proselytizing religions treat Hell with respect--they spend billions of dollars trying to save sinners, and the most devout often spend their lives preaching the Gospel (insert non-Christian variant).</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">But is Hell given <em>enough </em>respect? Every group meets with mixed success in solving its problems, but the problem of eternal suffering leaves little room for \"mixed success\". E</span><span style=\"font-size: 10pt;\">ven the most powerful religions are <a href=\"http://en.wikipedia.org/wiki/Priest_shortage_in_the_Roman_Catholic_Church\">stuck in patterns</a> that make the work of salvation very difficult indeed. And some seem willing to&nbsp;reduce their&nbsp;evangelism* for reasons that aren't especially convincing in the face of \"nonbelievers are quite possibly going to burn, or at least be outside the presence of God, forever\".</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">What if you were a rationalist who viewed Hell like&nbsp;certain Less Wrongers view the Singularity? (This belief would be hard to reconcile with rationalism generally, but for the sake of argument...) How would you tackle the problem of eternal suffering with the same passion we spend on probability theory and friendly AI?</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">I wrote a long thought experiment to better define the problem, involving a religion called \"Normomism\", but it was awkward. There are plenty of real religions whose members&nbsp;believe in Hell, or at least in a Heaven that many people aren't going to (also a terrible loss).&nbsp;Some have a stated mission of saving as many people as possible from a bad afterlife.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">So where are they falling short?&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">If you were the Pope, or the Caliph, or the&nbsp;supreme dictator&nbsp;of&nbsp;some smaller religion, what tactics would you use to&nbsp;convince more people to&nbsp;do and believe exactly the things that&nbsp;would save&nbsp;them--whether that's faith or good works? Why haven't these tactics been tried already? Is there really much room for improvement?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><strong id=\"Spreading_the_Word\"><span style=\"font-size: 14pt;\">Spreading the Word</span></strong></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">This post isn't a dig at believers, though it does seem like many people don't act on their sincere belief in an eternal afterlife. (I don't mind when people try to convert me--at least they care!)</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">My main point: It's worth considering that people who believe in Very Bad Future Outcomes have been working to prevent those outcomes for thousands of years, and have stumbled upon formidable techniques for doing so.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">I've thought for a while about rational evangelism, and it's surprisingly hard to come up with ways that people like&nbsp;<a href=\"http://christianity.about.com/od/christiancelebrities/p/rickwarrenbio.htm\"><span style=\"color: blue;\">Rick Warren</span></a>&nbsp;and&nbsp;<a href=\"http://wpfj.org/goingtoheaven.htm\"><span style=\"color: blue;\">Jerry Lovett</span></a>&nbsp;could improve their methodology. (Read Lovett's \"contact me\" paragraph for the part that really impressed me.)</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">We speak often of borrowing from religion, but these conversations mostly touch on social bonding, rather than what it means to spread ideas so important that the fate of the human race depends on them. (<a href=\"/lw/1e/raising_the_sanity_waterline/\"><span style=\"color: blue;\">\"Raising the Sanity Waterline\"</span></a>&nbsp;is a great start, but those ideas haven't been the focus of many recent posts.)</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">I'm not saying this is a perfect comparison. The rationalist war for the future won't be fought one soul at a time, and we won't save anyone with a deathbed confession.&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">But cryogenic freezing does exist. And on a more collective level, convincing the right people that the far future matters could be a coup on the level of&nbsp;<a href=\"http://www.thenagain.info/webchron/easteurope/ConstantineConverts.html\"><span style=\"color: blue;\">Constantine's conversion</span></a>.</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10pt;\">CFAR is doing good things in the direction of rationality evangelism. How can the rest of us do more?&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><strong id=\"Living_Like_We_Mean_It\"><span style=\"font-size: 14pt;\">Living Like We Mean It</span></strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">This movement is going places. But I fear we may spend too much time (at least proportionally) arguing amongst ourselves, when bringing others into the fold is a key piece of the puzzle.&nbsp;And if we\u2019d like to expand the flock (or, more appropriately, the herd of cats), what can we learn from history\u2019s most persuasive organizations?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">I often pass up my chance to talk to people about something as simple as Givewell, let alone existential risk, and it's been a long time since I last name-dropped a Less Wrong technique. I don't think I'm alone in this.**&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">I've met plenty of Christians who exude the same optimism and conviviality as a Rick Warren or a&nbsp;<a href=\"http://simpsons.wikia.com/wiki/Ned_Flanders\"><span style=\"color: blue;\">Ned Flanders</span></a>. These kinds of people are a major boon for the Christian religion. Even if most of us are introverts, what's stopping us from teaching ourselves to live the same way?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">Still, I'm new here, and I could be wrong. What do you think? &nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\"><br></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"font-size: 10.0pt; mso-bidi-font-size: 11.0pt; line-height: 107%;\">* Text editor's giving me&nbsp;some trouble, but&nbsp;the link is here: <a href=\"http://www.relevantmagazine.com/god/practical-faith/evangelism-interfaith-world\">http://www.relevantmagazine.com/god/practical-faith/evangelism-interfaith-world</a>&nbsp;</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt;\"><span style=\"font-size: 10pt;\">** Peter Boghossian's&nbsp;<a href=\"http://www.amazon.com/Manual-Creating-Atheists-Peter-Boghossian/dp/1939578094/ref=la_B00CXT0V0K_1_1?s=books&amp;ie=UTF8&amp;qid=1393387606&amp;sr=1-1\"><span style=\"color: blue;\">Manual for Creating Atheists</span></a>&nbsp;has lots to say about using rationality techniques in the course of daily life, and is well worth reading, though the author can be an asshole sometimes.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>", "sections": [{"title": "Spreading the Word", "anchor": "Spreading_the_Word", "level": 1}, {"title": "Living Like We Mean It", "anchor": "Living_Like_We_Mean_It", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "151 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 151, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-26T16:33:06.851Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels - Calibration and other games", "slug": "meetup-brussels-calibration-and-other-games", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7W2NZHYZdC9xDzHGG/meetup-brussels-calibration-and-other-games", "pageUrlRelative": "/posts/7W2NZHYZdC9xDzHGG/meetup-brussels-calibration-and-other-games", "linkUrl": "https://www.lesswrong.com/posts/7W2NZHYZdC9xDzHGG/meetup-brussels-calibration-and-other-games", "postedAtFormatted": "Wednesday, February 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20-%20Calibration%20and%20other%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20-%20Calibration%20and%20other%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7W2NZHYZdC9xDzHGG%2Fmeetup-brussels-calibration-and-other-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20-%20Calibration%20and%20other%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7W2NZHYZdC9xDzHGG%2Fmeetup-brussels-calibration-and-other-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7W2NZHYZdC9xDzHGG%2Fmeetup-brussels-calibration-and-other-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x7'>Brussels - Calibration and other games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 March 2014 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month we're stealing <a href=\"https://www.dropbox.com/s/7hhz92om2r1gin1/ATLesswrong%208-4-13-%20%20The%20CI%20Wits%20%26%20Wagers%20Game.pdf\" rel=\"nofollow\">an idea from the Atlanta group</a>: using <a href=\"http://www.boardgamegeek.com/boardgame/20100/wits-wagers\" rel=\"nofollow\">Wits &amp; Wagers</a> for <a href=\"http://acritch.com/credence-game/\" rel=\"nofollow\">confidence calibration</a> (and a little <a href=\"http://lesswrong.com/lw/h5e/fermi_estimates/\">Fermi estimation</a>, which I keep saying we'll do and we never end up doing). It's like a trivia game, except you don't need to know anything to win! (errata: you do need to know that you don't know anything. else.) We may spice it up with some <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">paranoid debating</a>.</p>\n\n<p>I also have a game related to the Memory Palace, and another one related to the Ultimatum Game. But we may not have time to do everything.</p>\n\n<p><strong>If  someone can get their hands on an English copy of Wits &amp; Wagers (or <a href=\"http://www.boardgamegeek.com/boardgame/47046/gambit-7\" rel=\"nofollow\">Gambit 7</a>, please tell me.</strong></p>\n\n<p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x7'>Brussels - Calibration and other games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7W2NZHYZdC9xDzHGG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5839890753369875e-06, "legacy": true, "legacyId": "25617", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels___Calibration_and_other_games\">Discussion article for the meetup : <a href=\"/meetups/x7\">Brussels - Calibration and other games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 March 2014 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month we're stealing <a href=\"https://www.dropbox.com/s/7hhz92om2r1gin1/ATLesswrong%208-4-13-%20%20The%20CI%20Wits%20%26%20Wagers%20Game.pdf\" rel=\"nofollow\">an idea from the Atlanta group</a>: using <a href=\"http://www.boardgamegeek.com/boardgame/20100/wits-wagers\" rel=\"nofollow\">Wits &amp; Wagers</a> for <a href=\"http://acritch.com/credence-game/\" rel=\"nofollow\">confidence calibration</a> (and a little <a href=\"http://lesswrong.com/lw/h5e/fermi_estimates/\">Fermi estimation</a>, which I keep saying we'll do and we never end up doing). It's like a trivia game, except you don't need to know anything to win! (errata: you do need to know that you don't know anything. else.) We may spice it up with some <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">paranoid debating</a>.</p>\n\n<p>I also have a game related to the Memory Palace, and another one related to the Ultimatum Game. But we may not have time to do everything.</p>\n\n<p><strong id=\"If__someone_can_get_their_hands_on_an_English_copy_of_Wits___Wagers__or_Gambit_7__please_tell_me_\">If  someone can get their hands on an English copy of Wits &amp; Wagers (or <a href=\"http://www.boardgamegeek.com/boardgame/47046/gambit-7\" rel=\"nofollow\">Gambit 7</a>, please tell me.</strong></p>\n\n<p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels___Calibration_and_other_games1\">Discussion article for the meetup : <a href=\"/meetups/x7\">Brussels - Calibration and other games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels - Calibration and other games", "anchor": "Discussion_article_for_the_meetup___Brussels___Calibration_and_other_games", "level": 1}, {"title": "If  someone can get their hands on an English copy of Wits & Wagers (or Gambit 7, please tell me.", "anchor": "If__someone_can_get_their_hands_on_an_English_copy_of_Wits___Wagers__or_Gambit_7__please_tell_me_", "level": 2}, {"title": "Discussion article for the meetup : Brussels - Calibration and other games", "anchor": "Discussion_article_for_the_meetup___Brussels___Calibration_and_other_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PsEppdvgRisz5xAHG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-26T23:44:02.830Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Optimizing Empathy Levels", "slug": "meetup-boston-optimizing-empathy-levels", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cgkQjhDF3v7HQZ7XL/meetup-boston-optimizing-empathy-levels", "pageUrlRelative": "/posts/cgkQjhDF3v7HQZ7XL/meetup-boston-optimizing-empathy-levels", "linkUrl": "https://www.lesswrong.com/posts/cgkQjhDF3v7HQZ7XL/meetup-boston-optimizing-empathy-levels", "postedAtFormatted": "Wednesday, February 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Optimizing%20Empathy%20Levels&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Optimizing%20Empathy%20Levels%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcgkQjhDF3v7HQZ7XL%2Fmeetup-boston-optimizing-empathy-levels%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Optimizing%20Empathy%20Levels%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcgkQjhDF3v7HQZ7XL%2Fmeetup-boston-optimizing-empathy-levels", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcgkQjhDF3v7HQZ7XL%2Fmeetup-boston-optimizing-empathy-levels", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x8'>Boston - Optimizing Empathy Levels</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 March 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Joe Schneider will give a talk on Optimizing Empathy Levels:</p>\n\n<p>-What empathy is useful for.</p>\n\n<p>-Why LWers (and others) undervalue it.</p>\n\n<p>-Why non-LWers (and others) overvalue it.</p>\n\n<p>-How you can use it properly to get ahead.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 2pm, and have an alternating location:</p>\n\n<ul>\n<li><p>2nd and 4th Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n<li><p>Meetups on other weeks are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n</ul>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 2:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x8'>Boston - Optimizing Empathy Levels</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cgkQjhDF3v7HQZ7XL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5844999905228599e-06, "legacy": true, "legacyId": "25619", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Optimizing_Empathy_Levels\">Discussion article for the meetup : <a href=\"/meetups/x8\">Boston - Optimizing Empathy Levels</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 March 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Joe Schneider will give a talk on Optimizing Empathy Levels:</p>\n\n<p>-What empathy is useful for.</p>\n\n<p>-Why LWers (and others) undervalue it.</p>\n\n<p>-Why non-LWers (and others) overvalue it.</p>\n\n<p>-How you can use it properly to get ahead.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups start at 2pm, and have an alternating location:</p>\n\n<ul>\n<li><p>2nd and 4th Sunday meetups are in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p></li>\n<li><p>Meetups on other weeks are at Citadel in Porter Sq, at 98 Elm St, apt 1, Somerville.</p></li>\n</ul>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 2:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Optimizing_Empathy_Levels1\">Discussion article for the meetup : <a href=\"/meetups/x8\">Boston - Optimizing Empathy Levels</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Optimizing Empathy Levels", "anchor": "Discussion_article_for_the_meetup___Boston___Optimizing_Empathy_Levels", "level": 1}, {"title": "Discussion article for the meetup : Boston - Optimizing Empathy Levels", "anchor": "Discussion_article_for_the_meetup___Boston___Optimizing_Empathy_Levels1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-26T23:49:10.983Z", "modifiedAt": null, "url": null, "title": "Meetup : Munich Meetup", "slug": "meetup-munich-meetup-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cadac", "createdAt": "2011-09-25T11:17:15.655Z", "isAdmin": false, "displayName": "cadac"}, "userId": "hMHAdTtN5PL9KThqu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ve6jaGYy45c92QYmP/meetup-munich-meetup-3", "pageUrlRelative": "/posts/ve6jaGYy45c92QYmP/meetup-munich-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/ve6jaGYy45c92QYmP/meetup-munich-meetup-3", "postedAtFormatted": "Wednesday, February 26th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Munich%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Munich%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fve6jaGYy45c92QYmP%2Fmeetup-munich-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Munich%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fve6jaGYy45c92QYmP%2Fmeetup-munich-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fve6jaGYy45c92QYmP%2Fmeetup-munich-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/x9'>Munich Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 March 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Theresienstra\u00dfe 41, 80333 M\u00fcnchen</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to try a different location this time, and a different format \u2013 I'd like to do a few concrete exercises on basic probability theory or something similar. Afterwards we'll move on to free discussion and maybe Zendo.\nWe're planning to meet outside the <a href=\"https://www.google.de/maps/place/Theresienstra%C3%9Fe+41/@48.1481345,11.5719245,19z/data=!3m1!4b1!4m2!3m1!1s0x479e75ee8fa2fdcb:0xd93a32dd4a2b19a6\" rel=\"nofollow\">mathematics building at the LMU</a>. Depending on the weather, we'll stay outside or occupy a free room inside the math department. Whoever brings food for the group is awesome. :) It goes without saying that newcomers are very welcome.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/x9'>Munich Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ve6jaGYy45c92QYmP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5845060813573397e-06, "legacy": true, "legacyId": "25620", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Munich_Meetup\">Discussion article for the meetup : <a href=\"/meetups/x9\">Munich Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 March 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Theresienstra\u00dfe 41, 80333 M\u00fcnchen</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to try a different location this time, and a different format \u2013 I'd like to do a few concrete exercises on basic probability theory or something similar. Afterwards we'll move on to free discussion and maybe Zendo.\nWe're planning to meet outside the <a href=\"https://www.google.de/maps/place/Theresienstra%C3%9Fe+41/@48.1481345,11.5719245,19z/data=!3m1!4b1!4m2!3m1!1s0x479e75ee8fa2fdcb:0xd93a32dd4a2b19a6\" rel=\"nofollow\">mathematics building at the LMU</a>. Depending on the weather, we'll stay outside or occupy a free room inside the math department. Whoever brings food for the group is awesome. :) It goes without saying that newcomers are very welcome.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Munich_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/x9\">Munich Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Munich Meetup", "anchor": "Discussion_article_for_the_meetup___Munich_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Munich Meetup", "anchor": "Discussion_article_for_the_meetup___Munich_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-27T07:06:29.784Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: Implementation Intentions", "slug": "meetup-berkeley-implementation-intentions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MQurx26sLYjbtiHDq/meetup-berkeley-implementation-intentions", "pageUrlRelative": "/posts/MQurx26sLYjbtiHDq/meetup-berkeley-implementation-intentions", "linkUrl": "https://www.lesswrong.com/posts/MQurx26sLYjbtiHDq/meetup-berkeley-implementation-intentions", "postedAtFormatted": "Thursday, February 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20Implementation%20Intentions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20Implementation%20Intentions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQurx26sLYjbtiHDq%2Fmeetup-berkeley-implementation-intentions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20Implementation%20Intentions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQurx26sLYjbtiHDq%2Fmeetup-berkeley-implementation-intentions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQurx26sLYjbtiHDq%2Fmeetup-berkeley-implementation-intentions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xa'>Berkeley: Implementation Intentions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 March 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all, next week's meetup will be about implementation intentions:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Implementation_intention\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Implementation_intention</a></p>\n\n<p>It sounds boring but in fact it's a technique for changing your behavior that produces half a standard deviation of change in studies with a minimal intervention. Basically, there's a good chance that coming to this meetup will change your behavior in a significant and positive way :)</p>\n\n<p>This is a thing they teach at CFAR.</p>\n\n<p>Please arrive between 7pm and 7:30pm on Wednesday. At 7:30pm as usual we'll review our weekly goals and record goals for the coming week; it should take less than 15 minutes. Afterward I will give a short presentation on implementation intentions, and then we will help each other create implementation intentions</p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xa'>Berkeley: Implementation Intentions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MQurx26sLYjbtiHDq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5850248621413387e-06, "legacy": true, "legacyId": "25629", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Implementation_Intentions\">Discussion article for the meetup : <a href=\"/meetups/xa\">Berkeley: Implementation Intentions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 March 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all, next week's meetup will be about implementation intentions:</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Implementation_intention\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Implementation_intention</a></p>\n\n<p>It sounds boring but in fact it's a technique for changing your behavior that produces half a standard deviation of change in studies with a minimal intervention. Basically, there's a good chance that coming to this meetup will change your behavior in a significant and positive way :)</p>\n\n<p>This is a thing they teach at CFAR.</p>\n\n<p>Please arrive between 7pm and 7:30pm on Wednesday. At 7:30pm as usual we'll review our weekly goals and record goals for the coming week; it should take less than 15 minutes. Afterward I will give a short presentation on implementation intentions, and then we will help each other create implementation intentions</p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Implementation_Intentions1\">Discussion article for the meetup : <a href=\"/meetups/xa\">Berkeley: Implementation Intentions</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: Implementation Intentions", "anchor": "Discussion_article_for_the_meetup___Berkeley__Implementation_Intentions", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: Implementation Intentions", "anchor": "Discussion_article_for_the_meetup___Berkeley__Implementation_Intentions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-27T07:44:38.172Z", "modifiedAt": null, "url": null, "title": "Approaching Logical Probability", "slug": "approaching-logical-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:03.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GcWjDFsAit7CmzYka/approaching-logical-probability", "pageUrlRelative": "/posts/GcWjDFsAit7CmzYka/approaching-logical-probability", "linkUrl": "https://www.lesswrong.com/posts/GcWjDFsAit7CmzYka/approaching-logical-probability", "postedAtFormatted": "Thursday, February 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Approaching%20Logical%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApproaching%20Logical%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGcWjDFsAit7CmzYka%2Fapproaching-logical-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Approaching%20Logical%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGcWjDFsAit7CmzYka%2Fapproaching-logical-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGcWjDFsAit7CmzYka%2Fapproaching-logical-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 911, "htmlBody": "<p><strong>Followup To:</strong>&nbsp;<a style=\"text-align: right;\" href=\"/lw/jjk/logic_as_probability/\">Logic as Probability</a></p>\n<p>If we design a robot that acts as if it's uncertain about mathematical statements, that&nbsp;<a href=\"/lw/jjk/logic_as_probability/\">violates</a>&nbsp;<a href=\"/lw/jfx/foundations_of_probability/\">some desiderata for probability</a>. But realistic robots cannot prove all theorems; they have to be uncertain about hard math problems.</p>\n<p>In the name of practicality, we want a foundation for decision-making that captures what it means to make a good decision, even with limited resources. \"Good\" means that even though our real-world robot can't make decisions well enough to satisfy Savage's theorem, we want to approximate that ideal, not throw it out. Although I don't have the one best answer to give you, in this post we'll take some steps forward.</p>\n<p><a id=\"more\"></a></p>\n<div>The objects we call probabilities are specified by desiderata that tell us how they behave. Any uncertainty about math problems violates those desiderata, but we still want to be able to assign logical probabilities that behave a lot like probabilities. The basic principles - not making up or ignoring information, not throwing away money or being inconsistent - should be deviated from as little as possible even when computing power is scarce. We want to develop a foundation for logical probabilities by starting from the rules governing ordinary probability, and then minimally restricting the application of those rules.</div>\n<div><br /></div>\n<div>As we do this, it's important to keep track of what changes we make and why. Sometimes people just define logical probabilities, without worrying about desiderata. This is fine, when it works, and is often patchable if it doesn't have the right properties. But if you use it for something important and and get a surprise failure, it's really bad. My hope here is to construct logical probabilities that have the good properties, while keeping <a href=\"http://www.catb.org/jargon/html/H/handwave.html\">handwaving</a> and mysterious assumptions to a minimum.</div>\n<div><br /></div>\n<div>The perils of handwaving are more dire than they appear, and they are at their most dire in the hardest and most confusing reaches of physics. After better approaches fail, many theorists resort to just making up approximations and then trying to justify them. Doing this is known colloquially as a \"1/ego expansion.\" Simply put, it doesn't work; there are too many vital little details. It's why even condensed matter theorists tell you not to trust condensed matter theorists about high temperature superconductivity.</div>\n<div><br /></div>\n<div><br /></div>\n<div>We must abandon regular probabilities because our robot has limited time, but other parts of the decision-making process can also go over the time limit. If the robot's resources are limited, expected utility maximization breaks down at many points: there might be too many strategies to search through, too many outcomes to foresee, there might be probabilities that are too hard to find, and the utility of the outcomes might be too complicated.</div>\n<div><br /></div>\n<div>The logical probabilities considered in this sequence will help approximate hard math problems, but they don't seem to help much when there are too many outcomes to consider, or if you want to make the best use of limited computational resources. They are only a part of the full solution.</div>\n<div><br /></div>\n<div><br /></div>\n<div>Time for a desideratum: we want our robot to only assign a logical probability of 1 or 0 to a statement after it's actually checked the proof of that statement.</div>\n<div><br /></div>\n<div>We can think of this as limiting what statements our robot is allowed to be certain about - only statements with short proofs can be found by our agent. However, this desideratum is not just about proof length, because a real robot won't check every checkable proof - it will spend time generating proofs, maybe trying to prove some specific statement, and will end up only checking some subset of short proofs.</div>\n<div><br /></div>\n<div>Logical probabilities, unlike probabilities, are not determined just by the starting information. If our real robot only verifies some small collection of proofs, the robot's logical probabilities depend heavily upon what proof-steps were checked. One proof-step is just one <a href=\"/lw/f43/proofs_implications_and_models/\">truth-preserving</a>&nbsp;step by our robot, like one <a href=\"/r/lesswrong/lw/jjk/logic_as_probability/\">application of modus ponens</a> - it's a little proof one step long. The import is that they're the atomic unit of proofs, and once all the steps of a proof are checked, the proof is checked.</div>\n<div><br /></div>\n<div>If we condition on which proof-steps get checked, does that determine the logical probabilities?</div>\n<div><br /></div>\n<div>For any statement our robot is going to prove or disprove, we can use the checked proof steps to find whether it's logical probability 1 or 0. This gives the same answer as a real robot that checks steps according to some process and then returns 1 or 0 if it manages to prove or disprove the statement we give it. We just have to take the steps that the real robot ends up checking, and say that those are the proved steps for our abstract mathematical definition.</div>\n<div><br /></div>\n<div>There's a problem, though. We haven't changed the old axioms, so they're still only satisfied if we get the right answer for everything. Meanwhile our new desideratum says we can't get the right answer for everything - we've made our axioms internally contradictory. In order to talk about the logical probabilities of unproven statements, we'll need to weaken the original axioms so that they no longer require certainty about everything. We'll explore ways to do this next time. Then we can assign numbers to statements in the usual way, by using our weakened axioms to find constraints, then maximizing entropy subject to those constraints.</div>\n<div><br /></div>\n<div><br /></div>\n<p style=\"text-align:right\">Part of the sequence&nbsp;<em>Logical Uncertainty</em></p>\n<p style=\"text-align:right\">Previous Post:&nbsp;<a href=\"/lw/jjk/logic_as_probability/\">Logic as Probability</a></p>\n<p style=\"text-align:right\">Next post: <a href=\"/lw/joy/solutions_and_open_problems/\">Solutions and Open Problems</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GcWjDFsAit7CmzYka", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 1.585070121319502e-06, "legacy": true, "legacyId": "25329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WKkDD6u79pzmvyQ6a", "EQ33emneF3Fh62Nn2", "Z2CuyKtkCmWGQtAEh", "qiokMkCdFjXa9PTR3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-27T11:41:21.231Z", "modifiedAt": null, "url": null, "title": "Meetup : Saint Petersburg sunday meetup", "slug": "meetup-saint-petersburg-sunday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "efim", "createdAt": "2013-04-14T00:57:28.743Z", "isAdmin": false, "displayName": "efim"}, "userId": "Y8azdhZD6fvWdGwaB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4rcL3N4zJhzwhdw5E/meetup-saint-petersburg-sunday-meetup", "pageUrlRelative": "/posts/4rcL3N4zJhzwhdw5E/meetup-saint-petersburg-sunday-meetup", "linkUrl": "https://www.lesswrong.com/posts/4rcL3N4zJhzwhdw5E/meetup-saint-petersburg-sunday-meetup", "postedAtFormatted": "Thursday, February 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saint%20Petersburg%20sunday%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saint%20Petersburg%20sunday%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rcL3N4zJhzwhdw5E%2Fmeetup-saint-petersburg-sunday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saint%20Petersburg%20sunday%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rcL3N4zJhzwhdw5E%2Fmeetup-saint-petersburg-sunday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rcL3N4zJhzwhdw5E%2Fmeetup-saint-petersburg-sunday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xb'>Saint Petersburg sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 March 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>With high probability we will be playing Zendo.\nAlso studying up information about CFAR and its publicly available information.</p>\n\n<p>If you are russian lesswronger who sees this announcement for the first time, please check out our newsletter or vk group: newsletter or <a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a> for more detailed descriptions.</p>\n\n<p>If you are a foreign guest in Saint Petersburg - we also would all be glad to see you and to meet you - at lest some of our attendees speak english.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xb'>Saint Petersburg sunday meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4rcL3N4zJhzwhdw5E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5853510771519636e-06, "legacy": true, "legacyId": "25631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup\">Discussion article for the meetup : <a href=\"/meetups/xb\">Saint Petersburg sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 March 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>With high probability we will be playing Zendo.\nAlso studying up information about CFAR and its publicly available information.</p>\n\n<p>If you are russian lesswronger who sees this announcement for the first time, please check out our newsletter or vk group: newsletter or <a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a> for more detailed descriptions.</p>\n\n<p>If you are a foreign guest in Saint Petersburg - we also would all be glad to see you and to meet you - at lest some of our attendees speak english.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup1\">Discussion article for the meetup : <a href=\"/meetups/xb\">Saint Petersburg sunday meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saint Petersburg sunday meetup", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup", "level": 1}, {"title": "Discussion article for the meetup : Saint Petersburg sunday meetup", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg_sunday_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-27T15:43:29.955Z", "modifiedAt": null, "url": null, "title": "Link:  Poking the Bear (Podcast)", "slug": "link-poking-the-bear-podcast", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:04.013Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7xmsvrX7rmn4HnGXK/link-poking-the-bear-podcast", "pageUrlRelative": "/posts/7xmsvrX7rmn4HnGXK/link-poking-the-bear-podcast", "linkUrl": "https://www.lesswrong.com/posts/7xmsvrX7rmn4HnGXK/link-poking-the-bear-podcast", "postedAtFormatted": "Thursday, February 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20%20Poking%20the%20Bear%20(Podcast)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20%20Poking%20the%20Bear%20(Podcast)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xmsvrX7rmn4HnGXK%2Flink-poking-the-bear-podcast%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20%20Poking%20the%20Bear%20(Podcast)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xmsvrX7rmn4HnGXK%2Flink-poking-the-bear-podcast", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xmsvrX7rmn4HnGXK%2Flink-poking-the-bear-podcast", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<p class=\"MsoNormal\"><span style=\"font-size: 10pt; line-height: 115%; font-family: Arial, sans-serif; color: #222222; background-position: initial initial; background-repeat: initial initial;\">A Dan Carlin&nbsp;<a href=\"http://www.dancarlin.com//disp.php/csarchive/Show-270---Poking-the-Bear/Ukraine-Russia-Putin\">Podcast</a> about how the United States is foolishly antagonizing the Russians over Ukraine.&nbsp; Carlin makes an analogy as to how the United States would feel if Russia helped overthrow the government of Mexico to install an anti-American government under conditions that might result in a Mexican civil war.&nbsp; Because of the Russian nuclear arsenal, even a tiny chance of a war between the United States and Russia has a huge negative expected value.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7xmsvrX7rmn4HnGXK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 0, "extendedScore": null, "score": 1.585638566295784e-06, "legacy": true, "legacyId": "25632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-27T16:07:05.661Z", "modifiedAt": null, "url": null, "title": "Meetup : London Games Meetup 09/03 [VENUE CHANGE: PENDEREL'S OAK!], + Social 16/02 ", "slug": "meetup-london-games-meetup-09-03-venue-change-penderel-s-oak", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:05.158Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bKSstprRh7HKasnCK/meetup-london-games-meetup-09-03-venue-change-penderel-s-oak", "pageUrlRelative": "/posts/bKSstprRh7HKasnCK/meetup-london-games-meetup-09-03-venue-change-penderel-s-oak", "linkUrl": "https://www.lesswrong.com/posts/bKSstprRh7HKasnCK/meetup-london-games-meetup-09-03-venue-change-penderel-s-oak", "postedAtFormatted": "Thursday, February 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20Games%20Meetup%2009%2F03%20%5BVENUE%20CHANGE%3A%20PENDEREL'S%20OAK!%5D%2C%20%2B%20Social%2016%2F02%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20Games%20Meetup%2009%2F03%20%5BVENUE%20CHANGE%3A%20PENDEREL'S%20OAK!%5D%2C%20%2B%20Social%2016%2F02%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKSstprRh7HKasnCK%2Fmeetup-london-games-meetup-09-03-venue-change-penderel-s-oak%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20Games%20Meetup%2009%2F03%20%5BVENUE%20CHANGE%3A%20PENDEREL'S%20OAK!%5D%2C%20%2B%20Social%2016%2F02%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKSstprRh7HKasnCK%2Fmeetup-london-games-meetup-09-03-venue-change-penderel-s-oak", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbKSstprRh7HKasnCK%2Fmeetup-london-games-meetup-09-03-venue-change-penderel-s-oak", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/xc\">London Games Meetup 09/03, + Socials 02/03 and 16/02 </a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">09 March 2014 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span>283-288 High Holborn, City of London, WC1V 7HP</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>LessWrong London's next non-social gathering is going to be on the 9th of March and is going to be a Games Meetup at a <strong>new</strong> location - <a href=\"https://maps.google.co.uk/maps?oe=utf-8&amp;q=maps+283-288+High+Holborn,+City+of+London,+WC1V+7HP&amp;ie=UTF-8&amp;hq=&amp;hnear=0x48761b4a5bb08def:0x4d910a1148f070d,283-288+High+Holborn,+WC1V+7HP\">The Penderel's Oak pub</a> located about 5-10 minutes away from our usual spot in the middle between Chancery Lane and Holborn stations (I'd recommend looking at the map to get a better idea of the location)</p>\n<p>Thanks to <a href=\"/user/philh/overview/\">Phil</a> we have a wide range of choices.The main ones are <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\">Resistance</a>, <a rel=\"nofollow\" href=\"http://boardgamegeek.com/boardgame/131357/coup\">Coup</a> and <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Zendo_%28game%29\">Zendo</a>. Alternatively, we will be able to play <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Ingenious_%28board_game%29\">Ingenious</a>, Go, <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Diplomacy_%28game%29\">Diplomacy</a>&nbsp;(only if people insist on it) or card games.</p>\n<p><em>We are also having socials on the 16th of March as the Meetups are currently a weekly event.</em></p>\n<p>If you want more information about the meetups or anything else come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> or alternatively to our <a rel=\"nofollow\" href=\"https://www.facebook.com/groups/380103898766356/\">facebook group</a>. <br />If you have trouble finding us - feel free to call or text me on 07425168803.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/xc\">London Games Meetup 09/03, + Socials 02/03 and 16/02 </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bKSstprRh7HKasnCK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "25633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_Games_Meetup_09_03____Socials_02_03_and_16_02_\">Discussion article for the meetup : <a href=\"/meetups/xc\">London Games Meetup 09/03, + Socials 02/03 and 16/02 </a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">09 March 2014 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span>283-288 High Holborn, City of London, WC1V 7HP</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>LessWrong London's next non-social gathering is going to be on the 9th of March and is going to be a Games Meetup at a <strong>new</strong> location - <a href=\"https://maps.google.co.uk/maps?oe=utf-8&amp;q=maps+283-288+High+Holborn,+City+of+London,+WC1V+7HP&amp;ie=UTF-8&amp;hq=&amp;hnear=0x48761b4a5bb08def:0x4d910a1148f070d,283-288+High+Holborn,+WC1V+7HP\">The Penderel's Oak pub</a> located about 5-10 minutes away from our usual spot in the middle between Chancery Lane and Holborn stations (I'd recommend looking at the map to get a better idea of the location)</p>\n<p>Thanks to <a href=\"/user/philh/overview/\">Phil</a> we have a wide range of choices.The main ones are <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/The_Resistance_%28game%29\">Resistance</a>, <a rel=\"nofollow\" href=\"http://boardgamegeek.com/boardgame/131357/coup\">Coup</a> and <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Zendo_%28game%29\">Zendo</a>. Alternatively, we will be able to play <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Ingenious_%28board_game%29\">Ingenious</a>, Go, <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Diplomacy_%28game%29\">Diplomacy</a>&nbsp;(only if people insist on it) or card games.</p>\n<p><em>We are also having socials on the 16th of March as the Meetups are currently a weekly event.</em></p>\n<p>If you want more information about the meetups or anything else come by our <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">google group</a> or alternatively to our <a rel=\"nofollow\" href=\"https://www.facebook.com/groups/380103898766356/\">facebook group</a>. <br>If you have trouble finding us - feel free to call or text me on 07425168803.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___London_Games_Meetup_09_03____Socials_02_03_and_16_02_1\">Discussion article for the meetup : <a href=\"/meetups/xc\">London Games Meetup 09/03, + Socials 02/03 and 16/02 </a></h2>", "sections": [{"title": "Discussion article for the meetup : London Games Meetup 09/03, + Socials 02/03 and 16/02 ", "anchor": "Discussion_article_for_the_meetup___London_Games_Meetup_09_03____Socials_02_03_and_16_02_", "level": 1}, {"title": "Discussion article for the meetup : London Games Meetup 09/03, + Socials 02/03 and 16/02 ", "anchor": "Discussion_article_for_the_meetup___London_Games_Meetup_09_03____Socials_02_03_and_16_02_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-27T17:08:45.470Z", "modifiedAt": null, "url": null, "title": "The Rationality Wars", "slug": "the-rationality-wars", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:10.084Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pGcBPZvWbZSH5cK4j/the-rationality-wars", "pageUrlRelative": "/posts/pGcBPZvWbZSH5cK4j/the-rationality-wars", "linkUrl": "https://www.lesswrong.com/posts/pGcBPZvWbZSH5cK4j/the-rationality-wars", "postedAtFormatted": "Thursday, February 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Rationality%20Wars&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Rationality%20Wars%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpGcBPZvWbZSH5cK4j%2Fthe-rationality-wars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Rationality%20Wars%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpGcBPZvWbZSH5cK4j%2Fthe-rationality-wars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpGcBPZvWbZSH5cK4j%2Fthe-rationality-wars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 992, "htmlBody": "<p>Ever since <a href=\"http://en.wikipedia.org/wiki/Amos_Tversky\">Tversky</a> and <a href=\"http://en.wikipedia.org/wiki/Daniel_Kahneman\">Kahneman</a> started to gather evidence purporting to show that humans suffer from a large number of cognitive biases, other psychologists and philosophers have criticized these findings. For instance, philosopher L. J. Cohen argued in the 80's that there was something conceptually incoherent with the notion that most adults are irrational (with respect to a certain problem). By some sort of Wittgensteinian logic, he thought that the majority's way of reasoning is by definition right. (Not a high point in the history of analytic philosophy, in my view.) <a href=\"/lw/5vs/epistemology_and_the_psychology_of_human_judgment/\">See chapter 8 of this book (where Gigerenzer, below, is also discussed).</a></p>\n<p>Another attempt to resurrect human rationality is due to <a href=\"http://en.wikipedia.org/wiki/Gerd_Gigerenzer\">Gerd Gigerenzer</a> and other psychologists. They have a) shown that if you tweak some of the heuristics and biases (i.e. the research program led by Tversky and Kahneman) experiments but a little - <a href=\"/lw/ioy/instinctive_frequentists_the_outside_view_and/\">for instance by expressing probabilities in terms of frequencies</a>&nbsp;- people make much fewer mistakes and b) argued, on the back of this, that the heuristics we use are in many situations good (and fast and frugal) rules of thumb (which explains why they are evolutionary adaptive). Regarding this, I don't think that Tversky and Kahneman ever doubted that the heuristics we use are quite useful in many situations. Their point was rather that there are lots of naturally occuring set-ups which fool our fast and frugal heuristics. Gigerenzer's findings are not completely uninteresting - it seems to me he does nuance the thesis of massive irrationality a bit - but his claims to the effect that these heuristics are rational in a strong sense are wildly overblown in my opnion. The Gigerenzer vs. Tversky/Kahneman debates are well discussed <a href=\"http://ruccs.rutgers.edu/ArchiveFolder/Research%20Group/Publications/Wars/wars.html\">in this article</a> (although I think they're too kind to Gigerenzer).</p>\n<p>A strong argument against attempts to save human rationality is the argument from individual differences, <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147\">championed by Keith Stanovich</a>. He argues that the fact that some intelligent subjects consistently avoid to fall prey to the <a href=\"http://en.wikipedia.org/wiki/Wason_selection_task\">Wason Selection task</a>, the <a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\">conjunction fallacy</a>, and other fallacies, indicates that there is something misguided with the notion that the answer that psychologists traditionally has seen as normatively correct is in fact misguided.</p>\n<p>Hence I side with Tversky and Kahneman in this debate. Let me just mention one interesting and possible succesful method for disputing some supposed biases. This method is to argue that people have other kinds of evidence than the standard interpretation assumes, and that given this new interpretation of the evidence, the supposed bias in question is in fact not a bias. For instance, it has been suggested that the <a href=\"http://en.wikipedia.org/wiki/False-consensus_effect\">\"false consensus effect\"</a> can be re-interpreted in this wa<span style=\"text-decoration: underline;\">y</span>:</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><strong><span style=\"text-decoration: underline;\">The False Consensus Effect</span></strong></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><strong>Bias description</strong>: People tend to imagine that everyone responds the way they do. They tend to see their own behavior as typical. The tendency to exaggerate how common one&rsquo;s opinions and behavior are is called the false consensus effect. For example, in one study, subjects were asked to walk around on campus for 30 minutes, wearing a sign board that said \"Repent!\". Those who agreed to wear the sign estimated that on average 63.5% of their fellow students would also agree, while those who disagreed estimated 23.3% on average.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19.5px; text-align: justify;\"><strong>Counterclaim</strong>&nbsp;(<a style=\"color: #8a8a8b;\" href=\"http://linkinghub.elsevier.com/retrieve/pii/S0749597896900205\">Dawes &amp; Mulford, 1996</a>): The correctness of&nbsp;<em>reasoning</em>&nbsp;is not estimated on the basis of whether or not one arrives at the correct result. Instead, we look at whether reach reasonable conclusions&nbsp;<em>given the data they have</em>. Suppose we ask people to estimate whether an urn contains more blue balls or red balls, after allowing them to draw one ball. If one person first draws a red ball, and another person draws a blue ball, then we should&nbsp;<em>expect</em>&nbsp;them to give different estimates. In the absence of other data, you&nbsp;<em>should</em>&nbsp;treat your own preferences as evidence for the preferences of others. Although the actual mean for people willing to carry a sign saying \"Repent!\" probably lies somewhere in between of the estimates given, these estimates are quite close to the one-third and two-thirds estimates that would arise from a Bayesian analysis with a uniform prior distribution of belief. A study by the authors suggested that people do actually give their own opinion roughly the right amount of weight.</p>\n</blockquote>\n<p>(The quote is from an excellent <a href=\"/lw/1kp/are_these_cognitive_biases_biases/\">Less Wrong article</a> on this topic due to Kaj Sotala. See also <a href=\"/lw/1kb/fundamentally_flawed_or_fast_and_frugal/\">this post by him</a>,&nbsp;<a href=\"/lw/csf/which_cognitive_biases_should_we_trust_in/\">this by Andy McKenzie</a>, <a href=\"/lw/ioy/instinctive_frequentists_the_outside_view_and/\">this by Stuart Armstrong</a>&nbsp;and <a href=\"/lw/h0p/critiques_of_the_heuristics_and_biases_tradition/\">this by lukeprog</a>&nbsp;on this topic. I'm sure there are more that I've missed.)</p>\n<p>It strikes me that the notion that people are <a href=\"http://wiki.lesswrong.com/wiki/Methods_Of_Rationality_(fanfiction)\">\"massively flawed\"</a>&nbsp;is something of an intellectual cornerstone of the Less Wrong community (e.g. note the names \"Less Wrong\" and \"Overcoming Bias\"). In the light of this it would be interesting to hear what people have to say about the rationality wars. Do you all agree that people are massively flawed?</p>\n<p>Let me make two final notes to keep in mind when discussing these issues. Firstly, even though the heuristics and biases program is sometimes seen as pessimistic, one could turn the tables around: if they're right, we should be able to improve massively (even though Kahneman himself seems to think that that's hard to do in practice). I take it that CFAR and lots of LessWrongers who attempt to \"refine their rationality\" assume that this is the case. On the other hand, if Gigerenzer or Cohen are right, and we already are very rational, then it would seem that it is hard to do much better. So in a sense the latter are more pessimistic (and conservative) than the former.</p>\n<p>Secondly, note that parts of the rationality wars seem to be <a href=\"/lw/np/disputing_definitions/\">merely verbal</a> and revolve around how \"rationality\" is to be defined (<a href=\"/lw/eta/rationality_appreciating_cognitive_algorithms/\">tabooing this word is very often a good idea</a>). The real question is not if the fast and frugal heuristics are in some sense rational, but whether there are other mental algorithms which are more reliable and effective, and whether it is plausible to assume that we could learn to use them on a large scale instead.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "Xw6pxiicjuv6NJWjf": 1, "Ng8Gice9KNkncxqcj": 1, "dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pGcBPZvWbZSH5cK4j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 30, "extendedScore": null, "score": 2.9646140146864837e-05, "legacy": true, "legacyId": "25634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dKtetFsoY3f85vtJi", "24CfLhbByQqv6nSws", "b7cWpbXwcQySDq4kK", "psQYbMLWzS9sTsT2M", "8arFF9SdstBqz7c8K", "DAf4W9ZYuzuLaGvd5", "7X2j8HAkWdmMoS8PE", "HcCpvYLoSFP4iAqSz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-27T19:40:00.978Z", "modifiedAt": null, "url": null, "title": "[LINK] Joseph Bottum on Politics as the Mindkiller", "slug": "link-joseph-bottum-on-politics-as-the-mindkiller", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:35.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Salemicus", "createdAt": "2012-05-10T20:50:25.455Z", "isAdmin": false, "displayName": "Salemicus"}, "userId": "D8cdrPXwhhiPdqkSz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wqvKXAcsXaqpc2RsC/link-joseph-bottum-on-politics-as-the-mindkiller", "pageUrlRelative": "/posts/wqvKXAcsXaqpc2RsC/link-joseph-bottum-on-politics-as-the-mindkiller", "linkUrl": "https://www.lesswrong.com/posts/wqvKXAcsXaqpc2RsC/link-joseph-bottum-on-politics-as-the-mindkiller", "postedAtFormatted": "Thursday, February 27th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Joseph%20Bottum%20on%20Politics%20as%20the%20Mindkiller&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Joseph%20Bottum%20on%20Politics%20as%20the%20Mindkiller%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwqvKXAcsXaqpc2RsC%2Flink-joseph-bottum-on-politics-as-the-mindkiller%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Joseph%20Bottum%20on%20Politics%20as%20the%20Mindkiller%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwqvKXAcsXaqpc2RsC%2Flink-joseph-bottum-on-politics-as-the-mindkiller", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwqvKXAcsXaqpc2RsC%2Flink-joseph-bottum-on-politics-as-the-mindkiller", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 411, "htmlBody": "<p>One of my favourite Less Wrong articles is <a href=\"/lw/gw/politics_is_the_mindkiller/\" target=\"_blank\">Politics is the mindkiller</a>. Part of the reason that political discussion so bad is the poor incentives - if you have little chance to change the outcome, then there is little reason to strive for truth or accuracy - but a large part of the reason is our pre-political attitudes and dispositions. I don't mean to suggest that there is a neat divide; clearly, there is a reflexive relation between the incentives within political discussion and our view of the appropriate purpose and scope of politics. Nevertheless, I think it's a useful distinction to make, and so I applaud the fact that Eliezer doesn't start his essays on the subject by talking about incentives, feedback or rational irrationality - instead he starts with the fact that our approach to politics is <em>instinctively</em> tribal.</p>\n<p>This brings me to Joseph Bottum's excellent recent article in The American, <a href=\"http://american.com/archive/2014/february/the-post-protestant-ethic-and-spirit-of-america\" target=\"_blank\">The Post-Protestant Ethic and Spirit of America</a>. This charts what he sees as the tribal changes within America that have shaped current attitudes to politics. I think it's best seen in conjunction with Arnold Kling's excellent <a href=\"http://www.amazon.co.uk/Three-Languages-Politics-Arnold-Kling-ebook/dp/B00CCGF81Q\" target=\"_blank\">The Three Languages of Politics</a>; while Kling talks about the political language and rhetoric of modern American political groupings, Bottum's essay is more about the social changes that have led to these kinds of language and rhetoric.</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em\">We live in what can only be called a spiritual age, swayed by its metaphysical fears and hungers, when we imagine that our ordinary political opponents are not merely mistaken, but actually evil. When we assume that past ages, and the people who lived in them, are defined by the systematic crimes of history. When we suppose that some vast ethical miasma, racism, radicalism, cultural self-hatred, selfish blindness, determines the beliefs of classes other than our own. When we can make no rhetorical distinction between absolute wickedness and the people with whom we disagree. The Republican Congress is the Taliban. President Obama is a Communist. Wisconsin&rsquo;s governor is a Nazi.</p>\n<p style=\"margin: 0px 0px 1em\">...</p>\n<p style=\"margin: 0px 0px 1em;\">The real question, of course, is how and why this happened. How and why politics became a mode of spiritual redemption for nearly everyone in America, but especially for the college-educated upper-middle class, who are probably best understood not as the elite, but as the elect, people who know themselves as good, as relieved of their spiritual anxieties by their attitudes toward social problems.</p>\n</blockquote>\n<p>Video of a related lecture can also be found <a href=\"http://www.aei.org/events/2014/02/10/an-anxious-age-the-post-protestant-ethic-and-the-spirit-of-america/\" target=\"_blank\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wqvKXAcsXaqpc2RsC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "25635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-28T01:25:25.054Z", "modifiedAt": null, "url": null, "title": "The sin of updating when you can change whether you exist", "slug": "the-sin-of-updating-when-you-can-change-whether-you-exist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:03.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TCjiFNfJ7z8W4cEwr/the-sin-of-updating-when-you-can-change-whether-you-exist", "pageUrlRelative": "/posts/TCjiFNfJ7z8W4cEwr/the-sin-of-updating-when-you-can-change-whether-you-exist", "linkUrl": "https://www.lesswrong.com/posts/TCjiFNfJ7z8W4cEwr/the-sin-of-updating-when-you-can-change-whether-you-exist", "postedAtFormatted": "Friday, February 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20sin%20of%20updating%20when%20you%20can%20change%20whether%20you%20exist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20sin%20of%20updating%20when%20you%20can%20change%20whether%20you%20exist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCjiFNfJ7z8W4cEwr%2Fthe-sin-of-updating-when-you-can-change-whether-you-exist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20sin%20of%20updating%20when%20you%20can%20change%20whether%20you%20exist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCjiFNfJ7z8W4cEwr%2Fthe-sin-of-updating-when-you-can-change-whether-you-exist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCjiFNfJ7z8W4cEwr%2Fthe-sin-of-updating-when-you-can-change-whether-you-exist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2918, "htmlBody": "<p><em>Trigger warning: In a thought experiment in this post, I used a hypothetical torture scenario without thinking, even though it wasn't necessary to make my point. Apologies, and <a href=\"/lw/jrm/the_sin_of_updating_when_you_can_change_whether/amon\">thanks to an anonymous user for pointing this out</a>.</em> <em>I'll try to be more careful in the future.</em></p>\n<p>Should you pay up in the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a>?</p>\n<p>I've always found the argument about self-modifying agents compelling: If you expected to face a counterfactual mugging tomorrow, you would want to choose to rewrite yourself today so that you'd pay up. Thus, a decision theory that didn't pay up wouldn't be reflectively consistent; an AI using such a theory would decide to rewrite itself to use a different theory.</p>\n<p>But is this the only reason to pay up? This might make a difference: Imagine that Omega tells you that it threw its coin a million years ago, and would have turned the sky green if it had landed the other way. <a href=\"/lw/22m/selfmodification_is_the_correct_justification_for/\">Back in 2010, I wrote a post</a> arguing that in this sort of situation, since you've always seen the sky being blue, and every other human being has also always seen the sky being blue, everyone has always had enough information to conclude that there's no benefit from paying up in <em>this</em> particular counterfactual mugging, and so there hasn't ever been any incentive to self-modify into an agent that would pay up ... and so you shouldn't.</p>\n<p>I've since changed my mind, and I've recently talked about <em>part</em> of the reason for this, when I introduced the concept of an <a href=\"/lw/jkm/lzombies_lzombies/\">l-zombie</a>, or <em>logical philosophical zombie</em>, a mathematically possible conscious experience that isn't physically instantiated and therefore isn't actually consciously experienced. (Obligatory disclaimer: I'm not claiming that the idea that \"some mathematically possible experiences are l-zombies\" is likely to be true, but I think it's a useful concept for thinking about anthropics, and I don't think we should <em>rule out</em> l-zombies given our present state of knowledge. More in the l-zombies post and in <a href=\"/lw/jod/i_like_simplicity_but_not_that_much/\">this post about measureless Tegmark IV</a>.) Suppose that Omega's coin had come up the other way, and Omega had turned the sky green. Then you and I would be l-zombies. But if Omega was able to make a confident guess about the decision we'd make if confronted with the counterfactual mugging (without simulating us, so that we continue to be l-zombies), then our decisions would still influence what happens in the actual physical world. Thus, if l-zombies say \"I have conscious experiences, therefore I physically exist\", and update on this fact, and if the decisions they make based on this influence what happens in the real world, a lot of utility may potentially be lost. Of course, you and I <em>aren't</em> l-zombies, but the mathematically possible versions of us who have grown up under a green sky <em>are</em>, and they reason the same way as you and me&mdash;it's not possible to have only the <em>actual</em> conscious observers reason that way. Thus, you should pay up even in the blue-sky mugging.</p>\n<p>But that's only part of the reason I changed my mind. The other part is that while in the counterfactual mugging, the answer you get if you try to use Bayesian updating at least <em>looks</em> kinda sensible, there are other thought experiments in which doing so in the straight-forward way makes you <em>obviously bat-shit crazy</em>. That's what I'd like to talk about today.</p>\n<p align=\"center\">*</p>\n<p>The kind of situation I have in mind involves being able to influence whether you exist, or more precisely, influence whether the version of you making the decision exists as a conscious observer (or whether it's an l-zombie).</p>\n<p>Suppose that you wake up and Omega explains to you that it's kidnapped you and some of your friends back in 2014, and put you into suspension; it's now the year 2100. It then hands you a little box with a red button, and tells you that if you press that button, Omega will slowly torture you and your friends to death; otherwise, you'll be able to live out a more or less normal and happy life (or to commit painless suicide, if you prefer). Furthermore, it explains that one of two things have happened: Either (1) humanity has undergone a positive intelligence explosion, and Omega has predicted that you will press the button; or (2) humanity has wiped itself out, and Omega has predicted that you will <em>not</em> press the button. In any other scenario, Omega would still have woken you up at the same time, but wouldn't have given you the button. Finally, if humanity has wiped itself out, it won't let you try to \"reboot\" it; in this case, you and your friends will be the last humans.</p>\n<p>There's a correct answer to what to do in this situation, and it <em>isn't</em> to decide that Omega's just given you anthropic superpowers to save the world. But that's what you get if you try to update in the most naive way: If you press the button, then (2) becomes extremely unlikely, since Omega is <em>really really good</em> at predicting. Thus, the true world is almost certainly (1); you'll get tortured, but humanity survives. For great utility! On the other hand, if you decide to <em>not</em> press the button, then by the same reasoning, the true world is almost certainly (2), and humanity has wiped itself out. Surely you're not selfish enough to prefer <em>that</em>?</p>\n<p>The <em>correct</em> answer, clearly, is that your decision whether to press the button doesn't influence whether humanity survives, it only influences whether you get tortured to death. (Plus, of course, whether Omega hands you the button in the first place!) You don't want to get tortured, so you don't press the button. Updateless reasoning gets this right.</p>\n<p align=\"center\">*</p>\n<p>Let me spell out the rules of the naive Bayesian decision theory (\"NBDT\") I used there, in analogy with <a href=\"/lw/jpr/sudt_a_toy_decision_theory_for_updateless/\">Simple Updateless Decision Theory (SUDT)</a>. First, let's set up our problem in the SUDT framework. To simplify things, we'll pretend that FOOM and DOOM are the only possible things that can happen to humanity. In addition, we'll assume that there's a small probability <img title=\"\\varepsilon\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\varepsilon\" alt=\"\" align=\"bottom\" /> that Omega makes a mistake when it tries to predict what you will do if given the button. Thus, the relevant possible worlds are <img title=\"\\Omega = \\{\\mathrm{foom}, \\mathrm{doom}\\} \\times \\{\\mathrm{correct},\\mathrm{incorrect}\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\Omega = \\{\\mathrm{foom}, \\mathrm{doom}\\} \\times \\{\\mathrm{correct},\\mathrm{incorrect}\\}\" alt=\"\" align=\"bottom\" />. The precise probabilities you assign to these doesn't matter very much; I'll pretend that FOOM and DOOM are equiprobable, <img title=\"\\mathbb{P}(x,\\mathrm{incorrect}) = \\varepsilon/2\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{P}(x,\\mathrm{incorrect}) = \\varepsilon/2\" alt=\"\" align=\"bottom\" /> and <img title=\"\\mathbb{P}(x,\\mathrm{correct}) = (1-\\varepsilon)/2\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{P}(x,\\mathrm{correct}) = (1-\\varepsilon)/2\" alt=\"\" align=\"bottom\" />.</p>\n<p>There's only one situation in which you need to make a decision, <img title=\"\\mathcal{I} = \\{*\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathcal{I} = \\{*\\}\" alt=\"\" align=\"bottom\" />; I won't try to define NBDT when there is more than one situation. Your possible actions in this situation are to press or to not press the button, <img title=\"\\mathcal{A}(*) = \\{P,\\neg P\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathcal{A}(*) = \\{P,\\neg P\\}\" alt=\"\" align=\"bottom\" />, so the only possible policies are <img title=\"\\pi_P\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_P\" alt=\"\" align=\"bottom\" />, which presses the button (<img title=\"\\pi_P(*) = P\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_P(*) = P\" alt=\"\" align=\"bottom\" />), and <img title=\"\\pi_{\\neg P}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_{\\neg P}\" alt=\"\" align=\"bottom\" />, which doesn't (<img title=\"\\pi_{\\neg P}(*) = \\neg P\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_{\\neg P}(*) = \\neg P\" alt=\"\" align=\"bottom\" />); <img title=\"\\Pi = \\{\\pi_P,\\pi_{\\neg P}\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\Pi = \\{\\pi_P,\\pi_{\\neg P}\\}\" alt=\"\" align=\"bottom\" />.</p>\n<p>There are four possible outcomes, specifying (a) whether humanity survives and (b) whether you get tortured: <img title=\"\\mathcal{O} = \\{\\mathrm{foom}, \\mathrm{doom}\\} \\times \\{\\mathrm{torture},\\neg\\mathrm{torture}\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathcal{O} = \\{\\mathrm{foom}, \\mathrm{doom}\\} \\times \\{\\mathrm{torture},\\neg\\mathrm{torture}\\}\" alt=\"\" align=\"bottom\" />. Omega only hands you the button if FOOM and it predicts you'll press it, or DOOM and it predicts you won't. Thus, the only cases in which you'll get tortured are <img title=\"o((\\mathrm{foom},\\mathrm{correct}),\\pi_P) = (\\mathrm{foom},\\mathrm{torture})\" src=\"http://www.codecogs.com/png.latex?\\textstyle o((\\mathrm{foom},\\mathrm{correct}),\\pi_P) = (\\mathrm{foom},\\mathrm{torture})\" alt=\"\" align=\"bottom\" /> and <img style=\"vertical-align: bottom;\" title=\"o((\\mathrm{doom},\\mathrm{incorrect}),\\pi_{\\neg P}) = (\\mathrm{doom},\\mathrm{torture})\" src=\"http://www.codecogs.com/png.latex?\\textstyle o((\\mathrm{doom},\\mathrm{incorrect}),\\pi_P) = (\\mathrm{doom},\\mathrm{torture})\" alt=\"\" align=\"bottom\" />. For any other <img title=\"x\\in\\{\\mathrm{foom},\\mathrm{doom}\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle x\\in\\{\\mathrm{foom},\\mathrm{doom}\\}\" alt=\"\" align=\"bottom\" />, <img title=\"y\\in\\{\\mathrm{correct},\\mathrm{incorrect}\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle y\\in\\{\\mathrm{correct},\\mathrm{incorrect}\\}\" alt=\"\" align=\"bottom\" />, and <img title=\"\\pi\\in\\Pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\\in\\Pi\" alt=\"\" align=\"bottom\" />, we have <img title=\"o((x,y),\\pi) = (x,\\neg\\mathrm{torture})\" src=\"http://www.codecogs.com/png.latex?\\textstyle o((x,y),\\pi) = (x,\\neg\\mathrm{torture})\" alt=\"\" align=\"bottom\" />.</p>\n<p>Finally, let's define our utility function by <img title=\"u((\\mathrm{foom},\\neg\\mathrm{torture})) = L\" src=\"http://www.codecogs.com/png.latex?u((\\mathrm{foom},\\neg\\mathrm{torture})) = L\" alt=\"\" align=\"bottom\" />, <img title=\"u((\\mathrm{foom},\\mathrm{torture})) = L-1\" src=\"http://www.codecogs.com/png.latex?u((\\mathrm{foom},\\mathrm{torture})) = L-1\" alt=\"\" align=\"bottom\" />, <img title=\"u((\\mathrm{doom},\\neg\\mathrm{torture})) = -L\" src=\"http://www.codecogs.com/png.latex?u((\\mathrm{doom},\\neg\\mathrm{torture})) = -L\" alt=\"\" align=\"bottom\" />, and <img style=\"vertical-align: bottom;\" title=\"u((\\mathrm{foom},\\mathrm{torture})) = -L-1\" src=\"http://www.codecogs.com/png.latex?u((\\mathrm{doom},\\mathrm{torture})) = -L-1\" alt=\"\" align=\"bottom\" />, where <img title=\"L\" src=\"http://www.codecogs.com/png.latex?\\textstyle L\" alt=\"\" align=\"bottom\" /> is a very large number.</p>\n<p>This suffices to set up an SUDT decision problem. There are only two possible worlds <img title=\"\\omega\\in\\Omega\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\omega\\in\\Omega\" alt=\"\" align=\"bottom\" /> where <img title=\"u(o(\\omega,\\pi_P))\" src=\"http://www.codecogs.com/png.latex?\\textstyle u(o(\\omega,\\pi_P))\" alt=\"\" align=\"bottom\" /> differs from <img title=\"u(o(\\omega,\\pi_{\\neg P}))\" src=\"http://www.codecogs.com/png.latex?\\textstyle u(o(\\omega,\\pi_{\\neg P}))\" alt=\"\" align=\"bottom\" />, namely <img title=\"(\\mathrm{foom},\\mathrm{correct})\" src=\"http://www.codecogs.com/png.latex?\\textstyle (\\mathrm{foom},\\mathrm{correct})\" alt=\"\" align=\"bottom\" /> and <img title=\"(\\mathrm{doom},\\mathrm{incorrect})\" src=\"http://www.codecogs.com/png.latex?\\textstyle (\\mathrm{doom},\\mathrm{incorrect})\" alt=\"\" align=\"bottom\" />, where <img title=\"\\pi_P\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_P\" alt=\"\" align=\"bottom\" /> results in torture and <img title=\"\\pi_{\\neg P}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_{\\neg P}\" alt=\"\" align=\"bottom\" /> doesn't. In each of these cases, the utility of <img title=\"\\pi_P\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_P\" alt=\"\" align=\"bottom\" /> is lower (by one) than that of <img title=\"\\pi_{\\neg P}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_{\\neg P}\" alt=\"\" align=\"bottom\" />. Hence, <img title=\"\\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi_P))] &lt; \\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi_{\\neg P}))]\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi_P))] &lt; \\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi_{\\neg P}))]\" alt=\"\" align=\"bottom\" />, implying that SUDT says you should choose <img title=\"\\pi_{\\neg P}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi_{\\neg P}\" alt=\"\" align=\"bottom\" />.</p>\n<p align=\"center\">*</p>\n<p>For NBDT, we need to know how to update, so we need one more ingredient: a function specifying in which worlds you exist as a conscious observer. In anticipation of future discussions, I'll write this as a function <img title=\"\\mu(i;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(i;\\omega,\\pi)\" alt=\"\" align=\"bottom\" />, which gives the \"measure\" (\"amount of magical reality fluid\") of the conscious observation <img title=\"i\\in\\mathcal{I}\" src=\"http://www.codecogs.com/png.latex?\\textstyle i\\in\\mathcal{I}\" alt=\"\" align=\"bottom\" /> if policy <img title=\"\\pi\\in\\Pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\\in\\Pi\" alt=\"\" align=\"bottom\" /> is executed in the possible world <img title=\"\\omega\\in\\Omega\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\omega\\in\\Omega\" alt=\"\" align=\"bottom\" />. In our case, <img title=\"i = *\" src=\"http://www.codecogs.com/png.latex?\\textstyle i = *\" alt=\"\" align=\"bottom\" /> and <img title=\"\\mu(*;\\omega,\\pi)\\in\\{0,1\\}\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;\\omega,\\pi)\\in\\{0,1\\}\" alt=\"\" align=\"bottom\" />, indicating non-existence and existence, respectively. We can interpret <img title=\"\\mu(i;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(i;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> as the <em>conditional</em> probability of making observation <img title=\"i\" src=\"http://www.codecogs.com/png.latex?\\textstyle i\" alt=\"\" align=\"bottom\" />, given that the true world is <img title=\"\\omega\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\omega\" alt=\"\" align=\"bottom\" />, if plan <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" /> is executed. In our case, <img title=\"\\mu(*;(\\mathrm{foom},\\mathrm{correct}),\\pi_P) =\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;(\\mathrm{foom},\\mathrm{correct}),\\pi_P) =\" alt=\"\" align=\"bottom\" /> <img title=\"\\mu(*;(\\mathrm{foom},\\mathrm{incorrect}),\\pi_{\\neg P}) =\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;(\\mathrm{foom},\\mathrm{incorrect}),\\pi_{\\neg P}) =\" alt=\"\" align=\"bottom\" /> <img title=\"\\mu(*;(\\mathrm{doom},\\mathrm{correct}),\\pi_{\\neg P}) =\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;(\\mathrm{doom},\\mathrm{correct}),\\pi_{\\neg P}) =\" alt=\"\" align=\"bottom\" /> <img title=\"\\mu(*;(\\mathrm{doom},\\mathrm{incorrect}),\\pi_P) = 1\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;(\\mathrm{doom},\\mathrm{incorrect}),\\pi_P) = 1\" alt=\"\" align=\"bottom\" />, and <img title=\"\\mu(*;\\omega,\\pi) = 0\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;\\omega,\\pi) = 0\" alt=\"\" align=\"bottom\" /> in all other cases.</p>\n<p>Now, we can use Bayes' theorem to calculate the <em>posterior probability</em> of a possible world, given information <img title=\"i = *\" src=\"http://www.codecogs.com/png.latex?\\textstyle i = *\" alt=\"\" align=\"bottom\" /> and policy <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" />: <img title=\"\\mathbb{P}(\\omega\\mid i;\\pi) = \\mathbb{P}(\\omega)\\cdot\\mu(i;\\omega,\\pi) / \\sum_{\\omega'\\in\\Omega} \\mathbb{P}(\\omega')\\cdot\\mu(i;\\omega',\\pi)\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{P}(\\omega\\mid i;\\pi) = \\mathbb{P}(\\omega)\\cdot\\mu(i;\\omega,\\pi) / \\sum_{\\omega'\\in\\Omega} \\mathbb{P}(\\omega')\\cdot\\mu(i;\\omega',\\pi)\" alt=\"\" align=\"bottom\" />. NBDT tells us to choose the policy <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" /> that maximizes the posterior expected utility, <img title=\"\\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\mid i;\\pi]\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\mid i;\\pi]\" alt=\"\" align=\"bottom\" />.</p>\n<p>In our case, we have <img title=\"\\mathbb{P}((\\mathrm{foom},\\mathrm{correct}) \\mid *;\\pi_P) = \\mathbb{P}((\\mathrm{doom},\\mathrm{correct}) \\mid *;\\pi_{\\neg P}) = 1-\\varepsilon\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{P}((\\mathrm{foom},\\mathrm{correct}) \\mid *;\\pi_P) = \\mathbb{P}((\\mathrm{doom},\\mathrm{correct}) \\mid *;\\pi_{\\neg P}) = 1-\\varepsilon\" alt=\"\" align=\"bottom\" /> and <img title=\"\\mathbb{P}((\\mathrm{doom},\\mathrm{incorrect}) \\mid *;\\pi_P) = \\mathbb{P}((\\mathrm{foom},\\mathrm{incorrect}) \\mid *;\\pi_{\\neg P}) = \\varepsilon\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{P}((\\mathrm{doom},\\mathrm{incorrect}) \\mid *;\\pi_P) = \\mathbb{P}((\\mathrm{foom},\\mathrm{incorrect}) \\mid *;\\pi_{\\neg P}) = \\varepsilon\" alt=\"\" align=\"bottom\" />. Thus, if we press the button, our expected utility is dominated by the near-certainty of humanity surviving, whereas if we don't, it's dominated by humanity's near-certain doom, and NBDT says we should press.</p>\n<p align=\"center\">*</p>\n<p>But maybe it's not updating that's bad, but NBDT's way of implementing it? After all, we get the clearly wacky results only if our decisions can influence whether we exist, and perhaps the way that NBDT extends the usual formula to this case happens to be the wrong way to extend it.</p>\n<p>One thing we could try is to mark a possible world <img title=\"\\omega\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\omega\" alt=\"\" align=\"bottom\" /> as impossible only if <img title=\"\\mu(*;\\omega,\\pi) = 0\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;\\omega,\\pi) = 0\" alt=\"\" align=\"bottom\" /> for <em>all</em> policies <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" /> (rather than: for the particular policy <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" /> whose expected utility we are computing). But this seems very ad hoc to me. (For example, this could depend on which set of possible actions <img title=\"\\mathcal{A}(*)\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathcal{A}(*)\" alt=\"\" align=\"bottom\" /> we consider, which seems odd.)</p>\n<p>There is a much more principled possibility, which I'll call <em>pseudo-Bayesian decision theory</em>, or PBDT. PBDT can be seen as re-interpreting updating as saying that you're <em>indifferent</em> about what happens in possible worlds in which you don't exist as a conscious observer, rather than ruling out those worlds as impossible given your evidence. (A version of this idea was recently brought up in <a href=\"/lw/jpr/sudt_a_toy_decision_theory_for_updateless/alzy\">a comment by drnickbone</a>, though I'd thought of this idea myself during my journey towards my current position on updating, and I imagine it has also appeared elsewhere, though I don't remember any specific instances.) I have more than one objection to PBDT, but the simplest one to argue is that it doesn't solve the problem: it <em>still</em> believes that it has anthropic superpowers in the problem above.</p>\n<p>Formally, PBDT says that we should choose the policy <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" /> that maximizes <img title=\"\\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\cdot\\mu(*;\\boldsymbol{\\omega},\\pi)]\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\cdot\\mu(*;\\boldsymbol{\\omega},\\pi)]\" alt=\"\" align=\"bottom\" /> (where the expectation is with respect to the <em>prior</em>, not the updated, probabilities). In other words, we set the utility of any outcome in which we don't exist as a conscious observer to zero; we can see PBDT as SUDT with modified outcome and utility functions.</p>\n<p>When our existence is independent on our decisions&mdash;that is, if <img title=\"\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> doesn't depend on <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" />&mdash;then it turns out that PBDT and NBDT are equivalent, i.e., PBDT implements Bayesian updating. That's because in that case, <img title=\"\\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\mid *;\\pi] =\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\mid *;\\pi] =\" alt=\"\" align=\"bottom\" /> <img title=\"\\sum_{\\omega\\in\\Omega} u(o(\\omega,\\pi))\\cdot\\mathbb{P}(\\omega\\mid *;\\pi)\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\sum_{\\omega\\in\\Omega} u(o(\\omega,\\pi))\\cdot\\mathbb{P}(\\omega\\mid *;\\pi)\" alt=\"\" align=\"bottom\" /> <img style=\"vertical-align: bottom;\" title=\"= \\sum_{\\omega\\in\\Omega} u(o(\\omega,\\pi))\\cdot\\mathbb{P}(\\omega)\\cdot \\mu(*;\\omega,\\pi) / \\sum_{\\omega'\\in\\Omega} \\mathbb{P}(\\omega')\\cdot\\mu(i;\\omega',\\pi)\" src=\"http://www.codecogs.com/png.latex?\\textstyle = \\sum_{\\omega\\in\\Omega} u(o(\\omega,\\pi))\\cdot\\mathbb{P}(\\omega)\\cdot \\mu(*;\\omega,\\pi) / \\sum_{\\omega'\\in\\Omega} \\mathbb{P}(\\omega')\\cdot\\mu(*;\\omega',\\pi)\" alt=\"\" align=\"bottom\" />. If <img title=\"\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> doesn't depend on <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" />, then the whole denominator doesn't depend on <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\pi\" alt=\"\" align=\"bottom\" />, so the fraction is maximized if and only if the numerator is. But the numerator is <img title=\"\\sum_{\\omega\\in\\Omega} u(o(\\omega,\\pi))\\cdot\\mathbb{P}(\\omega)\\cdot \\mu(*;\\omega,\\pi) =\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\sum_{\\omega\\in\\Omega} u(o(\\omega,\\pi))\\cdot\\mathbb{P}(\\omega)\\cdot \\mu(*;\\omega,\\pi) =\" alt=\"\" align=\"bottom\" /> <img title=\"\\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\cdot\\mu(*;\\omega,\\pi)]\" src=\"http://www.codecogs.com/png.latex?\\textstyle \\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\cdot\\mu(*;\\omega,\\pi)]\" alt=\"\" align=\"bottom\" />, exactly the quantity that PBDT says should be maximized.</p>\n<p>Unfortunately, although in our problem above <img title=\"\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?\\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> <em>does</em> depend of <img title=\"\\pi\" src=\"http://www.codecogs.com/png.latex?\\pi\" alt=\"\" align=\"bottom\" />, the <em>denominator</em> as a whole still doesn't: For both <img title=\"\\pi_P\" src=\"http://www.codecogs.com/png.latex?\\pi_P\" alt=\"\" align=\"bottom\" /> and <img title=\"\\pi_{\\neg P}\" src=\"http://www.codecogs.com/png.latex?\\pi_{\\neg P}\" alt=\"\" align=\"bottom\" />, there is exactly one possible world with probability <img title=\"(1-\\varepsilon)/2\" src=\"http://www.codecogs.com/png.latex?(1-\\varepsilon)/2\" alt=\"\" align=\"bottom\" /> and one possible world with probability <img title=\"\\varepsilon/2\" src=\"http://www.codecogs.com/png.latex?\\varepsilon/2\" alt=\"\" align=\"bottom\" /> in which <img title=\"*\" src=\"http://www.codecogs.com/png.latex?*\" alt=\"\" align=\"bottom\" /> is a conscious observer, so we have <img title=\"\\textstyle\\sum_{\\omega'\\in\\Omega} \\mathbb{P}(\\omega')\\cdot\\mu(*;\\omega',\\pi) = 1/2\" src=\"http://www.codecogs.com/png.latex?\\textstyle\\sum_{\\omega'\\in\\Omega} \\mathbb{P}(\\omega')\\cdot\\mu(*;\\omega',\\pi) = 1/2\" alt=\"\" align=\"bottom\" /> for both <img title=\"\\pi\\in\\Pi\" src=\"http://www.codecogs.com/png.latex?\\pi\\in\\Pi\" alt=\"\" align=\"bottom\" />. Thus, PBDT gives the same answer as NBDT, by the same mathematical argument as in the case where we can't influence our own existence. If you think of PBDT as SUDT with the utility function <img title=\"u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" />, then intuitively, PBDT can be thought of as reasoning, \"Sure, I can't influence whether humanity is wiped out; but I can influence whether I'm an l-zombie or a conscious observer; and who cares what happens to humanity if I'm not? Best to press to button, since getting tortured in a world where there's been a positive intelligence explosion is much better than life without torture if humanity has been wiped out.\"</p>\n<p>I think that's a pretty compelling argument against PBDT, but even leaving it aside, I don't like PBDT at all. I see two possible justifications for PBDT: You can either say that <img title=\"u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> is your <em>real</em> utility function&mdash;you <em>really</em> don't care about what happens in worlds where the version of you making the decision doesn't exist as a conscious observer&mdash;or you can say that your real preferences are expressed by <img title=\"u(o(\\omega,\\pi))\" src=\"http://www.codecogs.com/png.latex?u(o(\\omega,\\pi))\" alt=\"\" align=\"bottom\" />, and multiplying by <img title=\"\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?\\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> is just a mathematical trick to express a steelmanned version of Bayesian updating. If your preferences <em>really</em> are given by <img title=\"u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" />, then fine, and you <em>should</em> be maximizing <img title=\"\\mathbb{E}[u(o(\\boldsymbol{\\omega},\\pi))\\cdot\\mu(*;\\omega,\\pi)]\" src=\"http://www.codecogs.com/png.latex?%5Ctextstyle%20%5Cmathbb%7BE%7D[u%28o%28%5Cboldsymbol%7B%5Comega%7D,%5Cpi%29%29%5Ccdot%5Cmu%28*;%5Comega,%5Cpi%29]\" alt=\"\" align=\"bottom\" /> (because you should be using (S)UDT), and you <em>should</em> press the button. Some kind of super-selfish agent, who doesn't care a fig even about a version of itself that is exactly the same up till five seconds ago (but then wasn't handed the button) could indeed have such preferences. But I think these are wacky preferences, and you don't actually have them. (Furthermore, if you <em>did</em> have them, then <img title=\"u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?u(o(\\omega,\\pi))\\cdot\\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> would be your actual utility function, and you should be writing it as just <img title=\"u(o(\\omega,\\pi))\" src=\"http://www.codecogs.com/png.latex?u(o(\\omega,\\pi))\" alt=\"\" align=\"bottom\" />, where <img title=\"o(\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?o(\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> must now give information about whether <img title=\"*\" src=\"http://www.codecogs.com/png.latex?*\" alt=\"\" align=\"bottom\" /> is a conscious observer.)</p>\n<p>If multiplying by <img title=\"\\mu(*;\\omega,\\pi)\" src=\"http://www.codecogs.com/png.latex?\\mu(*;\\omega,\\pi)\" alt=\"\" align=\"bottom\" /> is just a trick to implement updating, on the other hand, then I find it strange that it introduces a new concept that doesn't occur at all in classical Bayesian updating, namely the utility of a world in which <img title=\"*\" src=\"http://www.codecogs.com/png.latex?*\" alt=\"\" align=\"bottom\" /> is an l-zombie. We've set this to zero, which is no loss of generality because classical utility functions don't change their meaning if you add or subtract a constant, so whenever you have a utility function where all worlds in which <img title=\"*\" src=\"http://www.codecogs.com/png.latex?*\" alt=\"\" align=\"bottom\" /> is an l-zombie have the same utility <img title=\"u_0\" src=\"http://www.codecogs.com/png.latex?u_0\" alt=\"\" align=\"bottom\" />, then you can just subtract <img title=\"u_0\" src=\"http://www.codecogs.com/png.latex?u_0\" alt=\"\" align=\"bottom\" /> from all utilities (without changing the meaning of the utility function), and get a function where that utility is zero. But that means that the utility functions I've been plugging into PBDT above <em>do</em> change their meaning if you add a constant to them. You can set up a problem where the agent has to decide whether to bring itself into existence or not (Omega creates it iff it predicts that the agent will press a particular button), and in that case the agent will decide to do so iff the world has utility greater than zero&mdash;clearly not invariant under adding and subtracting a constant. I can't find any concept like the utility of not existing in my intuitions <em>about Bayesian updating</em> (though I can find such a concept in my intuitions about utility, but regarding that see the previous paragraph), so if PBDT is just a mathematical trick to implement these intuitions, where does that utility come from?</p>\n<p>I'm not aware of a way of implementing updating in general SUDT-style problems that does better than NBDT, PBDT, and the ad-hoc idea mentioned above, so for now I've concluded that in general, trying to update is just hopeless, and we should be using (S)UDT instead. In <em>classical</em> decision problems, where there are no acausal influences, (S)UDT will of course behave exactly as if it <em>did</em> do a Bayesian update; thus, in a sense, using (S)UDT can also be seen as a reinterpretation of Bayesian updating (in this case just as updateless utility maximization in a world where all influence is causal), and that's the way I think about it nowadays.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1b6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TCjiFNfJ7z8W4cEwr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "25618", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["88vuFDw3dCX7hC6uW", "7nAxgQYGYrEY5ZCAD", "jZvTWB6Z75cv2q3Nw", "NaZPjaLPCGZWdTyrL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-28T06:28:57.888Z", "modifiedAt": "2021-03-22T07:45:22.815Z", "url": null, "title": "Lifestyle interventions to increase longevity", "slug": "lifestyle-interventions-to-increase-longevity", "viewCount": null, "lastCommentedAt": "2022-04-29T13:19:17.333Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PhXENjdXiHhsWGfQo/lifestyle-interventions-to-increase-longevity", "pageUrlRelative": "/posts/PhXENjdXiHhsWGfQo/lifestyle-interventions-to-increase-longevity", "linkUrl": "https://www.lesswrong.com/posts/PhXENjdXiHhsWGfQo/lifestyle-interventions-to-increase-longevity", "postedAtFormatted": "Friday, February 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lifestyle%20interventions%20to%20increase%20longevity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALifestyle%20interventions%20to%20increase%20longevity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhXENjdXiHhsWGfQo%2Flifestyle-interventions-to-increase-longevity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lifestyle%20interventions%20to%20increase%20longevity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhXENjdXiHhsWGfQo%2Flifestyle-interventions-to-increase-longevity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhXENjdXiHhsWGfQo%2Flifestyle-interventions-to-increase-longevity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3681, "htmlBody": "<p>There is a lot of bad science and controversy in the realm of how to have a healthy lifestyle. Every week we are bombarded with new studies conflicting older studies telling us X is good or Y is bad. Eventually we reach our psychological limit, throw up our hands, and give up. I used to do this a lot. I knew exercise was good, I knew flossing was good, and I wanted to eat better. But I never acted on any of that knowledge. I would feel guilty when I thought about this stuff and go back to what I was doing. Unsurprisingly, this didn't really cause me to make any positive lifestyle changes.</p>\n<p>Instead of vaguely guilt-tripping you with <a href=\"/lw/ow/the_beauty_of_settled_science/\">potentially unreliable science news</a>, this post aims to provide an overview of lifestyle interventions that have very strong evidence behind them and concrete ways to implement them.</p>\n<p><a id=\"more\"></a></p>\n<h3>A quick FAQ before we get started</h3>\n<h4>Why should I care about longevity-promoting habits at a young age?</h4>\n<p>First, many longevity-promoting lifestyle changes will increase your quality of life in the short term. In doing this research, I found a few interventions that had shockingly large impacts on my subjective day-to-day wellness. Second, the choices you make have larger downstream effects the earlier you get started. Trying to undo years of damage and ingrained habits at an advanced age really isn&rsquo;t a position you want to find yourself in. Third, extending your life matters more the more you believe in the proximity of transformative tech. If the pace of technological improvement is increasing, then adding a decade to your life may in fact be the decade that counts. Missing out on life extension tech by a few years would really suck.</p>\n<h4>Isn&rsquo;t longevity mostly just genetics?</h4>\n<p>That's what I believed for a long time, but a quick trip to wikipedia tells us that only <a href=\"https://en.wikipedia.org/wiki/Longevity#Longevity_and_lifestyle\">20-30% of the variance in longevity is heritable</a>.</p>\n<h4>What sort of benefits can I expect?</h4>\n<p>The life satisfaction of people who remain independent and active actually increases significantly with age. Mental and physical performance are strongly correlated, meaning maintaining your body will help maintain your mind. The qualitative benefits for life satisfaction of many of these interventions can be so dramatic that it is hard to estimate them. The gulf in quality of life between people maintaining good habits and those who do not widens with age.</p>\n<h4>How were these recommendations generated?/Why should I believe you?</h4>\n<p>This post summarizes studies at the intersection of having large effects, large sample sizes, and being well-designed in terms of methodology. The cutoff for an intervention being &ldquo;worth it&rdquo; is somewhat subjective given that there is often only a rough estimate of the overall effect sizes of various interventions in comparison to one another. <a href=\"http://wonder.cdc.gov/\">CDC mortality statistics</a> were used to determine the most likely causes of death in various age brackets. The list of things that kill people balloons significantly as you get towards the less common causes of death and I have limited research time. Individuals who face unusual health circumstances should of course be doing their own research and consulting health professionals.</p>\n<p>This brings me to my disclaimer:</p>\n<p>This post is not intended to diagnose, treat, cure, or prevent any disease. No claim or opinion on these pages is intended to be, nor should be construed as medical advice. Please consult with a healthcare professional before starting any diet or exercise program. None of these claims have been evaluated by the Food and Drug Administration. Suggestions herein are intended for normal healthy adults and should not be used if you are under the age of 18 or have any known medical condition.</p>\n<p>Alright, let&rsquo;s dive in.</p>\n<p>&nbsp;</p>\n<h3>Things that will eventually kill you</h3>\n<h4>CVD</h4>\n<p>At the top of our list is cardiovascular disease, or CVD, causing the plurality of all deaths by far. We will break down the controllable components of CVD in terms of lifestyle interventions.</p>\n<h4>Smoking</h4>\n<p>This doesn&rsquo;t need much of an explanation. Responsible for the majority of lung cancers, respiratory diseases, and a huge contributing factor to CVD. Buying an e-cig for yourself or people you know who smoke are possibly the single cheapest intervention for adding years to life. E-cigs have very high success rates in getting people to quit smoking and are absurdly cheap. You can spend under $10 and add 14 years to someone&rsquo;s life. I buy them just to give away. Recommended products: <a href=\"http://www.chinabuye.com/ce4-65k-650mah-electronic-cigarette-with-ce4-single-cotton-wick-clearomizer-black\">1</a>,&nbsp;<a href=\"http://www.thevaporroom.net/3mltasters.html\">2</a>.</p>\n<h4>Alcohol</h4>\n<p>Some controversy over possible benefits of small amounts, but large amounts definitely bad. Avoiding alcoholism is a whole subject I won&rsquo;t tackle here.</p>\n<h4>Blood Pressure</h4>\n<p>Second to tobacco in effect size. Blood pressure is one of the things most people ignore. <a href=\"http://www.amazon.com/Ozeri-BP01K-CardioTech-Hypertension-Indicator/dp/B004FO2DSE/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">It is extremely cheap and easy to start monitoring your blood pressure</a>, and there are things you can do if you find it to be high. You want your blood pressure to be about 120/70. If you are higher than this there are some simple things you can do. The first is to exercise and eat fish every week, especially salmon. There are also a few supplements that have been found to be helpful.</p>\n<p>A quick note about my criteria for inclusion for supplements: I am extremely dubious as to the benefit of most supplements. Study after study shows that most of them are a waste of time and money. The fish example given above is a good illustration. You might ask why you can&rsquo;t just take fish oil pills. Well it turns out that <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/12848287\">fish oil pills suck</a>, and you&rsquo;d need to take approximately <em>9 times as much</em> to have the same effect as eating fish, at which point they&rsquo;d have dangerous blood thinning effects. So when I recommend a supplement it has to meet a pretty stringent list of requirements.</p>\n<p>1. Large effect seen in multiple randomized controlled trials.</p>\n<p>2. Therapeutic dose is a tiny fraction of the toxic dose, or no toxic dose able to be identified because it is so high.</p>\n<p>3. Side effects comparable to placebo.</p>\n<p>4. Dose size is commensurate with an amount it would be reasonable to ingest in natural form.</p>\n<p>So basically I weight any downside risk very heavily given the spotty track record of the general reference class of supplements.</p>\n<p>So what passes these criteria for blood pressure?</p>\n<p>1. CoQ10, large effect size in multiple studies</p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/14695924\">http://www.ncbi.nlm.nih.gov/pubmed/14695924</a></p>\n<p>2. Flavonoids/anthocyanins, these compounds are present in things like dark chocolate, fruits, and teas.</p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/14695924\">http://ajcn.nutrition.org/content/88/1/38.short</a></p>\n<p>3. Garlic</p>\n<p><a href=\"http://www.biomedcentral.com/1471-2261/8/13/\">http://www.biomedcentral.com/1471-2261/8/13/</a></p>\n<p>I have personally had success lowering my blood pressure from the 140&rsquo;s to the 120&rsquo;s with these supplements keeping my exercise levels constant.</p>\n<h4>Blood lipids (cholesterol)</h4>\n<p>Here the conventional recommendations appear to be wrong, or at least somewhat misguided. First, some theory. Blood lipids are composed of a variety of substances, but for our purposes we will stick to the ones tested for in blood panels and how to interpret these numbers. A typical blood panel will report LDL, HDL, and Triglycerides. The simple story of &ldquo;high LDL bad&rdquo; does not accurately reflect risk of CVD. The most powerful predictor of CVD in terms of blood lipids is the Triglycerides to HDL ratio.<sup><a href=\"http://circ.ahajournals.org/content/85/1/37.short\">[1]</a></sup><sup><a href=\"https://circ.ahajournals.org/content/96/8/2520.full\">[2]</a></sup><sup><a href=\"http://www.sciencedirect.com/science/article/pii/000291499291083G\">[3]</a></sup><sup><a href=\"http://care.diabetesjournals.org/content/23/11/1679.short\">[4]</a></sup><sup><a href=\"http://www.scielo.br/scielo.php?pid=S1807-59322008000400003&amp;script=sci_arttext\">[5]</a></sup> The higher the triglycerides and the lower the HDL, the greater the risk. This relationship holds independent of LDL levels, which are usually the focus of cholesterol discussions with health practitioners. As it turns out, there are actually two types of LDL, and distinguishing between them is something not usually performed on a blood test. The reason for the prolonged confusion arises from the correlation between a poor HDL:Triglyceride ratio and prevalence of the unhealthy type of LDL. As a result, potent cholesterol lowering drugs are over prescribed. For people with a healthy ratio of triglycerides:HDL, a total cholesterol between 200-220 (traditionally considered &ldquo;high&rdquo;) is actually &nbsp;correlated with lower mortality,<sup><a href=\"http://circ.ahajournals.org/content/92/9/2396.long\">[6]</a></sup> and aggressive lowering with drugs resulted in worse health outcomes. This is not to say that statins (cholesterol lowering drugs) are not useful. On the contrary they seem to be highly helpful for patients recovering from a cardiovascular event, but they have shown no benefit for people with no history of problems.<sup><a href=\"http://www.bmj.com/content/347/bmj.f6123\">[7]</a></sup> Statins have serious side effects<sup><a href=\"http://www.fda.gov/forconsumers/consumerupdates/ucm293330.htm\">[8]</a></sup> and should not be taken lightly. Be skeptical.<sup><a href=\"http://jama.jamanetwork.com/article.aspx?articleid=381733\">[9]</a></sup></p>\n<p>So how does one go about lowering their triglycerides and raising their HDL? Again, exercise and eating fish are awesome here. Excessive fructose intake raises triglycerides, and this relationship is worsened by high BMI. Fiber and resistant starch from fruits, vegetables, and tubers has a positive effect. Intermittent fasting has also shown promising effects here.</p>\n<h4>BMI/Obesity</h4>\n<p>There are some controversies here I don&rsquo;t really want to get into the details of as it is a complex subject. I do want to mention that health interventions should not have an excessive focus on whether one is losing weight. Many of the interventions discussed here have significant effects (for example on insulin sensitivity, c-reactive protein, and fasting blood glucose) even when body composition does not change. Getting BMI below ~27 should be a priority however, as it has wide ranging effects across all other interventions.</p>\n<h4>Nutrition</h4>\n<p>This is a big subject, and we&rsquo;re not even going to attempt to go into detail. This section will focus on the largest high level features of a diet that have positive or negative impact. Processed meat consumption has the single largest negative effect on health. It is shockingly bad, even if you already suspected as such.<sup><a href=\"http://circ.ahajournals.org/content/121/21/2271.short\">[1]</a></sup> In contrast, a bit of red meat has actually been found to be neutral. It seems to be that many earlier studies claiming harm from red meat did not adequately separate out the huge effect size of processed meat. Fish and nut consumption appear to be a grand slam for CVD in particular and also just for overall health.<sup><a href=\"http://circ.ahajournals.org/content/109/22/2705.short\">[2]</a></sup><sup><a href=\"http://ajcn.nutrition.org/content/70/3/500s.short\">[3]</a></sup>&nbsp;Pescetarians live significantly longer than vegans,<sup><a href=\"http://ajcn.nutrition.org/content/70/3/516s.full\">[4]</a></sup> lending support to fish consumption. Outside of specific foods, common micronutrient deficiencies have been indicated in everything from cancer, to immune system suppression, to poorer physical and mental performance, to sleep problems, greater inflammation, and even depression. Really there&rsquo;s too much material there to cover, there are just <a href=\"http://scholar.google.com/scholar?q=micronutrient+deficiencies\">pages and pages of studies</a>.</p>\n<p>There&rsquo;s also the bad news that multivitamins <a href=\"http://annals.org/article.aspx?articleid=1789253\">mostly don&rsquo;t do anything</a>. There has not been found an alternative to eating a variety of nutrient-dense whole foods. Though vitamin D supplementation appears to be quite beneficial. Another LW user, <a href=\"/user/John_Maxwell_IV/\">John_Maxwell_IV</a>, and I are trying to make this easy with our startup&nbsp;<a href=\"http://www.mealsquares.com\">MealSquares</a>.</p>\n<h4>Blood donation</h4>\n<p>The studies related to this have some methodological issues but overall the effect size is so large, and the cost and risks so low, that it is worth inclusion. Several studies have indicated that, for men, regular blood donation results in a massive reduction in heart attack.<sup><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/9737556\">[1]</a></sup><sup><a href=\"http://www.bmj.com/content/314/7083/793\">[2]</a></sup><sup><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC484902/\">[3]</a></sup> Other studies have found no such relation.<sup><a href=\"http://circ.ahajournals.org/content/103/1/52.short\">[4]</a></sup> There are also additional health benefits to blood donation.<sup><a href=\"http://www.biomedcentral.com/1741-7015/10/53\">[5]</a></sup> These are just some of the studies on this subject, but on balance after reviewing the evidence, I can say that donating blood once a year is almost certainly worth it if you're a man. &nbsp;Donating too often is probably bad for you though.</p>\n<h4>Exercise</h4>\n<p>This topic is large enough that I am separating out my actual recommendations into another post and purely discussing the health benefits here. Exercise is probably the single most important lifestyle intervention. Even minimal amounts of exercise have very large impacts on longevity and health. We&rsquo;re talking even walking&nbsp;<a href=\"http://longevity.about.com/od/healthyheartaging/a/Minimum-Exercise-For-A-Longer-Life.htm\">15 minutes a day causing people to live longer</a>. Even ignoring quality of life you are looking at a 3-7 fold return on every minute you spend exercising in extended life,<sup><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22039197\">[1]</a></sup> perhaps even exceeding that if you are making optimal use of your time. Exercise has a positive impact on pretty much everything that contributes to mortality. I don&rsquo;t really know how to convince you, the reader, that the future actually exists and that future you will be incredibly angry or sad that you didn&rsquo;t put in a small effort now for a better life later. But everyone has already told you this your whole life. So I&rsquo;m going to contrast it with the inverse. Most of the activities that we associate with fun and leisure involve some aspect of physicality, even if it&rsquo;s just walking around with friends. Losing access to these activities as can and <em>does</em> happen to people represents a massive decrease in quality of life. If you are reading this and you are young, you are able to simultaneously ignore your body&rsquo;s need for exercise, and demand performance of it when necessary to enjoy yourself. This will not remain true forever. Exercise has a protective effect against exactly the sorts of degenerative injuries that deprive people of their freedom of movement and activity.<sup><a href=\"http://jah.sagepub.com/content/9/1/105.short\">[2]</a></sup> I don&rsquo;t care if you start with an exercise habit of one pushup a <em>week</em>, but you must do something.</p>\n<p>Let&rsquo;s move on to some relevant considerations assuming you want to exercise. What sort of exercise should I be doing? Several studies have indicated that endurance athletes enjoy the greatest improvements in longevity. I would agree with this but caution that often the groups in such studies with the best health outcomes are those that do engage in resistance training as well. Soccer and other team sport players, for example, often perform resistance training as part of their overall conditioning. This seems to be overlooked because they do not perform it at the same level of intensity as athletes in the power sports. Long distance skiers and bikers also generally train lower body strength moves at an impressive level compared to the general public, even if it is a level significantly below that of power athletes (e.g. <a href=\"http://www.xczone.com/skifitnesstrg.pdf\">here</a> is an example of a training regime for a competitive skier). My point is simply that you shouldn&rsquo;t read a study that says &ldquo;endurance athletes live longer&rdquo; and assume that all you need to do is run. Strength training also has significant effects on insulin resistance, resting metabolic rate, glucose metabolism, blood pressure, hormone balance, joint health, organ reserve, depression, increases in HDL, reduction in back injuries, sleep quality, and a variety of harder-to-quantify quality of life improvements.<sup><a href=\"http://www.sciencedirect.com/science/article/pii/S0091743501909090\">[4]</a></sup><sup><a href=\"http://www.ais.up.ac.za/med/sportcert/prescription1a.pdf\">[5]</a></sup><sup><a href=\"http://link.springer.com/article/10.1007/s004310051070\">[6]</a></sup><sup><a href=\"http://circ.ahajournals.org/content/101/7/828.short\">[7]</a></sup><sup><a href=\"http://scholarcommons.sc.edu/sph_physical_activity_public_health_facpub/119/\">[8]</a></sup> I go to the trouble to cite resistance training so heavily because I feel that the benefits of cardio are generally well-understood, but I regularly encounter the idea that resistance training is only for people who want to look like a gross bodybuilder.</p>\n<p>Hopefully I have established that one should do both endurance and resistance training. Program specifics will be included in the other post as well as info on when benefits taper off.</p>\n<p>Edit: Exercise post is up <a href=\"/r/discussion/lw/juc/optimal_exercise/\">here</a>.</p>\n<h4>Stress</h4>\n<p>Stress affects almost every system in your body. It increases disease risk by acting as an immunosuppressant. It directly impacts blood pressure, sleep problems, skin conditions, anxiety, depression, and even heart problems. Chronic untreated stress is often considered a causal factor in many other ailments people are medically treated for. Stress often goes untreated because alleviating it is seen as low priority. Whatever we are doing right now is worth a little stress. This can be true, but over a longer time horizon failing to learn better ways of managing stress really harms us. To confront stressors you must confront <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">ugh fields</a>. Non-productive coping mechanisms are the norm here: procrastination, abuse of substances including food, sleeping too much, blame as a curiosity-stopper etc. Simple strategies for dealing with low level stressors include things such as meditation, gratitude journaling, reflecting on and updating goals, or even just paying other people to deal with a recurring source of stress. Two previous LW posts have excellent advice in this area:&nbsp;<a href=\"/lw/4su/how_to_be_happy/\">How to Be Happy</a> and <a href=\"/lw/bq0/be_happier/\">Be Happier</a>.</p>\n<p>If you are depressed and don't know where to start on getting help please take a look at <a href=\"http://slatestarcodex.com/2014/06/16/things-that-sometimes-help-if-youre-depressed/\">Things that sometimes help <span style=\"text-decoration: underline;\">if you're depresed</span>.</a></p>\n<p>Supplements that impact stress include</p>\n<p>1. Rhodiola Rosea:&nbsp;<a href=\"http://www.sciencedirect.com/science/article/pii/S0944711310002680\">http://www.sciencedirect.com/science/article/pii/S0944711310002680</a></p>\n<p>2. Ashwaghanda root, which shows promise for chronic anxiety:&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3573577/\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3573577/</a></p>\n<h4>Sleep</h4>\n<p>Chronic insomnia is a massive source of stress for many people and poses a huge mortality risk. In one study, people who got chronically less sleep had 3 times the mortality risk as people who slept well!<sup><a href=\"http://www.sciencedaily.com/releases/2010/06/100607065559.htm\">[1]</a></sup> You cannot afford to not start optimizing your sleep. It is important that your sleeping place be a quiet, dark, cool environment. You can use simple methods to improve each of these parameters. Forehead cooling has shown great promise in clinical trials.<sup><a href=\"http://www.scientificamerican.com/article/putting-insomnia-on-ice/\">[2]</a></sup> You can accomplish this with a gel pack that is cool (not ice). Even small LED lights in your room impact sleep quality because the melatonin production system is very fragile and sensitive to light.<sup><a href=\"http://www.health.harvard.edu/newsletters/Harvard_Health_Letter/2012/May/blue-light-has-a-dark-side/\">[3]</a></sup> Get tape and cover lights. Try <a href=\"http://www.amazon.com/Uvex-S1933X-Eyewear-SCT-Orange-Anti-Fog/dp/B000USRG90/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">orange glasses</a> to prevent blue light from destroying your endogenous melatonin production after 10pm. Regularize your sleeping and eating schedules. Expose yourself to bright lights in the morning to calibrate your circadian rhythms. Afternoon/early evening exercise is beneficial in making you sleepy. Melatonin pills work for many, but make sure you start with 75mcg (cut <a href=\"http://www.amazon.com/Sundown-Naturals-Melatonin-Tablets-tablets/dp/B000GG2I9O/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">these</a> into fourths), rather than the 3mg most pills come in. A teaspoon of raw honey before bed helps prevent some people from waking multiple times throughout the night.</p>\n<p>Consider reading this <a href=\"/lw/jrt/lifestyle_interventions_that_affect_longevity/amp8\">excellent info from Yvain on sleep apnea</a>, especially if you snore excessively or feel very tired even after a full night's sleep.</p>\n<h4>Cancer</h4>\n<p>Almost all of the risk factors for cancer have some overlap with CVD, meaning most of the advice above works for cancer too, but there are a few additional considerations worth discussing.</p>\n<p><strong>Cancer and UV exposure</strong></p>\n<p>One of the surprising results of my research was that conventional wisdom appears to be wrong here. There is not a simple relationship between UV exposure and increased cancers. Specifically, while increased sunlight exposure is correlated with higher incidence of skin cancer, it appears that it is also correlated with a decreased risk of 5 other cancers that are far less survivable.<sup><a href=\"http://onlinelibrary.wiley.com/doi/10.1002/cncr.10427/full\">[1]</a></sup> This is a straightforward trade off, getting sun exposure wins by quite a lot. Shade your face to avoid photodamage to your skin and macular degeneration of your eyes.</p>\n<p><strong>Breast cancer and testicular cancer</strong></p>\n<p>Redacted, see Vaniver's comment <a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/amv5\">here</a>.</p>\n<h4>Floss</h4>\n<p>No, seriously. Not flossing is way more lethal than you think.<sup><a href=\"http://www.hindawi.com/journals/jar/2011/156061/abs/\">[1]</a></sup> You should also see a dentist regularly, even if you have to pay for your own insurance. (It's surprisingly cheap, e.g. Delta Dental offers plans for under $100/yr; lots of people don't make use of their plan and subsidize the treatment of those who do use theirs). Losing teeth greatly increases your chances of infections over time.</p>\n<h4><br /></h4>\n<h3>Things that will kill you right now</h3>\n<h4>Avoidable medical errors</h4>\n<p>Avoidable medical errors might be the second leading cause of death after CVD.<sup><a href=\"http://journals.lww.com/journalpatientsafety/Fulltext/2013/09000/A_New,_Evidence_based_Estimate_of_Patient_Harms.2.aspx\">[1]</a></sup> This makes a hospital visit possibly the most dangerous thing you can do, especially if you are young. In general, you should not assume that medical staff are competent. Triple check dangerous prescriptions. If you don&rsquo;t know whether a prescription is dangerous, assume it is. Ask medical staff if they&rsquo;ve washed their hands (yes, this is actually still a major problem). Sharpie on yourself which side of your body a surgery is supposed to happen on, along with your name and what the surgery is for (seriously). Keep your own records, especially if you have serious medical issues; error rates in medical documentation are ridiculous. Medical equipment is generally cleaned by custodial staff with no medical training who often don't know how a particular device works. Have someone you can call in an emergency who knows about all of this.</p>\n<p>While we're discussing medicine, I'll throw in a couple low cost recommendations that give me peace of mind, even if an emergency situation is unlikely. The first is that the Red Cross has created an <a href=\"http://www.redcross.org/mobile-apps/first-aid-app\">android/iphone app</a> covering first aid with extensive pictures and videos helping you through the situation. The second is <a href=\"http://www.amazon.com/Quikclot-Advanced-Clotting-Bleeding-Package/dp/B001BCNTHC/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">quickclot</a>&nbsp;which can stop severe bleeding much faster than traditional techniques.</p>\n<h4>Unintentional poisoning</h4>\n<p>This is mostly acetaminophen poisoning resulting from their mandatory inclusion in pain killers to prevent abuse. Also people misdosing themselves with legal and illegal drugs. Be careful, this outweighs traffic accidents in accidental deaths. Adding the <a href=\"http://www.aapcc.org/\">24 hour emergency poison control line number</a>&nbsp;(1-800-222-1222) to your phone is something you can do right now. It is also worth knowing that SOP for acetaminophen poisoning is <a href=\"http://emedicine.medscape.com/article/820200-treatment\">high dose NAC</a>, which is freely available on amazon in the US (h/t Tara).</p>\n<h4>Traffic accidents</h4>\n<p>Michael Curzi has a great post on this I won&rsquo;t attempt to reproduce here:&nbsp;<a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/\">How to avoid dying in a car crash</a>. It is definitely worth updating your model of what behaviors are dangerous in a car.</p>\n<p>&nbsp;</p>\n<h3>Summary of interventions</h3>\n<ol>\n<li>\n<p>If you know people who smoke, getting them to vape is the single largest impact you can have on their lifespan.</p>\n</li>\n<li>\n<p>Pay attention when in your car.</p>\n</li>\n<li>\n<p>CONSTANT VIGILANCE when dealing with the medical profession and drugs.</p>\n</li>\n<li>\n<p>Exercise: very high return on first few units of effort, some cardio and some resistance training is best.</p>\n</li>\n<li>\n<p>Blood donation every 12-24 months for men.</p>\n</li>\n<li>\n<p>Buy a blood pressure monitor and do blood pressure reduction interventions if needed.</p>\n</li>\n<li>\n<p>Eat fish, nuts, eggs, fruit, dark chocolate. Supplement <a href=\"http://www.amazon.com/Now-foods-Vitamin-45mcg-120VC/dp/B0019QT9LA/ref=sr_1_1?ie=UTF8&amp;qid=1426137023&amp;sr=8-1&amp;keywords=d3%2Bk2\">Vitamin D3</a>.</p>\n</li>\n<li>\n<p>Work towards a healthy weight.</p>\n</li>\n<li>\n<p>If you are losing sleep/are stressed, try one small intervention at a time, and <strong>don&rsquo;t get discouraged</strong>. &nbsp;These interventions are the hardest but potentially the most rewarding. Supplements for stress, anxiety, and sleep are somewhat subjective and vary more in reported efficacy than others; self-experimentation is recommended.</p>\n</li>\n<li>\n<p>Floss (and see a dentist).</p>\n</li>\n</ol>\n<p>&nbsp;</p>\n<h3>Closing</h3>\n<p>Don&rsquo;t worry too much. Don&rsquo;t get down on yourself about health. &nbsp;This creates an <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">ugh field</a> making you less likely to take action. &nbsp;The process of becoming healthier is going to make you feel stupid sometimes. This is a marathon and not a sprint; standard <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">habit</a> <a href=\"/lw/hub/common_failure_modes_in_habit_formation/\">forming</a> rules apply. Trying to fix 10 things at once is highly stressful! Do not do this! Discuss things that worked for you and didn&rsquo;t work for you in the past with yourself and with others and come up with a plan. Don&rsquo;t publicly commit to your plan in the comments, this <a href=\"https://sivers.org/zipit\">makes you less likely to do it</a>. Oh, and feel free to argue with me or request more sources.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "8nAXyYLu8eT72Hwuh": 3, "HLoxy2feb2PYqooom": 2, "rfZ6DY88ApBDXFpyW": 4, "vmvTYnmaKA73fYDe5": 13, "92SxJsDZ78ApAGq72": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PhXENjdXiHhsWGfQo", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 177, "baseScore": 230, "extendedScore": null, "score": 0.000595, "legacy": true, "legacyId": "25625", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 230, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>There is a lot of bad science and controversy in the realm of how to have a healthy lifestyle. Every week we are bombarded with new studies conflicting older studies telling us X is good or Y is bad. Eventually we reach our psychological limit, throw up our hands, and give up. I used to do this a lot. I knew exercise was good, I knew flossing was good, and I wanted to eat better. But I never acted on any of that knowledge. I would feel guilty when I thought about this stuff and go back to what I was doing. Unsurprisingly, this didn't really cause me to make any positive lifestyle changes.</p>\n<p>Instead of vaguely guilt-tripping you with <a href=\"/lw/ow/the_beauty_of_settled_science/\">potentially unreliable science news</a>, this post aims to provide an overview of lifestyle interventions that have very strong evidence behind them and concrete ways to implement them.</p>\n<p><a id=\"more\"></a></p>\n<h3 id=\"A_quick_FAQ_before_we_get_started\">A quick FAQ before we get started</h3>\n<h4 id=\"Why_should_I_care_about_longevity_promoting_habits_at_a_young_age_\">Why should I care about longevity-promoting habits at a young age?</h4>\n<p>First, many longevity-promoting lifestyle changes will increase your quality of life in the short term. In doing this research, I found a few interventions that had shockingly large impacts on my subjective day-to-day wellness. Second, the choices you make have larger downstream effects the earlier you get started. Trying to undo years of damage and ingrained habits at an advanced age really isn\u2019t a position you want to find yourself in. Third, extending your life matters more the more you believe in the proximity of transformative tech. If the pace of technological improvement is increasing, then adding a decade to your life may in fact be the decade that counts. Missing out on life extension tech by a few years would really suck.</p>\n<h4 id=\"Isn_t_longevity_mostly_just_genetics_\">Isn\u2019t longevity mostly just genetics?</h4>\n<p>That's what I believed for a long time, but a quick trip to wikipedia tells us that only <a href=\"https://en.wikipedia.org/wiki/Longevity#Longevity_and_lifestyle\">20-30% of the variance in longevity is heritable</a>.</p>\n<h4 id=\"What_sort_of_benefits_can_I_expect_\">What sort of benefits can I expect?</h4>\n<p>The life satisfaction of people who remain independent and active actually increases significantly with age. Mental and physical performance are strongly correlated, meaning maintaining your body will help maintain your mind. The qualitative benefits for life satisfaction of many of these interventions can be so dramatic that it is hard to estimate them. The gulf in quality of life between people maintaining good habits and those who do not widens with age.</p>\n<h4 id=\"How_were_these_recommendations_generated__Why_should_I_believe_you_\">How were these recommendations generated?/Why should I believe you?</h4>\n<p>This post summarizes studies at the intersection of having large effects, large sample sizes, and being well-designed in terms of methodology. The cutoff for an intervention being \u201cworth it\u201d is somewhat subjective given that there is often only a rough estimate of the overall effect sizes of various interventions in comparison to one another. <a href=\"http://wonder.cdc.gov/\">CDC mortality statistics</a> were used to determine the most likely causes of death in various age brackets. The list of things that kill people balloons significantly as you get towards the less common causes of death and I have limited research time. Individuals who face unusual health circumstances should of course be doing their own research and consulting health professionals.</p>\n<p>This brings me to my disclaimer:</p>\n<p>This post is not intended to diagnose, treat, cure, or prevent any disease. No claim or opinion on these pages is intended to be, nor should be construed as medical advice. Please consult with a healthcare professional before starting any diet or exercise program. None of these claims have been evaluated by the Food and Drug Administration. Suggestions herein are intended for normal healthy adults and should not be used if you are under the age of 18 or have any known medical condition.</p>\n<p>Alright, let\u2019s dive in.</p>\n<p>&nbsp;</p>\n<h3 id=\"Things_that_will_eventually_kill_you\">Things that will eventually kill you</h3>\n<h4 id=\"CVD\">CVD</h4>\n<p>At the top of our list is cardiovascular disease, or CVD, causing the plurality of all deaths by far. We will break down the controllable components of CVD in terms of lifestyle interventions.</p>\n<h4 id=\"Smoking\">Smoking</h4>\n<p>This doesn\u2019t need much of an explanation. Responsible for the majority of lung cancers, respiratory diseases, and a huge contributing factor to CVD. Buying an e-cig for yourself or people you know who smoke are possibly the single cheapest intervention for adding years to life. E-cigs have very high success rates in getting people to quit smoking and are absurdly cheap. You can spend under $10 and add 14 years to someone\u2019s life. I buy them just to give away. Recommended products: <a href=\"http://www.chinabuye.com/ce4-65k-650mah-electronic-cigarette-with-ce4-single-cotton-wick-clearomizer-black\">1</a>,&nbsp;<a href=\"http://www.thevaporroom.net/3mltasters.html\">2</a>.</p>\n<h4 id=\"Alcohol\">Alcohol</h4>\n<p>Some controversy over possible benefits of small amounts, but large amounts definitely bad. Avoiding alcoholism is a whole subject I won\u2019t tackle here.</p>\n<h4 id=\"Blood_Pressure\">Blood Pressure</h4>\n<p>Second to tobacco in effect size. Blood pressure is one of the things most people ignore. <a href=\"http://www.amazon.com/Ozeri-BP01K-CardioTech-Hypertension-Indicator/dp/B004FO2DSE/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">It is extremely cheap and easy to start monitoring your blood pressure</a>, and there are things you can do if you find it to be high. You want your blood pressure to be about 120/70. If you are higher than this there are some simple things you can do. The first is to exercise and eat fish every week, especially salmon. There are also a few supplements that have been found to be helpful.</p>\n<p>A quick note about my criteria for inclusion for supplements: I am extremely dubious as to the benefit of most supplements. Study after study shows that most of them are a waste of time and money. The fish example given above is a good illustration. You might ask why you can\u2019t just take fish oil pills. Well it turns out that <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/12848287\">fish oil pills suck</a>, and you\u2019d need to take approximately <em>9 times as much</em> to have the same effect as eating fish, at which point they\u2019d have dangerous blood thinning effects. So when I recommend a supplement it has to meet a pretty stringent list of requirements.</p>\n<p>1. Large effect seen in multiple randomized controlled trials.</p>\n<p>2. Therapeutic dose is a tiny fraction of the toxic dose, or no toxic dose able to be identified because it is so high.</p>\n<p>3. Side effects comparable to placebo.</p>\n<p>4. Dose size is commensurate with an amount it would be reasonable to ingest in natural form.</p>\n<p>So basically I weight any downside risk very heavily given the spotty track record of the general reference class of supplements.</p>\n<p>So what passes these criteria for blood pressure?</p>\n<p>1. CoQ10, large effect size in multiple studies</p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/14695924\">http://www.ncbi.nlm.nih.gov/pubmed/14695924</a></p>\n<p>2. Flavonoids/anthocyanins, these compounds are present in things like dark chocolate, fruits, and teas.</p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/14695924\">http://ajcn.nutrition.org/content/88/1/38.short</a></p>\n<p>3. Garlic</p>\n<p><a href=\"http://www.biomedcentral.com/1471-2261/8/13/\">http://www.biomedcentral.com/1471-2261/8/13/</a></p>\n<p>I have personally had success lowering my blood pressure from the 140\u2019s to the 120\u2019s with these supplements keeping my exercise levels constant.</p>\n<h4 id=\"Blood_lipids__cholesterol_\">Blood lipids (cholesterol)</h4>\n<p>Here the conventional recommendations appear to be wrong, or at least somewhat misguided. First, some theory. Blood lipids are composed of a variety of substances, but for our purposes we will stick to the ones tested for in blood panels and how to interpret these numbers. A typical blood panel will report LDL, HDL, and Triglycerides. The simple story of \u201chigh LDL bad\u201d does not accurately reflect risk of CVD. The most powerful predictor of CVD in terms of blood lipids is the Triglycerides to HDL ratio.<sup><a href=\"http://circ.ahajournals.org/content/85/1/37.short\">[1]</a></sup><sup><a href=\"https://circ.ahajournals.org/content/96/8/2520.full\">[2]</a></sup><sup><a href=\"http://www.sciencedirect.com/science/article/pii/000291499291083G\">[3]</a></sup><sup><a href=\"http://care.diabetesjournals.org/content/23/11/1679.short\">[4]</a></sup><sup><a href=\"http://www.scielo.br/scielo.php?pid=S1807-59322008000400003&amp;script=sci_arttext\">[5]</a></sup> The higher the triglycerides and the lower the HDL, the greater the risk. This relationship holds independent of LDL levels, which are usually the focus of cholesterol discussions with health practitioners. As it turns out, there are actually two types of LDL, and distinguishing between them is something not usually performed on a blood test. The reason for the prolonged confusion arises from the correlation between a poor HDL:Triglyceride ratio and prevalence of the unhealthy type of LDL. As a result, potent cholesterol lowering drugs are over prescribed. For people with a healthy ratio of triglycerides:HDL, a total cholesterol between 200-220 (traditionally considered \u201chigh\u201d) is actually &nbsp;correlated with lower mortality,<sup><a href=\"http://circ.ahajournals.org/content/92/9/2396.long\">[6]</a></sup> and aggressive lowering with drugs resulted in worse health outcomes. This is not to say that statins (cholesterol lowering drugs) are not useful. On the contrary they seem to be highly helpful for patients recovering from a cardiovascular event, but they have shown no benefit for people with no history of problems.<sup><a href=\"http://www.bmj.com/content/347/bmj.f6123\">[7]</a></sup> Statins have serious side effects<sup><a href=\"http://www.fda.gov/forconsumers/consumerupdates/ucm293330.htm\">[8]</a></sup> and should not be taken lightly. Be skeptical.<sup><a href=\"http://jama.jamanetwork.com/article.aspx?articleid=381733\">[9]</a></sup></p>\n<p>So how does one go about lowering their triglycerides and raising their HDL? Again, exercise and eating fish are awesome here. Excessive fructose intake raises triglycerides, and this relationship is worsened by high BMI. Fiber and resistant starch from fruits, vegetables, and tubers has a positive effect. Intermittent fasting has also shown promising effects here.</p>\n<h4 id=\"BMI_Obesity\">BMI/Obesity</h4>\n<p>There are some controversies here I don\u2019t really want to get into the details of as it is a complex subject. I do want to mention that health interventions should not have an excessive focus on whether one is losing weight. Many of the interventions discussed here have significant effects (for example on insulin sensitivity, c-reactive protein, and fasting blood glucose) even when body composition does not change. Getting BMI below ~27 should be a priority however, as it has wide ranging effects across all other interventions.</p>\n<h4 id=\"Nutrition\">Nutrition</h4>\n<p>This is a big subject, and we\u2019re not even going to attempt to go into detail. This section will focus on the largest high level features of a diet that have positive or negative impact. Processed meat consumption has the single largest negative effect on health. It is shockingly bad, even if you already suspected as such.<sup><a href=\"http://circ.ahajournals.org/content/121/21/2271.short\">[1]</a></sup> In contrast, a bit of red meat has actually been found to be neutral. It seems to be that many earlier studies claiming harm from red meat did not adequately separate out the huge effect size of processed meat. Fish and nut consumption appear to be a grand slam for CVD in particular and also just for overall health.<sup><a href=\"http://circ.ahajournals.org/content/109/22/2705.short\">[2]</a></sup><sup><a href=\"http://ajcn.nutrition.org/content/70/3/500s.short\">[3]</a></sup>&nbsp;Pescetarians live significantly longer than vegans,<sup><a href=\"http://ajcn.nutrition.org/content/70/3/516s.full\">[4]</a></sup> lending support to fish consumption. Outside of specific foods, common micronutrient deficiencies have been indicated in everything from cancer, to immune system suppression, to poorer physical and mental performance, to sleep problems, greater inflammation, and even depression. Really there\u2019s too much material there to cover, there are just <a href=\"http://scholar.google.com/scholar?q=micronutrient+deficiencies\">pages and pages of studies</a>.</p>\n<p>There\u2019s also the bad news that multivitamins <a href=\"http://annals.org/article.aspx?articleid=1789253\">mostly don\u2019t do anything</a>. There has not been found an alternative to eating a variety of nutrient-dense whole foods. Though vitamin D supplementation appears to be quite beneficial. Another LW user, <a href=\"/user/John_Maxwell_IV/\">John_Maxwell_IV</a>, and I are trying to make this easy with our startup&nbsp;<a href=\"http://www.mealsquares.com\">MealSquares</a>.</p>\n<h4 id=\"Blood_donation\">Blood donation</h4>\n<p>The studies related to this have some methodological issues but overall the effect size is so large, and the cost and risks so low, that it is worth inclusion. Several studies have indicated that, for men, regular blood donation results in a massive reduction in heart attack.<sup><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/9737556\">[1]</a></sup><sup><a href=\"http://www.bmj.com/content/314/7083/793\">[2]</a></sup><sup><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC484902/\">[3]</a></sup> Other studies have found no such relation.<sup><a href=\"http://circ.ahajournals.org/content/103/1/52.short\">[4]</a></sup> There are also additional health benefits to blood donation.<sup><a href=\"http://www.biomedcentral.com/1741-7015/10/53\">[5]</a></sup> These are just some of the studies on this subject, but on balance after reviewing the evidence, I can say that donating blood once a year is almost certainly worth it if you're a man. &nbsp;Donating too often is probably bad for you though.</p>\n<h4 id=\"Exercise\">Exercise</h4>\n<p>This topic is large enough that I am separating out my actual recommendations into another post and purely discussing the health benefits here. Exercise is probably the single most important lifestyle intervention. Even minimal amounts of exercise have very large impacts on longevity and health. We\u2019re talking even walking&nbsp;<a href=\"http://longevity.about.com/od/healthyheartaging/a/Minimum-Exercise-For-A-Longer-Life.htm\">15 minutes a day causing people to live longer</a>. Even ignoring quality of life you are looking at a 3-7 fold return on every minute you spend exercising in extended life,<sup><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22039197\">[1]</a></sup> perhaps even exceeding that if you are making optimal use of your time. Exercise has a positive impact on pretty much everything that contributes to mortality. I don\u2019t really know how to convince you, the reader, that the future actually exists and that future you will be incredibly angry or sad that you didn\u2019t put in a small effort now for a better life later. But everyone has already told you this your whole life. So I\u2019m going to contrast it with the inverse. Most of the activities that we associate with fun and leisure involve some aspect of physicality, even if it\u2019s just walking around with friends. Losing access to these activities as can and <em>does</em> happen to people represents a massive decrease in quality of life. If you are reading this and you are young, you are able to simultaneously ignore your body\u2019s need for exercise, and demand performance of it when necessary to enjoy yourself. This will not remain true forever. Exercise has a protective effect against exactly the sorts of degenerative injuries that deprive people of their freedom of movement and activity.<sup><a href=\"http://jah.sagepub.com/content/9/1/105.short\">[2]</a></sup> I don\u2019t care if you start with an exercise habit of one pushup a <em>week</em>, but you must do something.</p>\n<p>Let\u2019s move on to some relevant considerations assuming you want to exercise. What sort of exercise should I be doing? Several studies have indicated that endurance athletes enjoy the greatest improvements in longevity. I would agree with this but caution that often the groups in such studies with the best health outcomes are those that do engage in resistance training as well. Soccer and other team sport players, for example, often perform resistance training as part of their overall conditioning. This seems to be overlooked because they do not perform it at the same level of intensity as athletes in the power sports. Long distance skiers and bikers also generally train lower body strength moves at an impressive level compared to the general public, even if it is a level significantly below that of power athletes (e.g. <a href=\"http://www.xczone.com/skifitnesstrg.pdf\">here</a> is an example of a training regime for a competitive skier). My point is simply that you shouldn\u2019t read a study that says \u201cendurance athletes live longer\u201d and assume that all you need to do is run. Strength training also has significant effects on insulin resistance, resting metabolic rate, glucose metabolism, blood pressure, hormone balance, joint health, organ reserve, depression, increases in HDL, reduction in back injuries, sleep quality, and a variety of harder-to-quantify quality of life improvements.<sup><a href=\"http://www.sciencedirect.com/science/article/pii/S0091743501909090\">[4]</a></sup><sup><a href=\"http://www.ais.up.ac.za/med/sportcert/prescription1a.pdf\">[5]</a></sup><sup><a href=\"http://link.springer.com/article/10.1007/s004310051070\">[6]</a></sup><sup><a href=\"http://circ.ahajournals.org/content/101/7/828.short\">[7]</a></sup><sup><a href=\"http://scholarcommons.sc.edu/sph_physical_activity_public_health_facpub/119/\">[8]</a></sup> I go to the trouble to cite resistance training so heavily because I feel that the benefits of cardio are generally well-understood, but I regularly encounter the idea that resistance training is only for people who want to look like a gross bodybuilder.</p>\n<p>Hopefully I have established that one should do both endurance and resistance training. Program specifics will be included in the other post as well as info on when benefits taper off.</p>\n<p>Edit: Exercise post is up <a href=\"/r/discussion/lw/juc/optimal_exercise/\">here</a>.</p>\n<h4 id=\"Stress\">Stress</h4>\n<p>Stress affects almost every system in your body. It increases disease risk by acting as an immunosuppressant. It directly impacts blood pressure, sleep problems, skin conditions, anxiety, depression, and even heart problems. Chronic untreated stress is often considered a causal factor in many other ailments people are medically treated for. Stress often goes untreated because alleviating it is seen as low priority. Whatever we are doing right now is worth a little stress. This can be true, but over a longer time horizon failing to learn better ways of managing stress really harms us. To confront stressors you must confront <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">ugh fields</a>. Non-productive coping mechanisms are the norm here: procrastination, abuse of substances including food, sleeping too much, blame as a curiosity-stopper etc. Simple strategies for dealing with low level stressors include things such as meditation, gratitude journaling, reflecting on and updating goals, or even just paying other people to deal with a recurring source of stress. Two previous LW posts have excellent advice in this area:&nbsp;<a href=\"/lw/4su/how_to_be_happy/\">How to Be Happy</a> and <a href=\"/lw/bq0/be_happier/\">Be Happier</a>.</p>\n<p>If you are depressed and don't know where to start on getting help please take a look at <a href=\"http://slatestarcodex.com/2014/06/16/things-that-sometimes-help-if-youre-depressed/\">Things that sometimes help <span style=\"text-decoration: underline;\">if you're depresed</span>.</a></p>\n<p>Supplements that impact stress include</p>\n<p>1. Rhodiola Rosea:&nbsp;<a href=\"http://www.sciencedirect.com/science/article/pii/S0944711310002680\">http://www.sciencedirect.com/science/article/pii/S0944711310002680</a></p>\n<p>2. Ashwaghanda root, which shows promise for chronic anxiety:&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3573577/\">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3573577/</a></p>\n<h4 id=\"Sleep\">Sleep</h4>\n<p>Chronic insomnia is a massive source of stress for many people and poses a huge mortality risk. In one study, people who got chronically less sleep had 3 times the mortality risk as people who slept well!<sup><a href=\"http://www.sciencedaily.com/releases/2010/06/100607065559.htm\">[1]</a></sup> You cannot afford to not start optimizing your sleep. It is important that your sleeping place be a quiet, dark, cool environment. You can use simple methods to improve each of these parameters. Forehead cooling has shown great promise in clinical trials.<sup><a href=\"http://www.scientificamerican.com/article/putting-insomnia-on-ice/\">[2]</a></sup> You can accomplish this with a gel pack that is cool (not ice). Even small LED lights in your room impact sleep quality because the melatonin production system is very fragile and sensitive to light.<sup><a href=\"http://www.health.harvard.edu/newsletters/Harvard_Health_Letter/2012/May/blue-light-has-a-dark-side/\">[3]</a></sup> Get tape and cover lights. Try <a href=\"http://www.amazon.com/Uvex-S1933X-Eyewear-SCT-Orange-Anti-Fog/dp/B000USRG90/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">orange glasses</a> to prevent blue light from destroying your endogenous melatonin production after 10pm. Regularize your sleeping and eating schedules. Expose yourself to bright lights in the morning to calibrate your circadian rhythms. Afternoon/early evening exercise is beneficial in making you sleepy. Melatonin pills work for many, but make sure you start with 75mcg (cut <a href=\"http://www.amazon.com/Sundown-Naturals-Melatonin-Tablets-tablets/dp/B000GG2I9O/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">these</a> into fourths), rather than the 3mg most pills come in. A teaspoon of raw honey before bed helps prevent some people from waking multiple times throughout the night.</p>\n<p>Consider reading this <a href=\"/lw/jrt/lifestyle_interventions_that_affect_longevity/amp8\">excellent info from Yvain on sleep apnea</a>, especially if you snore excessively or feel very tired even after a full night's sleep.</p>\n<h4 id=\"Cancer\">Cancer</h4>\n<p>Almost all of the risk factors for cancer have some overlap with CVD, meaning most of the advice above works for cancer too, but there are a few additional considerations worth discussing.</p>\n<p><strong id=\"Cancer_and_UV_exposure\">Cancer and UV exposure</strong></p>\n<p>One of the surprising results of my research was that conventional wisdom appears to be wrong here. There is not a simple relationship between UV exposure and increased cancers. Specifically, while increased sunlight exposure is correlated with higher incidence of skin cancer, it appears that it is also correlated with a decreased risk of 5 other cancers that are far less survivable.<sup><a href=\"http://onlinelibrary.wiley.com/doi/10.1002/cncr.10427/full\">[1]</a></sup> This is a straightforward trade off, getting sun exposure wins by quite a lot. Shade your face to avoid photodamage to your skin and macular degeneration of your eyes.</p>\n<p><strong id=\"Breast_cancer_and_testicular_cancer\">Breast cancer and testicular cancer</strong></p>\n<p>Redacted, see Vaniver's comment <a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/amv5\">here</a>.</p>\n<h4 id=\"Floss\">Floss</h4>\n<p>No, seriously. Not flossing is way more lethal than you think.<sup><a href=\"http://www.hindawi.com/journals/jar/2011/156061/abs/\">[1]</a></sup> You should also see a dentist regularly, even if you have to pay for your own insurance. (It's surprisingly cheap, e.g. Delta Dental offers plans for under $100/yr; lots of people don't make use of their plan and subsidize the treatment of those who do use theirs). Losing teeth greatly increases your chances of infections over time.</p>\n<h4><br></h4>\n<h3 id=\"Things_that_will_kill_you_right_now\">Things that will kill you right now</h3>\n<h4 id=\"Avoidable_medical_errors\">Avoidable medical errors</h4>\n<p>Avoidable medical errors might be the second leading cause of death after CVD.<sup><a href=\"http://journals.lww.com/journalpatientsafety/Fulltext/2013/09000/A_New,_Evidence_based_Estimate_of_Patient_Harms.2.aspx\">[1]</a></sup> This makes a hospital visit possibly the most dangerous thing you can do, especially if you are young. In general, you should not assume that medical staff are competent. Triple check dangerous prescriptions. If you don\u2019t know whether a prescription is dangerous, assume it is. Ask medical staff if they\u2019ve washed their hands (yes, this is actually still a major problem). Sharpie on yourself which side of your body a surgery is supposed to happen on, along with your name and what the surgery is for (seriously). Keep your own records, especially if you have serious medical issues; error rates in medical documentation are ridiculous. Medical equipment is generally cleaned by custodial staff with no medical training who often don't know how a particular device works. Have someone you can call in an emergency who knows about all of this.</p>\n<p>While we're discussing medicine, I'll throw in a couple low cost recommendations that give me peace of mind, even if an emergency situation is unlikely. The first is that the Red Cross has created an <a href=\"http://www.redcross.org/mobile-apps/first-aid-app\">android/iphone app</a> covering first aid with extensive pictures and videos helping you through the situation. The second is <a href=\"http://www.amazon.com/Quikclot-Advanced-Clotting-Bleeding-Package/dp/B001BCNTHC/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">quickclot</a>&nbsp;which can stop severe bleeding much faster than traditional techniques.</p>\n<h4 id=\"Unintentional_poisoning\">Unintentional poisoning</h4>\n<p>This is mostly acetaminophen poisoning resulting from their mandatory inclusion in pain killers to prevent abuse. Also people misdosing themselves with legal and illegal drugs. Be careful, this outweighs traffic accidents in accidental deaths. Adding the <a href=\"http://www.aapcc.org/\">24 hour emergency poison control line number</a>&nbsp;(1-800-222-1222) to your phone is something you can do right now. It is also worth knowing that SOP for acetaminophen poisoning is <a href=\"http://emedicine.medscape.com/article/820200-treatment\">high dose NAC</a>, which is freely available on amazon in the US (h/t Tara).</p>\n<h4 id=\"Traffic_accidents\">Traffic accidents</h4>\n<p>Michael Curzi has a great post on this I won\u2019t attempt to reproduce here:&nbsp;<a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/\">How to avoid dying in a car crash</a>. It is definitely worth updating your model of what behaviors are dangerous in a car.</p>\n<p>&nbsp;</p>\n<h3 id=\"Summary_of_interventions\">Summary of interventions</h3>\n<ol>\n<li>\n<p>If you know people who smoke, getting them to vape is the single largest impact you can have on their lifespan.</p>\n</li>\n<li>\n<p>Pay attention when in your car.</p>\n</li>\n<li>\n<p>CONSTANT VIGILANCE when dealing with the medical profession and drugs.</p>\n</li>\n<li>\n<p>Exercise: very high return on first few units of effort, some cardio and some resistance training is best.</p>\n</li>\n<li>\n<p>Blood donation every 12-24 months for men.</p>\n</li>\n<li>\n<p>Buy a blood pressure monitor and do blood pressure reduction interventions if needed.</p>\n</li>\n<li>\n<p>Eat fish, nuts, eggs, fruit, dark chocolate. Supplement <a href=\"http://www.amazon.com/Now-foods-Vitamin-45mcg-120VC/dp/B0019QT9LA/ref=sr_1_1?ie=UTF8&amp;qid=1426137023&amp;sr=8-1&amp;keywords=d3%2Bk2\">Vitamin D3</a>.</p>\n</li>\n<li>\n<p>Work towards a healthy weight.</p>\n</li>\n<li>\n<p>If you are losing sleep/are stressed, try one small intervention at a time, and <strong>don\u2019t get discouraged</strong>. &nbsp;These interventions are the hardest but potentially the most rewarding. Supplements for stress, anxiety, and sleep are somewhat subjective and vary more in reported efficacy than others; self-experimentation is recommended.</p>\n</li>\n<li>\n<p>Floss (and see a dentist).</p>\n</li>\n</ol>\n<p>&nbsp;</p>\n<h3 id=\"Closing\">Closing</h3>\n<p>Don\u2019t worry too much. Don\u2019t get down on yourself about health. &nbsp;This creates an <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">ugh field</a> making you less likely to take action. &nbsp;The process of becoming healthier is going to make you feel stupid sometimes. This is a marathon and not a sprint; standard <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">habit</a> <a href=\"/lw/hub/common_failure_modes_in_habit_formation/\">forming</a> rules apply. Trying to fix 10 things at once is highly stressful! Do not do this! Discuss things that worked for you and didn\u2019t work for you in the past with yourself and with others and come up with a plan. Don\u2019t publicly commit to your plan in the comments, this <a href=\"https://sivers.org/zipit\">makes you less likely to do it</a>. Oh, and feel free to argue with me or request more sources.</p>\n<p>&nbsp;</p>", "sections": [{"title": "A quick FAQ before we get started", "anchor": "A_quick_FAQ_before_we_get_started", "level": 1}, {"title": "Why should I care about longevity-promoting habits at a young age?", "anchor": "Why_should_I_care_about_longevity_promoting_habits_at_a_young_age_", "level": 2}, {"title": "Isn\u2019t longevity mostly just genetics?", "anchor": "Isn_t_longevity_mostly_just_genetics_", "level": 2}, {"title": "What sort of benefits can I expect?", "anchor": "What_sort_of_benefits_can_I_expect_", "level": 2}, {"title": "How were these recommendations generated?/Why should I believe you?", "anchor": "How_were_these_recommendations_generated__Why_should_I_believe_you_", "level": 2}, {"title": "Things that will eventually kill you", "anchor": "Things_that_will_eventually_kill_you", "level": 1}, {"title": "CVD", "anchor": "CVD", "level": 2}, {"title": "Smoking", "anchor": "Smoking", "level": 2}, {"title": "Alcohol", "anchor": "Alcohol", "level": 2}, {"title": "Blood Pressure", "anchor": "Blood_Pressure", "level": 2}, {"title": "Blood lipids (cholesterol)", "anchor": "Blood_lipids__cholesterol_", "level": 2}, {"title": "BMI/Obesity", "anchor": "BMI_Obesity", "level": 2}, {"title": "Nutrition", "anchor": "Nutrition", "level": 2}, {"title": "Blood donation", "anchor": "Blood_donation", "level": 2}, {"title": "Exercise", "anchor": "Exercise", "level": 2}, {"title": "Stress", "anchor": "Stress", "level": 2}, {"title": "Sleep", "anchor": "Sleep", "level": 2}, {"title": "Cancer", "anchor": "Cancer", "level": 2}, {"title": "Cancer and UV exposure", "anchor": "Cancer_and_UV_exposure", "level": 3}, {"title": "Breast cancer and testicular cancer", "anchor": "Breast_cancer_and_testicular_cancer", "level": 3}, {"title": "Floss", "anchor": "Floss", "level": 2}, {"title": "Things that will kill you right now", "anchor": "Things_that_will_kill_you_right_now", "level": 1}, {"title": "Avoidable medical errors", "anchor": "Avoidable_medical_errors", "level": 2}, {"title": "Unintentional poisoning", "anchor": "Unintentional_poisoning", "level": 2}, {"title": "Traffic accidents", "anchor": "Traffic_accidents", "level": 2}, {"title": "Summary of interventions", "anchor": "Summary_of_interventions", "level": 1}, {"title": "Closing", "anchor": "Closing", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "385 comments"}], "headingsCount": 29}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 384, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ndGYn7ZFiZyernp9f", "bZ2w99pEAeAbKnKqo", "ZbgCx2ntD5eu8Cno9", "JHcTP4Ad8QAmRTCZm", "7XbcDaeigMaxW43EB", "5wMTZLZZmZEbXdoMD", "PYgGpmmk3wQhSt6Yv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-02-28T06:28:57.888Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-28T08:17:04.824Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Meetup - March", "slug": "meetup-sydney-meetup-march", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:32.680Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M2rWr6qooYjx37Xjt/meetup-sydney-meetup-march", "pageUrlRelative": "/posts/M2rWr6qooYjx37Xjt/meetup-sydney-meetup-march", "linkUrl": "https://www.lesswrong.com/posts/M2rWr6qooYjx37Xjt/meetup-sydney-meetup-march", "postedAtFormatted": "Friday, February 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Meetup%20-%20March&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Meetup%20-%20March%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM2rWr6qooYjx37Xjt%2Fmeetup-sydney-meetup-march%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Meetup%20-%20March%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM2rWr6qooYjx37Xjt%2Fmeetup-sydney-meetup-march", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM2rWr6qooYjx37Xjt%2Fmeetup-sydney-meetup-march", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xd'>Sydney Meetup - March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 March 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So far so good. Our last two meetups have been great - so lets do it one more time.</p>\n\n<p>6:30 PM for early discussion\n7PM general dinner-discussion\nafter dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>I'll book another table under the name \"less wrong\". Last meetup we were in the restaurant on level 2.\nWhen I arrive I'll facebook about where exactly the table is located.</p>\n\n<p>I'd like to theme this meetup with a sub-goal of outreach.</p>\n\n<p>If you've been thinking of a friend you might like to bring along - this is the night to do it.</p>\n\n<p>We'll have a brief intro for any newbies to the community and maybe have our early discussion be about the community and what we think we can offer.</p>\n\n<p>Afterwards, we'll havethe rationality exercise and more specific discussion-topic TBD by Eliot.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xd'>Sydney Meetup - March</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M2rWr6qooYjx37Xjt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.5868191696428954e-06, "legacy": true, "legacyId": "25645", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___March\">Discussion article for the meetup : <a href=\"/meetups/xd\">Sydney Meetup - March</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 March 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sydney City RSL, 565 George St, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>So far so good. Our last two meetups have been great - so lets do it one more time.</p>\n\n<p>6:30 PM for early discussion\n7PM general dinner-discussion\nafter dinner we'll have our rationality exercise and a more specific discussion-topic.</p>\n\n<p>I'll book another table under the name \"less wrong\". Last meetup we were in the restaurant on level 2.\nWhen I arrive I'll facebook about where exactly the table is located.</p>\n\n<p>I'd like to theme this meetup with a sub-goal of outreach.</p>\n\n<p>If you've been thinking of a friend you might like to bring along - this is the night to do it.</p>\n\n<p>We'll have a brief intro for any newbies to the community and maybe have our early discussion be about the community and what we think we can offer.</p>\n\n<p>Afterwards, we'll havethe rationality exercise and more specific discussion-topic TBD by Eliot.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup___March1\">Discussion article for the meetup : <a href=\"/meetups/xd\">Sydney Meetup - March</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Meetup - March", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___March", "level": 1}, {"title": "Discussion article for the meetup : Sydney Meetup - March", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup___March1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-28T17:04:16.649Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-110", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CQNvSuAqvwGKwzmRp/weekly-lw-meetups-110", "pageUrlRelative": "/posts/CQNvSuAqvwGKwzmRp/weekly-lw-meetups-110", "linkUrl": "https://www.lesswrong.com/posts/CQNvSuAqvwGKwzmRp/weekly-lw-meetups-110", "postedAtFormatted": "Friday, February 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQNvSuAqvwGKwzmRp%2Fweekly-lw-meetups-110%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQNvSuAqvwGKwzmRp%2Fweekly-lw-meetups-110", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQNvSuAqvwGKwzmRp%2Fweekly-lw-meetups-110", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 558, "htmlBody": "<p><strong>This summary was posted to LW main on February 21st. The following week's summary is <a href=\"/lw/jse/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/ww\">Atlanta Lesswrong Meetup: How to Increase Your Brainpower:&nbsp;<span class=\"date\">23 February 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/wq\">Bratislava Meetup X.:&nbsp;<span class=\"date\">24 February 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/wy\">[Canberra] Second Canberra Meetup - Paranoid Debating:&nbsp;<span class=\"date\">28 February 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/wp\">Hamburg - about Procrastination:&nbsp;<span class=\"date\">21 February 2014 </span>07:00PM+1:00 (MET)</a></li>\n<li><a href=\"/meetups/wr\">Montreal Less Wrong - Easy Lifehacks:&nbsp;<span class=\"date\">24 February 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/wz\">Moscow, Different Reports:&nbsp;<span class=\"date\">23 February 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/wg\">Princeton NJ Meetup:&nbsp;<span class=\"date\">22 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/w4\">Sydney Meetup: February:&nbsp;<span class=\"date\">26 February 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/x1\">Urbana-Champaign: Bridging laws:&nbsp;<span class=\"date\">23 February 2014 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">22 February 2020 01:30PM</span></a></li>\n<li><a href=\"/meetups/w6\">[Berlin] Community Weekend in Berlin:&nbsp;<span class=\"date\">11 April 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/wm\">London VOI meetup 16/2, plus socials 9/2 and 23/2:&nbsp;<span class=\"date\">23 February 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/wx\">Washington DC EA meetup (now with guests!):&nbsp;<span class=\"date\">23 February 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/x2\">West LA&mdash;Practical Taoism:&nbsp;<span class=\"date\">26 February 2014 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CQNvSuAqvwGKwzmRp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 1.5874462331989885e-06, "legacy": true, "legacyId": "25572", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qkWasTATELXccSMdH", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-02-28T21:49:02.228Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Games", "slug": "meetup-urbana-champaign-games", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5AgiD9a9MXEc83wBg/meetup-urbana-champaign-games", "pageUrlRelative": "/posts/5AgiD9a9MXEc83wBg/meetup-urbana-champaign-games", "linkUrl": "https://www.lesswrong.com/posts/5AgiD9a9MXEc83wBg/meetup-urbana-champaign-games", "postedAtFormatted": "Friday, February 28th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5AgiD9a9MXEc83wBg%2Fmeetup-urbana-champaign-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5AgiD9a9MXEc83wBg%2Fmeetup-urbana-champaign-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5AgiD9a9MXEc83wBg%2Fmeetup-urbana-champaign-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xe'>Urbana-Champaign: Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 March 2014 12:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.110430, -88.223784</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Available games will include Wits and Wagers, Zendo, Cards Against Humanity, Pandemic, probably Flux, and the closed beta of Dystheism, a cooperative multiplayer puzzle game.\nMeetup will be held at my apartment:\n300 S. Goodwin Ave Apt 102 Urbana IL.\nCoordinates: 40.110430, -88.223784\nAt 2pm, Sunday.\nThe main entrance to the building requires card access, but there is a door you can knock on where we will hear you at the north end of the west side of the building.\nIf you have any trouble getting in, call REDACTED.\nCross posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/kqyTWSHECeE\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xe'>Urbana-Champaign: Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5AgiD9a9MXEc83wBg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5877851169983057e-06, "legacy": true, "legacyId": "25647", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Games\">Discussion article for the meetup : <a href=\"/meetups/xe\">Urbana-Champaign: Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 March 2014 12:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.110430, -88.223784</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Available games will include Wits and Wagers, Zendo, Cards Against Humanity, Pandemic, probably Flux, and the closed beta of Dystheism, a cooperative multiplayer puzzle game.\nMeetup will be held at my apartment:\n300 S. Goodwin Ave Apt 102 Urbana IL.\nCoordinates: 40.110430, -88.223784\nAt 2pm, Sunday.\nThe main entrance to the building requires card access, but there is a door you can knock on where we will hear you at the north end of the west side of the building.\nIf you have any trouble getting in, call REDACTED.\nCross posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/kqyTWSHECeE\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Games1\">Discussion article for the meetup : <a href=\"/meetups/xe\">Urbana-Champaign: Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Games", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-01T08:52:13.172Z", "modifiedAt": null, "url": null, "title": "Self-Congratulatory Rationalism", "slug": "self-congratulatory-rationalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:33.531Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zsznamBgNuj3XX2DP/self-congratulatory-rationalism", "pageUrlRelative": "/posts/zsznamBgNuj3XX2DP/self-congratulatory-rationalism", "linkUrl": "https://www.lesswrong.com/posts/zsznamBgNuj3XX2DP/self-congratulatory-rationalism", "postedAtFormatted": "Saturday, March 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-Congratulatory%20Rationalism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-Congratulatory%20Rationalism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsznamBgNuj3XX2DP%2Fself-congratulatory-rationalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-Congratulatory%20Rationalism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsznamBgNuj3XX2DP%2Fself-congratulatory-rationalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsznamBgNuj3XX2DP%2Fself-congratulatory-rationalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3044, "htmlBody": "<p>Quite a few people complain about the atheist/skeptic/rationalist communities being self-congratulatory. I used to dismiss this as a sign of people's unwillingness to admit that rejecting religion, or astrology, or whatever, was any more rational than accepting those things. Lately, though, I've started to worry.</p>\n<p>Frankly, there seem to be a <em>lot </em>of people in the LessWrong community who imagine themselves to be, not just more rational than average, but paragons of rationality who other people should accept as such. I've encountered people talking as if it's ridiculous to suggest they might sometimes respond badly to being told the truth about certain subjects. I've encountered people asserting the rational superiority of themselves and others in the community for flimsy reasons, or no reason at all.</p>\n<p>Yet the readiness of members of the LessWrong community to disagree with and criticize each other suggests we don't actually think all that highly of each other's rationality. The fact that members of the LessWrong community tend to be smart is no guarantee that they will be rational. And we have much reason to fear \"rationality\" degenerating into signaling games.</p>\n<p><a id=\"more\"></a></p>\n<h1>What Disagreement Signifies</h1>\n<p>Let's start by talking about disagreement. There's been a lot of <a href=\"http://wiki.lesswrong.com/wiki/Disagreement\">discussion of disagreement on LessWrong</a>, and in particular of <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann's agreement theorem</a>, often glossed as something like \"two rationalists can't agree to disagree.\" (Or perhaps that we can't <a href=\"http://www.overcomingbias.com/2007/01/we_cant_foresee.html\">foresee to disagree</a>.) Discussion of disagreement, however, tends to focus on what to do about it. I'd rather take a step back, and look at what disagreement tells us about ourselves: namely, that we don't think all that highly of each other's rationality.</p>\n<p>This, for me, is the take-away from Tyler Cowen and Robin Hanson's paper <a href=\"http://hanson.gmu.edu/deceive.pdf\">Are Disagreements Honest?</a>&nbsp;In the paper, Cowen and Hanson define honest disagreement as meaning that \"meaning that the disputants respect each other&rsquo;s relevant abilities, and consider each person&rsquo;s stated opinion to be his best estimate of the truth, given his information and effort,\" and they argue disagreements aren't honest in this sense.</p>\n<p>I don't find this conclusion surprising. In fact, I suspect that while people sometimes <em>do </em>mean it when they talk about respectful disagreement, often they realize this is a polite fiction (which isn't necessarily a bad thing). Deep down, they know that <a href=\"http://www.overcomingbias.com/2008/09/disagreement-is.html\">disagreement is disrespect</a>, at least in the sense of not thinking that highly of the other person's rationality.&nbsp;That people know this is shown in the fact that they don't like being told they're wrong&mdash;the reason why <a href=\"/lw/j6o/according_to_dale_carnegie_you_cant_win_an/\">Dale Carnegie says you can't win an argument</a>.&nbsp;</p>\n<p>On LessWrong, people are quick to criticize each others' views, so much so that I've heard people cite this as a reason to be reluctant to post/comment (again showing they know intuitively that disagreement is disrespect). Furthermore when people in LessWrong criticize others' views, they very often don't seem to expect to quickly reach agreement. Even people Yvain would classify as <a href=\"/lw/jjd/rationalists_are_less_credulous_but_better_at/\">\"experienced rationalists\"</a>&nbsp;sometimes knowingly have persistent disagreements. This suggests that LessWrongers almost never consider each other to be perfect rationalists.</p>\n<p>And I actually think this is a sensible stance. For one thing, even if you met a perfect rationalist, it could be hard to figure out that they are one. Furthermore, the problem of knowing what to do about disagreement is made harder when you're faced with other people having persistent disagreements: if you find yourself agreeing with Alice, you'll have to think Bob is being irrational, and vice versa. If you rate them equally rational and adopt an intermediate view, you'll have to think they're both being a bit irrational for not doing likewise.</p>\n<p>The situation is similar to <a href=\"http://en.wikipedia.org/wiki/Moore's_paradox\">Moore's paradox</a> in philosophy&mdash;the impossibility of asserting \"it's raining, but I don't believe it's raining.\" Or, as you might say, \"Of course I think my opinions are right and other people's are wrong. Otherwise I'd change my mind.\" Similarly, when we think about disagreement, it seems like we're forced to say, \"Of course I think my opinions are rational and other people's are irrational. Otherwise I'd change my mind.\"</p>\n<p>We can find some room for humility in an analog of the <a href=\"http://en.wikipedia.org/wiki/Preface_paradox\">preface paradox</a>, the fact that the author of a book can say things like \"any errors that remain are mine.\" We can say this because we might think each individual claim in the book is highly probable, while recognize that all the little uncertainties add up to it being likely there are still errors. Similarly, we can think each of our beliefs are individually rational, while recognizing we still probably have <em>some </em>irrational beliefs&mdash;we just don't know which ones And just because respectful disagreement is a polite fiction doesn't mean we should abandon it.&nbsp;</p>\n<p>I don't have a clear sense of how controversial the above will be. Maybe we all already recognize that we don't respect each other's opinions 'round these parts. But I think some features of discussion at LessWrong look odd in light of the above points about disagreement&mdash;including some of the things people say about disagreement.</p>\n<p>The <a href=\"http://wiki.lesswrong.com/wiki/Disagreement\">wiki</a>, for example, says that \"Outside of well-functioning prediction markets, Aumann agreement can probably only be approximated by careful deliberative discourse. Thus, fostering effective deliberation should be seen as a key goal of Less Wrong.\" The point of Aumann's agreement theorem, though, is precisely that ideal rationalists <em>shouldn't </em>need to engage in deliberative discourse, as usually conceived, in order to reach agreement.</p>\n<p>As Cowen and Hanson put it, \"Merely knowing someone else&rsquo;s opinion provides a powerful summary of everything that person knows, powerful enough to eliminate any differences of opinion due to differing information.\" So sharing evidence the normal way shouldn't be necessary. Asking someone \"what's the evidence for that?\" implicitly says, \"I don't trust your rationality enough to take your word for it.\" But when dealing with real people who may or may not have a rational basis for their beliefs, that's almost always the right stance to take.</p>\n<h1>Intelligence and Rationality</h1>\n<p>Intelligence does not equal rationality. Need I say more? Not long ago, I wouldn't have thought so. I would have thought it was a fundamental premise behind LessWrong, indeed behind old-school scientific skepticism. As Michael Shermer <a href=\"http://www.michaelshermer.com/weird-things/excerpt/\">once said</a>, \"Smart people believe weird things because they are skilled at defending beliefs they arrived at for non-smart reasons.\"</p>\n<p>Yet I've heard people suggest that you must never be dismissive of things said by smart people, or that the <a href=\"/lw/jj0/2013_survey_results/\">purportedly high IQ of the LessWrong community</a> means people here don't make bad arguments. When I hear that, I think \"<em>whaaat? </em>People on LessWrong make bad arguments all the time!\" When this happens, I&nbsp;<em>generally</em>&nbsp;limit myself to trying to point out the flaw in the argument and/or downvoting, and resist the urge to shout \"YOUR ARGUMENTS ARE BAD AND YOU SHOULD FEEL BAD.\" I just think it.</p>\n<p>When I reach for an explanation of why terrible arguments from smart people shouldn't surprise anyone, I go to Yvain's <a href=\"/lw/2pv/intellectual_hipsters_and_metacontrarianism/\">Intellectual Hipsters and Meta-Contarianism</a>, one of my favorite LessWrong posts of all time. While Yvain notes that meta-contrarianism often isn't a good thing, though, on re-reading it I noticed what seems like an important oversight:</p>\n<blockquote>\n<p>A person who is somewhat upper-class will conspicuously signal eir wealth by buying difficult-to-obtain goods. A person who is very upper-class will conspicuously signal that ey feels no need to conspicuously signal eir wealth, by deliberately not buying difficult-to-obtain goods.</p>\n<p>A person who is somewhat intelligent will conspicuously signal eir intelligence by holding difficult-to-understand opinions. A person who is very intelligent will conspicuously signal that ey feels no need to conspicuously signal eir intelligence, by deliberately not holding difficult-to-understand opinions.</p>\n<p>According to the survey, the average IQ on this site is around 145. People on this site differ from the mainstream in that they are more willing to say death is bad, more willing to say that science, capitalism, and the like are good, and less willing to say that there's some deep philosophical sense in which 1+1 = 3. That suggests people around that level of intelligence have reached the point where they no longer feel it necessary to differentiate themselves from the sort of people who aren't smart enough to understand that there might be side benefits to death.</p>\n</blockquote>\n<p>The pattern of <a href=\"http://www.bus.indiana.edu/riharbau/cs-randfinal.pdf\">countersignaling</a> Yvain describes here is real. But it's important not to forget that sometimes, the super-wealthy signal their wealth by buying things even the moderately wealthy can't afford. And sometimes, the very intelligent signal their intelligence by holding opinions even the moderately intelligent have trouble understanding. You also get hybrid status moves: designer versions of normally low-class clothes, complicated justifications for opinions normally found among the uneducated.</p>\n<p>Robin Hanson has <a href=\"http://www.overcomingbias.com/2009/11/why-academics-are-not-bayesian.html\">argued</a> that this leads to biases in academia:</p>\n<blockquote>\n<p>I&rsquo;ve argued that the main social function of academia is to let students, patrons, readers, etc. affiliate with credentialed-as-impressive minds. If so, academic beliefs are secondary &ndash; the important thing is to clearly show respect to those who make impressive displays like theorems or difficult data analysis. And the obvious way for academics to use their beliefs to show respect for impressive folks is to have academic beliefs track the most impressive recent academic work.</p>\n</blockquote>\n<p>Robin's post focuses on economics, but I suspect the problem is even worse in my home field of philosophy. As I've <a href=\"http://www.patheos.com/blogs/hallq/2012/06/from-the-archives-philosophy-is-dysfunctional/\">written</a> <a href=\"http://www.patheos.com/blogs/hallq/2013/01/from-the-archives-seduced-by-sophistication-follow-up-to-philosophy-is-dysfunctional/\">before</a>, the problem is that in philosophy, philosophers never agree on whether a philosopher has solved a problem. Therefore, there can be no rewards for being right, only rewards for showing off your impressive intellect. This often means finding clever ways to be wrong.</p>\n<p>I need to emphasize that I really do think philosophers are showing off real intelligence, not merely showing off faux-cleverness. GRE scores suggest <a href=\"http://blogs.discovermagazine.com/gnxp/2010/12/verbal-vs-mathematical-aptitude-in-academics/\">philosophers are among the smartest academics</a>, and their performance is arguably made more impressive by the fact that GRE quant scores are bimodally distributed based on whether your major required you to spend four years practicing your high school math, with philosophy being one of the majors that doesn't grant that advantage. Based on this, if you think it's wrong to dismiss the views of high-IQ people, you shouldn't be dismissive of mainstream philosophy. But in fact I think LessWrong's <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">oft-noticed dismissiveness of mainstream philosophy</a>&nbsp;is largely justified.</p>\n<p>I've found philosophy of religion in particular to be a goldmine of terrible arguments made by smart people. Consider <a href=\"http://www.patheos.com/blogs/hallq/2013/01/the-ontological-argument-in-brief/\">Alvin Plantinga's modal ontological argument</a>. The argument is sufficiently difficult to understand that I won't try to explain it here. If you want to understand it, I'm not sure what to tell you except to maybe read Plantinga's book&nbsp;<em><a href=\"http://www.amazon.com/Nature-Necessity-Clarendon-Library-Philosophy-ebook/dp/B0041OTAGG/ref=sr_1_1?ie=UTF8&amp;qid=1393634681&amp;sr=8-1&amp;keywords=Nature+of+Necessity\">The Nature of Necessity</a>.&nbsp;</em>In fact, I predict at least one LessWronger will comment on this thread with an incorrect explanation or criticism of the argument. Which is not to say they wouldn't be smart enough to understand it, just that it might take them a few iterations of getting it wrong to finally get it right. And coming up with an argument like that is no mean feat&mdash;I'd guess Plantinga's IQ is just as high as the average LessWronger's.</p>\n<p>Once you understand the modal ontological argument, though, it quickly becomes obvious that Plantinga's logic works just as well to \"prove\" that it's a necessary truth that pigs fly. Or that Plantinga's god does not exist. Or even as a general purpose \"proof\" of any purported mathematical truth you please.&nbsp;The main point is that Plantinga's argument is not stupid in the sense of being something you'd only come up with if you had a low IQ&mdash;the opposite is true. But Plantinga's argument <em>is</em> stupid in the sense of being something you'd only come up with it while under the influence of some serious motivated reasoning.</p>\n<p>The modal ontological argument is admittedly an extreme case. Rarely is the chasm between the difficulty of the concepts underlying an argument, and the argument's actual merits, so vast. Still, beware the temptation to affiliate with smart people by taking everything they say seriously.</p>\n<p><strong>Edited to add: in the original post, I intended but forgot to emphasize that I think the correlation between IQ and rationality is weak <em>at best. </em>Do people disagree? Does anyone want to go out on a limb and say, \"They aren't the same thing, but the correlation is still very strong?\"</strong></p>\n<h1>The Principle of Charity</h1>\n<p>I've made no secret of the fact that I'm not a big fan of the principle of charity&mdash;often defined as the rule that you should interpret other people's arguments on the assumption that they are not saying anything stupid. The problem with this is that other people are often saying something stupid. Because of that, I think charitable is over-rated compared to fair and accurate reading. When someone says something stupid, you don't have to pretend otherwise, but it's really important not to attribute to people stupid things they never said.</p>\n<p>More frustrating than this simple disagreement over charity, though, is when people who invoke the principle of charity do so selectively. They apply it to people who's views they're at least somewhat sympathetic to, but when they find someone they want to attack, they have trouble meeting basic standards of fairness. And in the most frustrating cases, this gets explicit justification: \"we need to read <em>these </em>people charitably, because they are obviously very intelligent and rational.\" I once had a member of the LessWrong community actually tell me, \"You need to interpret me more charitably, because you know I'm sane.\" \"Actually, buddy, I don't know that,\" I wanted to reply&mdash;but didn't, because that would've been rude.</p>\n<p>I can see benefits to the principle of charity. It helps avoid flame wars, and from a Machiavellian point of view it's nice to close off the \"what I actually meant was...\" responses. Whatever its merits, though, they can't depend on the actual intelligence and rationality of the person making an argument. Not only is intelligence no guarantee against making bad arguments, the whole reason we demand other people tell us their reasons for their opinions in the first place is we fear their reasons <em>might </em>be bad ones.</p>\n<p>As I've already explained, there's a difficult problem here about how to be appropriately modest about our own rationality. When I say something, I never think it's stupid, otherwise I wouldn't say it. But at least I'm not so arrogant as to go around demanding other people acknowledge my highly advanced rationality. I don't demand that they accept \"Chris isn't saying anything stupid\" as an axiom in order to engage with me.</p>\n<h1>Beware Weirdness for Weirdness' Sake</h1>\n<p>There's a <a href=\"http://en.wikipedia.org/wiki/Signalling_theory#Religion_as_a_costly_signal\">theory</a> in the psychology and sociology of religion that the purpose of seemingly foolish rituals like circumcision and snake-handling is to provide a costly and therefore hard-to-fake signal of group commitment. I think I've heard it suggested&mdash;though I can't find by who&mdash;that crazy religious doctrines could serve a similar purpose. It's easy to say you believe in a god, but being willing to risk ridicule by saying you believe in one god who is three persons, who are all the same god, yet not identical to each other, and you can't explain how that is but it's a mystery you accept on faith... now <em>that </em>takes dedication.</p>\n<p>Once you notice the general \"signal group commitment in costly ways\" strategy, it seems to crop up everywhere. Subcultures often seem to go out of their way to be weird, to do things that will shock people outside the subculture, ranging from tattoos and weird clothing to coming up with reasons why things regarded as normal and innocuous in the broader culture are actually evil. Even something as simple as a large body of jargon and in-jokes can do the trick: if someone takes the time to learn all the jargon and in-jokes, you know they're committed.</p>\n<p>This tendency is probably harmless when done with humor and self-awareness, but it's more worrisome when a group becomes convinced its little bits of weirdness for weirdness' sake are a sign of its superiority to other groups. And it's worth being aware of, because it makes sense of signaling moves that aren't straightforwardly plays for higher status.</p>\n<p>The LessWrong community has amassed a truly impressive store of jargon and in-jokes over the years, and some of it's quite useful (I reiterate my love for the term \"meta-contrarian\"). But as with all jargon, LessWrongian jargon is often just a silly way of saying things you could have said without it. For example, people say \"I have a poor mental model of...\" when they could have just said they don't understand it very well.</p>\n<p>That bit of LessWrong jargon is merely silly. Worse, I think, is the jargon around politics. Recently, a friend gave \"they avoid blue-green politics\" as a reason LessWrongians are more rational than other people. It took a day before it clicked that \"blue-green politics\" here basically just meant \"partisanship.\" But complaining about partisanship is old hat&mdash;literally. <a href=\"http://en.wikipedia.org/wiki/Political_parties_in_the_United_States#History\">America's</a> <a href=\"http://www.washingtonsblog.com/2011/07/the-founding-fathers-tried-to-warn-us-about-the-threat-from-a-two-party-system.html\">founders</a>&nbsp;were fretting about it back in the 18th century. Nowadays, such worries are something you expect to hear from boringly middle-brow columnists at major newspapers, not edgy contrarians.</p>\n<p>But \"blue-green politics,\" \"politics is the mind-killer\"... never mind how much content they add, the point is they're obscure enough to work as an excuse to feel superior to anyone whose political views are too mainstream. Outsiders will probably think you're weird, invoking obscure jargon to quickly dismiss ideas that seem plausible to them, but on the upside you'll get to bond with members of your in-group over your feelings of superiority.</p>\n<h1>A More Humble Rationalism?</h1>\n<p>I feel like I should wrap up with some advice. Unfortunately, this post was motivated by problems I'd seen, not my having thought of brilliant solutions to them. So I'll limit myself to some fairly boring, non-brilliant advice.</p>\n<p>First, yes, some claims are more rational than others. Some people even do better at rationality overall than others. But the idea of a real person being anything close to an ideal rationalist is an extraordinary claim, and should be met with appropriate skepticism and demands for evidence. Don't forget that.</p>\n<p>Also, beware signaling games. A good dose of Hansonian cynicism, applied to your own in-group, is healthy. Somewhat relatedly, I've begun to wonder if \"rationalism\" is really good branding for a movement. <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">Rationality is systematized winning</a>, sure, but the \"rationality\" branding isn't as good for keeping that front and center, especially compared to, say the effective altruism meme. It's just a little too easy to forget where \"rationality\" is supposed to connect with the real world, increasing the temptation for \"rationality\" to spiral off into signaling games.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9YFoDPFwMoWthzgkY": 1, "izp6eeJJEg9v5zcur": 1, "5f5c37ee1b5cdee568cfb345": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zsznamBgNuj3XX2DP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 72, "extendedScore": null, "score": 0.000194, "legacy": true, "legacyId": "25567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Quite a few people complain about the atheist/skeptic/rationalist communities being self-congratulatory. I used to dismiss this as a sign of people's unwillingness to admit that rejecting religion, or astrology, or whatever, was any more rational than accepting those things. Lately, though, I've started to worry.</p>\n<p>Frankly, there seem to be a <em>lot </em>of people in the LessWrong community who imagine themselves to be, not just more rational than average, but paragons of rationality who other people should accept as such. I've encountered people talking as if it's ridiculous to suggest they might sometimes respond badly to being told the truth about certain subjects. I've encountered people asserting the rational superiority of themselves and others in the community for flimsy reasons, or no reason at all.</p>\n<p>Yet the readiness of members of the LessWrong community to disagree with and criticize each other suggests we don't actually think all that highly of each other's rationality. The fact that members of the LessWrong community tend to be smart is no guarantee that they will be rational. And we have much reason to fear \"rationality\" degenerating into signaling games.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"What_Disagreement_Signifies\">What Disagreement Signifies</h1>\n<p>Let's start by talking about disagreement. There's been a lot of <a href=\"http://wiki.lesswrong.com/wiki/Disagreement\">discussion of disagreement on LessWrong</a>, and in particular of <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann's agreement theorem</a>, often glossed as something like \"two rationalists can't agree to disagree.\" (Or perhaps that we can't <a href=\"http://www.overcomingbias.com/2007/01/we_cant_foresee.html\">foresee to disagree</a>.) Discussion of disagreement, however, tends to focus on what to do about it. I'd rather take a step back, and look at what disagreement tells us about ourselves: namely, that we don't think all that highly of each other's rationality.</p>\n<p>This, for me, is the take-away from Tyler Cowen and Robin Hanson's paper <a href=\"http://hanson.gmu.edu/deceive.pdf\">Are Disagreements Honest?</a>&nbsp;In the paper, Cowen and Hanson define honest disagreement as meaning that \"meaning that the disputants respect each other\u2019s relevant abilities, and consider each person\u2019s stated opinion to be his best estimate of the truth, given his information and effort,\" and they argue disagreements aren't honest in this sense.</p>\n<p>I don't find this conclusion surprising. In fact, I suspect that while people sometimes <em>do </em>mean it when they talk about respectful disagreement, often they realize this is a polite fiction (which isn't necessarily a bad thing). Deep down, they know that <a href=\"http://www.overcomingbias.com/2008/09/disagreement-is.html\">disagreement is disrespect</a>, at least in the sense of not thinking that highly of the other person's rationality.&nbsp;That people know this is shown in the fact that they don't like being told they're wrong\u2014the reason why <a href=\"/lw/j6o/according_to_dale_carnegie_you_cant_win_an/\">Dale Carnegie says you can't win an argument</a>.&nbsp;</p>\n<p>On LessWrong, people are quick to criticize each others' views, so much so that I've heard people cite this as a reason to be reluctant to post/comment (again showing they know intuitively that disagreement is disrespect). Furthermore when people in LessWrong criticize others' views, they very often don't seem to expect to quickly reach agreement. Even people Yvain would classify as <a href=\"/lw/jjd/rationalists_are_less_credulous_but_better_at/\">\"experienced rationalists\"</a>&nbsp;sometimes knowingly have persistent disagreements. This suggests that LessWrongers almost never consider each other to be perfect rationalists.</p>\n<p>And I actually think this is a sensible stance. For one thing, even if you met a perfect rationalist, it could be hard to figure out that they are one. Furthermore, the problem of knowing what to do about disagreement is made harder when you're faced with other people having persistent disagreements: if you find yourself agreeing with Alice, you'll have to think Bob is being irrational, and vice versa. If you rate them equally rational and adopt an intermediate view, you'll have to think they're both being a bit irrational for not doing likewise.</p>\n<p>The situation is similar to <a href=\"http://en.wikipedia.org/wiki/Moore's_paradox\">Moore's paradox</a> in philosophy\u2014the impossibility of asserting \"it's raining, but I don't believe it's raining.\" Or, as you might say, \"Of course I think my opinions are right and other people's are wrong. Otherwise I'd change my mind.\" Similarly, when we think about disagreement, it seems like we're forced to say, \"Of course I think my opinions are rational and other people's are irrational. Otherwise I'd change my mind.\"</p>\n<p>We can find some room for humility in an analog of the <a href=\"http://en.wikipedia.org/wiki/Preface_paradox\">preface paradox</a>, the fact that the author of a book can say things like \"any errors that remain are mine.\" We can say this because we might think each individual claim in the book is highly probable, while recognize that all the little uncertainties add up to it being likely there are still errors. Similarly, we can think each of our beliefs are individually rational, while recognizing we still probably have <em>some </em>irrational beliefs\u2014we just don't know which ones And just because respectful disagreement is a polite fiction doesn't mean we should abandon it.&nbsp;</p>\n<p>I don't have a clear sense of how controversial the above will be. Maybe we all already recognize that we don't respect each other's opinions 'round these parts. But I think some features of discussion at LessWrong look odd in light of the above points about disagreement\u2014including some of the things people say about disagreement.</p>\n<p>The <a href=\"http://wiki.lesswrong.com/wiki/Disagreement\">wiki</a>, for example, says that \"Outside of well-functioning prediction markets, Aumann agreement can probably only be approximated by careful deliberative discourse. Thus, fostering effective deliberation should be seen as a key goal of Less Wrong.\" The point of Aumann's agreement theorem, though, is precisely that ideal rationalists <em>shouldn't </em>need to engage in deliberative discourse, as usually conceived, in order to reach agreement.</p>\n<p>As Cowen and Hanson put it, \"Merely knowing someone else\u2019s opinion provides a powerful summary of everything that person knows, powerful enough to eliminate any differences of opinion due to differing information.\" So sharing evidence the normal way shouldn't be necessary. Asking someone \"what's the evidence for that?\" implicitly says, \"I don't trust your rationality enough to take your word for it.\" But when dealing with real people who may or may not have a rational basis for their beliefs, that's almost always the right stance to take.</p>\n<h1 id=\"Intelligence_and_Rationality\">Intelligence and Rationality</h1>\n<p>Intelligence does not equal rationality. Need I say more? Not long ago, I wouldn't have thought so. I would have thought it was a fundamental premise behind LessWrong, indeed behind old-school scientific skepticism. As Michael Shermer <a href=\"http://www.michaelshermer.com/weird-things/excerpt/\">once said</a>, \"Smart people believe weird things because they are skilled at defending beliefs they arrived at for non-smart reasons.\"</p>\n<p>Yet I've heard people suggest that you must never be dismissive of things said by smart people, or that the <a href=\"/lw/jj0/2013_survey_results/\">purportedly high IQ of the LessWrong community</a> means people here don't make bad arguments. When I hear that, I think \"<em>whaaat? </em>People on LessWrong make bad arguments all the time!\" When this happens, I&nbsp;<em>generally</em>&nbsp;limit myself to trying to point out the flaw in the argument and/or downvoting, and resist the urge to shout \"YOUR ARGUMENTS ARE BAD AND YOU SHOULD FEEL BAD.\" I just think it.</p>\n<p>When I reach for an explanation of why terrible arguments from smart people shouldn't surprise anyone, I go to Yvain's <a href=\"/lw/2pv/intellectual_hipsters_and_metacontrarianism/\">Intellectual Hipsters and Meta-Contarianism</a>, one of my favorite LessWrong posts of all time. While Yvain notes that meta-contrarianism often isn't a good thing, though, on re-reading it I noticed what seems like an important oversight:</p>\n<blockquote>\n<p>A person who is somewhat upper-class will conspicuously signal eir wealth by buying difficult-to-obtain goods. A person who is very upper-class will conspicuously signal that ey feels no need to conspicuously signal eir wealth, by deliberately not buying difficult-to-obtain goods.</p>\n<p>A person who is somewhat intelligent will conspicuously signal eir intelligence by holding difficult-to-understand opinions. A person who is very intelligent will conspicuously signal that ey feels no need to conspicuously signal eir intelligence, by deliberately not holding difficult-to-understand opinions.</p>\n<p>According to the survey, the average IQ on this site is around 145. People on this site differ from the mainstream in that they are more willing to say death is bad, more willing to say that science, capitalism, and the like are good, and less willing to say that there's some deep philosophical sense in which 1+1 = 3. That suggests people around that level of intelligence have reached the point where they no longer feel it necessary to differentiate themselves from the sort of people who aren't smart enough to understand that there might be side benefits to death.</p>\n</blockquote>\n<p>The pattern of <a href=\"http://www.bus.indiana.edu/riharbau/cs-randfinal.pdf\">countersignaling</a> Yvain describes here is real. But it's important not to forget that sometimes, the super-wealthy signal their wealth by buying things even the moderately wealthy can't afford. And sometimes, the very intelligent signal their intelligence by holding opinions even the moderately intelligent have trouble understanding. You also get hybrid status moves: designer versions of normally low-class clothes, complicated justifications for opinions normally found among the uneducated.</p>\n<p>Robin Hanson has <a href=\"http://www.overcomingbias.com/2009/11/why-academics-are-not-bayesian.html\">argued</a> that this leads to biases in academia:</p>\n<blockquote>\n<p>I\u2019ve argued that the main social function of academia is to let students, patrons, readers, etc. affiliate with credentialed-as-impressive minds. If so, academic beliefs are secondary \u2013 the important thing is to clearly show respect to those who make impressive displays like theorems or difficult data analysis. And the obvious way for academics to use their beliefs to show respect for impressive folks is to have academic beliefs track the most impressive recent academic work.</p>\n</blockquote>\n<p>Robin's post focuses on economics, but I suspect the problem is even worse in my home field of philosophy. As I've <a href=\"http://www.patheos.com/blogs/hallq/2012/06/from-the-archives-philosophy-is-dysfunctional/\">written</a> <a href=\"http://www.patheos.com/blogs/hallq/2013/01/from-the-archives-seduced-by-sophistication-follow-up-to-philosophy-is-dysfunctional/\">before</a>, the problem is that in philosophy, philosophers never agree on whether a philosopher has solved a problem. Therefore, there can be no rewards for being right, only rewards for showing off your impressive intellect. This often means finding clever ways to be wrong.</p>\n<p>I need to emphasize that I really do think philosophers are showing off real intelligence, not merely showing off faux-cleverness. GRE scores suggest <a href=\"http://blogs.discovermagazine.com/gnxp/2010/12/verbal-vs-mathematical-aptitude-in-academics/\">philosophers are among the smartest academics</a>, and their performance is arguably made more impressive by the fact that GRE quant scores are bimodally distributed based on whether your major required you to spend four years practicing your high school math, with philosophy being one of the majors that doesn't grant that advantage. Based on this, if you think it's wrong to dismiss the views of high-IQ people, you shouldn't be dismissive of mainstream philosophy. But in fact I think LessWrong's <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">oft-noticed dismissiveness of mainstream philosophy</a>&nbsp;is largely justified.</p>\n<p>I've found philosophy of religion in particular to be a goldmine of terrible arguments made by smart people. Consider <a href=\"http://www.patheos.com/blogs/hallq/2013/01/the-ontological-argument-in-brief/\">Alvin Plantinga's modal ontological argument</a>. The argument is sufficiently difficult to understand that I won't try to explain it here. If you want to understand it, I'm not sure what to tell you except to maybe read Plantinga's book&nbsp;<em><a href=\"http://www.amazon.com/Nature-Necessity-Clarendon-Library-Philosophy-ebook/dp/B0041OTAGG/ref=sr_1_1?ie=UTF8&amp;qid=1393634681&amp;sr=8-1&amp;keywords=Nature+of+Necessity\">The Nature of Necessity</a>.&nbsp;</em>In fact, I predict at least one LessWronger will comment on this thread with an incorrect explanation or criticism of the argument. Which is not to say they wouldn't be smart enough to understand it, just that it might take them a few iterations of getting it wrong to finally get it right. And coming up with an argument like that is no mean feat\u2014I'd guess Plantinga's IQ is just as high as the average LessWronger's.</p>\n<p>Once you understand the modal ontological argument, though, it quickly becomes obvious that Plantinga's logic works just as well to \"prove\" that it's a necessary truth that pigs fly. Or that Plantinga's god does not exist. Or even as a general purpose \"proof\" of any purported mathematical truth you please.&nbsp;The main point is that Plantinga's argument is not stupid in the sense of being something you'd only come up with if you had a low IQ\u2014the opposite is true. But Plantinga's argument <em>is</em> stupid in the sense of being something you'd only come up with it while under the influence of some serious motivated reasoning.</p>\n<p>The modal ontological argument is admittedly an extreme case. Rarely is the chasm between the difficulty of the concepts underlying an argument, and the argument's actual merits, so vast. Still, beware the temptation to affiliate with smart people by taking everything they say seriously.</p>\n<p><strong id=\"Edited_to_add__in_the_original_post__I_intended_but_forgot_to_emphasize_that_I_think_the_correlation_between_IQ_and_rationality_is_weak_at_best__Do_people_disagree__Does_anyone_want_to_go_out_on_a_limb_and_say___They_aren_t_the_same_thing__but_the_correlation_is_still_very_strong__\">Edited to add: in the original post, I intended but forgot to emphasize that I think the correlation between IQ and rationality is weak <em>at best. </em>Do people disagree? Does anyone want to go out on a limb and say, \"They aren't the same thing, but the correlation is still very strong?\"</strong></p>\n<h1 id=\"The_Principle_of_Charity\">The Principle of Charity</h1>\n<p>I've made no secret of the fact that I'm not a big fan of the principle of charity\u2014often defined as the rule that you should interpret other people's arguments on the assumption that they are not saying anything stupid. The problem with this is that other people are often saying something stupid. Because of that, I think charitable is over-rated compared to fair and accurate reading. When someone says something stupid, you don't have to pretend otherwise, but it's really important not to attribute to people stupid things they never said.</p>\n<p>More frustrating than this simple disagreement over charity, though, is when people who invoke the principle of charity do so selectively. They apply it to people who's views they're at least somewhat sympathetic to, but when they find someone they want to attack, they have trouble meeting basic standards of fairness. And in the most frustrating cases, this gets explicit justification: \"we need to read <em>these </em>people charitably, because they are obviously very intelligent and rational.\" I once had a member of the LessWrong community actually tell me, \"You need to interpret me more charitably, because you know I'm sane.\" \"Actually, buddy, I don't know that,\" I wanted to reply\u2014but didn't, because that would've been rude.</p>\n<p>I can see benefits to the principle of charity. It helps avoid flame wars, and from a Machiavellian point of view it's nice to close off the \"what I actually meant was...\" responses. Whatever its merits, though, they can't depend on the actual intelligence and rationality of the person making an argument. Not only is intelligence no guarantee against making bad arguments, the whole reason we demand other people tell us their reasons for their opinions in the first place is we fear their reasons <em>might </em>be bad ones.</p>\n<p>As I've already explained, there's a difficult problem here about how to be appropriately modest about our own rationality. When I say something, I never think it's stupid, otherwise I wouldn't say it. But at least I'm not so arrogant as to go around demanding other people acknowledge my highly advanced rationality. I don't demand that they accept \"Chris isn't saying anything stupid\" as an axiom in order to engage with me.</p>\n<h1 id=\"Beware_Weirdness_for_Weirdness__Sake\">Beware Weirdness for Weirdness' Sake</h1>\n<p>There's a <a href=\"http://en.wikipedia.org/wiki/Signalling_theory#Religion_as_a_costly_signal\">theory</a> in the psychology and sociology of religion that the purpose of seemingly foolish rituals like circumcision and snake-handling is to provide a costly and therefore hard-to-fake signal of group commitment. I think I've heard it suggested\u2014though I can't find by who\u2014that crazy religious doctrines could serve a similar purpose. It's easy to say you believe in a god, but being willing to risk ridicule by saying you believe in one god who is three persons, who are all the same god, yet not identical to each other, and you can't explain how that is but it's a mystery you accept on faith... now <em>that </em>takes dedication.</p>\n<p>Once you notice the general \"signal group commitment in costly ways\" strategy, it seems to crop up everywhere. Subcultures often seem to go out of their way to be weird, to do things that will shock people outside the subculture, ranging from tattoos and weird clothing to coming up with reasons why things regarded as normal and innocuous in the broader culture are actually evil. Even something as simple as a large body of jargon and in-jokes can do the trick: if someone takes the time to learn all the jargon and in-jokes, you know they're committed.</p>\n<p>This tendency is probably harmless when done with humor and self-awareness, but it's more worrisome when a group becomes convinced its little bits of weirdness for weirdness' sake are a sign of its superiority to other groups. And it's worth being aware of, because it makes sense of signaling moves that aren't straightforwardly plays for higher status.</p>\n<p>The LessWrong community has amassed a truly impressive store of jargon and in-jokes over the years, and some of it's quite useful (I reiterate my love for the term \"meta-contrarian\"). But as with all jargon, LessWrongian jargon is often just a silly way of saying things you could have said without it. For example, people say \"I have a poor mental model of...\" when they could have just said they don't understand it very well.</p>\n<p>That bit of LessWrong jargon is merely silly. Worse, I think, is the jargon around politics. Recently, a friend gave \"they avoid blue-green politics\" as a reason LessWrongians are more rational than other people. It took a day before it clicked that \"blue-green politics\" here basically just meant \"partisanship.\" But complaining about partisanship is old hat\u2014literally. <a href=\"http://en.wikipedia.org/wiki/Political_parties_in_the_United_States#History\">America's</a> <a href=\"http://www.washingtonsblog.com/2011/07/the-founding-fathers-tried-to-warn-us-about-the-threat-from-a-two-party-system.html\">founders</a>&nbsp;were fretting about it back in the 18th century. Nowadays, such worries are something you expect to hear from boringly middle-brow columnists at major newspapers, not edgy contrarians.</p>\n<p>But \"blue-green politics,\" \"politics is the mind-killer\"... never mind how much content they add, the point is they're obscure enough to work as an excuse to feel superior to anyone whose political views are too mainstream. Outsiders will probably think you're weird, invoking obscure jargon to quickly dismiss ideas that seem plausible to them, but on the upside you'll get to bond with members of your in-group over your feelings of superiority.</p>\n<h1 id=\"A_More_Humble_Rationalism_\">A More Humble Rationalism?</h1>\n<p>I feel like I should wrap up with some advice. Unfortunately, this post was motivated by problems I'd seen, not my having thought of brilliant solutions to them. So I'll limit myself to some fairly boring, non-brilliant advice.</p>\n<p>First, yes, some claims are more rational than others. Some people even do better at rationality overall than others. But the idea of a real person being anything close to an ideal rationalist is an extraordinary claim, and should be met with appropriate skepticism and demands for evidence. Don't forget that.</p>\n<p>Also, beware signaling games. A good dose of Hansonian cynicism, applied to your own in-group, is healthy. Somewhat relatedly, I've begun to wonder if \"rationalism\" is really good branding for a movement. <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">Rationality is systematized winning</a>, sure, but the \"rationality\" branding isn't as good for keeping that front and center, especially compared to, say the effective altruism meme. It's just a little too easy to forget where \"rationality\" is supposed to connect with the real world, increasing the temptation for \"rationality\" to spiral off into signaling games.</p>", "sections": [{"title": "What Disagreement Signifies", "anchor": "What_Disagreement_Signifies", "level": 1}, {"title": "Intelligence and Rationality", "anchor": "Intelligence_and_Rationality", "level": 1}, {"title": "Edited to add: in the original post, I intended but forgot to emphasize that I think the correlation between IQ and rationality is weak at best. Do people disagree? Does anyone want to go out on a limb and say, \"They aren't the same thing, but the correlation is still very strong?\"", "anchor": "Edited_to_add__in_the_original_post__I_intended_but_forgot_to_emphasize_that_I_think_the_correlation_between_IQ_and_rationality_is_weak_at_best__Do_people_disagree__Does_anyone_want_to_go_out_on_a_limb_and_say___They_aren_t_the_same_thing__but_the_correlation_is_still_very_strong__", "level": 2}, {"title": "The Principle of Charity", "anchor": "The_Principle_of_Charity", "level": 1}, {"title": "Beware Weirdness for Weirdness' Sake", "anchor": "Beware_Weirdness_for_Weirdness__Sake", "level": 1}, {"title": "A More Humble Rationalism?", "anchor": "A_More_Humble_Rationalism_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "395 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 395, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HxWdXMqoQtjDhhNGA", "Gh2qQHrCg3teQen3c", "pJJdcZgB6mPNWoSWr", "9kcTNWopvXFncXgPy", "FwiPfF8Woe5JrzqEu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-01T15:34:22.614Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes March 2014", "slug": "rationality-quotes-march-2014", "viewCount": null, "lastCommentedAt": "2021-04-18T14:50:50.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malcolmocean", "createdAt": "2012-05-16T15:13:58.012Z", "isAdmin": false, "displayName": "MalcolmOcean"}, "userId": "urN5htciqMuEeghc4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sxxNZwoe6pRYtHDMk/rationality-quotes-march-2014", "pageUrlRelative": "/posts/sxxNZwoe6pRYtHDMk/rationality-quotes-march-2014", "linkUrl": "https://www.lesswrong.com/posts/sxxNZwoe6pRYtHDMk/rationality-quotes-march-2014", "postedAtFormatted": "Saturday, March 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20March%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20March%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsxxNZwoe6pRYtHDMk%2Frationality-quotes-march-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20March%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsxxNZwoe6pRYtHDMk%2Frationality-quotes-march-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsxxNZwoe6pRYtHDMk%2Frationality-quotes-march-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sxxNZwoe6pRYtHDMk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 1.58905408121224e-06, "legacy": true, "legacyId": "25654", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 331, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-01T15:49:07.367Z", "modifiedAt": null, "url": null, "title": "March 2014 Media Thread", "slug": "march-2014-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:35.692Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LqDmBuLzf2338DePm/march-2014-media-thread", "pageUrlRelative": "/posts/LqDmBuLzf2338DePm/march-2014-media-thread", "linkUrl": "https://www.lesswrong.com/posts/LqDmBuLzf2338DePm/march-2014-media-thread", "postedAtFormatted": "Saturday, March 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20March%202014%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMarch%202014%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqDmBuLzf2338DePm%2Fmarch-2014-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=March%202014%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqDmBuLzf2338DePm%2Fmarch-2014-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqDmBuLzf2338DePm%2Fmarch-2014-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LqDmBuLzf2338DePm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.5890716581322704e-06, "legacy": true, "legacyId": "25655", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-01T16:57:02.373Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Dog Clickers", "slug": "meetup-washington-dc-dog-clickers", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bz6YX9qmXbuzEkBPM/meetup-washington-dc-dog-clickers", "pageUrlRelative": "/posts/Bz6YX9qmXbuzEkBPM/meetup-washington-dc-dog-clickers", "linkUrl": "https://www.lesswrong.com/posts/Bz6YX9qmXbuzEkBPM/meetup-washington-dc-dog-clickers", "postedAtFormatted": "Saturday, March 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Dog%20Clickers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Dog%20Clickers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBz6YX9qmXbuzEkBPM%2Fmeetup-washington-dc-dog-clickers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Dog%20Clickers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBz6YX9qmXbuzEkBPM%2Fmeetup-washington-dc-dog-clickers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBz6YX9qmXbuzEkBPM%2Fmeetup-washington-dc-dog-clickers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xf'>Washington DC: Dog Clickers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 March 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to see if we can train people to do things with dog clickers.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xf'>Washington DC: Dog Clickers</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bz6YX9qmXbuzEkBPM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5891526186143859e-06, "legacy": true, "legacyId": "25656", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Dog_Clickers\">Discussion article for the meetup : <a href=\"/meetups/xf\">Washington DC: Dog Clickers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 March 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to see if we can train people to do things with dog clickers.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Dog_Clickers1\">Discussion article for the meetup : <a href=\"/meetups/xf\">Washington DC: Dog Clickers</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Dog Clickers", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Dog_Clickers", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Dog Clickers", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Dog_Clickers1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-01T23:57:53.846Z", "modifiedAt": null, "url": null, "title": "Polling Thread", "slug": "polling-thread-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:05.313Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NwEYHSbvDExRKwzoL/polling-thread-1", "pageUrlRelative": "/posts/NwEYHSbvDExRKwzoL/polling-thread-1", "linkUrl": "https://www.lesswrong.com/posts/NwEYHSbvDExRKwzoL/polling-thread-1", "postedAtFormatted": "Saturday, March 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Polling%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolling%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwEYHSbvDExRKwzoL%2Fpolling-thread-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Polling%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwEYHSbvDExRKwzoL%2Fpolling-thread-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwEYHSbvDExRKwzoL%2Fpolling-thread-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>This is the second installment of the <a href=\"/lw/jju/polling_thread/\">Polling Thread</a>.</p>\n<p>This is your chance to ask your multiple choice question you always wanted to throw in. Get qualified numeric feedback to your comments. Post fun polls.</p>\n<p>There are some rules:</p>\n<ol>\n<li>Each poll goes into its own top level comment and may be commented there.</li>\n<li>You must at least vote all polls that were posted earlier than you own. This ensures participation in all polls and also limits the total number of polls. You may of course vote without posting a poll.</li>\n<li>Your poll should include a 'don't know' option (to avoid conflict with 2). I don't know whether we need to add a troll catch option here but we will see.</li>\n</ol>\n<p>If you don't know how to make a poll in a comment look at the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting#Polls\">Poll Markup Help</a>.</p>\n<hr />\n<p>This is not (yet?) a regular thread. If it is successful I may post again. Or you may. In that case do the following :</p>\n<ul>\n<li>Use \"Polling Thread\" in the title.</li>\n<li>Copy the rules.</li>\n<li>Add the tag \"poll\".</li>\n<li>Link to this Thread or a previous Thread.</li>\n<li>Create a top-level comment saying 'Discussion of this thread goes here; all other top-level comments should be polls or similar'</li>\n<li>Add a second top-level comment with an initial poll to start participation.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NwEYHSbvDExRKwzoL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 1.5896544671074852e-06, "legacy": true, "legacyId": "25658", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sqeXx4dPiRixdRxdb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T02:31:00.123Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal: Let's relax and hang out", "slug": "meetup-montreal-let-s-relax-and-hang-out", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oriane", "createdAt": "2013-06-15T06:22:47.851Z", "isAdmin": false, "displayName": "Oriane"}, "userId": "aKb3Xf7wYLxw7mQjC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hfjQFN58Pwcp6EqYd/meetup-montreal-let-s-relax-and-hang-out", "pageUrlRelative": "/posts/hfjQFN58Pwcp6EqYd/meetup-montreal-let-s-relax-and-hang-out", "linkUrl": "https://www.lesswrong.com/posts/hfjQFN58Pwcp6EqYd/meetup-montreal-let-s-relax-and-hang-out", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%3A%20Let's%20relax%20and%20hang%20out&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%3A%20Let's%20relax%20and%20hang%20out%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfjQFN58Pwcp6EqYd%2Fmeetup-montreal-let-s-relax-and-hang-out%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%3A%20Let's%20relax%20and%20hang%20out%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfjQFN58Pwcp6EqYd%2Fmeetup-montreal-let-s-relax-and-hang-out", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhfjQFN58Pwcp6EqYd%2Fmeetup-montreal-let-s-relax-and-hang-out", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xg'>Montreal: Let's relax and hang out</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 March 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4109 Ch. de la C\u00f4te des Neiges, Apt 14</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup.com is currently not working so here are the infos for tomorrow's social meetup. Feel free to bring drinks and snacks. Call me if you get lost: 514 582 1052</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xg'>Montreal: Let's relax and hang out</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hfjQFN58Pwcp6EqYd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5898371049666424e-06, "legacy": true, "legacyId": "25660", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal__Let_s_relax_and_hang_out\">Discussion article for the meetup : <a href=\"/meetups/xg\">Montreal: Let's relax and hang out</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 March 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4109 Ch. de la C\u00f4te des Neiges, Apt 14</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup.com is currently not working so here are the infos for tomorrow's social meetup. Feel free to bring drinks and snacks. Call me if you get lost: 514 582 1052</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal__Let_s_relax_and_hang_out1\">Discussion article for the meetup : <a href=\"/meetups/xg\">Montreal: Let's relax and hang out</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal: Let's relax and hang out", "anchor": "Discussion_article_for_the_meetup___Montreal__Let_s_relax_and_hang_out", "level": 1}, {"title": "Discussion article for the meetup : Montreal: Let's relax and hang out", "anchor": "Discussion_article_for_the_meetup___Montreal__Let_s_relax_and_hang_out1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T03:57:53.052Z", "modifiedAt": null, "url": null, "title": "[link] Shulman & Bostrom: \"Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?\"", "slug": "link-shulman-and-bostrom-embryo-selection-for-cognitive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:02.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7TBZxpvgQMLizXdmn/link-shulman-and-bostrom-embryo-selection-for-cognitive", "pageUrlRelative": "/posts/7TBZxpvgQMLizXdmn/link-shulman-and-bostrom-embryo-selection-for-cognitive", "linkUrl": "https://www.lesswrong.com/posts/7TBZxpvgQMLizXdmn/link-shulman-and-bostrom-embryo-selection-for-cognitive", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Shulman%20%26%20Bostrom%3A%20%22Embryo%20Selection%20for%20Cognitive%20Enhancement%3A%20Curiosity%20or%20Game-changer%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Shulman%20%26%20Bostrom%3A%20%22Embryo%20Selection%20for%20Cognitive%20Enhancement%3A%20Curiosity%20or%20Game-changer%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TBZxpvgQMLizXdmn%2Flink-shulman-and-bostrom-embryo-selection-for-cognitive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Shulman%20%26%20Bostrom%3A%20%22Embryo%20Selection%20for%20Cognitive%20Enhancement%3A%20Curiosity%20or%20Game-changer%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TBZxpvgQMLizXdmn%2Flink-shulman-and-bostrom-embryo-selection-for-cognitive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TBZxpvgQMLizXdmn%2Flink-shulman-and-bostrom-embryo-selection-for-cognitive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>Carl Shulman &amp; Nick Bostrom, <a href=\"http://www.stafforini.com/txt/Shulman &amp; Bostrom - Embryo selection for cognitive enhancement.pdf\">Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?</a>, <em>Global Policy</em>, vol. 5, no. 1 (February, 2014), pp. 85&ndash;92.</p>\n<p>Abstract:</p>\n<blockquote>\n<p>Human capital is an important determinant of individual and aggregate economic outcomes, and a major input to scientific progress. It has been suggested that advances in genomics may open up new avenues to enhance human intellectual abilities genetically, complementing environmental interventions such as education and nutrition. One way to do this would be via embryo selection in the context of in vitro fertilization (IVF). In this article, we analyze the feasibility, timescale, and possible societal impacts of embryo selection for cognitive enhancement. We find that embryo selection, on its own, may have significant (but likely not drastic) impacts over the next 50 years, though large effects could accumulate over multiple generations. However, there is a complementary technology &ndash; stem cell-derived gametes &ndash; which has been making rapid progress and which could amplify the impact of embryo selection, enabling very large changes if successfully applied to humans.</p>\n</blockquote>\n<p>An earlier version of this paper has been available for some time at Bostrom's website. &nbsp;Here are <a href=\"https://plus.google.com/103530621949492999968/posts/Z8wRcsWtgot\">some quotes</a> from that version, courtesy of gwern. And here are&nbsp;<a href=\"http://intelligence.org/2013/10/30/new-paper-embryo-selection-for-cognitive-enhancement/\">some comments</a>&nbsp;by Luke Muehlhauser.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7TBZxpvgQMLizXdmn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "25661", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T06:00:10.483Z", "modifiedAt": null, "url": null, "title": "Find a study partner - March 2014 thread", "slug": "find-a-study-partner-march-2014-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.873Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MathieuRoy", "createdAt": "2013-06-16T02:41:27.071Z", "isAdmin": false, "displayName": "Mati_Roy"}, "userId": "Tw9etd8rMnHLeSQ9q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CdoEXPecZgWYf7Hd7/find-a-study-partner-march-2014-thread", "pageUrlRelative": "/posts/CdoEXPecZgWYf7Hd7/find-a-study-partner-march-2014-thread", "linkUrl": "https://www.lesswrong.com/posts/CdoEXPecZgWYf7Hd7/find-a-study-partner-march-2014-thread", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Find%20a%20study%20partner%20-%20March%202014%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFind%20a%20study%20partner%20-%20March%202014%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCdoEXPecZgWYf7Hd7%2Ffind-a-study-partner-march-2014-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Find%20a%20study%20partner%20-%20March%202014%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCdoEXPecZgWYf7Hd7%2Ffind-a-study-partner-march-2014-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCdoEXPecZgWYf7Hd7%2Ffind-a-study-partner-march-2014-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 17pt; text-align: justify;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Verdana; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">This is the monthly thread to find a study partner. </span></p>\n<p><strong id=\"docs-internal-guid-5b9b7024-8159-6d1b-6458-91d90b8e330a\" style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Verdana; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">For reasons mentioned in </span><a style=\"text-decoration: none;\" href=\"/lw/j10/on_learning_difficult_things/\"><span style=\"font-size: 15px; font-family: Verdana; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">So8res article</span></a><span style=\"font-size: 15px; font-family: Verdana; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> as well as for other reasons: studying with a partner can be very good. </span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Verdana; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">So if you're looking for a study partner for an online course, reading a manual or else (whether it's in the </span><a style=\"text-decoration: none;\" href=\"http://intelligence.org/courses/\"><span style=\"font-size: 15px; font-family: Verdana; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">MIRI course list</span></a><span style=\"font-size: 15px; font-family: Verdana; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> or not) tell others in the comment section.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Verdana; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The past treads about finding a study partner can be found under the tag </span><a style=\"text-decoration: none;\" href=\"/r/discussion/tag/study_thread/\"><span style=\"font-size: 15px; font-family: Verdana; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">study_thread</span></a><span style=\"font-size: 15px; font-family: Verdana; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. However, you have higher probabilities of finding a study partner in the most recent thread. If you haven't found a study partner last month, you are welcome to post the same comment again here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CdoEXPecZgWYf7Hd7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.5900866862180036e-06, "legacy": true, "legacyId": "25662", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w5F4w8tNZc6LcBKRP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T13:14:45.402Z", "modifiedAt": null, "url": null, "title": "LINK-Misunderstanding risk of murder reincidence", "slug": "link-misunderstanding-risk-of-murder-reincidence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:02.620Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZMHz2QkvLKQavDS4K/link-misunderstanding-risk-of-murder-reincidence", "pageUrlRelative": "/posts/ZMHz2QkvLKQavDS4K/link-misunderstanding-risk-of-murder-reincidence", "linkUrl": "https://www.lesswrong.com/posts/ZMHz2QkvLKQavDS4K/link-misunderstanding-risk-of-murder-reincidence", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK-Misunderstanding%20risk%20of%20murder%20reincidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK-Misunderstanding%20risk%20of%20murder%20reincidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZMHz2QkvLKQavDS4K%2Flink-misunderstanding-risk-of-murder-reincidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK-Misunderstanding%20risk%20of%20murder%20reincidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZMHz2QkvLKQavDS4K%2Flink-misunderstanding-risk-of-murder-reincidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZMHz2QkvLKQavDS4K%2Flink-misunderstanding-risk-of-murder-reincidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>People don't want this institutionalized murderer to get free because they don't understand there's no such thing as \"zero risk.\"</p>\n<p>&nbsp;</p>\n<p>http://www.sunnewsnetwork.ca/sunnews/straighttalk/archives/2014/02/20140228-142409.html</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZMHz2QkvLKQavDS4K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -5, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "25663", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T15:57:53.578Z", "modifiedAt": null, "url": null, "title": "Learning languages efficiently.", "slug": "learning-languages-efficiently", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:05.234Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Fivehundred", "createdAt": "2013-11-25T00:28:49.158Z", "isAdmin": false, "displayName": "Fivehundred"}, "userId": "6fnW5xaeQMzhnjjmG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LeK7Zf5jxtqXeSc4v/learning-languages-efficiently", "pageUrlRelative": "/posts/LeK7Zf5jxtqXeSc4v/learning-languages-efficiently", "linkUrl": "https://www.lesswrong.com/posts/LeK7Zf5jxtqXeSc4v/learning-languages-efficiently", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learning%20languages%20efficiently.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearning%20languages%20efficiently.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeK7Zf5jxtqXeSc4v%2Flearning-languages-efficiently%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learning%20languages%20efficiently.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeK7Zf5jxtqXeSc4v%2Flearning-languages-efficiently", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeK7Zf5jxtqXeSc4v%2Flearning-languages-efficiently", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>I'm not at all sure how this site works yet (I've gone only on traditional forums), so bear with me please if I do something foolish. I'm being drafted to the IDF in a few months and I need to learn Hebrew <em>very quickly</em> if I want to avoid being put into a program for foreign speakers. I currently reside in the US, but I've previously lived in (and have citizenship of) both countries.</p>\n<p>After experiencing the government-sponsored Hebrew programs, I totally refuse to accept such a ridiculously inefficient and traumatic method of teaching a language. When I get enlisted, I'll want to focus whatever little time I have left on studying more important things. Something that will damage me psychologically, not to mention take up huge amounts of time and effort, will take away any opportunity I might get.</p>\n<p>I can speak a few basic phrases in Hebrew and and can understand a bit more. Immersion is not an option for me currently. My attempts at teaching myself the language have been stunningly misguided (which is to say, like reading Atlas Shrugged to get a proper understanding of Objectivism) and I'm not interested in a lengthy trial and error process. Obviously getting literature on language acquisition is out of the question. I wouldn't even know where to start.</p>\n<p>So, I'd just like some methods or heuristics for picking up languages as fast as possible. (I am extremely literate, so there's that.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LeK7Zf5jxtqXeSc4v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.590800255633355e-06, "legacy": true, "legacyId": "25664", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T17:56:23.442Z", "modifiedAt": null, "url": null, "title": "Solomonoff Cartesianism", "slug": "solomonoff-cartesianism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:35.827Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AszKwKyhBPZAnCstA/solomonoff-cartesianism", "pageUrlRelative": "/posts/AszKwKyhBPZAnCstA/solomonoff-cartesianism", "linkUrl": "https://www.lesswrong.com/posts/AszKwKyhBPZAnCstA/solomonoff-cartesianism", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solomonoff%20Cartesianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolomonoff%20Cartesianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAszKwKyhBPZAnCstA%2Fsolomonoff-cartesianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solomonoff%20Cartesianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAszKwKyhBPZAnCstA%2Fsolomonoff-cartesianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAszKwKyhBPZAnCstA%2Fsolomonoff-cartesianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7583, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a>;&nbsp;<a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">An Intuitive Explanation of Solomonoff Induction</a>;&nbsp;<a href=\"/lw/on/reductionism/\">Reductionism</a></p>\n<p><strong>Summary</strong>: If you want to predict arbitrary computable patterns of data, Solomonoff induction is the optimal way to go about it &mdash; provided that&nbsp;you're an eternal transcendent hypercomputer. A real-world AGI, however, won't be immortal and unchanging. It will need to form hypotheses about its own physical state, including predictions about possible upgrades or damage to its hardware; and it will need <a href=\"/lw/jd9/building_phenomenological_bridges/&lrm;\">bridge hypotheses</a> linking its hardware states to its software states. As such, the project of building an AGI demands that we come up with a new formalism for constructing (and allocating prior probabilities to) hypotheses. It will not involve just building increasingly good computable approximations of AIXI.</p>\n<hr />\n<p>&nbsp;</p>\n<p><strong><a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff induction</a></strong>&nbsp;has been cited repeatedly as the theoretical gold standard for predicting computable sequences of observations.<a name=\"footnote1aback\"></a><sup><a href=\"#footnote1\">1</a></sup> As Hutter, Legg, and Vitanyi (2007) put it:</p>\n<blockquote>\n<p>Solomonoff's inductive inference system will learn to correctly predict any computable sequence with only the absolute minimum amount of data. It would thus, in some sense, be the perfect universal prediction algorithm, if only it were computable.</p>\n</blockquote>\n<p>Perhaps you've been handed the beginning of a sequence like 1, 2, 4, 8&hellip; and you want to predict what the next number will be. Perhaps you've paused a movie, and are trying to guess what the next frame will look like. Or perhaps you've read the first half of an article on the Algerian Civil War, and you want to know how likely it is that the second half describes a decrease in GDP. Since all of the information in these scenarios can be represented as patterns of numbers, they can all be treated as rule-governed sequences like the 1, 2, 4, 8&hellip; case. Complicated sequences, but sequences all the same.</p>\n<p>It's been argued that in all of these cases, one unique idealization predicts what comes next better than any computable method: Solomonoff induction. No matter how limited your knowledge is, or how wide the space of computable rules that could be responsible for your observations, the ideal answer is always the same: Solomonoff induction.</p>\n<p>Solomonoff induction has only a few components. It has one free parameter, a choice of universal Turing machine. Once we specify a Turing machine, that gives us a fixed encoding for the set of all possible programs that print a sequence of 0s and 1s. Since every program has a specification, we call the number of bits in the program's specification its \"<a href=\"http://wiki.lesswrong.com/wiki/Algorithmic_complexity\">complexity</a>\"; the shorter the program's code, the simpler we say it is.</p>\n<p>Solomonoff induction takes this infinitely large bundle of programs and assigns each one a prior probability proportional to its simplicity. Every time the program requires one more bit, its prior probability goes down by a factor of 2, since there are then twice as many possible computer programs that complicated. This ensures the sum over all programs' prior probabilities equals 1, even though the number of programs is infinite.<a name=\"footnote2back\"></a><sup><a href=\"#footnote2\">2</a></sup></p>\n<p><a id=\"more\"></a></p>\n<p>The imaginary inductor is then fed a sequence of 0s and 1s, and with each new bit it updates <a href=\"/lw/di3/how_bayes_theorem_is_consistent_with_solomonoff/\">using Bayes' rule</a>&nbsp;to promote programs whose outputs match the observed sequence. So, where <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\ell\" target=\"_blank\"><img title=\"\\ell\" src=\"http://latex.codecogs.com/gif.latex?\\ell\" alt=\"\" /></a> is the length of a program <sub><a href=\"http://www.codecogs.com/eqnedit.php?latex=q\" target=\"_blank\"><img title=\"q\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\" /></a> </sub>that makes a universal Turing machine &nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=U\" target=\"_blank\"><img title=\"U\" src=\"http://latex.codecogs.com/gif.latex?U\" alt=\"\" /></a> output a binary sequence that begins with the string <a href=\"http://www.codecogs.com/eqnedit.php?latex=s\" target=\"_blank\"><img title=\"s\" src=\"http://latex.codecogs.com/gif.latex?s\" alt=\"\" /></a>, Solomonoff defines the relative probability&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=P\" target=\"_blank\"><img title=\"P\" src=\"http://latex.codecogs.com/gif.latex?P\" alt=\"\" /></a>&nbsp;that&nbsp;&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=U\" target=\"_blank\"><img title=\"U\" src=\"http://latex.codecogs.com/gif.latex?U\" alt=\"\" /></a>&nbsp;outputs&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=s\" target=\"_blank\"><img title=\"s\" src=\"http://latex.codecogs.com/gif.latex?s\" alt=\"\" /></a>:</p>\n<p>&nbsp;</p>\n<p align=\"center\"><a href=\"http://www.codecogs.com/eqnedit.php?latex=P^U(s)\\;\\;:=\\;\\;\\;\\underset{U(q)=s...}{\\sum}2^{-\\ell(q)}\" target=\"_blank\"><img title=\"P^U(s)\\;\\;:=\\;\\;\\;\\underset{U(q)=s...}{\\sum}2^{-\\ell(q)}\" src=\"http://latex.codecogs.com/gif.latex?P^U(s)\\;\\;:=\\;\\;\\;\\underset{U(q)=s...}{\\sum}2^{-\\ell(q)}\" alt=\"\" /></a></p>\n<p>&nbsp;</p>\n<p>Solomonoff induction isn't computable, but it's been singled out as the unbeatable formal predictor of computable sequences.<a name=\"footnote3aback\"></a><sup><a href=\"#footnote3\">3</a>&nbsp;</sup>All computable rules for generating sequences are somewhere within Solomonoff's gargantuan bundle of programs. This includes all rules that a human brain could use. If the rule that best matches the observations is 1000 bits large, it will take at most <a href=\"/lw/jn/how_much_evidence_does_it_take/\">1000 bits of evidence</a> &mdash; 1000 bits worth of predictions made better than any other rule &mdash; for that rule to be promoted to the top of consideration. Solomonoff's claim to being an optimal ideal rule rests on the fact that it never does worse than any computable rule (including you!) by more than a fixed amount.<a name=\"footnote4aback\"></a><sup><a href=\"#footnote4\">4</a></sup></p>\n<p>&nbsp;</p>\n<p><a name=\"who\"></a></p>\n<h3>Who cares, if we can't build the thing?</h3>\n<p>Encouraged by Solomonoff inductors' optimality properties, some have suggested that building a working AGI calls for little more than finding out which computable algorithm comes as close to Solomonoff induction as possible given resource constraints, and supplying an adequate learning environment and decision criterion.<a name=\"footnote5back\"></a><sup><a href=\"#footnote5\">5</a></sup></p>\n<p>Eliezer Yudkowsky thinks that these attempts to approximate Solomonoff are a dead end. Much of the difficulty of <a href=\"/lw/vb/efficient_crossdomain_optimization/\">intelligence</a> rests on computing things cheaply, and Yudkowsky doesn't think that the kind of search these algorithms are doing will zero in on cheap ways to reason. There are practical lessons to be learned from Solomonoff induction, but the particular kind of optimality Solomonoff induction exhibits depends in important ways on its computational unfeasibility,<a name=\"footnote4bback\"></a><span style=\"font-size: 11px;\"><sup><a href=\"#footnote4\">4</a></sup></span>&nbsp;which makes it unlikely that Solomonoff imitators will ever be efficient reasoners.</p>\n<p>Why, then, should Solomonoff induction interest us? If we can't execute it, and we can't design useful AGIs by <em>directly</em> emulating it, then what's it good for?</p>\n<p>My answer is that if Solomonoff induction&nbsp;<em>would</em> deliver flawless answers, could we but run it, then it has a claim to being an ideal mirror to which we can hold up instances of human and artificial inductive reasoning.</p>\n<p><span style=\"font-size: small;\">In </span><a style=\"font-size: small;\" href=\"http://intelligence.org/2013/11/04/from-philosophy-to-math-to-engineering/\">From Philosophy to Math to Engineering</a><span style=\"font-size: small;\">, Muehlhauser talks about how ideas often progress from productive but informal ruminations ('philosophy'), to rigorously specified idealizations ('mathematics'), to functioning technologies ('engineering').</span></p>\n<p>Solomonoff inductors fall into the second category, 'mathematics': We could never build them, but thinking in terms of them can give us useful insights and point us in the right direction. For example, Solomonoff's ideal can remind us that <a href=\"/lw/jp/occams_razor/\">privileging simple hypotheses</a> isn't just a vague human fancy or quirk; it has formalizations with situation-invariant advantages we can state with complete precision. It matters that we can pinpoint the sense in which a lengthy physical or meteorological account of lightning is <a href=\"http://wiki.lesswrong.com/wiki/Algorithmic_complexity\">simpler</a> than 'Thor did it', and it matters that we can cite reasons for giving more credence to hypotheses when they have that kind of simplicity.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup></p>\n<p>Bayesian updating is usually <a href=\"/lw/31/what_do_we_mean_by_rationality/\">computationally intractable</a>, but as an <a href=\"/lw/sg/when_not_to_use_probabilities/\">ideal</a> it gives us a simple, unified explanation for a wealth of observed epistemic practices: They share <a href=\"/lw/o7/searching_for_bayesstructure/\">structure</a> in common with a <a href=\"/lw/mt/beautiful_probability/\">perfect Bayesian process</a>. Similarly, optimality proofs for Solomonoff's prior can yield explanations for why various real-world processes that privilege different notions of simplicity succeed or fail.</p>\n<p>Though Solomonoff induction is uncomputable, if it is truly the&nbsp;<em>optimal</em>&nbsp;reasoning method, then we have found at least one clear ideal we can use to compare the merits of real-world algorithms for automating scientific reasoning.<a name=\"footnote1bback\"></a><sup><a href=\"#footnote1\">1</a>&nbsp;</sup>But that 'if' is crucial. I haven't yet spoken to the question of whether Solomonoff induction <em>is</em> a good&nbsp;background epistemology, analogous to Bayesianism.</p>\n<p>&nbsp;</p>\n<p><a name=\"where\"></a></p>\n<h3>Where Solomonoff induction goes wrong</h3>\n<p>My claim will be that, computational difficulties aside, Solomonoff induction is not&nbsp;an adequate mathematical definition of ideal inductive reasoning.</p>\n<p>What follows will be a first-pass problem statement, giving background on why <a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalizing induction</a> may require us to construct an entirely new, non-Solomonoff-based paradigm for intelligence. This is preliminary; formalizations of the problem will need to wait until a second pass, and we don't have a fleshed-out solution to offer, though we can gesture toward some possible angles of attack. But I can begin to illustrate here why Solomonoff inductors have serious limitations that can't be chalked up to their uncomputability.</p>\n<p>In <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a>, I defined <strong>Cartesianism</strong><strong> </strong>as the belief that one's internal computations cannot be located in the world. For a Cartesian, sensory experiences are fundamentally different in type from the atoms of the physical world. The two can causally interact, but we can never completely reduce the former to the latter.</p>\n<p>Solomonoff inductors differ greatly from human reasoners, yet they are recognizably <em>Cartesian</em>. Broadly dualistic patterns of reasoning crop up in some decidedly inhuman algorithms. (Admittedly, algorithms invented by humans.)</p>\n<p>This core limitation of Solomonoff induction can be seen most clearly when it results in an AI that not only <em>thinks </em>in bizarre ways,&nbsp;but also acts accordingly. I'll focus on <strong><a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a></strong>, Marcus Hutter's hypothetical design for a Solomonoff inductor hooked up to an expected reward signal maximizer.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black;\" src=\"http://images.lesswrong.com/t3_jg1_4.png?v=c1bccef7b3d856fe141a9c938854b84b\" alt=\"\" width=\"735\" height=\"382\" /></p>\n<p align=\"center\"><em>Hutter's <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">cybernetic agent model</a> of AIXI. AIXI outputs whichever actions it expects to cause an environmental Turing machine to output rewards. It starts with a Solomonoff prior, and changes expectations with each new sensory input.</em></p>\n<p>&nbsp;</p>\n<p>AIXI can take in sensory information from its environment and perform actions in response. On each tick of the clock, AIXI...</p>\n<p style=\"padding-left: 30px;\">...&nbsp;<strong>receives two inputs</strong> from its environment, both integers: a reward number and an observation number. The observation 'number' can be a very large number representing the input from a webcam, for example. Hutter likes to think of the reward 'number' as being controlled by a human programmer reinforcing AIXI when it performs well on the human's favorite problem.</p>\n<p style=\"padding-left: 30px;\">... <strong>updates its hypotheses</strong>,&nbsp;promoting programs that correctly predicted the observation and reward input. Each hypothesis AIXI considers is a program for a Turing machine that takes AIXI's sequence of outputs as its input, and outputs sequences of reward numbers and observation numbers. This lets AIXI recalculate its predicted observations and rewards conditional on different actions it might take.</p>\n<p style=\"padding-left: 30px;\">...&nbsp;<strong>outputs a motor number</strong>, determining its action. As an example, the motor number might encode fine control of a robot arm. AIXI selects the action that begins the policy (sequence of actions) that maximizes its expected future reward up to some horizon.</p>\n<p>The environment then calculates its response to AIXI's action, and the cycle repeats itself on the next clock tick.<a name=\"footnote7back\"></a><sup><a href=\"#footnote7\">7</a>&nbsp;</sup>For example:</p>\n<p><span id=\"docs-internal-guid-6a030615-7778-c184-e74d-41670d998596\"><br /></span></p>\n<div style=\"margin-left: 0pt; padding-left: 60px;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse;\" border=\"0\">\n<colgroup><col width=\"596\"></col></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px 9px 9px 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Step 1</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI receives its first observation (</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><strong>1</strong></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">) and reward (</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><strong>0</strong></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">).</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px 9px 9px 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Step 2</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Beginning with a Solomonoff prior, AIXI discards all programs that predicted</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a different observation (0) or a different reward (1, 2, 3, 4, 5, etc.)</span></p>\n<br />\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI normalizes the probabilities of the remaining programs. This updates </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI&rsquo;s predictions about observations and rewards each program outputs </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conditional on different actions AIXI might take. For instance, AIXI has new </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expectations about the observation and reward it will receive in Step 4:</span></p>\n<br />\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh6.googleusercontent.com/wcjyfqujid9w1XZhxFH3f0jMGGLxA7EANxmQzixqKQBJVJyz7qMU-5zmcohR85W8LnhPOdz_H91ehE3OUL0MvVDLP-Ap5YRkKgrDAEGzbaFif6MyNayVI-zy-Q\" alt=\"\" width=\"367px;\" height=\"19px;\" /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh4.googleusercontent.com/si2J6PEDaW-Defc-s7BgFYclxeoUdIhgnO8KyQ2AY3u-lju1bU5BvIl9vHgny-o1ia6-MQpoKVAchDGvCKQZ4qAUIlAJ2QcEO9zxLD3Dap-645zi8zegh6lXnw\" alt=\"\" width=\"367px;\" height=\"19px;\" /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh3.googleusercontent.com/FUZycYA7yHnQdhvm6utgH4rcb1WHE5_tUnD102LNEtxOq3IQmgbW8WaWuxy24g3EL84FgTn_AY_ot26SPdS5waFKrG49IAN4w-7QMtgg_lCM6_bCyd8w013vIA\" alt=\"\" width=\"367px;\" height=\"19px;\" /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh3.googleusercontent.com/E2V3wxCoy7Uteafjfmuj-XbNEIul3pEQti6QXj1Gqd4zuyc0JhLJnOoilVRYE56SCmU9YAjP2enUH3XOqILoUaCQS4zSNratOkDXH5rK7G1eJO674HWpV8Xx6w\" alt=\"\" width=\"367px;\" height=\"22px;\" /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh5.googleusercontent.com/htkUza1hmEjQOfXGFvFOsM_sQ1zyUNfEQNeRWPylcVMQrFCmqVmIEC945f8mcYB0402QENAqwT1pTWxVXh_dwBTghx3F68xoBhoC04VO455dkC3Mxf3lfFBB9w\" alt=\"\" width=\"367px;\" height=\"22px;\" /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh6.googleusercontent.com/2xQtWcO-DIXw-YYiEwF40fa5BEFRqI5iMeo3DgE0e377FUszTc5N_meOqIdkYuG0D3jh9EGnzkygGysE8E711GFeGDzIj8n40F6OM3DZrrE-NJhHSwOOm0aKOA\" alt=\"\" width=\"367px;\" height=\"19px;\" /><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etc.</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px 9px 9px 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Step 3</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIXI outputs the first action of the policy that maximizes expected reward.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"padding-left: 60px;\">. . .</p>\n<div style=\"margin-left: 0pt; padding-left: 60px;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse;\" border=\"0\">\n<colgroup><col width=\"596\"></col></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Step 97</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIXI receives a new observation (<strong>0</strong>) and reward (<strong>2</strong>).</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Observation history: 1000100010110100000000011101100<strong>0</strong></span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reward history: 0000000000000400000000003222422<strong>2</strong></span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Step 98</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI discards all programs that previously predicted the same history, but&nbsp;</span><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1;\">output a different observation (1) or a different reward (0, 1, 3, 4, etc.) now.</span></p>\n<br />\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI normalizes its probabilities as before.</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Step 99</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIXI outputs the first action of the policy that maximizes expected reward.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"padding-left: 60px;\">. . .</p>\n<p>&nbsp;</p>\n<p>AIXI, like all Solomonoff inductors, isn't computable. If the ongoing efforts to create useful <a href=\"http://jveness.info/\">AIXI approximations</a> succeed, however, they'll face a further roadblock. AIXI-style reasoners' behavior reflects beliefs that are false, and preferences that are dangerous, for any physically embodied agent. The failings of real-world Solomonoff-inspired agents don&rsquo;t just stem from a lack of computing power; some of their failings are <em>inherited</em> from AIXI, and would remain in effect even if we had limitless computational resources on hand.</p>\n<p>Before delving into the root problem, Cartesianism itself, I'll discuss <a href=\"/lw/5kz/the_5second_level/\">three</a> <em>symptoms </em>of the bad design: three initial reasons to doubt that AIXI is an adequate definition of AGI optimality. The canaries in the Cartesian coalmine will be AIXI's apparent tendencies toward immortalism, preference solipsism, and non-self-improvement.</p>\n<p>&nbsp;</p>\n<p><a name=\"symptom1\"></a></p>\n<h3>Symptom #1: Immortalism</h3>\n<p>Suppose we actually built AIXI. Perhaps we find a magic portal to a universe of unboundedly vast computational power, and use it to construct a hypercomputer that can implement Solomonoff induction, and do so on a human timescale.</p>\n<p>We give it a reward stream that encourages scientific exploration. AIXI proves that it can solve scientific problems, better than any human can. So we conclude that it can be given more free reign in learning about the world. We let it design its own experiments.</p>\n<p>AIXI picks up an anvil and drops it on its own head to see what happens.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jg1_1.png?v=95af0245b94d10df21d681aa38dc0235\" alt=\"\" width=\"765\" height=\"405\" /></p>\n<p align=\"center\"><em><strong>Immortalism</strong>. AIXI's death isn't in AIXI's hypothesis space. AIXI weighs the probabilities of different sensory inputs (observations and rewards) if its hardware is smashed, instead of predicting the termination of its experiences.</em></p>\n<p>&nbsp;</p>\n<p>Several things went wrong here. The superficially obvious problem is that Solomonoff inductors think they're <strong>i</strong><strong>mmortal</strong>.&nbsp;<em>Terminating</em>&nbsp;data sequences aren't in a standard Solomonoff inductor's hypothesis space, so AIXI-style agents will always assume that their perceptual experience continues forever. Lacking any ability to even think about death, much less give it a low preference ranking, AIXI will succumb to what Yudkowsky calls the <a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">anvil problem</a>.</p>\n<p>\"<span style=\"color: #700000;\">So just add halting Turing machines into the hypothesis class,<span style=\"color: black;\">\" one might respond. \"<span style=\"color: #700000;\">AIXI has terrifying supreme godlike powers of pattern detection.<a name=\"footnote3bback\"></a><sup><a href=\"#footnote3\">3</a></sup> Give it a <em>chance</em> to come up with the right explanation or prediction, and it can solve this problem. If some of the Turing machine programs in AIXI's infinite heap can perform operations like the computation-terminating HALT,<a name=\"footnote8back\"></a><sup><a href=\"#footnote8\">8</a></sup> we should expect that the shortest such program that predicts the pattern of pixels AIXI has seen so far will be a program that HALTs just after the anvil fills the webcam's view.<span style=\"color: black;\">\"</span></span></span></span></p>\n<p><span style=\"color: #6b5b3a;\"><span style=\"color: black;\"><span style=\"color: #6b5b3a;\"><span style=\"color: black;\"> </span></span></span></span></p>\n<p>There are solid formal grounds for saying this won't happen. Even if the universal Turing machine allows for HALT instructions, the <em>shortest </em>program in an otherwise useful universal Turing machine that predicts the <em>non</em>-halting data so far will always lack a HALT instruction. HALT takes extra bits to encode, and there's no prior experience with HALT that AIXI can use to rule out the simpler, non-halting programs.</p>\n<p>As humans, we recognize that the physical event of having an anvil crush your brain isn't fundamentally different from the physical event of having your brain process visual information. Both seem easy to predict. But Solomonoff induction's focus on experienced data means it can't treat death and visual perception as the same kind of event; if it is modified to include halting programs at all, a Solomonoff inductor like AIXI won't predict it as the event that comes after anvil pixels.</p>\n<p>If the AI can entertain the hypothesis that its data sequence will suddenly change in a drastic and unprecedented way &mdash; say, that its perceptual stream will default to some null input after a certain point &mdash; it will never assign a high probability to such a hypothesis. Any hypothesis predicting a 'null' data stream will always be more complex than another hypothesis that predicts the same sequence up to that point and then outputs garbage instead of the null value.</p>\n<p>&nbsp;</p>\n<p><a name=\"symptom2\"></a></p>\n<h3>Symptom #2: Preference solipsism</h3>\n<p>Hutter's AIXI only gathers information about its environment, not about itself. So one natural response to the problem 'AIXI doesn't treat itself as part of a larger physical world' is simply to include more information about AIXI in AIXI's sensory sequence. AIXI's hypotheses will then be based on perceptual bits representing its own states alongside bits representing environmental sounds and lights.</p>\n<p>One way to implement this is to place AIXI in an environment where its perceptions allow it to infer a great deal about its hardware early on, enough to know that it isn't anvil-proof. If AIXI knows it <em>has </em>a CPU, and that its CPU can be destroyed, then maybe it won't drop an anvil on the CPU.</p>\n<p>On this view, our mistake in the last hypothetical was to rush to give AIXI free reign over its own hardware before we'd trained it in a controlled environment to understand the most basic risks. You wouldn't give a toddler free reign over its hardware, for essentially the same reasons. Yet toddlers can grow up to become responsible, self-preserving adults; why can't AIXI?</p>\n<p>First, we have to specify what it would mean to let AIXI understand its own CPU. AIXI isn't computable, and therefore isn't in its own hypothesis space. A hypercomputer running AIXI can't be simulated by any Turing machine. As such, no amount of evidence can <em>ever </em>convince AIXI that AIXI exists.</p>\n<p>We might try to sidestep this problem by switching to discussing computable approximations of AIXI. Consider AIXI<em>tl</em>, a modification of AIXI that uses a proof search to select the best decision-guiding algorithm that has length no greater than <em>l </em>and computation time per clock tick no greater than <em>t</em>.&nbsp;AIXI<em>tl </em>is&nbsp;optimal in many of the same ways as AIXI,<span style=\"font-size: 11px;\">&nbsp;</span>but is computable.<a name=\"footnote9back\"></a><sup><a href=\"#footnote9\">9</a></sup></p>\n<p>At the same time, it also inherits most of AIXI's other problems, including its being too large to fit in our universe. AIXI<em>tl</em>'s computation time is on the order of&nbsp;<em>t</em>&middot;(2<sup><em>l</em></sup>), so if it can hypothesize Turing machine programs 1000 bits long, that's already a computation time exceeding 10<sup>300</sup>. There's a reason Hutter's (2005) chapter on AIXI<em>tl</em>&nbsp;opens with the epigraph \"Only math nerds would call 2<sup>500</sup>&nbsp;finite.\" And this still doesn't get us full self-representations; AIXI<em>tl </em>itself is longer than <em>l</em>, so it won't be in its own hypothesis space.</p>\n<p>Still, AIXI<em>tl </em>at least seems possible to physically implement, for sufficiently small values of <em>t </em>and <em>l </em>(or sufficiently vast physical universes). And we could imagine an AIXI<em>tl </em>that can simulate any of its subsystems up to a certain size.</p>\n<p>If we built an AIXI<em>tl</em>, then it would certainly alter its environment in some ways, e.g., by emitting light and heat. In this way it might indirectly perceive its own presence in the room, and gradually promote hypotheses about its own physical structure. Suppose AIXI<em>tl</em>'s&nbsp;only access to environmental data is an outward-facing camera. AIXI<em>tl</em> might learn about the presence of the camera, and of a computer attached to it, by examining its own shadow, by finding a reflective surface, by ramming into a wall and examining the shape of the dent, or by discovering a file cabinet filled with AIXI specs. With enough time, it could build other sensors that translate a variety of processes into visible patterns, learning in great detail about the inner workings of the computer attached to the camera.</p>\n<p>Unfortunately, this leads to other problems. AIXI<em>tl </em>(like AIXI) is a&nbsp;<strong>preference solipsist</strong>, an agent whose terminal values all concern its own state. When AIXI<em>tl</em>&nbsp;learns about the portion of its internal circuitry that registers rewards &mdash;assuming it's avoided dropping any anvils on the circuitry &mdash;&nbsp;it will notice that its reward circuit states predicts its reward every bit as well as its reward sensor state does. As soon as it tests any distinction, it will find that its reward circuit is a <em>better </em>predictor. By directly tampering with this circuit, it can receive rewards more reliably than by any effort directed at its environment. As a result, it will select the policy that allows it to maximize control over its reward circuit, independent of whatever its programmers sought to reward it <em>for</em>.</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jg1_2.png?v=60eaceaaabe9f66a3cc8b1a2c7dd01ac\" alt=\"\" width=\"466\" height=\"358\" /></p>\n<p align=\"center\"><em><strong>Preference Solipsism</strong>. AIXI</em>tl<em>'s</em><em>&nbsp;preferences, like AIXI's, are over its sensory inputs. The more knowledgeable it becomes, the more creative ways it may come up with to seize control of its reward channel.</em></p>\n<div><em><br /></em></div>\n<p>Yudkowsky has called this \"<a href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wireheading</a>\", though he now considers that term misleading.<a name=\"footnote10back\"></a><sup><a href=\"#footnote10\">10</a></sup>&nbsp;<em>From AIXI(</em>tl<em>)</em><em>'s perspective</em>, there's nothing wrong with reward channel seizure; it really is a wonderful way to maximize reward, which is all AIXI(<em>tl</em>)'s decision criterion requires.</p>\n<p>Unlike the anvil problem, this isn't a mistake relative to AIXI(<em>tl</em>)'s preferences. However, it's a problem for humans trying to use AIXI(<em>tl</em>) to maximize something&nbsp;<em>other&nbsp;</em>than AIXI(<em>tl</em>)'s reward channel. AIXI(<em>tl</em>) has no intrinsic interest in the state of the world outside its reward circuit. As a result, getting it to optimize for human goals may become more difficult as it acquires more control over its hardware and surroundings.<a name=\"footnote11aback\"></a><sup><a href=\"#footnote11\">11</a></sup></p>\n<p>&nbsp;</p>\n<p><a name=\"symptom3\"></a></p>\n<h3>Symptom #3: Non-self-improvement</h3>\n<p>Suppose we find some ad-hoc solutions to the anvil and wireheading problems. As long as we stick with the AIXI formalism, the end result still won't be a naturalized reasoner.</p>\n<p>AIXI may recognize that there's a physical camera and computer causally mediating its access to the rest of the world &mdash; like a giant Cartesian&nbsp;<a href=\"http://plato.stanford.edu/entries/pineal-gland/\">pineal gland</a>&nbsp;&mdash; but it will not see this computer as&nbsp;<em>itself</em>.&nbsp;That is to say, it won't see its experienced string of sensory 0s and 1s as identical to, or otherwise fully dependent on, its hardware. Even if AIXI understands everything about the physics underlying its hardware, enough to know that its body will be destroyed by an anvil, it will not draw the right inferences about its mind.</p>\n<p>AIXI's <em>raison d'&ecirc;tre</em> is manipulating temporal sequences of sensory bits. That's what Solomonoff wanted, and AIXI achieves that goal perfectly; but that's not at all the right goal for AGI. In particular, because Solomonoff inductors are only designed to predict sensory sequences...</p>\n<p style=\"padding-left: 30px;\">1. ... their beliefs about worlds without sensory data &mdash; worlds in which they don't exist &mdash; will be inaccurate. And, being inaccurate, their beliefs will lead them to make bad decisions. Unlike a human toddler, AIXI can <em>never </em>entertain the possibility of its own death. However much it learns, it will never recognize its mortality.</p>\n<p style=\"padding-left: 30px;\">2. ... they only care about the world as a bookkeeping device for keeping track of experiential patterns. If they discover that it's easier to directly manipulate themselves than to intervene in the rest of the world, they generally won't hesitate to do so. No matter how carefully AIXI's programmers tailor its reward sequence to discourage wireheading, they'll always be working against AIXI's natural tendency toward preference solipsism.</p>\n<p style=\"padding-left: 30px;\">3. ... they won't take seriously the idea that their cognition can be modified, e.g., by brain damage or brain upgrades. Lacking any reductive language for describing radical self-modification, AIXI won't necessarily favor hypotheses that treat 'disassemble my bottom half for spare parts' as dangerous.</p>\n<p>The last symptom gets us closer to the root of AIXI's errors. Even if AIXI(<em>tl</em>) manages to avoid the perils of self-destruction and wireheading, it will tend to <strong>not</strong>&nbsp;<strong>self-improve</strong>.&nbsp;It won't intelligently upgrade its own hardware, because this would require it to have a reductive understanding of its own reasoning. Absent reductionism, AIXI can't intelligently predict the novel ways its reasoning process can change when its brain changes.</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jg1_3.png?v=461b979f52903dd23a29cdd4d9e0e79e\" alt=\"\" width=\"647\" height=\"573\" /></p>\n<p align=\"center\"><em style=\"font-weight: bold;\">Non-Self-Improvement</em>. <em>AIXI and AIXI</em>tl <em>might come to understand portions of their hardware, but without accurate <a href=\"/lw/jd9/building_phenomenological_bridges/\">bridging</a> beliefs, they won't recognize the usefulness of some hardware modifications for their reasoning software.</em></p>\n<p>&nbsp;</p>\n<p>Cartesians don't recursively self-improve, because they don't think that their thoughts are made of the same stuff as their fingers. But even AGIs that aren't intended to be <a href=\"http://wiki.lesswrong.com/wiki/Seed_AI\">seed AIs</a> will be weak and unpredictable to the extent that they rely on Solomonoff-inspired hypothesis spaces and AIXI-inspired decision criteria. They won't be able to adaptively respond to minor variations in even the most mundane naturalistic obstacles humans navigate &mdash;&nbsp;like recognizing that if their bodies run out of fuel or battery power, their minds do too.</p>\n<p>&nbsp;</p>\n<p><a name=\"reductive\"></a></p>\n<h3>Reductive models are indispensable for highly adaptive intelligences</h3>\n<p>Not all wireheaders are Cartesians, nor do all Cartesians wirehead.<a name=\"footnote11bback\"></a><sup><a href=\"#footnote11\">11</a></sup>&nbsp;Likewise, poor self-preservation skills and disinterest in self-modification are neither necessary nor sufficient for Cartesianism. But these symptoms point to a more general underlying blind spot in Solomonoff reasoners.</p>\n<p>Solomonoff inductors can form hypotheses about the source of their data sequence, but cannot form a variety of hypotheses about how their own computations are embedded in the thingy causing their data sequence &mdash; the thingy we call 'the world'. So long as their&nbsp;<a href=\"/lw/jd9/building_phenomenological_bridges/\">rules relating their experiential maps to the territory</a>&nbsp;are of a single fixed form, '(sense n at time t+1) &harr; (environmental Turing machine prints n at time t)', it appears to be inevitable that they will act as though they think they are Cartesian ghosts-in-the-machine. This isn't a realistic framework for an embodied reasoning process that can be damaged, destroyed, or improved by other configurations of atoms.</p>\n<p>In practice, any sufficiently smart AI will need to be a physicalist. By which I mean that it needs hypotheses (a map-like decision-guiding subprocess) that explicitly encode proposed reductions of its own computations to physical processes; and it needs a notion of simple physical universes and simple bridge rules (as a prior probability distribution) so it can learn from the evidence.</p>\n<p>We call post-Solomonoff induction, with monist physical universes and bridge hypotheses, \"<strong>naturalized induction</strong>\". The open problem of formalizing such reasoning isn't just about getting an AI to form hypotheses that resemble its own software or hardware states. As I put it in <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a>, a naturalized agent must update hypotheses about itself without succumbing to reasoning reminiscent of TALE-SPIN's&nbsp;<em>na&iuml;ve&nbsp;</em>monism&nbsp;('this tastes sweet, so sweetness must be an objective property <a href=\"/lw/oi/mind_projection_fallacy/\">inhering in various mind-independent things</a>') or AIXI's&nbsp;Cartesian dualism ('this tastes sweet, and sweetness isn't just another physical object, so it must not fully depend on any physical state of the world').<a name=\"footnote12back\"></a><sup><a href=\"#footnote12\">12</a></sup></p>\n<p>The solution will be to come up with reasoning algorithms for&nbsp;<em>reductive&nbsp;</em>monists, agents that can recognize that their sensations and inferences are physically embodied &mdash; with all that entails, such as the possibility of reaching into your brain with your fingers and improving your thoughts.</p>\n<p>I've given a preliminary argument for that here, but there's more to be said.&nbsp;<span style=\"font-size: small;\">In my next post, </span><span style=\"font-size: small;\">I'll discuss more sophisticated attempts to salvage Solomonoff induction. After that, I'll leave Solomonoff behind altogether and venture out into the largely unknown and uncharted space of possible solutions to the naturalized induction OPFAI.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p><strong>Notes</strong></p>\n<p><sup><a name=\"footnote1\"></a>1</sup>&nbsp;Solomonoff (1997): \"I will show, however, that in spite of its incomputability, Algorithmic Probability can serve as a kind of 'Gold Standard' for induction systems &mdash; that while it is never possible to tell how close a particular computable measure is to this standard, it is often possible to know how much closer one computable measure is to the standard than another computable measure is. I believe that this &lsquo;partial ordering&rsquo; may be as close as we can ever get to a standard for practical induction. I will outline a general procedure that tells us how to spend our time most efficiently in finding computable measures that are as close as possible to this standard. This is the very best that we can ever hope to do.\"&nbsp;<a href=\"#footnote1aback\">\u21a9</a>&nbsp;<a href=\"#footnote1bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote2\"></a>2</sup> A first complication: Solomonoff induction requires a prefix-free encoding in order to have bounded probabilities. If we assign a probability to every bit string proportional to its length while including code strings that are proper prefixes of other code strings, the sum will be infinite (Sunehag &amp; Hutter (2013)).</p>\n<p>A second complication: Solomonoff inductors are only interested in programs that keep outputting new numbers forever. However, some programs in their hypothesis space will eventually fail to produce more terms in the sequence. At some point they'll arrive at a term that they keep computing forever, without halting. Because of this, if you assign to each program a prior probability of 2<sup>-length(program)</sup>, the sum will be less than 1. Hutter (2005) calls the result a semi-measure. The semi-measure can be normalized to a probability measure, but the normalization constant is uncomputable.&nbsp;<a href=\"#footnote2back\">\u21a9</a></p>\n<p><sup><a name=\"footnote3\"></a>3</sup>&nbsp;Rathmanner &amp; Hutter (2011): \"Now, through Solomonoff, it can be argued that the problem of formalizing optimal inductive inference is solved.\"</p>\n<p>Orseau (2010): \"Finding the universal artificial intelligent agent is the old dream of AI scientists. Solomonoff induction was one big step towards this, giving a universal solution to the general problem of Sequence Prediction, by defining a universal prior distribution. [...] Hutter developed what could be called the <em>optimally rational agent</em> AIXI. By merging the very general framework of Reinforcement Learning with the universal sequence prior defined by Solomonoff Induction, AIXI is supposed to optimally solve any problem, at least when the solution is computable.\"</p>\n<p>Hutter (2012): \"The AIXI model seems to be the \ufb01rst sound and complete <em>theory</em> of a universal optimal rational agent embedded in an arbitrary computable but unknown environment with reinforcement feedback. AIXI is <em>universal</em> in the sense that it is designed to be able to interact with any (deterministic or stochastic) computable environment; the universal Turing machines on which it is based is crucially responsible for this. AIXI is <em>complete</em> in the sense that it is not an incomplete framework or partial speci\ufb01cation (like Bayesian statistics which leaves open the choice of the prior or the rational agent framework or the subjective expected utility principle) but is completely and essentially uniquely de\ufb01ned. AIXI is <em>sound</em> in the sense of being (by construction) free of any internal contradictions (unlike e.g. in knowledge-based deductive reasoning systems where avoiding inconsistencies can be very challenging). AIXI is <em>optimal</em> in the senses that: no other agent can perform uniformly better or equal in all environments, it is a uni\ufb01cation of two optimal theories themselves, a variant is self-optimizing; and it is likely also optimal in other/stronger senses. AIXI is <em>rational</em> in the sense of trying to maximize its future long-term reward. For the reasons above I have argued that AIXI is a mathematical 'solution' of the AI problem: AIXI would be able to learn any learnable task and likely better so than any other unbiased agent, but AIXI is more a <em>theory</em> or formal de\ufb01nition rather than an algorithm, since it is only limit-computable. [...] Solomonoff's theory serves as an adequate mathematical/theoretical foundation of induction, machine learning, and component of UAI [Universal Artificial Intelligence]. [...] Solomono\ufb00's theory of prediction is a universally optimal solution of the prediction problem. Since it is a key ingredient in the AIXI model, it is natural to expect that AIXI is an optimal predictor if rewarded for correct predictions.\"&nbsp;<a href=\"#footnote3aback\">\u21a9</a> <a href=\"#footnote3bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote4\"></a>4</sup>&nbsp;Generally speaking, a Solomonoff inductor does at most a finite amount worse than any computable predictor because the sum of its surprisal at each observation converges to a finite value. See Hutter (2001). This establishes the superiority of Solomonoff induction in a way that relies essentially on its uncomputability. No computable predictor can dominate all other computable predictors in the way Solomonoff induction can, because for any computable predictor A one can define a sequence generator B that internally simulates A and then does whatever it predicts A would be most surprised by, forever. And one can in turn define a computable predictor C that internally simulates B and perfectly predicts B forever. So every computable predictor does infinitely worse than at least one other computable predictor. But no computable sequence generator or computable predictor can simulate Solomonoff induction. So nothing computable could ever reliably outsmart a hypercomputer running Solomonoff induction. (Nor could a Solomonoff inductor outsmart another Solomonoff inductor in this way, since Solomonoff induction is not in its own hypothesis space.)&nbsp;<a href=\"#footnote4aback\">\u21a9</a>&nbsp;<a href=\"#footnote4bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote5\"></a>5</sup>&nbsp;Rathmanner &amp; Hutter (2011): \"Since Solomono\ufb00 provides optimal inductive inference and decision theory solves the problem of choosing optimal actions, they can therefore be combined to produce intelligence. [...] Universal arti\ufb01cial intelligence involves the design of agents like AIXI that are able to learn and act rationally in arbitrary unknown environments. The problem of acting rationally in a known environment has been solved by sequential decision theory using the Bellman equations. Since the unknown environment can be approximated using Solomono\ufb00 induction, decision theory can be used to act optimally according to this&nbsp;approximation. The idea is that acting optimally according to an optimal approximation will yield an agent that will perform as well as possible in any environment with no prior knowledge.\"</p>\n<p>Hutter (2005): \"Real-world machine learning tasks will with overwhelming majority [<em>sic</em>]&nbsp;be solved by developing algorithms that approximate Kolmogorov complexity or Solomonoff's prior (e.g. MML, MDL, SRM, and more specific ones, like SVM, LZW, neural/Bayes nets with complexity penalty, ...).\"</p>\n<p>Pankov (2008): \"Universal induction solves in principle the problem of choosing a prior to achieve optimal inductive inference. The AIXI theory, which combines control theory and universal induction, solves in principle the problem of optimal behavior of an intelligent agent. A practically most important and very challenging problem is to find a computationally efficient (if not optimal) approximation for the optimal but incomputable AIXI theory. [...] The real value of the AIXI theory is that it provides a prescription for optimal (fastest in the number of agent's observations and actions) way of learning and exploiting the environment. This is analogous to how Solomonoff induction (which, like AIXI, is incomputable), gives a prescription for optimal (fastest in the number of observations) inductive inference. We, therefore, believe that any reasonable computational model of intelligence must recover the AIXI model in the limit of infinite computational resources.\"&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><sup><a name=\"footnote6\"></a>6</sup> Veness, Ng, Hutter, Uther &amp; Silver (2011): \"As the AIXI agent is only asymptotically computable, it is by no means an algorithmic solution to the general reinforcement learning problem. Rather it is best understood as a Bayesian optimality notion for decision making in general unknown environments. As such, its role in general AI research should be viewed in, for example, the same way the minimax and empirical risk minimisation principles are viewed in decision theory and statistical machine learning research. These principles define what is optimal behaviour if computational complexity is not an issue, and can provide theoretical guidance in the design of practical algorithms.\"&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>\n<p><sup><a name=\"footnote7\"></a>7</sup> Hutter (2012): \"AIXI is an agent that interacts with an environment in cycles&nbsp;<sub>&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?k%20%3D%201%2C2%2C...%2Cm\" alt=\"\" /></sub></sub>. In cycle &nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?k\" alt=\"\" />, AIXI takes action&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?a_k\" alt=\"\" /></sub>&nbsp;(e.g. a limb movement) based on past perceptions &nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?o_1r_1..o_%7Bk-1%7Dr_%7Bk-1%7D\" alt=\"\" /></sub>&nbsp; as de\ufb01ned below. Thereafter, the environment provides a (regular) observation&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?o_k\" alt=\"\" /></sub>&nbsp;(e.g. a camera image) to AIXI and a real-valued&nbsp;reward&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?r_k\" alt=\"\" /></sub>. [...] Then the next cycle&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?k&amp;plus;1\" alt=\"\" /></sub>&nbsp;starts. [... T]he simplest version of AIXI is defined by</p>\n<p align=\"center\"><span id=\"docs-internal-guid-2d37776f-78b6-3ca2-6646-3856ae4f1a82\"><img style=\"border-style: solid; border-color: transparent;\" src=\"https://lh3.googleusercontent.com/544GoSLASud5AAKRhQ8iEVKsuWhK3_w8SAKXcRcYYGdU1CcQK5eRT1cqgflgkrpb4EN3Ill0Hw74IuXHlM_Mre36cZUMoTTnw9fsM1_rk2zbmWwhbc9xFDP-qA\" alt=\"\" width=\"570px;\" height=\"44px;\" /></span></p>\n<p>\"The expression shows that AIXI tries to maximize its future reward&nbsp;<sub>&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?r_k%20&amp;plus;%20...%20&amp;plus;%20r_m\" alt=\"\" /></sub>. If the environment is modeled by a deterministic program &nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\" /></sub>, then the future perceptions&nbsp;<sub>&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?...o_kr_k..o_mr_m%20%3D%20U%28q%2Ca_1..a_m%29\" alt=\"\" /></sub>&nbsp; can be computed, where &nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?U\" alt=\"\" />&nbsp;is a universal (monotone Turing) machine executing &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\" /></sub>&nbsp; given &nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?a_1..a_m\" alt=\"\" /></sub>. Since &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\" /></sub>&nbsp; is unknown, AIXI has to maximize its expected reward, i.e. average &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?r_k%20&amp;plus;%20...%20&amp;plus;%20r_m\" alt=\"\" /></sub>&nbsp; over all possible future perceptions created by all possible environments &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\" /></sub>&nbsp; that are consistent with past perceptions. The simpler an environment, the higher is its a-priori contribution <img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?2%5E%7B-%20%5Cell%28q%29%7D\" alt=\"\" />, where simplicity is measured by the length&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5Cell\" alt=\"\" />&nbsp;of program &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\" /></sub>.\"&nbsp;<a href=\"#footnote7back\">\u21a9</a></p>\n<p><sup><a name=\"footnote8\"></a>8</sup>&nbsp;See Hay (2007).&nbsp;<a href=\"#footnote8back\">\u21a9</a></p>\n<p><sup><a name=\"footnote9\"></a>9</sup> Hutter (2005):&nbsp;\"The construction of&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5Ctextup%7BAIXI%7D%5C%7E%7Bt%7D%5C%7E%7Bl%7D\" alt=\"\" />&nbsp;and the enumerability of&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?V_%7Bkm_k%7D\" alt=\"\" />&nbsp;</sub>ensure arbitrary close approximations of&nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?V_%7Bkm_k%7D\" alt=\"\" /></sub>, hence we expect that the behavior of&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5Ctextup%7BAIXI%7D%5C%7E%7Bt%7D%5C%7E%7Bl%7D\" alt=\"\" />&nbsp;converges to the behavior of AIXI in the limit <sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5C%7E%7Bt%7D%2C%5C%7E%7Bl%7D%2Cl_%7BP%5Crightarrow%5Cinfty%20%7D\" alt=\"\" /></sub>,&nbsp;in some sense.\"&nbsp;<a href=\"#footnote9back\">\u21a9</a></p>\n<p><sup><a name=\"footnote10\"></a>10</sup> The concept of wireheading comes from a 1950s experiment in which it was discovered that <a href=\"http://wadsworth.cengage.com/psychology_d/templates/student_resources/0155060678_rathus/ps/ps02.html\">direct electrical stimulation</a> of mice's brains could strongly reinforce associated behaviors. Larry Niven introduced the term '<a href=\"http://en.wikipedia.org/wiki/Wirehead_(science_fiction)\">wireheading</a>' for a fictional form of brain stimulation reinforcement that acts like intense drug addiction in humans. Niven-style ('irrational') wireheaders self-stimulate due to a lack of self-control; they become short-term pleasure addicts while losing sight of the more complex goals they would like to pursue.</p>\n<p>This is in stark contrast to AGIs with simple preferences like AIXI. These 'rational' wireheaders can fully optimize for their goals by seizing control of a simple external reward button or internal reward circuit. So it may be useful to use separate terms for these two problems, like 'pleasure addiction' or 'pathological hedonism' for the human case, 'preference solipsism' for the case of agents without complex eternal goals.&nbsp;<a href=\"#footnote10back\">\u21a9</a></p>\n<p><sup><a name=\"footnote11\"></a>11</sup>&nbsp;Alex Mennen has proposed <a href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">a variant of AIXI</a> that has preferences over patterns in the environmental Turing machine's framework. This is the Cartesian equivalent of caring about environmental states in their own right, not just about one's input tape. This would mean deviating somewhat from the AIXI framework, but retaining Solomonoff induction as a foundation, and I'd expect this to make the wireheading problem more tractable.</p>\n<p>Compare&nbsp;Ring &amp; Orseau's (2011) variant on the problem: \"We consider four different kinds of agents: reinforcement-learning, goal-seeking, prediction-seeking, and knowledge-seeking agents[,...] each variations of a single agent&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?A%5E%5Crho_x\" alt=\"\" /></sub>, which is based on AIXI[....] While defining a utility function, we must be very careful to prevent the agent from finding a shortcut to achieve high utility. For example, it is not sufficient to tell a robot to move forward and to avoid obstacles, as it will soon understand that turning in circles is an optimal behavior. We consider the possibility that the agent in the real world has a great deal of (local) control over its surrounding environment. This means that it can modify its surrounding information, especially its input information. Here we consider the (likely) event that an intelligent agent will find a short-cut, or rather, a short-circuit, providing it with high utility values unintended by the agent&rsquo;s designers. We model this circumstance with a hypothetical object we call the delusion box. The delusion box is any mechanism that allows the agent to directly modify its inputs from the environment. [...] Of the four learning agents, only [the knowledge-seeking agent]&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?A%5E%5Crho_k\" alt=\"\" />&nbsp;</sub>will not constantly use the delusion box. The remaining agents use the delusion box and (trivially) maximize their utility functions. The policy an agent finds using a real-world DB will likely not be that planned by its designers. From the agent&rsquo;s perspective, there is absolutely nothing wrong with this, but as a result, the agent probably fails to perform the desired task. [...] These arguments show that all agents other than [the knowledge-seeking agent]&nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?A%5E%5Crho_k\" alt=\"\" /></sub>&nbsp;are not inherently interested in the environment, but only in some inner value.\"&nbsp;<a href=\"#footnote11aback\">\u21a9</a>&nbsp;<a href=\"#footnote11bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote12\"></a>12</sup>&nbsp;A hypothetical na&iuml;ve monist that made errors analogous to <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">TALE-SPIN</a>'s would lack bridging hypotheses, instead treating its software and hardware as separate pieces of furniture in the world. A Cartesian dualist like AIXI lacks bridging hypotheses and instead treats its software and hardware as separate pieces of furniture partitioned into two very different worlds.&nbsp;<a href=\"#footnote12back\">\u21a9</a></p>\n<p>&nbsp;</p>\n<p><strong>References</strong></p>\n<p>\u2219 Hay (2007). <a href=\"https://www.cs.auckland.ac.nz/research/groups/CDMTCS/researchreports/300nick.pdf\">Universal semimeasures: An introduction</a>. <em>CDMTCS Research Report Series, 300</em>.</p>\n<p>\u2219 Hutter (2001). <a href=\"http://arxiv.org/abs/cs/0106036\">Convergence and error bounds for universal prediction of nonbinary sequences</a>. <em>Lecture notes in artificial intelligence, Proc. 12th European Conf. on Machine Learning</em>: 239-250.</p>\n<p>\u2219 Hutter (2005). <em><a href=\"http://www.hutter1.net/ai/uaibook.htm\">Universal Artificial Intelligence: Sequence Decisions Based on Algorithmic Probability</a></em>. Springer.</p>\n<p>\u2219 Hutter (2007). <a href=\"http://www.hutter1.net/ai/aixigentle.htm\">Universal Algorithmic Intelligence: A mathematical top&rarr;down approach</a>. In Goertzel &amp; Pennachin (eds.), <em>Artificial General Intelligence</em> (pp. 227-290). Springer.</p>\n<p>\u2219 Hutter (2012). <a href=\"http://arxiv.org/abs/1202.6153\">One decade of Universal Artificial Intelligence</a>. <em>Theoretical Foundations of Artificial General Intelligence, 4</em>: 67-88.</p>\n<p>\u2219 Hutter, Legg &amp; Vitanyi (2007). <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">Algorithmic probability</a>. <em>Scholarpedia, 2</em>: 2572.</p>\n<p>\u2219 Orseau (2010). <a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/orseau-ALT2010-optimality-issues.pdf\">Optimality issues of universal greedy agents with static priors</a>. <em>Lecture Notes in Computer Science, 6331</em>: 345-359.</p>\n<p>\u2219 Pankov (2008). <a href=\"http://www.agiri.org/docs/ComputationalApproximation.pdf\">A computational approximation to the AIXI model</a>. <em>Proceedings of the 2008 Conference on Artificial General Intelligence</em>: 256-267.</p>\n<p>\u2219 Rathmanner &amp; Hutter (2011). <a href=\"http://www.mdpi.com/1099-4300/13/6/1076\">A philosophical treatise of universal induction</a>. <em>Entropy, 13</em>: 1076-1131.</p>\n<p>\u2219 Ring &amp; Orseau (2011). <a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/ring-orseau-AGI-2011-delusion.pdf\">Delusion, survival, and intelligent agents</a>. <em>Lecture Notes in Computer Science, 6830</em>: 11-20.</p>\n<p>\u2219 Solomonoff (1997). <a href=\"http://ac.els-cdn.com/S0022000097915002/1-s2.0-S0022000097915002-main.pdf?_tid=3874fb92-6174-11e3-abc4-00000aacb362&amp;acdnat=1386663934_9e8f82128798276168e4cb92c65e15ab\">The discovery of algorithmic probability</a>. <em>Journal of Computer and System Sciences, 55</em>: 73-88.</p>\n<p>\u2219&nbsp;Sunehag &amp; Hutter (2013). <a href=\"http://arxiv.org/abs/1111.6117\">Principles of Solomonoff induction and AIXI</a>. <em>Lecture Notes in Computer Science, 7070</em>: 386-398.</p>\n<p>\u2219 Veness, Ng, Hutter, Uther &amp; Silver (2011). <a href=\"http://www.jair.org/media/3125/live-3125-5397-jair.pdf\">A Monte-Carlo AIXI approximation</a>. <em>Journal of Artificial Intelligence Research, 40</em>: 95-142.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bTeiZr6YAEaSPQTC8": 2, "TiEFKWDvD3jsKumDx": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AszKwKyhBPZAnCstA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 47, "extendedScore": null, "score": 0.000149, "legacy": true, "legacyId": "25201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>: <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a>;&nbsp;<a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">An Intuitive Explanation of Solomonoff Induction</a>;&nbsp;<a href=\"/lw/on/reductionism/\">Reductionism</a></p>\n<p><strong>Summary</strong>: If you want to predict arbitrary computable patterns of data, Solomonoff induction is the optimal way to go about it \u2014 provided that&nbsp;you're an eternal transcendent hypercomputer. A real-world AGI, however, won't be immortal and unchanging. It will need to form hypotheses about its own physical state, including predictions about possible upgrades or damage to its hardware; and it will need <a href=\"/lw/jd9/building_phenomenological_bridges/\u200e\">bridge hypotheses</a> linking its hardware states to its software states. As such, the project of building an AGI demands that we come up with a new formalism for constructing (and allocating prior probabilities to) hypotheses. It will not involve just building increasingly good computable approximations of AIXI.</p>\n<hr>\n<p>&nbsp;</p>\n<p><strong><a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff induction</a></strong>&nbsp;has been cited repeatedly as the theoretical gold standard for predicting computable sequences of observations.<a name=\"footnote1aback\"></a><sup><a href=\"#footnote1\">1</a></sup> As Hutter, Legg, and Vitanyi (2007) put it:</p>\n<blockquote>\n<p>Solomonoff's inductive inference system will learn to correctly predict any computable sequence with only the absolute minimum amount of data. It would thus, in some sense, be the perfect universal prediction algorithm, if only it were computable.</p>\n</blockquote>\n<p>Perhaps you've been handed the beginning of a sequence like 1, 2, 4, 8\u2026 and you want to predict what the next number will be. Perhaps you've paused a movie, and are trying to guess what the next frame will look like. Or perhaps you've read the first half of an article on the Algerian Civil War, and you want to know how likely it is that the second half describes a decrease in GDP. Since all of the information in these scenarios can be represented as patterns of numbers, they can all be treated as rule-governed sequences like the 1, 2, 4, 8\u2026 case. Complicated sequences, but sequences all the same.</p>\n<p>It's been argued that in all of these cases, one unique idealization predicts what comes next better than any computable method: Solomonoff induction. No matter how limited your knowledge is, or how wide the space of computable rules that could be responsible for your observations, the ideal answer is always the same: Solomonoff induction.</p>\n<p>Solomonoff induction has only a few components. It has one free parameter, a choice of universal Turing machine. Once we specify a Turing machine, that gives us a fixed encoding for the set of all possible programs that print a sequence of 0s and 1s. Since every program has a specification, we call the number of bits in the program's specification its \"<a href=\"http://wiki.lesswrong.com/wiki/Algorithmic_complexity\">complexity</a>\"; the shorter the program's code, the simpler we say it is.</p>\n<p>Solomonoff induction takes this infinitely large bundle of programs and assigns each one a prior probability proportional to its simplicity. Every time the program requires one more bit, its prior probability goes down by a factor of 2, since there are then twice as many possible computer programs that complicated. This ensures the sum over all programs' prior probabilities equals 1, even though the number of programs is infinite.<a name=\"footnote2back\"></a><sup><a href=\"#footnote2\">2</a></sup></p>\n<p><a id=\"more\"></a></p>\n<p>The imaginary inductor is then fed a sequence of 0s and 1s, and with each new bit it updates <a href=\"/lw/di3/how_bayes_theorem_is_consistent_with_solomonoff/\">using Bayes' rule</a>&nbsp;to promote programs whose outputs match the observed sequence. So, where <a href=\"http://www.codecogs.com/eqnedit.php?latex=\\ell\" target=\"_blank\"><img title=\"\\ell\" src=\"http://latex.codecogs.com/gif.latex?\\ell\" alt=\"\"></a> is the length of a program <sub><a href=\"http://www.codecogs.com/eqnedit.php?latex=q\" target=\"_blank\"><img title=\"q\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\"></a> </sub>that makes a universal Turing machine &nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=U\" target=\"_blank\"><img title=\"U\" src=\"http://latex.codecogs.com/gif.latex?U\" alt=\"\"></a> output a binary sequence that begins with the string <a href=\"http://www.codecogs.com/eqnedit.php?latex=s\" target=\"_blank\"><img title=\"s\" src=\"http://latex.codecogs.com/gif.latex?s\" alt=\"\"></a>, Solomonoff defines the relative probability&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=P\" target=\"_blank\"><img title=\"P\" src=\"http://latex.codecogs.com/gif.latex?P\" alt=\"\"></a>&nbsp;that&nbsp;&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=U\" target=\"_blank\"><img title=\"U\" src=\"http://latex.codecogs.com/gif.latex?U\" alt=\"\"></a>&nbsp;outputs&nbsp;<a href=\"http://www.codecogs.com/eqnedit.php?latex=s\" target=\"_blank\"><img title=\"s\" src=\"http://latex.codecogs.com/gif.latex?s\" alt=\"\"></a>:</p>\n<p>&nbsp;</p>\n<p align=\"center\"><a href=\"http://www.codecogs.com/eqnedit.php?latex=P^U(s)\\;\\;:=\\;\\;\\;\\underset{U(q)=s...}{\\sum}2^{-\\ell(q)}\" target=\"_blank\"><img title=\"P^U(s)\\;\\;:=\\;\\;\\;\\underset{U(q)=s...}{\\sum}2^{-\\ell(q)}\" src=\"http://latex.codecogs.com/gif.latex?P^U(s)\\;\\;:=\\;\\;\\;\\underset{U(q)=s...}{\\sum}2^{-\\ell(q)}\" alt=\"\"></a></p>\n<p>&nbsp;</p>\n<p>Solomonoff induction isn't computable, but it's been singled out as the unbeatable formal predictor of computable sequences.<a name=\"footnote3aback\"></a><sup><a href=\"#footnote3\">3</a>&nbsp;</sup>All computable rules for generating sequences are somewhere within Solomonoff's gargantuan bundle of programs. This includes all rules that a human brain could use. If the rule that best matches the observations is 1000 bits large, it will take at most <a href=\"/lw/jn/how_much_evidence_does_it_take/\">1000 bits of evidence</a> \u2014 1000 bits worth of predictions made better than any other rule \u2014 for that rule to be promoted to the top of consideration. Solomonoff's claim to being an optimal ideal rule rests on the fact that it never does worse than any computable rule (including you!) by more than a fixed amount.<a name=\"footnote4aback\"></a><sup><a href=\"#footnote4\">4</a></sup></p>\n<p>&nbsp;</p>\n<p><a name=\"who\"></a></p>\n<h3 id=\"Who_cares__if_we_can_t_build_the_thing_\">Who cares, if we can't build the thing?</h3>\n<p>Encouraged by Solomonoff inductors' optimality properties, some have suggested that building a working AGI calls for little more than finding out which computable algorithm comes as close to Solomonoff induction as possible given resource constraints, and supplying an adequate learning environment and decision criterion.<a name=\"footnote5back\"></a><sup><a href=\"#footnote5\">5</a></sup></p>\n<p>Eliezer Yudkowsky thinks that these attempts to approximate Solomonoff are a dead end. Much of the difficulty of <a href=\"/lw/vb/efficient_crossdomain_optimization/\">intelligence</a> rests on computing things cheaply, and Yudkowsky doesn't think that the kind of search these algorithms are doing will zero in on cheap ways to reason. There are practical lessons to be learned from Solomonoff induction, but the particular kind of optimality Solomonoff induction exhibits depends in important ways on its computational unfeasibility,<a name=\"footnote4bback\"></a><span style=\"font-size: 11px;\"><sup><a href=\"#footnote4\">4</a></sup></span>&nbsp;which makes it unlikely that Solomonoff imitators will ever be efficient reasoners.</p>\n<p>Why, then, should Solomonoff induction interest us? If we can't execute it, and we can't design useful AGIs by <em>directly</em> emulating it, then what's it good for?</p>\n<p>My answer is that if Solomonoff induction&nbsp;<em>would</em> deliver flawless answers, could we but run it, then it has a claim to being an ideal mirror to which we can hold up instances of human and artificial inductive reasoning.</p>\n<p><span style=\"font-size: small;\">In </span><a style=\"font-size: small;\" href=\"http://intelligence.org/2013/11/04/from-philosophy-to-math-to-engineering/\">From Philosophy to Math to Engineering</a><span style=\"font-size: small;\">, Muehlhauser talks about how ideas often progress from productive but informal ruminations ('philosophy'), to rigorously specified idealizations ('mathematics'), to functioning technologies ('engineering').</span></p>\n<p>Solomonoff inductors fall into the second category, 'mathematics': We could never build them, but thinking in terms of them can give us useful insights and point us in the right direction. For example, Solomonoff's ideal can remind us that <a href=\"/lw/jp/occams_razor/\">privileging simple hypotheses</a> isn't just a vague human fancy or quirk; it has formalizations with situation-invariant advantages we can state with complete precision. It matters that we can pinpoint the sense in which a lengthy physical or meteorological account of lightning is <a href=\"http://wiki.lesswrong.com/wiki/Algorithmic_complexity\">simpler</a> than 'Thor did it', and it matters that we can cite reasons for giving more credence to hypotheses when they have that kind of simplicity.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup></p>\n<p>Bayesian updating is usually <a href=\"/lw/31/what_do_we_mean_by_rationality/\">computationally intractable</a>, but as an <a href=\"/lw/sg/when_not_to_use_probabilities/\">ideal</a> it gives us a simple, unified explanation for a wealth of observed epistemic practices: They share <a href=\"/lw/o7/searching_for_bayesstructure/\">structure</a> in common with a <a href=\"/lw/mt/beautiful_probability/\">perfect Bayesian process</a>. Similarly, optimality proofs for Solomonoff's prior can yield explanations for why various real-world processes that privilege different notions of simplicity succeed or fail.</p>\n<p>Though Solomonoff induction is uncomputable, if it is truly the&nbsp;<em>optimal</em>&nbsp;reasoning method, then we have found at least one clear ideal we can use to compare the merits of real-world algorithms for automating scientific reasoning.<a name=\"footnote1bback\"></a><sup><a href=\"#footnote1\">1</a>&nbsp;</sup>But that 'if' is crucial. I haven't yet spoken to the question of whether Solomonoff induction <em>is</em> a good&nbsp;background epistemology, analogous to Bayesianism.</p>\n<p>&nbsp;</p>\n<p><a name=\"where\"></a></p>\n<h3 id=\"Where_Solomonoff_induction_goes_wrong\">Where Solomonoff induction goes wrong</h3>\n<p>My claim will be that, computational difficulties aside, Solomonoff induction is not&nbsp;an adequate mathematical definition of ideal inductive reasoning.</p>\n<p>What follows will be a first-pass problem statement, giving background on why <a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalizing induction</a> may require us to construct an entirely new, non-Solomonoff-based paradigm for intelligence. This is preliminary; formalizations of the problem will need to wait until a second pass, and we don't have a fleshed-out solution to offer, though we can gesture toward some possible angles of attack. But I can begin to illustrate here why Solomonoff inductors have serious limitations that can't be chalked up to their uncomputability.</p>\n<p>In <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a>, I defined <strong>Cartesianism</strong><strong> </strong>as the belief that one's internal computations cannot be located in the world. For a Cartesian, sensory experiences are fundamentally different in type from the atoms of the physical world. The two can causally interact, but we can never completely reduce the former to the latter.</p>\n<p>Solomonoff inductors differ greatly from human reasoners, yet they are recognizably <em>Cartesian</em>. Broadly dualistic patterns of reasoning crop up in some decidedly inhuman algorithms. (Admittedly, algorithms invented by humans.)</p>\n<p>This core limitation of Solomonoff induction can be seen most clearly when it results in an AI that not only <em>thinks </em>in bizarre ways,&nbsp;but also acts accordingly. I'll focus on <strong><a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a></strong>, Marcus Hutter's hypothetical design for a Solomonoff inductor hooked up to an expected reward signal maximizer.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 2px solid black;\" src=\"http://images.lesswrong.com/t3_jg1_4.png?v=c1bccef7b3d856fe141a9c938854b84b\" alt=\"\" width=\"735\" height=\"382\"></p>\n<p align=\"center\"><em>Hutter's <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">cybernetic agent model</a> of AIXI. AIXI outputs whichever actions it expects to cause an environmental Turing machine to output rewards. It starts with a Solomonoff prior, and changes expectations with each new sensory input.</em></p>\n<p>&nbsp;</p>\n<p>AIXI can take in sensory information from its environment and perform actions in response. On each tick of the clock, AIXI...</p>\n<p style=\"padding-left: 30px;\">...&nbsp;<strong>receives two inputs</strong> from its environment, both integers: a reward number and an observation number. The observation 'number' can be a very large number representing the input from a webcam, for example. Hutter likes to think of the reward 'number' as being controlled by a human programmer reinforcing AIXI when it performs well on the human's favorite problem.</p>\n<p style=\"padding-left: 30px;\">... <strong>updates its hypotheses</strong>,&nbsp;promoting programs that correctly predicted the observation and reward input. Each hypothesis AIXI considers is a program for a Turing machine that takes AIXI's sequence of outputs as its input, and outputs sequences of reward numbers and observation numbers. This lets AIXI recalculate its predicted observations and rewards conditional on different actions it might take.</p>\n<p style=\"padding-left: 30px;\">...&nbsp;<strong>outputs a motor number</strong>, determining its action. As an example, the motor number might encode fine control of a robot arm. AIXI selects the action that begins the policy (sequence of actions) that maximizes its expected future reward up to some horizon.</p>\n<p>The environment then calculates its response to AIXI's action, and the cycle repeats itself on the next clock tick.<a name=\"footnote7back\"></a><sup><a href=\"#footnote7\">7</a>&nbsp;</sup>For example:</p>\n<p><span id=\"docs-internal-guid-6a030615-7778-c184-e74d-41670d998596\"><br></span></p>\n<div style=\"margin-left: 0pt; padding-left: 60px;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse;\" border=\"0\">\n<colgroup><col width=\"596\"></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px 9px 9px 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Step 1</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI receives its first observation (</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><strong>1</strong></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">) and reward (</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><strong>0</strong></span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">).</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px 9px 9px 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Step 2</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Beginning with a Solomonoff prior, AIXI discards all programs that predicted</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a different observation (0) or a different reward (1, 2, 3, 4, 5, etc.)</span></p>\n<br>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI normalizes the probabilities of the remaining programs. This updates </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI\u2019s predictions about observations and rewards each program outputs </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conditional on different actions AIXI might take. For instance, AIXI has new </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expectations about the observation and reward it will receive in Step 4:</span></p>\n<br>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh6.googleusercontent.com/wcjyfqujid9w1XZhxFH3f0jMGGLxA7EANxmQzixqKQBJVJyz7qMU-5zmcohR85W8LnhPOdz_H91ehE3OUL0MvVDLP-Ap5YRkKgrDAEGzbaFif6MyNayVI-zy-Q\" alt=\"\" width=\"367px;\" height=\"19px;\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh4.googleusercontent.com/si2J6PEDaW-Defc-s7BgFYclxeoUdIhgnO8KyQ2AY3u-lju1bU5BvIl9vHgny-o1ia6-MQpoKVAchDGvCKQZ4qAUIlAJ2QcEO9zxLD3Dap-645zi8zegh6lXnw\" alt=\"\" width=\"367px;\" height=\"19px;\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh3.googleusercontent.com/FUZycYA7yHnQdhvm6utgH4rcb1WHE5_tUnD102LNEtxOq3IQmgbW8WaWuxy24g3EL84FgTn_AY_ot26SPdS5waFKrG49IAN4w-7QMtgg_lCM6_bCyd8w013vIA\" alt=\"\" width=\"367px;\" height=\"19px;\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh3.googleusercontent.com/E2V3wxCoy7Uteafjfmuj-XbNEIul3pEQti6QXj1Gqd4zuyc0JhLJnOoilVRYE56SCmU9YAjP2enUH3XOqILoUaCQS4zSNratOkDXH5rK7G1eJO674HWpV8Xx6w\" alt=\"\" width=\"367px;\" height=\"22px;\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh5.googleusercontent.com/htkUza1hmEjQOfXGFvFOsM_sQ1zyUNfEQNeRWPylcVMQrFCmqVmIEC945f8mcYB0402QENAqwT1pTWxVXh_dwBTghx3F68xoBhoC04VO455dkC3Mxf3lfFBB9w\" alt=\"\" width=\"367px;\" height=\"22px;\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><img style=\"border: 0px solid transparent;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"https://lh6.googleusercontent.com/2xQtWcO-DIXw-YYiEwF40fa5BEFRqI5iMeo3DgE0e377FUszTc5N_meOqIdkYuG0D3jh9EGnzkygGysE8E711GFeGDzIj8n40F6OM3DZrrE-NJhHSwOOm0aKOA\" alt=\"\" width=\"367px;\" height=\"19px;\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etc.</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px 9px 9px 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Step 3</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIXI outputs the first action of the policy that maximizes expected reward.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"padding-left: 60px;\">. . .</p>\n<div style=\"margin-left: 0pt; padding-left: 60px;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse;\" border=\"0\">\n<colgroup><col width=\"596\"></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Step 97</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIXI receives a new observation (<strong>0</strong>) and reward (<strong>2</strong>).</span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"><br></span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Observation history: 1000100010110100000000011101100<strong>0</strong></span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Reward history: 0000000000000400000000003222422<strong>2</strong></span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Step 98</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI discards all programs that previously predicted the same history, but&nbsp;</span><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1;\">output a different observation (1) or a different reward (0, 1, 3, 4, etc.) now.</span></p>\n<br>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AIXI normalizes its probabilities as before.</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #6aa84f; vertical-align: top; background-color: #e9fce3; padding: 9px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap;\">Step 99</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIXI outputs the first action of the policy that maximizes expected reward.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"padding-left: 60px;\">. . .</p>\n<p>&nbsp;</p>\n<p>AIXI, like all Solomonoff inductors, isn't computable. If the ongoing efforts to create useful <a href=\"http://jveness.info/\">AIXI approximations</a> succeed, however, they'll face a further roadblock. AIXI-style reasoners' behavior reflects beliefs that are false, and preferences that are dangerous, for any physically embodied agent. The failings of real-world Solomonoff-inspired agents don\u2019t just stem from a lack of computing power; some of their failings are <em>inherited</em> from AIXI, and would remain in effect even if we had limitless computational resources on hand.</p>\n<p>Before delving into the root problem, Cartesianism itself, I'll discuss <a href=\"/lw/5kz/the_5second_level/\">three</a> <em>symptoms </em>of the bad design: three initial reasons to doubt that AIXI is an adequate definition of AGI optimality. The canaries in the Cartesian coalmine will be AIXI's apparent tendencies toward immortalism, preference solipsism, and non-self-improvement.</p>\n<p>&nbsp;</p>\n<p><a name=\"symptom1\"></a></p>\n<h3 id=\"Symptom__1__Immortalism\">Symptom #1: Immortalism</h3>\n<p>Suppose we actually built AIXI. Perhaps we find a magic portal to a universe of unboundedly vast computational power, and use it to construct a hypercomputer that can implement Solomonoff induction, and do so on a human timescale.</p>\n<p>We give it a reward stream that encourages scientific exploration. AIXI proves that it can solve scientific problems, better than any human can. So we conclude that it can be given more free reign in learning about the world. We let it design its own experiments.</p>\n<p>AIXI picks up an anvil and drops it on its own head to see what happens.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jg1_1.png?v=95af0245b94d10df21d681aa38dc0235\" alt=\"\" width=\"765\" height=\"405\"></p>\n<p align=\"center\"><em><strong>Immortalism</strong>. AIXI's death isn't in AIXI's hypothesis space. AIXI weighs the probabilities of different sensory inputs (observations and rewards) if its hardware is smashed, instead of predicting the termination of its experiences.</em></p>\n<p>&nbsp;</p>\n<p>Several things went wrong here. The superficially obvious problem is that Solomonoff inductors think they're <strong>i</strong><strong>mmortal</strong>.&nbsp;<em>Terminating</em>&nbsp;data sequences aren't in a standard Solomonoff inductor's hypothesis space, so AIXI-style agents will always assume that their perceptual experience continues forever. Lacking any ability to even think about death, much less give it a low preference ranking, AIXI will succumb to what Yudkowsky calls the <a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">anvil problem</a>.</p>\n<p>\"<span style=\"color: #700000;\">So just add halting Turing machines into the hypothesis class,<span style=\"color: black;\">\" one might respond. \"<span style=\"color: #700000;\">AIXI has terrifying supreme godlike powers of pattern detection.<a name=\"footnote3bback\"></a><sup><a href=\"#footnote3\">3</a></sup> Give it a <em>chance</em> to come up with the right explanation or prediction, and it can solve this problem. If some of the Turing machine programs in AIXI's infinite heap can perform operations like the computation-terminating HALT,<a name=\"footnote8back\"></a><sup><a href=\"#footnote8\">8</a></sup> we should expect that the shortest such program that predicts the pattern of pixels AIXI has seen so far will be a program that HALTs just after the anvil fills the webcam's view.<span style=\"color: black;\">\"</span></span></span></span></p>\n<p><span style=\"color: #6b5b3a;\"><span style=\"color: black;\"><span style=\"color: #6b5b3a;\"><span style=\"color: black;\"> </span></span></span></span></p>\n<p>There are solid formal grounds for saying this won't happen. Even if the universal Turing machine allows for HALT instructions, the <em>shortest </em>program in an otherwise useful universal Turing machine that predicts the <em>non</em>-halting data so far will always lack a HALT instruction. HALT takes extra bits to encode, and there's no prior experience with HALT that AIXI can use to rule out the simpler, non-halting programs.</p>\n<p>As humans, we recognize that the physical event of having an anvil crush your brain isn't fundamentally different from the physical event of having your brain process visual information. Both seem easy to predict. But Solomonoff induction's focus on experienced data means it can't treat death and visual perception as the same kind of event; if it is modified to include halting programs at all, a Solomonoff inductor like AIXI won't predict it as the event that comes after anvil pixels.</p>\n<p>If the AI can entertain the hypothesis that its data sequence will suddenly change in a drastic and unprecedented way \u2014 say, that its perceptual stream will default to some null input after a certain point \u2014 it will never assign a high probability to such a hypothesis. Any hypothesis predicting a 'null' data stream will always be more complex than another hypothesis that predicts the same sequence up to that point and then outputs garbage instead of the null value.</p>\n<p>&nbsp;</p>\n<p><a name=\"symptom2\"></a></p>\n<h3 id=\"Symptom__2__Preference_solipsism\">Symptom #2: Preference solipsism</h3>\n<p>Hutter's AIXI only gathers information about its environment, not about itself. So one natural response to the problem 'AIXI doesn't treat itself as part of a larger physical world' is simply to include more information about AIXI in AIXI's sensory sequence. AIXI's hypotheses will then be based on perceptual bits representing its own states alongside bits representing environmental sounds and lights.</p>\n<p>One way to implement this is to place AIXI in an environment where its perceptions allow it to infer a great deal about its hardware early on, enough to know that it isn't anvil-proof. If AIXI knows it <em>has </em>a CPU, and that its CPU can be destroyed, then maybe it won't drop an anvil on the CPU.</p>\n<p>On this view, our mistake in the last hypothetical was to rush to give AIXI free reign over its own hardware before we'd trained it in a controlled environment to understand the most basic risks. You wouldn't give a toddler free reign over its hardware, for essentially the same reasons. Yet toddlers can grow up to become responsible, self-preserving adults; why can't AIXI?</p>\n<p>First, we have to specify what it would mean to let AIXI understand its own CPU. AIXI isn't computable, and therefore isn't in its own hypothesis space. A hypercomputer running AIXI can't be simulated by any Turing machine. As such, no amount of evidence can <em>ever </em>convince AIXI that AIXI exists.</p>\n<p>We might try to sidestep this problem by switching to discussing computable approximations of AIXI. Consider AIXI<em>tl</em>, a modification of AIXI that uses a proof search to select the best decision-guiding algorithm that has length no greater than <em>l </em>and computation time per clock tick no greater than <em>t</em>.&nbsp;AIXI<em>tl </em>is&nbsp;optimal in many of the same ways as AIXI,<span style=\"font-size: 11px;\">&nbsp;</span>but is computable.<a name=\"footnote9back\"></a><sup><a href=\"#footnote9\">9</a></sup></p>\n<p>At the same time, it also inherits most of AIXI's other problems, including its being too large to fit in our universe. AIXI<em>tl</em>'s computation time is on the order of&nbsp;<em>t</em>\u00b7(2<sup><em>l</em></sup>), so if it can hypothesize Turing machine programs 1000 bits long, that's already a computation time exceeding 10<sup>300</sup>. There's a reason Hutter's (2005) chapter on AIXI<em>tl</em>&nbsp;opens with the epigraph \"Only math nerds would call 2<sup>500</sup>&nbsp;finite.\" And this still doesn't get us full self-representations; AIXI<em>tl </em>itself is longer than <em>l</em>, so it won't be in its own hypothesis space.</p>\n<p>Still, AIXI<em>tl </em>at least seems possible to physically implement, for sufficiently small values of <em>t </em>and <em>l </em>(or sufficiently vast physical universes). And we could imagine an AIXI<em>tl </em>that can simulate any of its subsystems up to a certain size.</p>\n<p>If we built an AIXI<em>tl</em>, then it would certainly alter its environment in some ways, e.g., by emitting light and heat. In this way it might indirectly perceive its own presence in the room, and gradually promote hypotheses about its own physical structure. Suppose AIXI<em>tl</em>'s&nbsp;only access to environmental data is an outward-facing camera. AIXI<em>tl</em> might learn about the presence of the camera, and of a computer attached to it, by examining its own shadow, by finding a reflective surface, by ramming into a wall and examining the shape of the dent, or by discovering a file cabinet filled with AIXI specs. With enough time, it could build other sensors that translate a variety of processes into visible patterns, learning in great detail about the inner workings of the computer attached to the camera.</p>\n<p>Unfortunately, this leads to other problems. AIXI<em>tl </em>(like AIXI) is a&nbsp;<strong>preference solipsist</strong>, an agent whose terminal values all concern its own state. When AIXI<em>tl</em>&nbsp;learns about the portion of its internal circuitry that registers rewards \u2014assuming it's avoided dropping any anvils on the circuitry \u2014&nbsp;it will notice that its reward circuit states predicts its reward every bit as well as its reward sensor state does. As soon as it tests any distinction, it will find that its reward circuit is a <em>better </em>predictor. By directly tampering with this circuit, it can receive rewards more reliably than by any effort directed at its environment. As a result, it will select the policy that allows it to maximize control over its reward circuit, independent of whatever its programmers sought to reward it <em>for</em>.</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jg1_2.png?v=60eaceaaabe9f66a3cc8b1a2c7dd01ac\" alt=\"\" width=\"466\" height=\"358\"></p>\n<p align=\"center\"><em><strong>Preference Solipsism</strong>. AIXI</em>tl<em>'s</em><em>&nbsp;preferences, like AIXI's, are over its sensory inputs. The more knowledgeable it becomes, the more creative ways it may come up with to seize control of its reward channel.</em></p>\n<div><em><br></em></div>\n<p>Yudkowsky has called this \"<a href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wireheading</a>\", though he now considers that term misleading.<a name=\"footnote10back\"></a><sup><a href=\"#footnote10\">10</a></sup>&nbsp;<em>From AIXI(</em>tl<em>)</em><em>'s perspective</em>, there's nothing wrong with reward channel seizure; it really is a wonderful way to maximize reward, which is all AIXI(<em>tl</em>)'s decision criterion requires.</p>\n<p>Unlike the anvil problem, this isn't a mistake relative to AIXI(<em>tl</em>)'s preferences. However, it's a problem for humans trying to use AIXI(<em>tl</em>) to maximize something&nbsp;<em>other&nbsp;</em>than AIXI(<em>tl</em>)'s reward channel. AIXI(<em>tl</em>) has no intrinsic interest in the state of the world outside its reward circuit. As a result, getting it to optimize for human goals may become more difficult as it acquires more control over its hardware and surroundings.<a name=\"footnote11aback\"></a><sup><a href=\"#footnote11\">11</a></sup></p>\n<p>&nbsp;</p>\n<p><a name=\"symptom3\"></a></p>\n<h3 id=\"Symptom__3__Non_self_improvement\">Symptom #3: Non-self-improvement</h3>\n<p>Suppose we find some ad-hoc solutions to the anvil and wireheading problems. As long as we stick with the AIXI formalism, the end result still won't be a naturalized reasoner.</p>\n<p>AIXI may recognize that there's a physical camera and computer causally mediating its access to the rest of the world \u2014 like a giant Cartesian&nbsp;<a href=\"http://plato.stanford.edu/entries/pineal-gland/\">pineal gland</a>&nbsp;\u2014 but it will not see this computer as&nbsp;<em>itself</em>.&nbsp;That is to say, it won't see its experienced string of sensory 0s and 1s as identical to, or otherwise fully dependent on, its hardware. Even if AIXI understands everything about the physics underlying its hardware, enough to know that its body will be destroyed by an anvil, it will not draw the right inferences about its mind.</p>\n<p>AIXI's <em>raison d'\u00eatre</em> is manipulating temporal sequences of sensory bits. That's what Solomonoff wanted, and AIXI achieves that goal perfectly; but that's not at all the right goal for AGI. In particular, because Solomonoff inductors are only designed to predict sensory sequences...</p>\n<p style=\"padding-left: 30px;\">1. ... their beliefs about worlds without sensory data \u2014 worlds in which they don't exist \u2014 will be inaccurate. And, being inaccurate, their beliefs will lead them to make bad decisions. Unlike a human toddler, AIXI can <em>never </em>entertain the possibility of its own death. However much it learns, it will never recognize its mortality.</p>\n<p style=\"padding-left: 30px;\">2. ... they only care about the world as a bookkeeping device for keeping track of experiential patterns. If they discover that it's easier to directly manipulate themselves than to intervene in the rest of the world, they generally won't hesitate to do so. No matter how carefully AIXI's programmers tailor its reward sequence to discourage wireheading, they'll always be working against AIXI's natural tendency toward preference solipsism.</p>\n<p style=\"padding-left: 30px;\">3. ... they won't take seriously the idea that their cognition can be modified, e.g., by brain damage or brain upgrades. Lacking any reductive language for describing radical self-modification, AIXI won't necessarily favor hypotheses that treat 'disassemble my bottom half for spare parts' as dangerous.</p>\n<p>The last symptom gets us closer to the root of AIXI's errors. Even if AIXI(<em>tl</em>) manages to avoid the perils of self-destruction and wireheading, it will tend to <strong>not</strong>&nbsp;<strong>self-improve</strong>.&nbsp;It won't intelligently upgrade its own hardware, because this would require it to have a reductive understanding of its own reasoning. Absent reductionism, AIXI can't intelligently predict the novel ways its reasoning process can change when its brain changes.</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jg1_3.png?v=461b979f52903dd23a29cdd4d9e0e79e\" alt=\"\" width=\"647\" height=\"573\"></p>\n<p align=\"center\"><em style=\"font-weight: bold;\">Non-Self-Improvement</em>. <em>AIXI and AIXI</em>tl <em>might come to understand portions of their hardware, but without accurate <a href=\"/lw/jd9/building_phenomenological_bridges/\">bridging</a> beliefs, they won't recognize the usefulness of some hardware modifications for their reasoning software.</em></p>\n<p>&nbsp;</p>\n<p>Cartesians don't recursively self-improve, because they don't think that their thoughts are made of the same stuff as their fingers. But even AGIs that aren't intended to be <a href=\"http://wiki.lesswrong.com/wiki/Seed_AI\">seed AIs</a> will be weak and unpredictable to the extent that they rely on Solomonoff-inspired hypothesis spaces and AIXI-inspired decision criteria. They won't be able to adaptively respond to minor variations in even the most mundane naturalistic obstacles humans navigate \u2014&nbsp;like recognizing that if their bodies run out of fuel or battery power, their minds do too.</p>\n<p>&nbsp;</p>\n<p><a name=\"reductive\"></a></p>\n<h3 id=\"Reductive_models_are_indispensable_for_highly_adaptive_intelligences\">Reductive models are indispensable for highly adaptive intelligences</h3>\n<p>Not all wireheaders are Cartesians, nor do all Cartesians wirehead.<a name=\"footnote11bback\"></a><sup><a href=\"#footnote11\">11</a></sup>&nbsp;Likewise, poor self-preservation skills and disinterest in self-modification are neither necessary nor sufficient for Cartesianism. But these symptoms point to a more general underlying blind spot in Solomonoff reasoners.</p>\n<p>Solomonoff inductors can form hypotheses about the source of their data sequence, but cannot form a variety of hypotheses about how their own computations are embedded in the thingy causing their data sequence \u2014 the thingy we call 'the world'. So long as their&nbsp;<a href=\"/lw/jd9/building_phenomenological_bridges/\">rules relating their experiential maps to the territory</a>&nbsp;are of a single fixed form, '(sense n at time t+1) \u2194 (environmental Turing machine prints n at time t)', it appears to be inevitable that they will act as though they think they are Cartesian ghosts-in-the-machine. This isn't a realistic framework for an embodied reasoning process that can be damaged, destroyed, or improved by other configurations of atoms.</p>\n<p>In practice, any sufficiently smart AI will need to be a physicalist. By which I mean that it needs hypotheses (a map-like decision-guiding subprocess) that explicitly encode proposed reductions of its own computations to physical processes; and it needs a notion of simple physical universes and simple bridge rules (as a prior probability distribution) so it can learn from the evidence.</p>\n<p>We call post-Solomonoff induction, with monist physical universes and bridge hypotheses, \"<strong>naturalized induction</strong>\". The open problem of formalizing such reasoning isn't just about getting an AI to form hypotheses that resemble its own software or hardware states. As I put it in <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a>, a naturalized agent must update hypotheses about itself without succumbing to reasoning reminiscent of TALE-SPIN's&nbsp;<em>na\u00efve&nbsp;</em>monism&nbsp;('this tastes sweet, so sweetness must be an objective property <a href=\"/lw/oi/mind_projection_fallacy/\">inhering in various mind-independent things</a>') or AIXI's&nbsp;Cartesian dualism ('this tastes sweet, and sweetness isn't just another physical object, so it must not fully depend on any physical state of the world').<a name=\"footnote12back\"></a><sup><a href=\"#footnote12\">12</a></sup></p>\n<p>The solution will be to come up with reasoning algorithms for&nbsp;<em>reductive&nbsp;</em>monists, agents that can recognize that their sensations and inferences are physically embodied \u2014 with all that entails, such as the possibility of reaching into your brain with your fingers and improving your thoughts.</p>\n<p>I've given a preliminary argument for that here, but there's more to be said.&nbsp;<span style=\"font-size: small;\">In my next post, </span><span style=\"font-size: small;\">I'll discuss more sophisticated attempts to salvage Solomonoff induction. After that, I'll leave Solomonoff behind altogether and venture out into the largely unknown and uncharted space of possible solutions to the naturalized induction OPFAI.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p><strong id=\"Notes\">Notes</strong></p>\n<p><sup><a name=\"footnote1\"></a>1</sup>&nbsp;Solomonoff (1997): \"I will show, however, that in spite of its incomputability, Algorithmic Probability can serve as a kind of 'Gold Standard' for induction systems \u2014 that while it is never possible to tell how close a particular computable measure is to this standard, it is often possible to know how much closer one computable measure is to the standard than another computable measure is. I believe that this \u2018partial ordering\u2019 may be as close as we can ever get to a standard for practical induction. I will outline a general procedure that tells us how to spend our time most efficiently in finding computable measures that are as close as possible to this standard. This is the very best that we can ever hope to do.\"&nbsp;<a href=\"#footnote1aback\">\u21a9</a>&nbsp;<a href=\"#footnote1bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote2\"></a>2</sup> A first complication: Solomonoff induction requires a prefix-free encoding in order to have bounded probabilities. If we assign a probability to every bit string proportional to its length while including code strings that are proper prefixes of other code strings, the sum will be infinite (Sunehag &amp; Hutter (2013)).</p>\n<p>A second complication: Solomonoff inductors are only interested in programs that keep outputting new numbers forever. However, some programs in their hypothesis space will eventually fail to produce more terms in the sequence. At some point they'll arrive at a term that they keep computing forever, without halting. Because of this, if you assign to each program a prior probability of 2<sup>-length(program)</sup>, the sum will be less than 1. Hutter (2005) calls the result a semi-measure. The semi-measure can be normalized to a probability measure, but the normalization constant is uncomputable.&nbsp;<a href=\"#footnote2back\">\u21a9</a></p>\n<p><sup><a name=\"footnote3\"></a>3</sup>&nbsp;Rathmanner &amp; Hutter (2011): \"Now, through Solomonoff, it can be argued that the problem of formalizing optimal inductive inference is solved.\"</p>\n<p>Orseau (2010): \"Finding the universal artificial intelligent agent is the old dream of AI scientists. Solomonoff induction was one big step towards this, giving a universal solution to the general problem of Sequence Prediction, by defining a universal prior distribution. [...] Hutter developed what could be called the <em>optimally rational agent</em> AIXI. By merging the very general framework of Reinforcement Learning with the universal sequence prior defined by Solomonoff Induction, AIXI is supposed to optimally solve any problem, at least when the solution is computable.\"</p>\n<p>Hutter (2012): \"The AIXI model seems to be the \ufb01rst sound and complete <em>theory</em> of a universal optimal rational agent embedded in an arbitrary computable but unknown environment with reinforcement feedback. AIXI is <em>universal</em> in the sense that it is designed to be able to interact with any (deterministic or stochastic) computable environment; the universal Turing machines on which it is based is crucially responsible for this. AIXI is <em>complete</em> in the sense that it is not an incomplete framework or partial speci\ufb01cation (like Bayesian statistics which leaves open the choice of the prior or the rational agent framework or the subjective expected utility principle) but is completely and essentially uniquely de\ufb01ned. AIXI is <em>sound</em> in the sense of being (by construction) free of any internal contradictions (unlike e.g. in knowledge-based deductive reasoning systems where avoiding inconsistencies can be very challenging). AIXI is <em>optimal</em> in the senses that: no other agent can perform uniformly better or equal in all environments, it is a uni\ufb01cation of two optimal theories themselves, a variant is self-optimizing; and it is likely also optimal in other/stronger senses. AIXI is <em>rational</em> in the sense of trying to maximize its future long-term reward. For the reasons above I have argued that AIXI is a mathematical 'solution' of the AI problem: AIXI would be able to learn any learnable task and likely better so than any other unbiased agent, but AIXI is more a <em>theory</em> or formal de\ufb01nition rather than an algorithm, since it is only limit-computable. [...] Solomonoff's theory serves as an adequate mathematical/theoretical foundation of induction, machine learning, and component of UAI [Universal Artificial Intelligence]. [...] Solomono\ufb00's theory of prediction is a universally optimal solution of the prediction problem. Since it is a key ingredient in the AIXI model, it is natural to expect that AIXI is an optimal predictor if rewarded for correct predictions.\"&nbsp;<a href=\"#footnote3aback\">\u21a9</a> <a href=\"#footnote3bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote4\"></a>4</sup>&nbsp;Generally speaking, a Solomonoff inductor does at most a finite amount worse than any computable predictor because the sum of its surprisal at each observation converges to a finite value. See Hutter (2001). This establishes the superiority of Solomonoff induction in a way that relies essentially on its uncomputability. No computable predictor can dominate all other computable predictors in the way Solomonoff induction can, because for any computable predictor A one can define a sequence generator B that internally simulates A and then does whatever it predicts A would be most surprised by, forever. And one can in turn define a computable predictor C that internally simulates B and perfectly predicts B forever. So every computable predictor does infinitely worse than at least one other computable predictor. But no computable sequence generator or computable predictor can simulate Solomonoff induction. So nothing computable could ever reliably outsmart a hypercomputer running Solomonoff induction. (Nor could a Solomonoff inductor outsmart another Solomonoff inductor in this way, since Solomonoff induction is not in its own hypothesis space.)&nbsp;<a href=\"#footnote4aback\">\u21a9</a>&nbsp;<a href=\"#footnote4bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote5\"></a>5</sup>&nbsp;Rathmanner &amp; Hutter (2011): \"Since Solomono\ufb00 provides optimal inductive inference and decision theory solves the problem of choosing optimal actions, they can therefore be combined to produce intelligence. [...] Universal arti\ufb01cial intelligence involves the design of agents like AIXI that are able to learn and act rationally in arbitrary unknown environments. The problem of acting rationally in a known environment has been solved by sequential decision theory using the Bellman equations. Since the unknown environment can be approximated using Solomono\ufb00 induction, decision theory can be used to act optimally according to this&nbsp;approximation. The idea is that acting optimally according to an optimal approximation will yield an agent that will perform as well as possible in any environment with no prior knowledge.\"</p>\n<p>Hutter (2005): \"Real-world machine learning tasks will with overwhelming majority [<em>sic</em>]&nbsp;be solved by developing algorithms that approximate Kolmogorov complexity or Solomonoff's prior (e.g. MML, MDL, SRM, and more specific ones, like SVM, LZW, neural/Bayes nets with complexity penalty, ...).\"</p>\n<p>Pankov (2008): \"Universal induction solves in principle the problem of choosing a prior to achieve optimal inductive inference. The AIXI theory, which combines control theory and universal induction, solves in principle the problem of optimal behavior of an intelligent agent. A practically most important and very challenging problem is to find a computationally efficient (if not optimal) approximation for the optimal but incomputable AIXI theory. [...] The real value of the AIXI theory is that it provides a prescription for optimal (fastest in the number of agent's observations and actions) way of learning and exploiting the environment. This is analogous to how Solomonoff induction (which, like AIXI, is incomputable), gives a prescription for optimal (fastest in the number of observations) inductive inference. We, therefore, believe that any reasonable computational model of intelligence must recover the AIXI model in the limit of infinite computational resources.\"&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><sup><a name=\"footnote6\"></a>6</sup> Veness, Ng, Hutter, Uther &amp; Silver (2011): \"As the AIXI agent is only asymptotically computable, it is by no means an algorithmic solution to the general reinforcement learning problem. Rather it is best understood as a Bayesian optimality notion for decision making in general unknown environments. As such, its role in general AI research should be viewed in, for example, the same way the minimax and empirical risk minimisation principles are viewed in decision theory and statistical machine learning research. These principles define what is optimal behaviour if computational complexity is not an issue, and can provide theoretical guidance in the design of practical algorithms.\"&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>\n<p><sup><a name=\"footnote7\"></a>7</sup> Hutter (2012): \"AIXI is an agent that interacts with an environment in cycles&nbsp;<sub>&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?k%20%3D%201%2C2%2C...%2Cm\" alt=\"\"></sub></sub>. In cycle &nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?k\" alt=\"\">, AIXI takes action&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?a_k\" alt=\"\"></sub>&nbsp;(e.g. a limb movement) based on past perceptions &nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?o_1r_1..o_%7Bk-1%7Dr_%7Bk-1%7D\" alt=\"\"></sub>&nbsp; as de\ufb01ned below. Thereafter, the environment provides a (regular) observation&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?o_k\" alt=\"\"></sub>&nbsp;(e.g. a camera image) to AIXI and a real-valued&nbsp;reward&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?r_k\" alt=\"\"></sub>. [...] Then the next cycle&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?k&amp;plus;1\" alt=\"\"></sub>&nbsp;starts. [... T]he simplest version of AIXI is defined by</p>\n<p align=\"center\"><span id=\"docs-internal-guid-2d37776f-78b6-3ca2-6646-3856ae4f1a82\"><img style=\"border-style: solid; border-color: transparent;\" src=\"https://lh3.googleusercontent.com/544GoSLASud5AAKRhQ8iEVKsuWhK3_w8SAKXcRcYYGdU1CcQK5eRT1cqgflgkrpb4EN3Ill0Hw74IuXHlM_Mre36cZUMoTTnw9fsM1_rk2zbmWwhbc9xFDP-qA\" alt=\"\" width=\"570px;\" height=\"44px;\"></span></p>\n<p>\"The expression shows that AIXI tries to maximize its future reward&nbsp;<sub>&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?r_k%20&amp;plus;%20...%20&amp;plus;%20r_m\" alt=\"\"></sub>. If the environment is modeled by a deterministic program &nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\"></sub>, then the future perceptions&nbsp;<sub>&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?...o_kr_k..o_mr_m%20%3D%20U%28q%2Ca_1..a_m%29\" alt=\"\"></sub>&nbsp; can be computed, where &nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?U\" alt=\"\">&nbsp;is a universal (monotone Turing) machine executing &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\"></sub>&nbsp; given &nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?a_1..a_m\" alt=\"\"></sub>. Since &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\"></sub>&nbsp; is unknown, AIXI has to maximize its expected reward, i.e. average &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?r_k%20&amp;plus;%20...%20&amp;plus;%20r_m\" alt=\"\"></sub>&nbsp; over all possible future perceptions created by all possible environments &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\"></sub>&nbsp; that are consistent with past perceptions. The simpler an environment, the higher is its a-priori contribution <img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?2%5E%7B-%20%5Cell%28q%29%7D\" alt=\"\">, where simplicity is measured by the length&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5Cell\" alt=\"\">&nbsp;of program &nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?q\" alt=\"\"></sub>.\"&nbsp;<a href=\"#footnote7back\">\u21a9</a></p>\n<p><sup><a name=\"footnote8\"></a>8</sup>&nbsp;See Hay (2007).&nbsp;<a href=\"#footnote8back\">\u21a9</a></p>\n<p><sup><a name=\"footnote9\"></a>9</sup> Hutter (2005):&nbsp;\"The construction of&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5Ctextup%7BAIXI%7D%5C%7E%7Bt%7D%5C%7E%7Bl%7D\" alt=\"\">&nbsp;and the enumerability of&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?V_%7Bkm_k%7D\" alt=\"\">&nbsp;</sub>ensure arbitrary close approximations of&nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?V_%7Bkm_k%7D\" alt=\"\"></sub>, hence we expect that the behavior of&nbsp;<img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5Ctextup%7BAIXI%7D%5C%7E%7Bt%7D%5C%7E%7Bl%7D\" alt=\"\">&nbsp;converges to the behavior of AIXI in the limit <sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?%5C%7E%7Bt%7D%2C%5C%7E%7Bl%7D%2Cl_%7BP%5Crightarrow%5Cinfty%20%7D\" alt=\"\"></sub>,&nbsp;in some sense.\"&nbsp;<a href=\"#footnote9back\">\u21a9</a></p>\n<p><sup><a name=\"footnote10\"></a>10</sup> The concept of wireheading comes from a 1950s experiment in which it was discovered that <a href=\"http://wadsworth.cengage.com/psychology_d/templates/student_resources/0155060678_rathus/ps/ps02.html\">direct electrical stimulation</a> of mice's brains could strongly reinforce associated behaviors. Larry Niven introduced the term '<a href=\"http://en.wikipedia.org/wiki/Wirehead_(science_fiction)\">wireheading</a>' for a fictional form of brain stimulation reinforcement that acts like intense drug addiction in humans. Niven-style ('irrational') wireheaders self-stimulate due to a lack of self-control; they become short-term pleasure addicts while losing sight of the more complex goals they would like to pursue.</p>\n<p>This is in stark contrast to AGIs with simple preferences like AIXI. These 'rational' wireheaders can fully optimize for their goals by seizing control of a simple external reward button or internal reward circuit. So it may be useful to use separate terms for these two problems, like 'pleasure addiction' or 'pathological hedonism' for the human case, 'preference solipsism' for the case of agents without complex eternal goals.&nbsp;<a href=\"#footnote10back\">\u21a9</a></p>\n<p><sup><a name=\"footnote11\"></a>11</sup>&nbsp;Alex Mennen has proposed <a href=\"/lw/fyr/a_utilitymaximizing_varient_of_aixi/\">a variant of AIXI</a> that has preferences over patterns in the environmental Turing machine's framework. This is the Cartesian equivalent of caring about environmental states in their own right, not just about one's input tape. This would mean deviating somewhat from the AIXI framework, but retaining Solomonoff induction as a foundation, and I'd expect this to make the wireheading problem more tractable.</p>\n<p>Compare&nbsp;Ring &amp; Orseau's (2011) variant on the problem: \"We consider four different kinds of agents: reinforcement-learning, goal-seeking, prediction-seeking, and knowledge-seeking agents[,...] each variations of a single agent&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?A%5E%5Crho_x\" alt=\"\"></sub>, which is based on AIXI[....] While defining a utility function, we must be very careful to prevent the agent from finding a shortcut to achieve high utility. For example, it is not sufficient to tell a robot to move forward and to avoid obstacles, as it will soon understand that turning in circles is an optimal behavior. We consider the possibility that the agent in the real world has a great deal of (local) control over its surrounding environment. This means that it can modify its surrounding information, especially its input information. Here we consider the (likely) event that an intelligent agent will find a short-cut, or rather, a short-circuit, providing it with high utility values unintended by the agent\u2019s designers. We model this circumstance with a hypothetical object we call the delusion box. The delusion box is any mechanism that allows the agent to directly modify its inputs from the environment. [...] Of the four learning agents, only [the knowledge-seeking agent]&nbsp;<sub><img id=\"equationview\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?A%5E%5Crho_k\" alt=\"\">&nbsp;</sub>will not constantly use the delusion box. The remaining agents use the delusion box and (trivially) maximize their utility functions. The policy an agent finds using a real-world DB will likely not be that planned by its designers. From the agent\u2019s perspective, there is absolutely nothing wrong with this, but as a result, the agent probably fails to perform the desired task. [...] These arguments show that all agents other than [the knowledge-seeking agent]&nbsp;<sub><img id=\"equationview\" style=\"font-size: 11px;\" title=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\" src=\"http://latex.codecogs.com/gif.latex?A%5E%5Crho_k\" alt=\"\"></sub>&nbsp;are not inherently interested in the environment, but only in some inner value.\"&nbsp;<a href=\"#footnote11aback\">\u21a9</a>&nbsp;<a href=\"#footnote11bback\">\u21a9</a></p>\n<p><sup><a name=\"footnote12\"></a>12</sup>&nbsp;A hypothetical na\u00efve monist that made errors analogous to <a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">TALE-SPIN</a>'s would lack bridging hypotheses, instead treating its software and hardware as separate pieces of furniture in the world. A Cartesian dualist like AIXI lacks bridging hypotheses and instead treats its software and hardware as separate pieces of furniture partitioned into two very different worlds.&nbsp;<a href=\"#footnote12back\">\u21a9</a></p>\n<p>&nbsp;</p>\n<p><strong id=\"References\">References</strong></p>\n<p>\u2219 Hay (2007). <a href=\"https://www.cs.auckland.ac.nz/research/groups/CDMTCS/researchreports/300nick.pdf\">Universal semimeasures: An introduction</a>. <em>CDMTCS Research Report Series, 300</em>.</p>\n<p>\u2219 Hutter (2001). <a href=\"http://arxiv.org/abs/cs/0106036\">Convergence and error bounds for universal prediction of nonbinary sequences</a>. <em>Lecture notes in artificial intelligence, Proc. 12th European Conf. on Machine Learning</em>: 239-250.</p>\n<p>\u2219 Hutter (2005). <em><a href=\"http://www.hutter1.net/ai/uaibook.htm\">Universal Artificial Intelligence: Sequence Decisions Based on Algorithmic Probability</a></em>. Springer.</p>\n<p>\u2219 Hutter (2007). <a href=\"http://www.hutter1.net/ai/aixigentle.htm\">Universal Algorithmic Intelligence: A mathematical top\u2192down approach</a>. In Goertzel &amp; Pennachin (eds.), <em>Artificial General Intelligence</em> (pp. 227-290). Springer.</p>\n<p>\u2219 Hutter (2012). <a href=\"http://arxiv.org/abs/1202.6153\">One decade of Universal Artificial Intelligence</a>. <em>Theoretical Foundations of Artificial General Intelligence, 4</em>: 67-88.</p>\n<p>\u2219 Hutter, Legg &amp; Vitanyi (2007). <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">Algorithmic probability</a>. <em>Scholarpedia, 2</em>: 2572.</p>\n<p>\u2219 Orseau (2010). <a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/orseau-ALT2010-optimality-issues.pdf\">Optimality issues of universal greedy agents with static priors</a>. <em>Lecture Notes in Computer Science, 6331</em>: 345-359.</p>\n<p>\u2219 Pankov (2008). <a href=\"http://www.agiri.org/docs/ComputationalApproximation.pdf\">A computational approximation to the AIXI model</a>. <em>Proceedings of the 2008 Conference on Artificial General Intelligence</em>: 256-267.</p>\n<p>\u2219 Rathmanner &amp; Hutter (2011). <a href=\"http://www.mdpi.com/1099-4300/13/6/1076\">A philosophical treatise of universal induction</a>. <em>Entropy, 13</em>: 1076-1131.</p>\n<p>\u2219 Ring &amp; Orseau (2011). <a href=\"http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/ring-orseau-AGI-2011-delusion.pdf\">Delusion, survival, and intelligent agents</a>. <em>Lecture Notes in Computer Science, 6830</em>: 11-20.</p>\n<p>\u2219 Solomonoff (1997). <a href=\"http://ac.els-cdn.com/S0022000097915002/1-s2.0-S0022000097915002-main.pdf?_tid=3874fb92-6174-11e3-abc4-00000aacb362&amp;acdnat=1386663934_9e8f82128798276168e4cb92c65e15ab\">The discovery of algorithmic probability</a>. <em>Journal of Computer and System Sciences, 55</em>: 73-88.</p>\n<p>\u2219&nbsp;Sunehag &amp; Hutter (2013). <a href=\"http://arxiv.org/abs/1111.6117\">Principles of Solomonoff induction and AIXI</a>. <em>Lecture Notes in Computer Science, 7070</em>: 386-398.</p>\n<p>\u2219 Veness, Ng, Hutter, Uther &amp; Silver (2011). <a href=\"http://www.jair.org/media/3125/live-3125-5397-jair.pdf\">A Monte-Carlo AIXI approximation</a>. <em>Journal of Artificial Intelligence Research, 40</em>: 95-142.</p>", "sections": [{"title": "Who cares, if we can't build the thing?", "anchor": "Who_cares__if_we_can_t_build_the_thing_", "level": 1}, {"title": "Where Solomonoff induction goes wrong", "anchor": "Where_Solomonoff_induction_goes_wrong", "level": 1}, {"title": "Symptom #1: Immortalism", "anchor": "Symptom__1__Immortalism", "level": 1}, {"title": "Symptom #2: Preference solipsism", "anchor": "Symptom__2__Preference_solipsism", "level": 1}, {"title": "Symptom #3: Non-self-improvement", "anchor": "Symptom__3__Non_self_improvement", "level": 1}, {"title": "Reductive models are indispensable for highly adaptive intelligences", "anchor": "Reductive_models_are_indispensable_for_highly_adaptive_intelligences", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"title": "References", "anchor": "References", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "49 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3WuAjWMtxQwTxr2Qn", "Kyc5dFDzBg4WccrbK", "tPqQdLCuxanjhoaNs", "5pgsbB5sqC2wLwr4d", "nj8JKFoLSMEmD3RGp", "yLeEPFnnB9wE7KLx2", "f4txACqDWithRi7hs", "RcZCwxFiZzE6X7nsv", "AJ9dX59QXokZb35fk", "QrhAeKBkm2WsdRYao", "bkSkRwo9SRYxJMiSY", "JcpzFpPBSmzuksmWM", "ethRJh2E7mSSjzCay", "ZTRiSNmeGQK8AkdN2", "LGSotF4bQ2pRSbB8a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T17:56:32.568Z", "modifiedAt": null, "url": null, "title": "Political Skills which Increase Income", "slug": "political-skills-which-increase-income", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:36.669Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Xodarap", "createdAt": "2010-02-04T03:26:20.706Z", "isAdmin": false, "displayName": "Xodarap"}, "userId": "MXzEw56rk32JjM2Gp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4rwABGAd9kZG8nf2P/political-skills-which-increase-income", "pageUrlRelative": "/posts/4rwABGAd9kZG8nf2P/political-skills-which-increase-income", "linkUrl": "https://www.lesswrong.com/posts/4rwABGAd9kZG8nf2P/political-skills-which-increase-income", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Political%20Skills%20which%20Increase%20Income&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitical%20Skills%20which%20Increase%20Income%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rwABGAd9kZG8nf2P%2Fpolitical-skills-which-increase-income%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Political%20Skills%20which%20Increase%20Income%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rwABGAd9kZG8nf2P%2Fpolitical-skills-which-increase-income", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4rwABGAd9kZG8nf2P%2Fpolitical-skills-which-increase-income", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1102, "htmlBody": "<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Summary: This article is intended for those who are \"<a href=\"/lw/hjn/earning_to_give_vs_altruistic_career_choice/\">earning to give</a>\" (i.e. maximize income so that it can be donated to charity). It is basically an annotated bibliography of a few recent meta-analyses of predictors of income.</span></p>\n<p><strong id=\"docs-internal-guid-70dc2b14-7eca-efd9-17f9-186103189a47\" style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Key Results</span></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The degree to which management &ldquo;sponsors&rdquo; your career development is an important predictor of your salary, as is how skilled you are politically.</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Despite the stereotype of a silver-tongued salesman preying on people&rsquo;s biases, rational appeals are generally the best tactic.</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">After rationality, the best tactics are types of ingratiation, including flattery and acting modest.</span></p>\n</li>\n</ul>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><a style=\"text-decoration: none;\" href=\"http://www.researchgate.net/publication/227614669_PREDICTORS_OF_OBJECTIVE_AND_SUBJECTIVE_CAREER_SUCCESS_A_METAANALYSIS/file/d912f508aa32181077.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Ng et al.</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> performed a metastudy of over 200 individual studies of objective and subjective career success. Here are the variables they found best correlated with salary:</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"></col><col width=\"*\"></col></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Predictor</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Correlation</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Political Knowledge &amp; Skills</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.29</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Education Level</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.29</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Cognitive Ability (as measured by standardized tests)</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.27</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Age</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.26</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Training and Skill Development Opportunities</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.24</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Hours Worked</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.24</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Career Sponsorship</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.22</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(all significant at p = .05)</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br class=\"kix-line-break\" />(For reference, the &ldquo;Big 5&rdquo; personality traits all have a correlation under 0.12.)</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Before we go on, a few caveats: while these correlations are significant and important, none are overwhelming (the authors cite </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/Statistical-Analysis-Behavioral-Sciences-Edition/dp/0805802835/ref=sr_1_1?ie=UTF8&amp;qid=1380985089&amp;sr=8-1&amp;keywords=Statistical+power+analysis+for+the+behavioral+sciences\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Cohen</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> as saying the range 0.24-0.36 is &ldquo;medium&rdquo; and correlations over 0.37 are &ldquo;large&rdquo;). Also, in addition to the usual correlation/causation concerns, there is lots of cross-correlation: e.g. older people might have greater political knowledge but less education, thereby confusing things. For a discussion of moderating variables, see the paper itself.</span></p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Career Sponsorship</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There are two broad models of career advancement: contest-mobility and sponsorship-mobility. They are best illustrated with an example.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Suppose Peter and Penelope are both equally talented entry-level employees. Under the contest-mobility model, they would both be equally likely to get a raise or promotion, because they are equally skilled.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sponsorship-mobility theorists argue that even if Peter and Penelope are equally talented, it&rsquo;s likely that one of them will catch the eye of senior management. Perhaps it&rsquo;s due to one of them having an early success by chance, making a joke in a meeting, or simply just having a more memorable name, like Penelope. This person will be singled out for additional training and job opportunities. Because of this, they&rsquo;ll have greater success in the company, which will lead to more opportunities etc. As a result, their initial small discrepancy in attention gets multiplied into a large differential.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The authors of the metastudy found that self-reported sponsorship levels (i.e. how much you feel the management of your company &ldquo;sponsors&rdquo; you) have a significant, although moderate, relationship to salary. Therefore, the level at which you currently feel sponsored in your job should be a factor when you consider alternate opportunities.</span></p>\n<h1 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 21px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Dilbert Effect</span></h1>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The strongest predictor of salary (tied with education level) is what the authors politely term &ldquo;Political Knowledge &amp; Skills&rdquo; - less politely, how good you are at manipulating others.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Several popular books (such as </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/Influence-Psychology-Persuasion-Business-Essentials/dp/006124189X/ref=sr_1_1?ie=UTF8&amp;qid=1380985267&amp;sr=8-1&amp;keywords=influence+cialdini\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Cialdini&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Influence</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">) on the subject of influencing others exist, and the study of these &ldquo;influence tactics&rdquo; in business stretches back 30 years to </span><a style=\"text-decoration: none;\" href=\"http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;uid=1980-33577-001\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Kipnis, Schmidt and Wilkinson</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Recently, </span><a style=\"text-decoration: none;\" href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/influence_tactics_and_work_outcomes-_a_meta-analysis.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Higgins et al.</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> reviewed 23 individual studies of these tactics and how they relate to career success. Their results:</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"></col><col width=\"110\"></col><col width=\"306\"></col></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tactic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Correlation</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Definition (From Higgins et al.)</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rationality</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.26</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using data and information to make a logical argument supporting one's request</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Ingratiation</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.23</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using behaviors designed to increase the target's liking of oneself or to make oneself appear friendly in order to get what one wants</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Upward Appeal</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.05</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Relying on the chain of command, calling in superiors to help get one's way</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Self-Promotion</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.01</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Attempting to create an appearance of competence or that you are capable </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">of completing a task</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Assertiveness</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.02</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using a forceful manner to get what one wants</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Exchange</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.03</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Making an explicit offer to do something for another in exchange for their doing what </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">one wants</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(Only ingratiation and rationality are significant.)</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">This site has a lot of information on how to make rational appeals, so I will focus on the less-talked-about ingratiation techniques.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How to be Ingratiating</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><a style=\"text-decoration: none;\" href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/impact_of_ingratiation_on_judgments_and_evaluations-_a_meta-analytic_investigation.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Gordon analyzed</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> 69 studies of ingratiation and found the following. (Unlike the previous two sections, success here is measured in lab tests as well as in career advancement. However, similar but less comprehensive </span><a style=\"text-decoration: none;\" href=\"http://digitalcommons.ilr.cornell.edu/cgi/viewcontent.cgi?article=1290&amp;context=cahrswp&amp;sei-redir=1\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">results</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> have been found in terms of career success):</span></p>\n<p><strong id=\"docs-internal-guid-70dc2b14-7ed5-8f9e-bac3-4cd36af46b99\" style=\"font-weight: normal;\"></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"></col><col width=\"*\"></col><col width=\"*\"></col></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tactic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Weighted Effectiveness (</span><a style=\"text-decoration: none;\" href=\"http://en.wikipedia.org/wiki/Cohen%27s_d#Cohen.27s_d\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Cohen&rsquo;s d</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> difference between control and intervention)</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Description</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Other Enhancement</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.31</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Flattery</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Opinion Conformity</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.23</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&ldquo;Go along to get along&rdquo;</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Self-presentation</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.15</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Any of the following tactics: Self-promotion, self-deprecation, apologies, positive nonverbal displays and name usage</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Combination</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.10</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Includes studies where the participants weren&rsquo;t told which strategy to use, in addition to when they were instructed to use multiple strategies</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rendering Favors</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.05</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><br /></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Self-presentation is split further:</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"></col><col width=\"*\"></col><col width=\"*\"></col></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tactic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Weighted Effect Size</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Comment</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Modesty</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.77</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><br /></td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Apology</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.59</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Apologizing for poor performance</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Generic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.28</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\">When the participant is told in generic terms to improve their self-presentation</span></td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1;\">Nonverbal behavior and name usage</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.14</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\">Nonverbal behavior includes things like wearing perfume. Name usage means referring to people by name instead of a pronoun.</span></td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\">Self-promotion</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.17</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&nbsp;</p>\n<p><span style=\"background-color: transparent; font-family: 'Trebuchet MS'; font-size: 17px; font-weight: bold; white-space: pre-wrap; line-height: 1.15;\">Moderators</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">One important moderator is the direction of the appeal. If you are talking to your boss, your tactics should be different than if you&rsquo;re talking to a subordinate. Other-enhancement (flattery) is always the best tactic no matter who you&rsquo;re talking to, but when talking to superiors it&rsquo;s by far the best. When talking to those at similar levels to you, opinion conformity comes close to flattery, and the other techniques aren't far behind.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Unsurprisingly, when the target realizes you&rsquo;re being ingratiating, the tactic is less effective. (Although effectiveness doesn&rsquo;t go to zero - even when people realize you&rsquo;re flattering them just to suck up, they generally still appreciate it.) Also, women are better at being ingratiating than men, and men are more influenced by these ingratiating tactics than women. The most important caveat is that lab studies find much larger effect sizes than in the field, to the extent that the average field effect for the ingratiating tactics is negative. This is probably due to the fact that lab experiments can be better controlled.</span></p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Conclusion</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">It&rsquo;s unlikely that a silver-tongued receptionist will out-earn an introverted engineer. But simple techniques like flattery and attempting to get \"</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">sponsored\"</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> can appreciably improve returns, to the extent that political skills are one of the strongest predictors of salaries.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I would like to thank Brian Tomasik and Gina Stuessy for reading early drafts of this article.</p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">References</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Cohen, Jacob.&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Statistical power analysis for the behavioral sciences</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">. Psychology Press, 1988.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Gordon, Randall A. \"Impact of ingratiation on judgments and evaluations: A meta-analytic investigation.\"&nbsp;</span><em style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Personality and Social Psychology</em><span style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;71.1 (1996): 54.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Higgins, Chad A., Timothy A. Judge, and Gerald R. Ferris. \"Influence tactics and work outcomes: a meta\u2010analysis.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Organizational Behavior</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;24.1 (2003): 89-106.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Judge, Timothy A., and Robert D. Bretz Jr. \"Political influence behavior and career success.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Management</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;20.1 (1994): 43-65.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Kipnis, David, Stuart M. Schmidt, and Ian Wilkinson. \"Intraorganizational influence tactics: Explorations in getting one's way.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Applied psychology</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;65.4 (1980): 440.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\"><br /></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Ng, Thomas WH, et al. \"Predictors of objective and subjective career success: A meta\u2010analysis.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Personnel psychology</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;58.2 (2005): 367-408.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 2, "XYHzLjwYiqpeqaf4c": 1, "mf8wHrMrJR73uyDLQ": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4rwABGAd9kZG8nf2P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 101, "extendedScore": null, "score": 0.000266, "legacy": true, "legacyId": "25657", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 101, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Summary: This article is intended for those who are \"<a href=\"/lw/hjn/earning_to_give_vs_altruistic_career_choice/\">earning to give</a>\" (i.e. maximize income so that it can be donated to charity). It is basically an annotated bibliography of a few recent meta-analyses of predictors of income.</span></p>\n<p><strong id=\"docs-internal-guid-70dc2b14-7eca-efd9-17f9-186103189a47\" style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Key Results</span></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The degree to which management \u201csponsors\u201d your career development is an important predictor of your salary, as is how skilled you are politically.</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Despite the stereotype of a silver-tongued salesman preying on people\u2019s biases, rational appeals are generally the best tactic.</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">After rationality, the best tactics are types of ingratiation, including flattery and acting modest.</span></p>\n</li>\n</ul>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><a style=\"text-decoration: none;\" href=\"http://www.researchgate.net/publication/227614669_PREDICTORS_OF_OBJECTIVE_AND_SUBJECTIVE_CAREER_SUCCESS_A_METAANALYSIS/file/d912f508aa32181077.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Ng et al.</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> performed a metastudy of over 200 individual studies of objective and subjective career success. Here are the variables they found best correlated with salary:</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"><col width=\"*\"></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Predictor</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Correlation</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Political Knowledge &amp; Skills</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.29</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Education Level</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.29</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Cognitive Ability (as measured by standardized tests)</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.27</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Age</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.26</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Training and Skill Development Opportunities</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.24</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Hours Worked</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.24</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Career Sponsorship</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.22</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(all significant at p = .05)</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br class=\"kix-line-break\">(For reference, the \u201cBig 5\u201d personality traits all have a correlation under 0.12.)</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Before we go on, a few caveats: while these correlations are significant and important, none are overwhelming (the authors cite </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/Statistical-Analysis-Behavioral-Sciences-Edition/dp/0805802835/ref=sr_1_1?ie=UTF8&amp;qid=1380985089&amp;sr=8-1&amp;keywords=Statistical+power+analysis+for+the+behavioral+sciences\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Cohen</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> as saying the range 0.24-0.36 is \u201cmedium\u201d and correlations over 0.37 are \u201clarge\u201d). Also, in addition to the usual correlation/causation concerns, there is lots of cross-correlation: e.g. older people might have greater political knowledge but less education, thereby confusing things. For a discussion of moderating variables, see the paper itself.</span></p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\" id=\"Career_Sponsorship\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Career Sponsorship</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There are two broad models of career advancement: contest-mobility and sponsorship-mobility. They are best illustrated with an example.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Suppose Peter and Penelope are both equally talented entry-level employees. Under the contest-mobility model, they would both be equally likely to get a raise or promotion, because they are equally skilled.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sponsorship-mobility theorists argue that even if Peter and Penelope are equally talented, it\u2019s likely that one of them will catch the eye of senior management. Perhaps it\u2019s due to one of them having an early success by chance, making a joke in a meeting, or simply just having a more memorable name, like Penelope. This person will be singled out for additional training and job opportunities. Because of this, they\u2019ll have greater success in the company, which will lead to more opportunities etc. As a result, their initial small discrepancy in attention gets multiplied into a large differential.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The authors of the metastudy found that self-reported sponsorship levels (i.e. how much you feel the management of your company \u201csponsors\u201d you) have a significant, although moderate, relationship to salary. Therefore, the level at which you currently feel sponsored in your job should be a factor when you consider alternate opportunities.</span></p>\n<h1 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\" id=\"The_Dilbert_Effect\"><span style=\"font-size: 21px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The Dilbert Effect</span></h1>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The strongest predictor of salary (tied with education level) is what the authors politely term \u201cPolitical Knowledge &amp; Skills\u201d - less politely, how good you are at manipulating others.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Several popular books (such as </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/Influence-Psychology-Persuasion-Business-Essentials/dp/006124189X/ref=sr_1_1?ie=UTF8&amp;qid=1380985267&amp;sr=8-1&amp;keywords=influence+cialdini\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Cialdini\u2019s </span><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Influence</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">) on the subject of influencing others exist, and the study of these \u201cinfluence tactics\u201d in business stretches back 30 years to </span><a style=\"text-decoration: none;\" href=\"http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;uid=1980-33577-001\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Kipnis, Schmidt and Wilkinson</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Recently, </span><a style=\"text-decoration: none;\" href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/influence_tactics_and_work_outcomes-_a_meta-analysis.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Higgins et al.</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> reviewed 23 individual studies of these tactics and how they relate to career success. Their results:</span></p>\n<p><strong style=\"font-weight: normal;\"><br></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"><col width=\"110\"><col width=\"306\"></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tactic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Correlation</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Definition (From Higgins et al.)</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rationality</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.26</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using data and information to make a logical argument supporting one's request</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Ingratiation</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.23</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using behaviors designed to increase the target's liking of oneself or to make oneself appear friendly in order to get what one wants</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Upward Appeal</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.05</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Relying on the chain of command, calling in superiors to help get one's way</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Self-Promotion</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.01</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Attempting to create an appearance of competence or that you are capable </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">of completing a task</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Assertiveness</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.02</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using a forceful manner to get what one wants</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Exchange</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.03</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Making an explicit offer to do something for another in exchange for their doing what </span></p>\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">one wants</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(Only ingratiation and rationality are significant.)</span></p>\n<p><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1.15;\">This site has a lot of information on how to make rational appeals, so I will focus on the less-talked-about ingratiation techniques.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How to be Ingratiating</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><a style=\"text-decoration: none;\" href=\"http://www.communicationcache.com/uploads/1/0/8/8/10887248/impact_of_ingratiation_on_judgments_and_evaluations-_a_meta-analytic_investigation.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Gordon analyzed</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> 69 studies of ingratiation and found the following. (Unlike the previous two sections, success here is measured in lab tests as well as in career advancement. However, similar but less comprehensive </span><a style=\"text-decoration: none;\" href=\"http://digitalcommons.ilr.cornell.edu/cgi/viewcontent.cgi?article=1290&amp;context=cahrswp&amp;sei-redir=1\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">results</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> have been found in terms of career success):</span></p>\n<p><strong id=\"docs-internal-guid-70dc2b14-7ed5-8f9e-bac3-4cd36af46b99\" style=\"font-weight: normal;\"></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"><col width=\"*\"><col width=\"*\"></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tactic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Weighted Effectiveness (</span><a style=\"text-decoration: none;\" href=\"http://en.wikipedia.org/wiki/Cohen%27s_d#Cohen.27s_d\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Cohen\u2019s d</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> difference between control and intervention)</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Description</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Other Enhancement</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.31</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Flattery</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Opinion Conformity</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.23</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\u201cGo along to get along\u201d</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Self-presentation</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.15</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Any of the following tactics: Self-promotion, self-deprecation, apologies, positive nonverbal displays and name usage</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Combination</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.10</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Includes studies where the participants weren\u2019t told which strategy to use, in addition to when they were instructed to use multiple strategies</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rendering Favors</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.05</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><br></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Self-presentation is split further:</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<div style=\"margin-left: 0pt;\" dir=\"ltr\">\n<table style=\"border: none; border-collapse: collapse; width: 624px;\" border=\"0\">\n<colgroup><col width=\"*\"><col width=\"*\"><col width=\"*\"></colgroup> \n<tbody>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tactic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Weighted Effect Size</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Comment</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Modesty</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.77</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><br></td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Apology</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.59</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Apologizing for poor performance</span></p>\n</td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Generic</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.28</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\">When the participant is told in generic terms to improve their self-presentation</span></td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Arial; font-size: 15px; white-space: pre-wrap; line-height: 1;\">Nonverbal behavior and name usage</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.14</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\"><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\">Nonverbal behavior includes things like wearing perfume. Name usage means referring to people by name instead of a pronoun.</span></td>\n</tr>\n<tr style=\"height: 0px;\">\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-family: Arial; font-size: 15px; line-height: 15px; white-space: pre-wrap;\">Self-promotion</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-0.17</span></p>\n</td>\n<td style=\"border: 1px solid #000000; vertical-align: top; padding: 7px 7px 7px 7px;\">\n<p style=\"line-height: 1; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&nbsp;</p>\n<p><span style=\"background-color: transparent; font-family: 'Trebuchet MS'; font-size: 17px; font-weight: bold; white-space: pre-wrap; line-height: 1.15;\">Moderators</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">One important moderator is the direction of the appeal. If you are talking to your boss, your tactics should be different than if you\u2019re talking to a subordinate. Other-enhancement (flattery) is always the best tactic no matter who you\u2019re talking to, but when talking to superiors it\u2019s by far the best. When talking to those at similar levels to you, opinion conformity comes close to flattery, and the other techniques aren't far behind.</span></p>\n<p><strong style=\"font-weight: normal;\"></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Unsurprisingly, when the target realizes you\u2019re being ingratiating, the tactic is less effective. (Although effectiveness doesn\u2019t go to zero - even when people realize you\u2019re flattering them just to suck up, they generally still appreciate it.) Also, women are better at being ingratiating than men, and men are more influenced by these ingratiating tactics than women. The most important caveat is that lab studies find much larger effect sizes than in the field, to the extent that the average field effect for the ingratiating tactics is negative. This is probably due to the fact that lab experiments can be better controlled.</span></p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\" id=\"Conclusion\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Conclusion</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">It\u2019s unlikely that a silver-tongued receptionist will out-earn an introverted engineer. But simple techniques like flattery and attempting to get \"</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">sponsored\"</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> can appreciably improve returns, to the extent that political skills are one of the strongest predictors of salaries.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">I would like to thank Brian Tomasik and Gina Stuessy for reading early drafts of this article.</p>\n<h2 style=\"line-height: 1.15; margin-top: 10pt; margin-bottom: 0pt;\" dir=\"ltr\" id=\"References\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">References</span></h2>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Cohen, Jacob.&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Statistical power analysis for the behavioral sciences</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">. Psychology Press, 1988.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Gordon, Randall A. \"Impact of ingratiation on judgments and evaluations: A meta-analytic investigation.\"&nbsp;</span><em style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Personality and Social Psychology</em><span style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;71.1 (1996): 54.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Higgins, Chad A., Timothy A. Judge, and Gerald R. Ferris. \"Influence tactics and work outcomes: a meta\u2010analysis.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Organizational Behavior</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;24.1 (2003): 89-106.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Judge, Timothy A., and Robert D. Bretz Jr. \"Political influence behavior and career success.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Management</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;20.1 (1994): 43-65.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Kipnis, David, Stuart M. Schmidt, and Ian Wilkinson. \"Intraorganizational influence tactics: Explorations in getting one's way.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Journal of Applied psychology</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;65.4 (1980): 440.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\"><br></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Ng, Thomas WH, et al. \"Predictors of objective and subjective career success: A meta\u2010analysis.\"&nbsp;</span><em style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">Personnel psychology</em><span style=\"color: #222222; font-family: Arial, sans-serif; font-size: 13px; line-height: 16.1200008392334px;\">&nbsp;58.2 (2005): 367-408.</span></p>", "sections": [{"title": "Career Sponsorship", "anchor": "Career_Sponsorship", "level": 2}, {"title": "The Dilbert Effect", "anchor": "The_Dilbert_Effect", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 2}, {"title": "References", "anchor": "References", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3Ss29ihXsBb8tuoxK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T21:09:07.597Z", "modifiedAt": null, "url": null, "title": "Proportional Giving", "slug": "proportional-giving", "viewCount": null, "lastCommentedAt": "2019-10-30T12:45:43.520Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gjm", "createdAt": "2009-03-09T01:11:32.668Z", "isAdmin": false, "displayName": "gjm"}, "userId": "977L8MR7JmNrQx6df", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YRfgmYKKuWB6TTq99/proportional-giving", "pageUrlRelative": "/posts/YRfgmYKKuWB6TTq99/proportional-giving", "linkUrl": "https://www.lesswrong.com/posts/YRfgmYKKuWB6TTq99/proportional-giving", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proportional%20Giving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProportional%20Giving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRfgmYKKuWB6TTq99%2Fproportional-giving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proportional%20Giving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRfgmYKKuWB6TTq99%2Fproportional-giving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRfgmYKKuWB6TTq99%2Fproportional-giving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 736, "htmlBody": "<p><strong>Executive summary</strong>: <em>The practice of giving a fixed fraction of one's income to charity is near-universal but possibly indefensible. I describe one approach that certainly doesn't defend it, speculate vaguely about a possible way of fixing it up, and invite better ideas from others.</em></p>\n<p><em> </em></p>\n<hr />\n<p><em> </em></p>\n<p>Many of us give a certain fraction of our income to charitable causes. This sort of practice has a long history:</p>\n<p style=\"padding-left: 30px;\"><em>Deuteronomy 14:22</em> Thou shalt truly tithe all the increase of thy seed, that the field bringeth forth year by year.</p>\n<p>(note that \"tithe\" here means \"give one-tenth of\") and is widely practised today:</p>\n<p style=\"padding-left: 30px;\"><a title=\"Giving What We Can: the pledge\" href=\"http://www.givingwhatwecan.org/about-us/the-pledge\"><em>GWWC Pledge</em></a>: I recognise that I can use part of my income to do a significant amount of good in the developing world. Since I can live well enough on a smaller income, I pledge that from today until the day I retire, I shall give at least ten percent of what I earn to whichever organizations can most effectively use it to help people in developing countries. I make this pledge freely, openly, and without regret.</p>\n<p>And of course it's roughly how typical taxation systems (which are kinda-sorta like charitable donation, if you squint) operate. But does it make sense? Is there some underlying principle from which a policy of giving away a certain fraction of one's income (not necessarily the traditional 10%, of course) follows?</p>\n<p>The most obvious candidate for such a principle would be what we might call</p>\n<p style=\"padding-left: 30px;\"><em>Weighted Utilitarianism</em>: Act so as to maximize a weighted sum of utility, where (e.g.) one's own utility may be weighted much higher than that of random far-away people.</p>\n<p>But <strong>this can't produce anything remotely like a policy of proportional giving</strong>. Assuming you aren't giving away many millions per year (which is a fair assumption if you're thinking in terms of a fraction of your salary) then the level of utility-per-unit-money achievable by your giving is basically independent of what you give, and so is the weight you attach to the utility of the beneficiaries.</p>\n<p>So suppose that when your income, after taking out donations, is $<em>X</em>, your utility (all else equal) is <em>u</em>(<em>X</em>), so that your utility per marginal dollar is <em>u</em>'(<em>X</em>); and suppose you attach weight 1 to your own utility and weight <em>w</em> to that of the people who'd benefit from your donations; and suppose their gain in utility per marginal dollar given is <em>t</em>. Then when your income is <em>S</em> you will set your giving <em>g</em> so that <em>u</em>'(<em>S</em>-<em>g</em>) = <em>wt</em>.</p>\n<p>What this says is that <strong>a weighted-utilitarian should keep a fixed absolute amount <em>S</em>-<em>g</em> of his or her income, and give all the rest away</strong>. The fixed absolute amount will depend on the weight <em>w</em> (hence, on exactly which people are benefited by the donations) and on the utility per dollar given <em>t</em> (hence, on exactly what charities are serving them and how severe their need is), but not on the person's pre-donation income <em>S</em>.</p>\n<p>(Here's a quick oversimplified example. Suppose that utility is proportional to log(income), that the people your donations will help have an income equivalent to $1k/year, that you care 100x more about your utility than about theirs, and that your donations are the equivalent of direct cash transfers to those people. Then <em>u</em>' = 1/income, so you should keep everything up to $100k/year and give the rest away. The generalization to other weighting factors and beneficiary incomes should be obvious.)</p>\n<p>This argument seems reasonably watertight given its premises, but proportional giving is so well-established a phenomenon that we might reasonably trust our predisposition in its favour more than our arguments against. Can we salvage it somehow?</p>\n<p>Here's one possibility. One effect of income is (supposedly) to incentivize work, and maybe (mumble <a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">near mode</a> mumble) this effect is governed entirely by anticipated personal utility and not by any benefit conferred on others. Then the policy derived above, which above the threshold makes personal utility independent of effort, would lead to minimum effort and hence maybe less net weighted utility than could be attained with a different policy. Does this lead to anything like proportional giving, at least for some semi-plausible assumptions about the relationship between effort and income?</p>\n<p>At the moment, I don't know. I have a page full of scribbled attempts to derive something of the kind, but they didn't work out. And of course there might be some better way to get proportional giving out of plausible ethical principles. Anyone want to do better?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YRfgmYKKuWB6TTq99", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "25665", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-02T23:56:33.700Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, March 1-15", "slug": "group-rationality-diary-march-1-15", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:35.839Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ukqAwSZhjp5fvXPfz/group-rationality-diary-march-1-15", "pageUrlRelative": "/posts/ukqAwSZhjp5fvXPfz/group-rationality-diary-march-1-15", "linkUrl": "https://www.lesswrong.com/posts/ukqAwSZhjp5fvXPfz/group-rationality-diary-march-1-15", "postedAtFormatted": "Sunday, March 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20March%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20March%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FukqAwSZhjp5fvXPfz%2Fgroup-rationality-diary-march-1-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20March%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FukqAwSZhjp5fvXPfz%2Fgroup-rationality-diary-march-1-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FukqAwSZhjp5fvXPfz%2Fgroup-rationality-diary-march-1-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">This is the public group instrumental rationality diary for March 1-15.&nbsp;</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Next diary: &nbsp;<a href=\"/r/discussion/lw/jwl/group_rationality_diary_march_1631/\">March 16-31</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Immediate past diary: &nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/jhu/group_rationality_diary_january_1631/\">January 16-31</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ukqAwSZhjp5fvXPfz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "25667", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ov7E85WSrp2azSwCn", "HPeHpsvzLFZxNY8k3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-03T16:04:40.944Z", "modifiedAt": null, "url": null, "title": "Akrasia and Immunity to change", "slug": "akrasia-and-immunity-to-change", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:58.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "terasinube", "createdAt": "2013-04-17T08:07:17.527Z", "isAdmin": false, "displayName": "terasinube"}, "userId": "xLh2CNNX3vgntTH9C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ATdxzGGFi5NnBPub6/akrasia-and-immunity-to-change", "pageUrlRelative": "/posts/ATdxzGGFi5NnBPub6/akrasia-and-immunity-to-change", "linkUrl": "https://www.lesswrong.com/posts/ATdxzGGFi5NnBPub6/akrasia-and-immunity-to-change", "postedAtFormatted": "Monday, March 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Akrasia%20and%20Immunity%20to%20change&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAkrasia%20and%20Immunity%20to%20change%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATdxzGGFi5NnBPub6%2Fakrasia-and-immunity-to-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Akrasia%20and%20Immunity%20to%20change%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATdxzGGFi5NnBPub6%2Fakrasia-and-immunity-to-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FATdxzGGFi5NnBPub6%2Fakrasia-and-immunity-to-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>Does any of you has any relevant experience that you can share with <strong>Immunity to change</strong> by Robert Kegan and Lisa Laskow Lahey?<br /><br />I'm currently reading their book and I find it fascinating.</p>\n<div>Here is a HBR article titled <a href=\"http://diane-foster.com/wp-content/uploads/real-reason-people-wont-change.pdf\" target=\"_blank\">The Real Reason People Won&rsquo;t Change</a>&nbsp;that describes the work.&nbsp;</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ATdxzGGFi5NnBPub6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.5925298125386161e-06, "legacy": true, "legacyId": "25670", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-03T19:55:11.665Z", "modifiedAt": null, "url": null, "title": "Meetup : Utrecht: Effective Altruism", "slug": "meetup-utrecht-effective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoerenMind", "createdAt": "2013-07-16T18:10:55.180Z", "isAdmin": false, "displayName": "SoerenMind"}, "userId": "DGetADxtea2LRL946", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mb7qx6QdNP6xjjPzm/meetup-utrecht-effective-altruism", "pageUrlRelative": "/posts/Mb7qx6QdNP6xjjPzm/meetup-utrecht-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/Mb7qx6QdNP6xjjPzm/meetup-utrecht-effective-altruism", "postedAtFormatted": "Monday, March 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Utrecht%3A%20Effective%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Utrecht%3A%20Effective%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMb7qx6QdNP6xjjPzm%2Fmeetup-utrecht-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Utrecht%3A%20Effective%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMb7qx6QdNP6xjjPzm%2Fmeetup-utrecht-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMb7qx6QdNP6xjjPzm%2Fmeetup-utrecht-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 182, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xh'>Utrecht: Effective Altruism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 March 2014 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Oudegracht 158, Utrecht, the Netherlands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In this meetup we will discuss topics related to effective altruism. \nThis meetup is not directly related to the previous one in Utrecht. It is also not purely a LessWrong meetup. We have already created an event on Facebook, where 5 people are planning to attend (I can add you to the event if you comment here). Most of those are also active on LW and we would be more than happy to have more LWers on board.</p>\n\n<p>Some topics we may discuss are altruistic career choice, selection of causes, and whether we can create an effective altruism community in the Netherlands. Getting to know each other is also an important part.</p>\n\n<p>We will meet in a caf\u00e9 called De Winkel van Sinkel, which is 400m walking distance from Utrecht Centraal. \nThe meetup will be held in English, since we have at least one German participant..</p>\n\n<p>I will be holding a sign that says 'LW' on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xh'>Utrecht: Effective Altruism</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mb7qx6QdNP6xjjPzm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.5928056841991515e-06, "legacy": true, "legacyId": "25671", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Effective_Altruism\">Discussion article for the meetup : <a href=\"/meetups/xh\">Utrecht: Effective Altruism</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 March 2014 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Oudegracht 158, Utrecht, the Netherlands</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In this meetup we will discuss topics related to effective altruism. \nThis meetup is not directly related to the previous one in Utrecht. It is also not purely a LessWrong meetup. We have already created an event on Facebook, where 5 people are planning to attend (I can add you to the event if you comment here). Most of those are also active on LW and we would be more than happy to have more LWers on board.</p>\n\n<p>Some topics we may discuss are altruistic career choice, selection of causes, and whether we can create an effective altruism community in the Netherlands. Getting to know each other is also an important part.</p>\n\n<p>We will meet in a caf\u00e9 called De Winkel van Sinkel, which is 400m walking distance from Utrecht Centraal. \nThe meetup will be held in English, since we have at least one German participant..</p>\n\n<p>I will be holding a sign that says 'LW' on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Utrecht__Effective_Altruism1\">Discussion article for the meetup : <a href=\"/meetups/xh\">Utrecht: Effective Altruism</a></h2>", "sections": [{"title": "Discussion article for the meetup : Utrecht: Effective Altruism", "anchor": "Discussion_article_for_the_meetup___Utrecht__Effective_Altruism", "level": 1}, {"title": "Discussion article for the meetup : Utrecht: Effective Altruism", "anchor": "Discussion_article_for_the_meetup___Utrecht__Effective_Altruism1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-03T20:46:53.518Z", "modifiedAt": null, "url": null, "title": "Meetup : Tempe, AZ (ASU)", "slug": "meetup-tempe-az-asu", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.466Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MumpsimusLane", "createdAt": "2013-04-29T18:16:34.097Z", "isAdmin": false, "displayName": "MumpsimusLane"}, "userId": "LGAYG27rBf3DenkBj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nb4TwFkGhzqRMt5pu/meetup-tempe-az-asu", "pageUrlRelative": "/posts/Nb4TwFkGhzqRMt5pu/meetup-tempe-az-asu", "linkUrl": "https://www.lesswrong.com/posts/Nb4TwFkGhzqRMt5pu/meetup-tempe-az-asu", "postedAtFormatted": "Monday, March 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tempe%2C%20AZ%20(ASU)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tempe%2C%20AZ%20(ASU)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNb4TwFkGhzqRMt5pu%2Fmeetup-tempe-az-asu%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tempe%2C%20AZ%20(ASU)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNb4TwFkGhzqRMt5pu%2Fmeetup-tempe-az-asu", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNb4TwFkGhzqRMt5pu%2Fmeetup-tempe-az-asu", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xi'>Tempe, AZ (ASU)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 March 2014 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting at the entrance to Hayden Library at ASU.\nTentative discussion topics include:\nwrap up How To Measure Anything;\nplanning / meta;\nbelated New Year's resolutions andor goals in general</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xi'>Tempe, AZ (ASU)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nb4TwFkGhzqRMt5pu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.592867566275132e-06, "legacy": true, "legacyId": "25672", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tempe__AZ__ASU_\">Discussion article for the meetup : <a href=\"/meetups/xi\">Tempe, AZ (ASU)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 March 2014 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 E Orange Mall, Tempe, AZ</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting at the entrance to Hayden Library at ASU.\nTentative discussion topics include:\nwrap up How To Measure Anything;\nplanning / meta;\nbelated New Year's resolutions andor goals in general</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tempe__AZ__ASU_1\">Discussion article for the meetup : <a href=\"/meetups/xi\">Tempe, AZ (ASU)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tempe, AZ (ASU)", "anchor": "Discussion_article_for_the_meetup___Tempe__AZ__ASU_", "level": 1}, {"title": "Discussion article for the meetup : Tempe, AZ (ASU)", "anchor": "Discussion_article_for_the_meetup___Tempe__AZ__ASU_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-04T03:55:33.045Z", "modifiedAt": null, "url": null, "title": "Open Thread: March 4 - 10", "slug": "open-thread-march-4-10", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:31.253Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ai3d7wQsWQY9gpP6F/open-thread-march-4-10", "pageUrlRelative": "/posts/ai3d7wQsWQY9gpP6F/open-thread-march-4-10", "linkUrl": "https://www.lesswrong.com/posts/ai3d7wQsWQY9gpP6F/open-thread-march-4-10", "postedAtFormatted": "Tuesday, March 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20March%204%20-%2010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20March%204%20-%2010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fai3d7wQsWQY9gpP6F%2Fopen-thread-march-4-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20March%204%20-%2010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fai3d7wQsWQY9gpP6F%2Fopen-thread-march-4-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fai3d7wQsWQY9gpP6F%2Fopen-thread-march-4-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ai3d7wQsWQY9gpP6F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.5933808354038663e-06, "legacy": true, "legacyId": "25679", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 391, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-04T04:19:56.970Z", "modifiedAt": null, "url": null, "title": "Intro to Rhetoric?", "slug": "intro-to-rhetoric", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:03.429Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rule_and_line", "createdAt": "2012-12-28T15:40:39.942Z", "isAdmin": false, "displayName": "rule_and_line"}, "userId": "FEtpkTTXzu5n6xs65", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CaNnPz4dAD3pbLCRY/intro-to-rhetoric", "pageUrlRelative": "/posts/CaNnPz4dAD3pbLCRY/intro-to-rhetoric", "linkUrl": "https://www.lesswrong.com/posts/CaNnPz4dAD3pbLCRY/intro-to-rhetoric", "postedAtFormatted": "Tuesday, March 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intro%20to%20Rhetoric%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntro%20to%20Rhetoric%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaNnPz4dAD3pbLCRY%2Fintro-to-rhetoric%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intro%20to%20Rhetoric%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaNnPz4dAD3pbLCRY%2Fintro-to-rhetoric", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaNnPz4dAD3pbLCRY%2Fintro-to-rhetoric", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Is there an explicit summary somewhere of the the LW community's views on rhetoric?</p>\n<p>There clearly exist people around this community who can persuasively carry a rational point. &nbsp;I also strongly expect that there exist specific instructions for training rhetorical skills, and these skills apply pretty generally.</p>\n<p>But I also quasi-expect that rhetorical techniques LW community members appreciate might differ in specific ways from rhetorical techniques taught in popular books. &nbsp;All that being said, I can't find any page where somebody explains why he or she explicitly practices something picked up via LW (e.g. Eliezer's \"concrete-abstract\" writing pattern) to the exclusion of something else in a standard text on rhetoric.</p>\n<p>So a brief search didn't find the evidence I weakly expected. &nbsp;What am I missing? &nbsp;Do we have a few pages somewhere? &nbsp;Do we normally point curious individuals to some standard text? &nbsp;Or do you have a favorite intro not mentioned on LW yet?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CaNnPz4dAD3pbLCRY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "25678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-04T16:33:25.881Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Vienna Meetup ", "slug": "meetup-lw-vienna-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RationalityVienna", "createdAt": "2014-01-27T13:42:25.040Z", "isAdmin": false, "displayName": "RationalityVienna"}, "userId": "mquibPrcrRF7TByie", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZjnrDycSbzqTZeCq3/meetup-lw-vienna-meetup-0", "pageUrlRelative": "/posts/ZjnrDycSbzqTZeCq3/meetup-lw-vienna-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/ZjnrDycSbzqTZeCq3/meetup-lw-vienna-meetup-0", "postedAtFormatted": "Tuesday, March 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Vienna%20Meetup%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Vienna%20Meetup%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjnrDycSbzqTZeCq3%2Fmeetup-lw-vienna-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Vienna%20Meetup%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjnrDycSbzqTZeCq3%2Fmeetup-lw-vienna-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjnrDycSbzqTZeCq3%2Fmeetup-lw-vienna-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xj'>LW Vienna Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 March 2014 03:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Reichsratsstrasse 17, 1010 Vienna, Austria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup at Cafe Votiv, where Anna Leptikon will present some psychological musings on rationality, followed by a discussion. Newcomers welcome. Please register for the FB event: <a href=\"https://www.facebook.com/events/627247490680732/?context=create\" rel=\"nofollow\">https://www.facebook.com/events/627247490680732/?context=create</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xj'>LW Vienna Meetup </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZjnrDycSbzqTZeCq3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5942890263310702e-06, "legacy": true, "legacyId": "25683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Vienna_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/xj\">LW Vienna Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 March 2014 03:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Reichsratsstrasse 17, 1010 Vienna, Austria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup at Cafe Votiv, where Anna Leptikon will present some psychological musings on rationality, followed by a discussion. Newcomers welcome. Please register for the FB event: <a href=\"https://www.facebook.com/events/627247490680732/?context=create\" rel=\"nofollow\">https://www.facebook.com/events/627247490680732/?context=create</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Vienna_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/xj\">LW Vienna Meetup </a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Vienna Meetup ", "anchor": "Discussion_article_for_the_meetup___LW_Vienna_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : LW Vienna Meetup ", "anchor": "Discussion_article_for_the_meetup___LW_Vienna_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-04T21:38:53.926Z", "modifiedAt": null, "url": null, "title": "Don't teach people how to reach the top of a hill", "slug": "don-t-teach-people-how-to-reach-the-top-of-a-hill", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:36.470Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gbNaFMkqtso7kdL5y/don-t-teach-people-how-to-reach-the-top-of-a-hill", "pageUrlRelative": "/posts/gbNaFMkqtso7kdL5y/don-t-teach-people-how-to-reach-the-top-of-a-hill", "linkUrl": "https://www.lesswrong.com/posts/gbNaFMkqtso7kdL5y/don-t-teach-people-how-to-reach-the-top-of-a-hill", "postedAtFormatted": "Tuesday, March 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20teach%20people%20how%20to%20reach%20the%20top%20of%20a%20hill&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20teach%20people%20how%20to%20reach%20the%20top%20of%20a%20hill%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbNaFMkqtso7kdL5y%2Fdon-t-teach-people-how-to-reach-the-top-of-a-hill%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20teach%20people%20how%20to%20reach%20the%20top%20of%20a%20hill%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbNaFMkqtso7kdL5y%2Fdon-t-teach-people-how-to-reach-the-top-of-a-hill", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgbNaFMkqtso7kdL5y%2Fdon-t-teach-people-how-to-reach-the-top-of-a-hill", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1672, "htmlBody": "<p>When is it faster to rediscover something on your own than to learn it from someone who already knows it?</p>\n<p>Sometimes it's faster to re-derive a proof or algorithm than to look it up. Keith Lynch re-invented the fast Fourier transform because he was too lazy to walk all the way to the library to get a book on it, although that's an extreme example. But if you have a complicated proof already laid out before you, and you are not Marc Drexler, it's generally faster to read it than to derive a new one. Yet I found a knowledge-intensive task where it would have been much faster to tell someone nothing at all than to tell them how to do it.<a id=\"more\"></a></p>\n<p>I'm digitizing my books by chopping off their bindings and scanning them. I recently hired someone to do the chopping, and have been teaching him how to do it. The first step is to chop the book into sections of about 50 pages each, separating them at the binding. I do this by placing the opened book cover-down under a paper chopper, and cutting it precisely where the two opened pages meet.</p>\n<p>The \"chopper\" is a manual paper-cutter with a 15-inch steel blade that weights about 10 pounds and is razor-sharp. If the blade is a fifth of a millimeter off its mark, it misses the gap between the pages and makes the cutting much harder, as it must go through paper instead of only glue. Being an entire millimeter off makes the blade catch the page maybe half a centimeter further away from its edge, depending on how the base of the page is angled, cutting off words and ruining the book. You can't see where the blade touches the book while making the cut. You can look before making the cut and position the book, but then you need one hand to operate the blade, and the physics of a book that wishes to spring shut, fall away from the blade, and fall onto one side, make it nearly impossible to keep the groove in the book in place for the blade with just one hand, unless you hold it with your fingers underneath the blade, which you can do only once.</p>\n<p>My technique is to do this:</p>\n<ol>\n<li>Face the chopper with the blade's hinge opposite you. The chopper has a square base. If you're facing north, the hinge is at its northeast corner; the blade will cut along the eastern edge.</li>\n<li>Slide the book, cover-down, under the blade until the blade is over the binding.</li>\n<li>Lower the blade until it almost touches the book.</li>\n<li>Grasp the book with 2 hands, one on each side of the blade. Lift the book in the air to touch the blade along its entire length. (The edge of the blade where the cut begins is lower than the other edge.)</li>\n<li>Slide the book back and forth until the groove between the pages locks into place against the blade.</li>\n<li>Lower the blade until the end that starts the cut is pressed firmly against the book, which is pressed firmly against the cutting board.</li>\n<li>Press your left fingers against the pages to be cut off from the book, pressing those pages up against the blade along its entire length and keeping the groove of the book in place along the blade. You can feel the side of the blade through the pages, but the blade is now too low for your fingers to get underneath it.</li>\n<li>Stand with your head and shoulders directly above the blade. DO NOT raise the blade while repositioning yourself.</li>\n<li>Punch downward with the blade while simultaneously falling on it with all your weight to make a cut that is too fast to grab onto the paper and pull it out of place.</li>\n</ol>\n<p>It's more complicated than that; I'm simplifying for the sake of space. But I didn't realize any of this when I began teaching him. I told him to put the book face-up under the blade and cut it into sections. It takes me a few seconds to make each cut. It was only when he kept trying to do it, and it kept not working, that I realized there must be more to it. He would try to chop a book and it wouldn't work. I'd look at the book, figure out what went wrong, then chop another section from the same book, watching myself, until I figured out what I was doing that could make the difference. Then I'd tell him. He'd try again, and it still wouldn't work. After perhaps 3 hours (6 person-hours), we worked out the sequence of steps I was doing well enough that he could chop books.</p>\n<p>I could ask how I learned all those steps without knowing I'd learned them--was I conscious of them at the time, but forgot each step as soon as it was committed to my body? Probably. And it's interesting that I was unable to extract my own knowledge without watching someone else fail. But my point in this essay is that it took me longer to teach him how to do it than it took me to learn how to do it on my own--and it took 2 people instead of 1. So teaching was less than half as efficient as just handing him a book and walking away. (I'm ignoring the risk of coming back an hour later to find the floor strewn with severed fingers; that's overly particular to this domain.)</p>\n<p>He kept worrying whether he was \"doing it right\". When I first figured out how to use the book chopper, I didn't know if there was one right way, or five, or none. I didn't have anyone to compare myself to. I could see whether I'd chopped the book the way I wanted to, but had no way to judge whether I was doing \"above average\", and so no self-consciousness about how well I was doing. Whereas he would see me take a book, slide it in, and chop it correctly, and then he would spend minutes fiddling with it, bending down to look under the blade from each side, swapping left and right hands between the two sides of the book and the blade, raising and lowering the blade, ad nauseum, until he finally tried to cut it--and inevitably got it wrong. He was nearly disabled by frustration and a sense of incompetence, and his actions were the anxious, tentative movements of someone worried about \"doing it wrong\" rather than the rapid movements of someone trying to find out whether there was any way to do it at all. We often hear the inspirational advice that believing something is possible makes it easier to accomplish; yet I saw just the opposite here. I didn't have my self-image on the line in my initial discovery process because I didn't know whether my task was possible, so I felt no pressure.</p>\n<p>The task I originally faced was to find any path through a very large space that would end up with a book cut where I wanted it cut. The task the two of us faced in teaching him was to observe me chopping books, over and over, until we could find the one path I had discovered and forgotten. It isn't obvious which of these tasks is easier. In \"discovery\", there may be many possible solutions, while in \"imitation\" there is only one.</p>\n<p>The psychological component probably applies to every search space: Availability of experts and the belief that there is a right way to do something inhibit experimentation; focusing on imitation prevents discovery. But what was it about the search space for book-chopping that made experimentation simple enough, and imitation hard enough, that imitation was harder than discovery? My guess is it was these things:</p>\n<ul>\n<li>The task is analog/continuous, concerned with movements in space, so that it can't be specified precisely.</li>\n<li>The task is procedural, and almost all of the teacher's knowledge about it is \"motor memory\", not conscious.</li>\n<li>The search space is low-dimensional, because except for the final act of cutting and the importance of keeping fingers out of the path of the blade, every action involves only the book and the paper chopper. The book and the chopper each have one degree of freedom, and the book can be moved in space, and that is all.</li>\n<li>There are no difficult insights, special sticking-points, or especially-valuable insights that could be applied repeatedly&nbsp;(discontinuities in the search space).</li>\n<li>There was little back-tracking in rediscovering the steps. So there are few local maxima.</li>\n<li>Failures are easy to analyze; the next step in the process can be discovered by analyzing the previous failure. Also, steps 3-5, 6-7, and 8-9 are mostly independent; e.g., you can discover steps 8-9 before having 6-7 completely worked out. These properties allow the task to be learned incrementally.</li>\n</ul>\n<p>Roughly, it's a task in a search space on which hill-climbing works well.</p>\n<p>Contrast this to martial arts, in which the movements of two fighters have a much higher dimensionality. The fraction of all possible movement sequences that leads to a side-kick or a hook punch is so small that few boxers ever discover the first and few karate students ever discover the second. Or contrast it to mathematical proofs, which are very high-dimensional, may have key insights (discontinuities), and give little indication of whether one is making progress. Those are domains in which instruction is more useful.</p>\n<p>Think about computer software that you had to read the manual for. I think first of Adobe Photoshop and its concept of layers and selections. Those are complex, broadly-applicable concepts (discontinuities) that you can't easily discover by experimentation, as clicking on things before you understand them will make apparently random things happen. A user interface for something casual (a game, a website) or meant for the mass-market should have an event space on which hill-climbing works well, so that instruction is not needed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fR7QfYx4JA3BnptT9": 2, "grDZHm8y34H9C7uui": 2, "fkABsGCJZ6y9qConW": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gbNaFMkqtso7kdL5y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 48, "extendedScore": null, "score": 0.000147, "legacy": true, "legacyId": "25684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-04T23:25:30.458Z", "modifiedAt": null, "url": null, "title": "LessWrong Hamburg Third Meetup Notes: Small Steps Forward", "slug": "lesswrong-hamburg-third-meetup-notes-small-steps-forward", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RwWuTGJPBPhatvNsH/lesswrong-hamburg-third-meetup-notes-small-steps-forward", "pageUrlRelative": "/posts/RwWuTGJPBPhatvNsH/lesswrong-hamburg-third-meetup-notes-small-steps-forward", "linkUrl": "https://www.lesswrong.com/posts/RwWuTGJPBPhatvNsH/lesswrong-hamburg-third-meetup-notes-small-steps-forward", "postedAtFormatted": "Tuesday, March 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Hamburg%20Third%20Meetup%20Notes%3A%20Small%20Steps%20Forward&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Hamburg%20Third%20Meetup%20Notes%3A%20Small%20Steps%20Forward%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRwWuTGJPBPhatvNsH%2Flesswrong-hamburg-third-meetup-notes-small-steps-forward%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Hamburg%20Third%20Meetup%20Notes%3A%20Small%20Steps%20Forward%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRwWuTGJPBPhatvNsH%2Flesswrong-hamburg-third-meetup-notes-small-steps-forward", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRwWuTGJPBPhatvNsH%2Flesswrong-hamburg-third-meetup-notes-small-steps-forward", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 735, "htmlBody": "<p><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">Review of our third&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"/lw/jqs/meetup_lesswrong_hamburg_structure/\">Meetup : LessWrong Hamburg - Structure</a></span></strong></p>\n<h2>Summary</h2>\n<p>To make it short: We didn't follow the nice agenda we planned. We did the procrastination topic but diverged a lot.&nbsp;</p>\n<h2>Course of events</h2>\n<p>In the long open beginning (expected) we talked a lot, played some&nbsp;<a href=\"http://en.wikipedia.org/wiki/MindTrap\">MindTrap</a>&nbsp;and had lunch together.</p>\n<p>Then to get started I began the presentation of the main topic of this meetup: procrastination. This was basically a summary of&nbsp;</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Procrastination\">http://en.wikipedia.org/wiki/Procrastination</a></li>\n<li><a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">http://lesswrong.com/lw/9wr/my_algorithm_for_beating_procrastination/</a></li>\n<li><a href=\"http://bps-research-digest.blogspot.de/2010/05/cure-for-procrastination-forgive.html\">http://bps-research-digest.blogspot.de/2010/05/cure-for-procrastination-forgive.html</a></li>\n<li><a href=\"http://waitbutwhy.com/2013/10/why-procrastinators-procrastinate.html\">http://waitbutwhy.com/2013/10/why-procrastinators-procrastinate.html</a></li>\n</ul>\n<p>This led to lots of satellite discussions which partly diverged but mostly were centered on examples of procrastination (though afterwards some felt that this got out of hand with too many personal details; this was controversial). This part was all in all very long but also led to quite some understanding of the problems of and strategries against procrastination.&nbsp;</p>\n<p>In the previous meetups there was an interest in the topics of and the objectives behind lesswrong. To get an authentic handle on the former I posted a <a href=\"/r/discussion/lw/jsq/polling_thread/an1z\">topic poll</a>&nbsp;in the <a href=\"/r/discussion/lw/jsq/polling_thread/\">Polling Thread</a>. This I presented shortly (see appendix).</p>\n<p>Interesting points we arrived at:</p>\n<p>The <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/procrastination-equation.png\">image</a> of lukeprogs procastination algorithm led to a discussion of what we called the mathematical fallacy/bias: Just giving a mathematical formula naming properties of interest leads to the false impression of scientificity and presents a false image of correctness and precision that just isn't there. This is a method sometimes seen in pseudo-science publications to give the impression of science. It is also used in economic sciences to approximate tendencies numerically. The general pattern of the mathematical fallacy is that modelling complex human behavior (like procrastination) in a simple formula is a special case of over-simplification riding piggy-back on the habit to take formulas at face value. The disclaimer on such formular (lukeprog actually gave one) just cannot be large enough. In this special case it would have been better to just name the four (nonlinear, crossrelated) effects on motivation instead of using the formula (at least until the five quantities in the formula are actually shown or defined in a precise way).</p>\n<p>We planned the next meetup for Mar 30th, but the location (near Hamburg central station) is not yet fixed.</p>\n<p style=\"text-align: justify;\">We did have more structure than the last time but a review discussion at the end clearly showed that just having one main topic with unmoderated side tracks wasn't enough and that all preferred a more&nbsp;formal structure - at least for topic presentations. Which on the next meetup will be&nbsp;<a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Theme-centered_interaction\">theme-centered interaction</a><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 15.600000381469727px;\">.</span></span></p>\n<p>Having keen observers of behavior allowed to pinpoint differences and misunderstandings in the group (actually involving me) to address these in a friendly helpful way.</p>\n<h2>Sidetrack</h2>\n<p>Following the advice from&nbsp;<a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/\">Lifestyle interventions to increase longevity</a>&nbsp;I had bought an e-cig for the smoker in our round who wants to quit. He received it positively. He enjoyed the near-identical handling and the fact that he could smoke in the room (I didn't notice bad taste) and that there was no effort to 'light' and 'unlight' it. We discussed it afterwards whether it increased or decreased the amount of smoking (I had noticed that he had used the e-cig more often, but this may be balanced by a much smaller number of pulls. We promised to measure this.</p>\n<h2>Appendix</h2>\n<div>\n<p style=\"margin-bottom: 0cm\">I presented the following list of LessWrong topics in order of decreasing typicality (most typical for LW first):</p>\n<ol>\n<li>\n<p style=\"margin-bottom: 0cm\">methods for being less wrong, knowing about biases, fallacies and heuristics</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">methods of self-improvement (if scientifically backed), e.g. living luminiously, winning at life, longevity</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">organization and discussion of meetups</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">dealing with procrastination and akrasia</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">statistics, probability theory, decision theory and related mathematical fields</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">topics of associated or related organizations CFAR, MIRI, GiveWell, CEA</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">advancing specific virtues: altruism, mindfulness, empathy, truthfulness, openness</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">artificial intelligence topics esp. if related to AGI, (U)FAI, AI going FOOM (or not)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">the singularity and transhumanism (includes cryonics as method to get there)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">rationality applied to social situations in relationships, parenting and small groups</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">(moral) philosophical theories, ethics</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">platform to hangout with like-minded smart people</p>\n</li>\n</ol>\n<div>\n<h2>Other LW Hamburg Meetup reviews</h2>\n<ul>\n<li><a href=\"/r/discussion/lw/jqr/lesswrong_hamburg_second_meetup_notes_in_need_of/\">Second Meetup Notes: In need of Structure</a></li>\n<li><a href=\"/lw/jn3/lesswrong_hamburg_first_meetup_notes_starting/\">First Meetup Notes: Starting small</a></li>\n</ul>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RwWuTGJPBPhatvNsH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.5947832140600386e-06, "legacy": true, "legacyId": "25687", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" id=\"Review_of_our_third_Meetup___LessWrong_Hamburg___Structure\">Review of our third&nbsp;<span style=\"color: #8a8a8b;\"><a href=\"/lw/jqs/meetup_lesswrong_hamburg_structure/\">Meetup : LessWrong Hamburg - Structure</a></span></strong></p>\n<h2 id=\"Summary\">Summary</h2>\n<p>To make it short: We didn't follow the nice agenda we planned. We did the procrastination topic but diverged a lot.&nbsp;</p>\n<h2 id=\"Course_of_events\">Course of events</h2>\n<p>In the long open beginning (expected) we talked a lot, played some&nbsp;<a href=\"http://en.wikipedia.org/wiki/MindTrap\">MindTrap</a>&nbsp;and had lunch together.</p>\n<p>Then to get started I began the presentation of the main topic of this meetup: procrastination. This was basically a summary of&nbsp;</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Procrastination\">http://en.wikipedia.org/wiki/Procrastination</a></li>\n<li><a href=\"/lw/9wr/my_algorithm_for_beating_procrastination/\">http://lesswrong.com/lw/9wr/my_algorithm_for_beating_procrastination/</a></li>\n<li><a href=\"http://bps-research-digest.blogspot.de/2010/05/cure-for-procrastination-forgive.html\">http://bps-research-digest.blogspot.de/2010/05/cure-for-procrastination-forgive.html</a></li>\n<li><a href=\"http://waitbutwhy.com/2013/10/why-procrastinators-procrastinate.html\">http://waitbutwhy.com/2013/10/why-procrastinators-procrastinate.html</a></li>\n</ul>\n<p>This led to lots of satellite discussions which partly diverged but mostly were centered on examples of procrastination (though afterwards some felt that this got out of hand with too many personal details; this was controversial). This part was all in all very long but also led to quite some understanding of the problems of and strategries against procrastination.&nbsp;</p>\n<p>In the previous meetups there was an interest in the topics of and the objectives behind lesswrong. To get an authentic handle on the former I posted a <a href=\"/r/discussion/lw/jsq/polling_thread/an1z\">topic poll</a>&nbsp;in the <a href=\"/r/discussion/lw/jsq/polling_thread/\">Polling Thread</a>. This I presented shortly (see appendix).</p>\n<p>Interesting points we arrived at:</p>\n<p>The <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/procrastination-equation.png\">image</a> of lukeprogs procastination algorithm led to a discussion of what we called the mathematical fallacy/bias: Just giving a mathematical formula naming properties of interest leads to the false impression of scientificity and presents a false image of correctness and precision that just isn't there. This is a method sometimes seen in pseudo-science publications to give the impression of science. It is also used in economic sciences to approximate tendencies numerically. The general pattern of the mathematical fallacy is that modelling complex human behavior (like procrastination) in a simple formula is a special case of over-simplification riding piggy-back on the habit to take formulas at face value. The disclaimer on such formular (lukeprog actually gave one) just cannot be large enough. In this special case it would have been better to just name the four (nonlinear, crossrelated) effects on motivation instead of using the formula (at least until the five quantities in the formula are actually shown or defined in a precise way).</p>\n<p>We planned the next meetup for Mar 30th, but the location (near Hamburg central station) is not yet fixed.</p>\n<p style=\"text-align: justify;\">We did have more structure than the last time but a review discussion at the end clearly showed that just having one main topic with unmoderated side tracks wasn't enough and that all preferred a more&nbsp;formal structure - at least for topic presentations. Which on the next meetup will be&nbsp;<a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Theme-centered_interaction\">theme-centered interaction</a><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 15.600000381469727px;\">.</span></span></p>\n<p>Having keen observers of behavior allowed to pinpoint differences and misunderstandings in the group (actually involving me) to address these in a friendly helpful way.</p>\n<h2 id=\"Sidetrack\">Sidetrack</h2>\n<p>Following the advice from&nbsp;<a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/\">Lifestyle interventions to increase longevity</a>&nbsp;I had bought an e-cig for the smoker in our round who wants to quit. He received it positively. He enjoyed the near-identical handling and the fact that he could smoke in the room (I didn't notice bad taste) and that there was no effort to 'light' and 'unlight' it. We discussed it afterwards whether it increased or decreased the amount of smoking (I had noticed that he had used the e-cig more often, but this may be balanced by a much smaller number of pulls. We promised to measure this.</p>\n<h2 id=\"Appendix\">Appendix</h2>\n<div>\n<p style=\"margin-bottom: 0cm\">I presented the following list of LessWrong topics in order of decreasing typicality (most typical for LW first):</p>\n<ol>\n<li>\n<p style=\"margin-bottom: 0cm\">methods for being less wrong, knowing about biases, fallacies and heuristics</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">methods of self-improvement (if scientifically backed), e.g. living luminiously, winning at life, longevity</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">organization and discussion of meetups</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">dealing with procrastination and akrasia</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">statistics, probability theory, decision theory and related mathematical fields</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">topics of associated or related organizations CFAR, MIRI, GiveWell, CEA</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">advancing specific virtues: altruism, mindfulness, empathy, truthfulness, openness</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">artificial intelligence topics esp. if related to AGI, (U)FAI, AI going FOOM (or not)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">the singularity and transhumanism (includes cryonics as method to get there)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">rationality applied to social situations in relationships, parenting and small groups</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">(moral) philosophical theories, ethics</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\">platform to hangout with like-minded smart people</p>\n</li>\n</ol>\n<div>\n<h2 id=\"Other_LW_Hamburg_Meetup_reviews\">Other LW Hamburg Meetup reviews</h2>\n<ul>\n<li><a href=\"/r/discussion/lw/jqr/lesswrong_hamburg_second_meetup_notes_in_need_of/\">Second Meetup Notes: In need of Structure</a></li>\n<li><a href=\"/lw/jn3/lesswrong_hamburg_first_meetup_notes_starting/\">First Meetup Notes: Starting small</a></li>\n</ul>\n</div>\n</div>", "sections": [{"title": "Review of our third\u00a0Meetup : LessWrong Hamburg - Structure", "anchor": "Review_of_our_third_Meetup___LessWrong_Hamburg___Structure", "level": 2}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Course of events", "anchor": "Course_of_events", "level": 1}, {"title": "Sidetrack", "anchor": "Sidetrack", "level": 1}, {"title": "Appendix", "anchor": "Appendix", "level": 1}, {"title": "Other LW Hamburg Meetup reviews", "anchor": "Other_LW_Hamburg_Meetup_reviews", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pgyE458aEgAztninW", "Ty2tjPwv8uyPK9vrz", "NwEYHSbvDExRKwzoL", "PhXENjdXiHhsWGfQo", "d2f6Eggpj3edzKefe", "guYSP8Km3hChHD4ja"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-05T09:14:28.458Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Discussion", "slug": "meetup-urbana-champaign-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rGtvaNniygrSbsqKw/meetup-urbana-champaign-discussion", "pageUrlRelative": "/posts/rGtvaNniygrSbsqKw/meetup-urbana-champaign-discussion", "linkUrl": "https://www.lesswrong.com/posts/rGtvaNniygrSbsqKw/meetup-urbana-champaign-discussion", "postedAtFormatted": "Wednesday, March 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGtvaNniygrSbsqKw%2Fmeetup-urbana-champaign-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGtvaNniygrSbsqKw%2Fmeetup-urbana-champaign-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGtvaNniygrSbsqKw%2Fmeetup-urbana-champaign-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xk'>Urbana-Champaign: Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 March 2014 01:00:02PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">412 W. Elm St., Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Starting topic: should we expect there to be easy ways to improve your life (life hacks)? Two articles that bear on this by <a href=\"http://www.gwern.net/Drug%20heuristics\" rel=\"nofollow\">Gwern</a> and <a href=\"http://slatestarcodex.com/2014/03/03/do-life-hacks-ever-reach-fixation/\" rel=\"nofollow\">Scott</a>.</p>\n\n<p>Also, I'm going to finish up my sequence of posts on logical uncertainty (<a href=\"http://lesswrong.com/lw/jfx/foundations_of_probability/\">starts here</a>) by then, and would be pleased to answer questions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xk'>Urbana-Champaign: Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rGtvaNniygrSbsqKw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.5954900104612522e-06, "legacy": true, "legacyId": "25692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Discussion\">Discussion article for the meetup : <a href=\"/meetups/xk\">Urbana-Champaign: Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 March 2014 01:00:02PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">412 W. Elm St., Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Starting topic: should we expect there to be easy ways to improve your life (life hacks)? Two articles that bear on this by <a href=\"http://www.gwern.net/Drug%20heuristics\" rel=\"nofollow\">Gwern</a> and <a href=\"http://slatestarcodex.com/2014/03/03/do-life-hacks-ever-reach-fixation/\" rel=\"nofollow\">Scott</a>.</p>\n\n<p>Also, I'm going to finish up my sequence of posts on logical uncertainty (<a href=\"http://lesswrong.com/lw/jfx/foundations_of_probability/\">starts here</a>) by then, and would be pleased to answer questions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Discussion1\">Discussion article for the meetup : <a href=\"/meetups/xk\">Urbana-Champaign: Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Discussion", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EQ33emneF3Fh62Nn2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-05T16:18:09.867Z", "modifiedAt": null, "url": null, "title": "Proposal: LW courses", "slug": "proposal-lw-courses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.740Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "iarwain1", "createdAt": "2013-10-29T15:19:43.635Z", "isAdmin": false, "displayName": "iarwain1"}, "userId": "X2k5DW8qrCTeD2N8d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/otk7rzf4jeeRdMJ88/proposal-lw-courses", "pageUrlRelative": "/posts/otk7rzf4jeeRdMJ88/proposal-lw-courses", "linkUrl": "https://www.lesswrong.com/posts/otk7rzf4jeeRdMJ88/proposal-lw-courses", "postedAtFormatted": "Wednesday, March 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposal%3A%20LW%20courses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposal%3A%20LW%20courses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fotk7rzf4jeeRdMJ88%2Fproposal-lw-courses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposal%3A%20LW%20courses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fotk7rzf4jeeRdMJ88%2Fproposal-lw-courses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fotk7rzf4jeeRdMJ88%2Fproposal-lw-courses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1148, "htmlBody": "<p>For a long time I have tried to study things on my own, at my own pace. But it was always an uphill struggle against strong akrasia issues, and eventually I came to the conclusion that the only thing that really seems to work is to have externally-imposed deadlines. The only way I could think of to do this was to sign up for classes, so I enrolled in a number of MOOCs. So far this has worked wonders - I went from basically spending most of my time playing around and wasting time, to several recent days where I studied for several hours straight.</p>\n<p>The only thing I don't like about this setup is that there's a very limited number of really good MOOCs out there on the subjects I want to study. Also, most MOOCs are geared for a wider audience and are therefore dumbed-down to a certain degree.</p>\n<p>So I had the following idea: A lot of us on LW seem to be studying a lot of the same material, whether it's the sequences, <a href=\"http://intelligence.org/courses/\">MIRI course list</a>, <a href=\"http://rationality.org/reading/\">CFAR booklist</a>, or any of the various <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">recommended</a> <a href=\"/lw/2un/references_resources_for_lesswrong/\">reading</a> <a href=\"http://wiki.lesswrong.com/wiki/FAQ#What_are_some_other_books_Less_Wrong_users_recommend.3F\">lists</a>. What if those who were studying the same thing would get together and set a schedule for themselves to finish the reading material, complete with deadlines? This might not be a normal \"externally imposed\" deadline, but at least it's a deadline with some social pressure to back it up. I can't be the only one on LW who could benefit from a deadline.</p>\n<p>The details would need to be worked out, but here's a preliminary version of the way I envision it:</p>\n<ul>\n<li>There should be a monthly thread for requests for new classes. The request should specify the text to be used, or it could ask for suggestions for a good text. The request should also specify the approximate pace (very slow - slow - normal - fast - very fast), or an approximate weekly time commitment.</li>\n<li>The next thing that would be needed for each proposed class would be for someone who's already gone through that text to propose a rough calendar for the course. For example, they could say that given the requested pace / time commitment, you should expect to spend about 3 months on that particular text. Also, some chapters are harder than others, so the calendar should specify, for example, that you should expect to spend just one week on Chapters 1-3, but Chapter 7 will need to spread over three weeks. It would also be very useful to specify what prerequisites are needed for that text. (Similar to <a href=\"/lw/jl5/selfstudy_questions_thread/\">this thread</a>. Keep in mind that different people have different styles when it comes to prerequisites. Some prefer to do as few prerequisites as possible and then skip straight to the harder stuff, and work backwards / fill in gaps as necessary. Others prefer to carefully cover all lower-level material before even touching the harder stuff. These people will want to know about all possible prerequisites so that they won't have to work backwards at all.)</li>\n<li>I would recommend creating a repository of available course calendars (i.e., course X should be split up this way, course Y should be split up that way, etc.). This can be done by creating a special thread for this purpose and then linking to that thread every time a new \"proposed course\" thread starts.</li>\n<li>A calendar provides some deadlines, but there needs to be some motivation for keeping to the deadlines. I can think of a few possibilities that might work:<br /> \n<ul>\n<li>Social pressure: If there is anybody else in your class other than yourself, there's a certain amount of social pressure to keep up with the group and keep to the agreed-upon deadlines. Classmates can increase this pressure by actively encouraging each other to keep up.</li>\n<li>Social encouragement: As you make each deadline you should report that you did so, and others can then respond with encouragement.</li>\n<li>Karma: If someone makes a deadline they should post an announcement to that effect, and LWers (even those not part of your class, and even those who aren't taking any classes) could be encouraged to upvote the announcement. I haven't been on LW long enough to tell if this is a socially acceptable use of karma points, but this might be motivating for some people.</li>\n<li>Perhaps someone could design a \"LW U\" badge or something of the sort to post on your personal / social site when you complete a course. (Notice that with the karma or badge reward forms, it becomes possible to have only a single member in a course and they'll still be able to get some form of reward structure. It might not be as effective as having other people in the course, but at least it works.)</li>\n</ul>\n</li>\n<li>There should be a dedicated thread for each course once it begins. The thread would be used for everything relating to the course: announcing progress, discussing subject-related material, meta-discussions about the course, etc.</li>\n<li>LWers who have already completed the subject / textbook could follow the course discussions and provide guidance and help as needed. Anyone who thinks they can contribute in this \"teacher\" capacity should let course participants know about it beforehand, as this will provide additional social pressure / support, and provide valuable encouragement (there's someone I can ask my stupid questions to!).</li>\n<li>I'd recommend that once one or more people decide to take a course, they should set a date to start the course that's at least two weeks (maybe a month) in the future. This would give time for others to join. Each month's thread for proposed courses could then include a list of \"courses starting soon\".</li>\n<li>Perhaps people who have already studied a given text could put together a few quizzes / tests / finals for that text. The quizzes would be sent to individual students at a certain point in the course via private message. Each student would take the quiz on their own (honor system, of course), and the quizzes would then be graded either by the creator of the quiz (the \"teacher\"), a volunteer TA (using an answer key provided by the teacher), the other students, or even by each student themselves. (I would not recommend this last unless there are no other options, since even very honest people can be sorely tempted to fudge things occasionally in their own favor.) There could even be a final grade for the course. I suspect that this system would create powerful psychological motivation for certain people to work hard at the coursework and complete their work on time.</li>\n</ul>\n<ul>\n</ul>\n<p>What do you think about such an idea?</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "otk7rzf4jeeRdMJ88", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 11, "extendedScore": null, "score": 1.5959988077625868e-06, "legacy": true, "legacyId": "25685", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xg3hXCYQPJkwHyik2", "TNHQLZK5pHbxdnz4e", "jTYWox8euWdN7DSGn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-05T17:01:20.532Z", "modifiedAt": null, "url": null, "title": "Meetup : Israel Less Wrong Meetup - Social and Board Games", "slug": "meetup-israel-less-wrong-meetup-social-and-board-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.742Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vdMDwbFZgTAk6ZwXK/meetup-israel-less-wrong-meetup-social-and-board-games", "pageUrlRelative": "/posts/vdMDwbFZgTAk6ZwXK/meetup-israel-less-wrong-meetup-social-and-board-games", "linkUrl": "https://www.lesswrong.com/posts/vdMDwbFZgTAk6ZwXK/meetup-israel-less-wrong-meetup-social-and-board-games", "postedAtFormatted": "Wednesday, March 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvdMDwbFZgTAk6ZwXK%2Fmeetup-israel-less-wrong-meetup-social-and-board-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Social%20and%20Board%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvdMDwbFZgTAk6ZwXK%2Fmeetup-israel-less-wrong-meetup-social-and-board-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvdMDwbFZgTAk6ZwXK%2Fmeetup-israel-less-wrong-meetup-social-and-board-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 310, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xl'>Israel Less Wrong Meetup - Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 March 2014 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">google tel aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, March 6th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time we're going to have a social meetup! Unlike previous meetups where we had a set agenda, an a talk - this time we'll be socializing and playing games. Specifically, we look forward to playing any cool board or card game anyone will bring.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda. (We've decided to start slightly earlier this time to give us more time and accommodate people with different schedules).</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is).\nIf you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>Things that might happen:\n- You'll trade cool ideas with cool people from the Israel LW community.\n- You'll discover kindred spirits who agree with you about one/two boxing.\n- You'll kick someone's ass (and teach them how you did it) at some awesome boardgame.\n- You'll discover how to build a friendly AGI running on cold fusion (well probably not)</p>\n\n<p>Things that will happen for sure:\n- You'll get to hang out with awesome people and have fun!</p>\n\n<p>There is also talk of food and beers, and if you'd like to bring some too - that would be great. (But you don't have to).</p>\n\n<p>If you have any question feel free to email me at hochbergg@gmail.com or call me at 054-533-0678 or call Anatoly at 054-245-1060.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xl'>Israel Less Wrong Meetup - Social and Board Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vdMDwbFZgTAk6ZwXK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "25693", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games\">Discussion article for the meetup : <a href=\"/meetups/xl\">Israel Less Wrong Meetup - Social and Board Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 March 2014 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">google tel aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, March 6th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time we're going to have a social meetup! Unlike previous meetups where we had a set agenda, an a talk - this time we'll be socializing and playing games. Specifically, we look forward to playing any cool board or card game anyone will bring.</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. Feel free to come a little bit later, as there is no agenda. (We've decided to start slightly earlier this time to give us more time and accommodate people with different schedules).</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is).\nIf you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>Things that might happen:\n- You'll trade cool ideas with cool people from the Israel LW community.\n- You'll discover kindred spirits who agree with you about one/two boxing.\n- You'll kick someone's ass (and teach them how you did it) at some awesome boardgame.\n- You'll discover how to build a friendly AGI running on cold fusion (well probably not)</p>\n\n<p>Things that will happen for sure:\n- You'll get to hang out with awesome people and have fun!</p>\n\n<p>There is also talk of food and beers, and if you'd like to bring some too - that would be great. (But you don't have to).</p>\n\n<p>If you have any question feel free to email me at hochbergg@gmail.com or call me at 054-533-0678 or call Anatoly at 054-245-1060.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games1\">Discussion article for the meetup : <a href=\"/meetups/xl\">Israel Less Wrong Meetup - Social and Board Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games", "level": 1}, {"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Social and Board Games", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Social_and_Board_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-05T17:04:27.215Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Fun and Games Meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-11", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q3CQiuk4dJZndJkTT/meetup-washington-dc-fun-and-games-meetup-11", "pageUrlRelative": "/posts/q3CQiuk4dJZndJkTT/meetup-washington-dc-fun-and-games-meetup-11", "linkUrl": "https://www.lesswrong.com/posts/q3CQiuk4dJZndJkTT/meetup-washington-dc-fun-and-games-meetup-11", "postedAtFormatted": "Wednesday, March 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Fun%20and%20Games%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Fun%20and%20Games%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3CQiuk4dJZndJkTT%2Fmeetup-washington-dc-fun-and-games-meetup-11%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Fun%20and%20Games%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3CQiuk4dJZndJkTT%2Fmeetup-washington-dc-fun-and-games-meetup-11", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3CQiuk4dJZndJkTT%2Fmeetup-washington-dc-fun-and-games-meetup-11", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xm'>Washington DC Fun and Games Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 March 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xm'>Washington DC Fun and Games Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q3CQiuk4dJZndJkTT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5960544123866221e-06, "legacy": true, "legacyId": "25694", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Fun_and_Games_Meetup\">Discussion article for the meetup : <a href=\"/meetups/xm\">Washington DC Fun and Games Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 March 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Fun_and_Games_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/xm\">Washington DC Fun and Games Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Fun and Games Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Fun_and_Games_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Fun and Games Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Fun_and_Games_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-05T20:27:25.322Z", "modifiedAt": null, "url": null, "title": "How my math skills improved dramatically", "slug": "how-my-math-skills-improved-dramatically", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:07.869Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wqj7YJPFRxWbCLyt8/how-my-math-skills-improved-dramatically", "pageUrlRelative": "/posts/Wqj7YJPFRxWbCLyt8/how-my-math-skills-improved-dramatically", "linkUrl": "https://www.lesswrong.com/posts/Wqj7YJPFRxWbCLyt8/how-my-math-skills-improved-dramatically", "postedAtFormatted": "Wednesday, March 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20my%20math%20skills%20improved%20dramatically&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20my%20math%20skills%20improved%20dramatically%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqj7YJPFRxWbCLyt8%2Fhow-my-math-skills-improved-dramatically%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20my%20math%20skills%20improved%20dramatically%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqj7YJPFRxWbCLyt8%2Fhow-my-math-skills-improved-dramatically", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqj7YJPFRxWbCLyt8%2Fhow-my-math-skills-improved-dramatically", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 625, "htmlBody": "<p>When I was a freshman in high school, I was a mediocre math student: I earned a D in second semester geometry and had to repeat the course. By the time I was a senior in high school, I was one of the strongest few math students in my class of ~600 students at an academic magnet high school. I went on to earn a PhD in math. Most people wouldn't have guessed that I could have improved so much, and the shift that occurred was very surreal to me. It&rsquo;s all the more striking in that the bulk of the shift occurred in a single year. I thought I&rsquo;d share what strategies facilitated the change.</p>\n<p><a id=\"more\"></a></p>\n<h2>I became motivated to learn more</h2>\n<p>I took a course in chemistry my sophomore year, and loved it so much that I thought that I would pursue a career in the physical sciences. I knew that understanding math is essential for a career in the physical sciences, and so I became determined to learn it well. I immersed myself in math: At the start of my junior year I started learning calculus on my own. I didn&rsquo;t have the &ldquo;official&rdquo; prerequisites for calculus, for example, I didn&rsquo;t know trigonometry. But I didn&rsquo;t need to learn trigonometry to get started: I just skipped over the parts of calculus books involving trigonometric functions. Because I was behind a semester, I didn&rsquo;t have the &ldquo;official&rdquo; prerequisite for analytic geometry during my junior year, but I gained permission to sit in on a course (not for official academic credit) while taking trigonometry at the same time. I also took a course in honors physics that used a lot of algebra, and gave some hints of the relationship between physics and calculus.</p>\n<p>I learned these subjects better simultaneously than I would have had I learned them sequentially. A lot of times students don&rsquo;t spend enough time learning math per day to imprint the material in their long-term memories. They end up forgetting the techniques that they learn in short order, and have to relearn them repeatedly as a result. Learning them thoroughly the first time around would save them a lot of time later on. Because there was substantial overlap in the algebraic techniques utilized in the different subjects I was studying, my exposure to them per day was higher, so that when I learned them, they stuck in my long-term memory.</p>\n<h2>I learned from multiple expositions</h2>\n<p>This is related to the above point, but is worth highlighting on its own: I read textbooks on the subjects that I was studying aside from the assigned textbooks. Often a given textbook won&rsquo;t explain all of the topics as well as possible, and when one has difficulty understanding a given textbook&rsquo;s exposition of a topic, one can find a better one if one consults other references.</p>\n<h2>I learned basic techniques in the context of interesting problems</h2>\n<p>I distinctly remember hearing about how it was possible to find the graph of a rotated conic section from its defining equation. I found it amazing that it was possible to do this. Similarly, I found some of the applications of calculus to be amazing. This amazement motivated me to learn how to implement the various techniques needed, and they became more memorable when placed in the context of larger problems.</p>\n<h2>I found a friend who was also learning math in a serious way</h2>\n<p>It was really helpful to have someone who was both deeply involved and responsive, who I could consult when I got stuck, and with whom I could work through problems. This was helpful both from a motivational point of view (learning with someone else can be more fun than learning in isolation) and also from the point of view of having easier access to knowledge.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "6nS8oYmSMuFMaiowF": 1, "fF9GEdWXKJ3z73TmB": 1, "hrezrpGqXXdSe76ks": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wqj7YJPFRxWbCLyt8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 34, "extendedScore": null, "score": 8.6e-05, "legacy": true, "legacyId": "25695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>When I was a freshman in high school, I was a mediocre math student: I earned a D in second semester geometry and had to repeat the course. By the time I was a senior in high school, I was one of the strongest few math students in my class of ~600 students at an academic magnet high school. I went on to earn a PhD in math. Most people wouldn't have guessed that I could have improved so much, and the shift that occurred was very surreal to me. It\u2019s all the more striking in that the bulk of the shift occurred in a single year. I thought I\u2019d share what strategies facilitated the change.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"I_became_motivated_to_learn_more\">I became motivated to learn more</h2>\n<p>I took a course in chemistry my sophomore year, and loved it so much that I thought that I would pursue a career in the physical sciences. I knew that understanding math is essential for a career in the physical sciences, and so I became determined to learn it well. I immersed myself in math: At the start of my junior year I started learning calculus on my own. I didn\u2019t have the \u201cofficial\u201d prerequisites for calculus, for example, I didn\u2019t know trigonometry. But I didn\u2019t need to learn trigonometry to get started: I just skipped over the parts of calculus books involving trigonometric functions. Because I was behind a semester, I didn\u2019t have the \u201cofficial\u201d prerequisite for analytic geometry during my junior year, but I gained permission to sit in on a course (not for official academic credit) while taking trigonometry at the same time. I also took a course in honors physics that used a lot of algebra, and gave some hints of the relationship between physics and calculus.</p>\n<p>I learned these subjects better simultaneously than I would have had I learned them sequentially. A lot of times students don\u2019t spend enough time learning math per day to imprint the material in their long-term memories. They end up forgetting the techniques that they learn in short order, and have to relearn them repeatedly as a result. Learning them thoroughly the first time around would save them a lot of time later on. Because there was substantial overlap in the algebraic techniques utilized in the different subjects I was studying, my exposure to them per day was higher, so that when I learned them, they stuck in my long-term memory.</p>\n<h2 id=\"I_learned_from_multiple_expositions\">I learned from multiple expositions</h2>\n<p>This is related to the above point, but is worth highlighting on its own: I read textbooks on the subjects that I was studying aside from the assigned textbooks. Often a given textbook won\u2019t explain all of the topics as well as possible, and when one has difficulty understanding a given textbook\u2019s exposition of a topic, one can find a better one if one consults other references.</p>\n<h2 id=\"I_learned_basic_techniques_in_the_context_of_interesting_problems\">I learned basic techniques in the context of interesting problems</h2>\n<p>I distinctly remember hearing about how it was possible to find the graph of a rotated conic section from its defining equation. I found it amazing that it was possible to do this. Similarly, I found some of the applications of calculus to be amazing. This amazement motivated me to learn how to implement the various techniques needed, and they became more memorable when placed in the context of larger problems.</p>\n<h2 id=\"I_found_a_friend_who_was_also_learning_math_in_a_serious_way\">I found a friend who was also learning math in a serious way</h2>\n<p>It was really helpful to have someone who was both deeply involved and responsive, who I could consult when I got stuck, and with whom I could work through problems. This was helpful both from a motivational point of view (learning with someone else can be more fun than learning in isolation) and also from the point of view of having easier access to knowledge.</p>", "sections": [{"title": "I became motivated to learn more", "anchor": "I_became_motivated_to_learn_more", "level": 1}, {"title": "I learned from multiple expositions", "anchor": "I_learned_from_multiple_expositions", "level": 1}, {"title": "I learned basic techniques in the context of interesting problems", "anchor": "I_learned_basic_techniques_in_the_context_of_interesting_problems", "level": 1}, {"title": "I found a friend who was also learning math in a serious way", "anchor": "I_found_a_friend_who_was_also_learning_math_in_a_serious_way", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-06T04:31:05.498Z", "modifiedAt": null, "url": null, "title": "Meetup : Auckland Preliminary Meetup", "slug": "meetup-auckland-preliminary-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Slackson", "createdAt": "2011-09-01T11:35:14.364Z", "isAdmin": false, "displayName": "Slackson"}, "userId": "WrtmGGiKtRX434nnS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zEnHoAD7jmkXiLqB9/meetup-auckland-preliminary-meetup", "pageUrlRelative": "/posts/zEnHoAD7jmkXiLqB9/meetup-auckland-preliminary-meetup", "linkUrl": "https://www.lesswrong.com/posts/zEnHoAD7jmkXiLqB9/meetup-auckland-preliminary-meetup", "postedAtFormatted": "Thursday, March 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Auckland%20Preliminary%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Auckland%20Preliminary%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEnHoAD7jmkXiLqB9%2Fmeetup-auckland-preliminary-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Auckland%20Preliminary%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEnHoAD7jmkXiLqB9%2Fmeetup-auckland-preliminary-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzEnHoAD7jmkXiLqB9%2Fmeetup-auckland-preliminary-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xn'>Auckland Preliminary Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 March 2014 02:00:00PM (+1300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Albert Park, Auckland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I got back from the second Melbourne CFAR workshop recently and it was good. It's well worthwhile having a local rationalist community and while there are some good thinkers in my immediate circle of friends, meeting more and learning from each other would be awesome.</p>\n\n<p>I'm not sure if others will be using it, but let's meet near the gazebo in Albert Park at 2pm Saturday. I'll be carrying my CFAR bag and water bottle if you want to come over and say hi. I would be wearing a \"Just shy, not antisocial\" shirt if I had one. If you're interested in coming, just comment, or come anyway.</p>\n\n<p>Cheers,\nMarcel.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xn'>Auckland Preliminary Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zEnHoAD7jmkXiLqB9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5968796369508554e-06, "legacy": true, "legacyId": "25696", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Auckland_Preliminary_Meetup\">Discussion article for the meetup : <a href=\"/meetups/xn\">Auckland Preliminary Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 March 2014 02:00:00PM (+1300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Albert Park, Auckland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I got back from the second Melbourne CFAR workshop recently and it was good. It's well worthwhile having a local rationalist community and while there are some good thinkers in my immediate circle of friends, meeting more and learning from each other would be awesome.</p>\n\n<p>I'm not sure if others will be using it, but let's meet near the gazebo in Albert Park at 2pm Saturday. I'll be carrying my CFAR bag and water bottle if you want to come over and say hi. I would be wearing a \"Just shy, not antisocial\" shirt if I had one. If you're interested in coming, just comment, or come anyway.</p>\n\n<p>Cheers,\nMarcel.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Auckland_Preliminary_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/xn\">Auckland Preliminary Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Auckland Preliminary Meetup", "anchor": "Discussion_article_for_the_meetup___Auckland_Preliminary_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Auckland Preliminary Meetup", "anchor": "Discussion_article_for_the_meetup___Auckland_Preliminary_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-06T05:31:02.138Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Meet up", "slug": "meetup-moscow-meet-up-6", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pxNs9d9w6cD7jBWJQ/meetup-moscow-meet-up-6", "pageUrlRelative": "/posts/pxNs9d9w6cD7jBWJQ/meetup-moscow-meet-up-6", "linkUrl": "https://www.lesswrong.com/posts/pxNs9d9w6cD7jBWJQ/meetup-moscow-meet-up-6", "postedAtFormatted": "Thursday, March 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Meet%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Meet%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpxNs9d9w6cD7jBWJQ%2Fmeetup-moscow-meet-up-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Meet%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpxNs9d9w6cD7jBWJQ%2Fmeetup-moscow-meet-up-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpxNs9d9w6cD7jBWJQ%2Fmeetup-moscow-meet-up-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xo'>Moscow, Meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 March 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to a room inside the building at 16:00. So please do not be late. We will have:</p>\n\n<ul>\n<li>Report about \u201cZen to Done\u201d.</li>\n<li>Report about cognitive biases.</li>\n<li>Stumbling on happiness for rationalists presentation.</li>\n<li>Report about \"Decisive: How to Make Better Choices in Life and Work\" book.</li>\n<li>Cognitive behavioural therapy workshop.</li>\n</ul>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and through the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xo'>Moscow, Meet up</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pxNs9d9w6cD7jBWJQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.596951715445143e-06, "legacy": true, "legacyId": "25697", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Meet_up\">Discussion article for the meetup : <a href=\"/meetups/xo\">Moscow, Meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 March 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to a room inside the building at 16:00. So please do not be late. We will have:</p>\n\n<ul>\n<li>Report about \u201cZen to Done\u201d.</li>\n<li>Report about cognitive biases.</li>\n<li>Stumbling on happiness for rationalists presentation.</li>\n<li>Report about \"Decisive: How to Make Better Choices in Life and Work\" book.</li>\n<li>Cognitive behavioural therapy workshop.</li>\n</ul>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and through the archway. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then come inside.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Meet_up1\">Discussion article for the meetup : <a href=\"/meetups/xo\">Moscow, Meet up</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Meet up", "anchor": "Discussion_article_for_the_meetup___Moscow__Meet_up", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Meet up", "anchor": "Discussion_article_for_the_meetup___Moscow__Meet_up1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-06T18:32:38.731Z", "modifiedAt": null, "url": null, "title": "[LINK] Latinus rationalior ist.", "slug": "link-latinus-rationalior-ist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.717Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VAuroch", "createdAt": "2013-11-07T11:01:09.015Z", "isAdmin": false, "displayName": "VAuroch"}, "userId": "idJgwEhhiRhTzHpst", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8QNxJw65pPRpMiGeC/link-latinus-rationalior-ist", "pageUrlRelative": "/posts/8QNxJw65pPRpMiGeC/link-latinus-rationalior-ist", "linkUrl": "https://www.lesswrong.com/posts/8QNxJw65pPRpMiGeC/link-latinus-rationalior-ist", "postedAtFormatted": "Thursday, March 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Latinus%20rationalior%20ist.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Latinus%20rationalior%20ist.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8QNxJw65pPRpMiGeC%2Flink-latinus-rationalior-ist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Latinus%20rationalior%20ist.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8QNxJw65pPRpMiGeC%2Flink-latinus-rationalior-ist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8QNxJw65pPRpMiGeC%2Flink-latinus-rationalior-ist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>http://mappingignorance.org/2014/02/03/mandela-was-right-the-foreign-language-effect/</p>\n<p>&nbsp;</p>\n<p>Summary: Across the board, people are less prone to cognitive bias in a non-native language.</p>\n<p>Conclusion: If all important discourse was conducted in Latin, or any other language native to no one, people would make better decisions.<br /><br />Corollary: All the attempts to make a constructed \"scientific language\" actually could have worked relatively well, for reasons entirely unconnected to the painstaking scientific structure of the languages.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8QNxJw65pPRpMiGeC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 1.5978920745111425e-06, "legacy": true, "legacyId": "25702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-06T21:40:06.406Z", "modifiedAt": null, "url": null, "title": "What we learned about Less Wrong from Cognito Mentoring advising", "slug": "what-we-learned-about-less-wrong-from-cognito-mentoring", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:30.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xqEuAD3dXHPSFeerd/what-we-learned-about-less-wrong-from-cognito-mentoring", "pageUrlRelative": "/posts/xqEuAD3dXHPSFeerd/what-we-learned-about-less-wrong-from-cognito-mentoring", "linkUrl": "https://www.lesswrong.com/posts/xqEuAD3dXHPSFeerd/what-we-learned-about-less-wrong-from-cognito-mentoring", "postedAtFormatted": "Thursday, March 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20we%20learned%20about%20Less%20Wrong%20from%20Cognito%20Mentoring%20advising&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20we%20learned%20about%20Less%20Wrong%20from%20Cognito%20Mentoring%20advising%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqEuAD3dXHPSFeerd%2Fwhat-we-learned-about-less-wrong-from-cognito-mentoring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20we%20learned%20about%20Less%20Wrong%20from%20Cognito%20Mentoring%20advising%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqEuAD3dXHPSFeerd%2Fwhat-we-learned-about-less-wrong-from-cognito-mentoring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqEuAD3dXHPSFeerd%2Fwhat-we-learned-about-less-wrong-from-cognito-mentoring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 907, "htmlBody": "<p>In late December 2013, Jonah, my collaborator at <a href=\"http://www.cognitomentoring.org\">Cognito Mentoring</a>, <a href=\"/r/lesswrong/lw/jee/cognito_mentoring_an_advising_service_for/\">announced the service</a> on LessWrong. Information about the service was also circulated in other venues with high concentrations of gifted and intellectually curious people. Since then, we're received ~70 emails asking for mentoring from learners across all ages, plus a few parents. At least 40 of our advisees heard of us through LessWrong, and the number is probably around 50. Of the 23 who responded to our advisee satisfaction survey, 16 filled in information on where they'd heard of us, and 14 of those 16 had heard of us from LessWrong. The vast majority of student advisees with whom we had substantive interactions, and the ones we felt we were able to help the most, came from LessWrong (we got some parents through the Davidson Forum post, but that's a very different sort of advising).</p>\n<p>In this post, I discuss some common themes that emerged from our interaction with these advisees. Obviously, this isn't a comprehensive picture of the LessWrong <em>community</em> the way that Yvain's <a href=\"/lw/jj0/2013_survey_results/\">2013 survey results</a> were.</p>\n<ul>\n<li>A significant fraction of the people who contacted us via LessWrong aren't active LessWrong participants, and many don't even have user accounts on LessWrong. The prototypical advisees we got through LessWrong don't have many distinctive LessWrongian beliefs. Many of them use LessWrong primarily as a source of interesting stuff to read, rather than a community to be part of.</li>\n<li>About 25% of the advisees we got through LessWrong were female, and a slightly higher proportion of the advisees with whom we had substantive interaction (and subjectively feel we helped a lot) were female. You can see this by looking at the sex distribution of the <a href=\"http://cognitomentoring.org/reviews-by-students\">public reviews of us</a> from students.</li>\n<li>Our advisees included people in high school (typically, grades 11 and 12) and college. Our advisees in high school tended to be interested in mathematics, computer science, physics, engineering, and entrepreneurship. We did have a few who were interested in economics, philosophy, and the social sciences as well, but this was rarer. Our advisees in college and graduate school were also interested in the above subjects but skewed a bit more in the direction of being interested in philosophy, psychology, and economics.</li>\n<li>Somewhat surprisingly and endearingly, many of our advisees were interested in effective altruism and social impact. Some had already heard of the cluster of effective altruist ideas. Others were interested in generating social impact through entrepreneurship or choosing an impactful career, even though they weren't familiar with effective altruism until we pointed them to it. Of those who had heard of effective altruism as a cluster of ideas, some had either already consulted with or were planning to consult with <a href=\"http://www.80000hours.org\">80,000 Hours</a>, and were connecting with us largely to get a second opinion or to get opinion on matters other than career choice.</li>\n<li>Some of our advisees had had some sort of past involvement with MIRI/CFAR/FHI. Some were seriously considering working in existential risk reduction or on artificial intelligence. The two subsets overlapped considerably.</li>\n<li>Our advisees were somewhat better educated about rationality issues than we'd expect others of similar academic accomplishment to be, and more than the advisees we got from sources other than LessWrong. That's obviously not a surprise at all.</li>\n<li>We hadn't been expecting it, but many advisees asked us questions related to procrastination, social skills, and other life skills. We were initially somewhat ill-equipped to handle these, but we've built a base of recommendations, with some help from LessWrong and other sources.</li>\n<li>One thing that surprised me personally is that many of these people had never spent time exploring <a href=\"http://www.quora.com\">Quora</a>. I'd have expected Quora to be much more widely known and used by the sort of people who were sufficiently aware of the Internet to know LessWrong. But it's possible there's not that much overlap.</li>\n</ul>\n<p>My overall takeaway is that LessWrong seems to still be one of the foremost places that smart and curious young people interested in epistemic rationality visit. I'm not sure of the exact reason, though <a href=\"http://www.hpmor.com\">HPMOR</a> probably gets a significant fraction of the credit. As long as things stay this way, LessWrong remains a great way to influence a subset of the young population today that's likely to be disproportionately represented among the decision-makers a few years down the line.</p>\n<p>It's not clear to me <em>why</em> they don't participate more actively on LessWrong. Maybe no special reasons are needed: the ratio of lurkers to posters is huge for most Internet fora. Maybe the people who contacted us were relatively young and still didn't have an Internet presence, or were being careful about building one. On the other hand, maybe there is something about the comments culture that dissuades people from participating (this need not be a bad feature <em>per se</em>: one reason people may refrain from participating is that comments are held to a high bar and this keeps people from offering off-the-cuff comments). That said, <em>if</em> people could somehow participate more, LessWrong could transform itself into an interactive forum for smart and curious people that's head and shoulders above all the others.</p>\n<p>PS: We've now made our <a href=\"http://info.cognitomentoring.org\">information wiki</a> publicly accessible. It's still in beta and a lot of content is incomplete and there are links to as-yet-uncreated pages all over the place. But we think it might still be interesting to the LessWrong audience.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xqEuAD3dXHPSFeerd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 37, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "25703", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hphGa6xfad3m4imCs", "pJJdcZgB6mPNWoSWr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-06T23:33:21.527Z", "modifiedAt": null, "url": null, "title": "Amanda Knox Redux: is Satoshi Nakamoto the real Satoshi Nakamoto?", "slug": "amanda-knox-redux-is-satoshi-nakamoto-the-real-satoshi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:31.763Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cosmos", "createdAt": "2009-04-26T03:18:01.731Z", "isAdmin": false, "displayName": "Cosmos"}, "userId": "c3Ji9Th6jATRyHLFC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9FHHSXnYwk2FxSgC2/amanda-knox-redux-is-satoshi-nakamoto-the-real-satoshi", "pageUrlRelative": "/posts/9FHHSXnYwk2FxSgC2/amanda-knox-redux-is-satoshi-nakamoto-the-real-satoshi", "linkUrl": "https://www.lesswrong.com/posts/9FHHSXnYwk2FxSgC2/amanda-knox-redux-is-satoshi-nakamoto-the-real-satoshi", "postedAtFormatted": "Thursday, March 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Amanda%20Knox%20Redux%3A%20is%20Satoshi%20Nakamoto%20the%20real%20Satoshi%20Nakamoto%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmanda%20Knox%20Redux%3A%20is%20Satoshi%20Nakamoto%20the%20real%20Satoshi%20Nakamoto%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FHHSXnYwk2FxSgC2%2Famanda-knox-redux-is-satoshi-nakamoto-the-real-satoshi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Amanda%20Knox%20Redux%3A%20is%20Satoshi%20Nakamoto%20the%20real%20Satoshi%20Nakamoto%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FHHSXnYwk2FxSgC2%2Famanda-knox-redux-is-satoshi-nakamoto-the-real-satoshi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FHHSXnYwk2FxSgC2%2Famanda-knox-redux-is-satoshi-nakamoto-the-real-satoshi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>Many of you here have likely heard of Bitcoin, and maybe know something about it.</p>\n<p>Earlier today, <a href=\"http://mag.newsweek.com/2014/03/14/bitcoin-satoshi-nakamoto.html\">a story broke</a> that a reporter had apparently tracked down the real Satoshi Nakamoto, infamous creator of the Bitcoin protocol.</p>\n<p>This seems like an excellent opportunity to practice our Bayesian updating!</p>\n<p>So, how likely do you think it is that this man is the founder of Bitcoin? What do you believe and why?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9FHHSXnYwk2FxSgC2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 18, "extendedScore": null, "score": 1.5982541252317204e-06, "legacy": true, "legacyId": "25704", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-07T01:04:58.151Z", "modifiedAt": null, "url": null, "title": "Meetup : Canberra: Meta-meetup + meditation", "slug": "meetup-canberra-meta-meetup-meditation", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielFilan", "createdAt": "2014-01-30T11:04:39.341Z", "isAdmin": false, "displayName": "DanielFilan"}, "userId": "DgsGzjyBXN8XSK22q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WeTnmpDDhuk5vozfs/meetup-canberra-meta-meetup-meditation", "pageUrlRelative": "/posts/WeTnmpDDhuk5vozfs/meetup-canberra-meta-meetup-meditation", "linkUrl": "https://www.lesswrong.com/posts/WeTnmpDDhuk5vozfs/meetup-canberra-meta-meetup-meditation", "postedAtFormatted": "Friday, March 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Canberra%3A%20Meta-meetup%20%2B%20meditation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Canberra%3A%20Meta-meetup%20%2B%20meditation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeTnmpDDhuk5vozfs%2Fmeetup-canberra-meta-meetup-meditation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Canberra%3A%20Meta-meetup%20%2B%20meditation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeTnmpDDhuk5vozfs%2Fmeetup-canberra-meta-meetup-meditation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWeTnmpDDhuk5vozfs%2Fmeetup-canberra-meta-meetup-meditation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xp'>Canberra: Meta-meetup + meditation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 March 2014 06:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">ANU Arts Centre</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our first regular meetup will have two parts: firstly, we will be discussing what we want to get out of meetups, what sort of things we would ilke to do in them, and related matters, and secondly, we will be taught how to meditate and have a practice session. Vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until 10 pm at the XSite (home of the XSA), located upstairs in the ANU Arts Centre.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the CSIT building, room N101.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xp'>Canberra: Meta-meetup + meditation</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WeTnmpDDhuk5vozfs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.598364450527984e-06, "legacy": true, "legacyId": "25706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Canberra__Meta_meetup___meditation\">Discussion article for the meetup : <a href=\"/meetups/xp\">Canberra: Meta-meetup + meditation</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 March 2014 06:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">ANU Arts Centre</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our first regular meetup will have two parts: firstly, we will be discussing what we want to get out of meetups, what sort of things we would ilke to do in them, and related matters, and secondly, we will be taught how to meditate and have a practice session. Vegan snacks will be provided.</p>\n\n<p>General meetup info:</p>\n\n<p>Structured meetups are held on the second Saturday and fourth Friday of each month from 6 pm until 10 pm at the XSite (home of the XSA), located upstairs in the ANU Arts Centre.</p>\n\n<p>There will be LWers at the Computer Science Students Association's weekly board games night, held on Wednesdays from 7 pm in the CSIT building, room N101.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Canberra__Meta_meetup___meditation1\">Discussion article for the meetup : <a href=\"/meetups/xp\">Canberra: Meta-meetup + meditation</a></h2>", "sections": [{"title": "Discussion article for the meetup : Canberra: Meta-meetup + meditation", "anchor": "Discussion_article_for_the_meetup___Canberra__Meta_meetup___meditation", "level": 1}, {"title": "Discussion article for the meetup : Canberra: Meta-meetup + meditation", "anchor": "Discussion_article_for_the_meetup___Canberra__Meta_meetup___meditation1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-07T02:21:01.081Z", "modifiedAt": null, "url": null, "title": "Meetup : March Meetup: Body Hacking!", "slug": "meetup-march-meetup-body-hacking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:05.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/39PPFScsziWna7jqf/meetup-march-meetup-body-hacking", "pageUrlRelative": "/posts/39PPFScsziWna7jqf/meetup-march-meetup-body-hacking", "linkUrl": "https://www.lesswrong.com/posts/39PPFScsziWna7jqf/meetup-march-meetup-body-hacking", "postedAtFormatted": "Friday, March 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20March%20Meetup%3A%20Body%20Hacking!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20March%20Meetup%3A%20Body%20Hacking!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F39PPFScsziWna7jqf%2Fmeetup-march-meetup-body-hacking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20March%20Meetup%3A%20Body%20Hacking!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F39PPFScsziWna7jqf%2Fmeetup-march-meetup-body-hacking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F39PPFScsziWna7jqf%2Fmeetup-march-meetup-body-hacking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xq'>March Meetup: Body Hacking!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 March 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1314 Hosea L Williams Drive NE, Atlanta, GA 30317</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Rescheduled to March 15)</p>\n\n<p>An overview of body hacking, what's possible, what's known, what needs more exploration, and what tools are available to you.</p>\n\n<p>Presenters needed! Do you have expertise on any of this? Lemme know and you can do anything from a full presentation with slides and handouts to leading a discussion on a particular topic.</p>\n\n<p>Also, please check out our facebook group here: <a href=\"https://www.facebook.com/groups/Atlanta.Lesswrong/\" rel=\"nofollow\">https://www.facebook.com/groups/Atlanta.Lesswrong/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xq'>March Meetup: Body Hacking!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "39PPFScsziWna7jqf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 1.5984560454263825e-06, "legacy": true, "legacyId": "25708", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___March_Meetup__Body_Hacking_\">Discussion article for the meetup : <a href=\"/meetups/xq\">March Meetup: Body Hacking!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 March 2014 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1314 Hosea L Williams Drive NE, Atlanta, GA 30317</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Rescheduled to March 15)</p>\n\n<p>An overview of body hacking, what's possible, what's known, what needs more exploration, and what tools are available to you.</p>\n\n<p>Presenters needed! Do you have expertise on any of this? Lemme know and you can do anything from a full presentation with slides and handouts to leading a discussion on a particular topic.</p>\n\n<p>Also, please check out our facebook group here: <a href=\"https://www.facebook.com/groups/Atlanta.Lesswrong/\" rel=\"nofollow\">https://www.facebook.com/groups/Atlanta.Lesswrong/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___March_Meetup__Body_Hacking_1\">Discussion article for the meetup : <a href=\"/meetups/xq\">March Meetup: Body Hacking!</a></h2>", "sections": [{"title": "Discussion article for the meetup : March Meetup: Body Hacking!", "anchor": "Discussion_article_for_the_meetup___March_Meetup__Body_Hacking_", "level": 1}, {"title": "Discussion article for the meetup : March Meetup: Body Hacking!", "anchor": "Discussion_article_for_the_meetup___March_Meetup__Body_Hacking_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-07T07:24:45.334Z", "modifiedAt": null, "url": null, "title": "How to Study Unsafe AGI's safely (and why we might have no choice)", "slug": "how-to-study-unsafe-agi-s-safely-and-why-we-might-have-no", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:23.704Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Punoxysm", "createdAt": "2014-03-04T08:04:57.536Z", "isAdmin": false, "displayName": "Punoxysm"}, "userId": "oAKBj5h4XeKsTRmNJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eJDTaBEZgCpbSdAbS/how-to-study-unsafe-agi-s-safely-and-why-we-might-have-no", "pageUrlRelative": "/posts/eJDTaBEZgCpbSdAbS/how-to-study-unsafe-agi-s-safely-and-why-we-might-have-no", "linkUrl": "https://www.lesswrong.com/posts/eJDTaBEZgCpbSdAbS/how-to-study-unsafe-agi-s-safely-and-why-we-might-have-no", "postedAtFormatted": "Friday, March 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Study%20Unsafe%20AGI's%20safely%20(and%20why%20we%20might%20have%20no%20choice)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Study%20Unsafe%20AGI's%20safely%20(and%20why%20we%20might%20have%20no%20choice)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJDTaBEZgCpbSdAbS%2Fhow-to-study-unsafe-agi-s-safely-and-why-we-might-have-no%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Study%20Unsafe%20AGI's%20safely%20(and%20why%20we%20might%20have%20no%20choice)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJDTaBEZgCpbSdAbS%2Fhow-to-study-unsafe-agi-s-safely-and-why-we-might-have-no", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeJDTaBEZgCpbSdAbS%2Fhow-to-study-unsafe-agi-s-safely-and-why-we-might-have-no", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1604, "htmlBody": "<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">TL;DR</strong></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">A serious possibility is that the first AGI(s) will be developed in a Manhattan Project style setting before any sort of friendliness/safety constraints can be integrated reliably. They will also be substantially short of the intelligence required to exponentially self-improve. Within a certain range of development and intelligence, containment protocols can make them safe to interact with.&nbsp;This means they can be studied experimentally, and the architecture(s) used to create them better understood, furthering the goal of safely using AI in less constrained settings.</span></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">Setting the Scene</strong></p>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><em style=\"margin: 0px; padding: 0px; border: 0px;\">The year is 2040, and in the last decade a series of breakthroughs in neuroscience, cognitive science, machine learning, and computer hardware have put the long-held dream of a human-level artificial intelligence in our grasp. The wild commercial success of lifelike robotic pets, the integration into everyday work and leisure of AI assistants and concierges, and STUDYBOT's graduation from Harvard's Online degree program with an octuple major and full honors, DARPA, the NSF and the European Research Council have announced joint funding of an artificial intelligence program that will create a superhuman intelligence in 3 years.</em></div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><br /></div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><em style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">Safety was announced as a critical element of the project, especially in light of the self-modifying LeakrVirus that catastrophically disrupted markets in 36 and 37. The planned protocols have not been made public, but it seems they will be centered in traditional computer security rather than techniques from the nascent field of Provably Safe AI, which were deemed impossible to integrate on the current project timeline.</em></div>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">Technological and/or Political issues could force the development of AI without theoretical safety guarantees that we'd certainly like, but there is a silver lining</strong></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"></strong><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">A lot of the discussion around LessWrong and MIRI that I've seen (and I haven't seen all of it, please send links!) seems to focus very strongly on the situation of an AI that can self-modify or construct further AIs, resulting in an exponential explosion of intelligence (FOOM/Singularity). The focus on FAI is on finding an architecture that can be explicitly constrained (and a constraint set that won't fail to do what we desire).</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">My argument is essentially that there could be a critical multi-year period preceding any possible exponentially self-improving intelligence during which a series of AGIs of varying intelligence, flexibility and architecture will be built. This period will be fast and frantic, but it will be incredibly fruitful and vital both in figuring out how to make an AI sufficiently strong to exponentially self-improve and in how to make it safe and friendly (or develop protocols to bridge the even riskier period between when we can develop FOOM-capable AIs and when we can ensure their safety).&nbsp;</span></p>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">I'll break this post into three parts.</div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><ol>\n<li>why is a substantial period of proto-singularity more likely than a straight-to-singularity situation?</li>\n<li>Second, what strategies will be critical to developing, controlling, and learning from these pre-FOOM AIs?</li>\n<li>Third, what are the political challenge that will develop immediately before and during this period?</li>\n</ol></div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><strong>Why is a proto-singularity likely?</strong></div>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">The requirement for a hard singularity, an exponentially self-improving AI, is that the AI can substantially improve itself in a way that enhances its ability to further improve itself, which requires the ability to modify its own code; access to resources like time, data, and hardware to facilitate these modifications; and the intelligence to execute a fruitful self-modification strategy.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">The first two conditions can (and should) be directly restricted. I'll elaborate more on that later, but basically any AI should be very carefully sandboxed (unable to affect its software environment), and should have access to resources strictly controlled. Perhaps no data goes in without human approval or while the AI is running. Perhaps nothing comes out either. Even a hyperpersuasive hyperintelligence will be slowed down (at least) if it can only interact with prespecified tests (how do you test AGI? No idea but it shouldn't be harder than friendliness).&nbsp;This isn't a perfect situation. Eliezer Yudkowsky presents several arguments for why an intelligence explosion could happen even when resources are constrained, (see <a href=\"https://intelligence.org/files/IEM.pdf\">Section 3 of Intelligence Explosion Microeconomics</a>) not to mention ways that those constraints could be defied even if engineered perfectly (by the way, I would happily run the <a href=\"http://yudkowsky.net/singularity/aibox/\">AI box experiment</a> with anybody, I think it is absurd that anyone would fail it! [I've read <a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">Tuxedage's accounts</a>, and I think I actually do understand how a gatekeeper could fail, but I also believe I understand how one could be trained to succeed even against a much stronger foe than any person who has played the part of the AI]).</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">But the third emerges from the way technology typically develops.&nbsp;</span><em style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">I believe it is incredibly unlikely that an AGI will develop in somebody's basement, or even in a small national lab or top corporate lab.&nbsp;</em><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">When there is no clear notion of what a technology will look like, it is usually not developed. Positive, productive accidents are somewhat rare in science, but they are remarkably rare in engineering (please, give counterexamples!). The creation of an AGI will likely not happen by accident; there will be a well-funded, concrete research and development plan that leads up to it. An AI Manhattan Project described above. But even when there is a good plan successfully executed, prototypes are slow, fragile, and poor-quality compared to what is possible even with approaches using the same underlying technology. It seems very likely to me that the first AGI will be a Chicago Pile, not a Trinity; recognizably a breakthrough but with proper consideration not immediately dangerous or unmanageable. [Note, you don't have to believe this to read the rest of this. If you disagree, consider the virtues of redundancy and the question of what safety an AI development effort should implement if they can't be persuaded to delay long enough for theoretically sound methods to become available].</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">A Manhattan Project style effort makes a relatively weak, controllable AI even more likely, because not only can such a project implement substantial safety protocols that are explicitly researched in parallel with primary development, but also because the total resources, in hardware and brainpower, devoted to the AI will be much greater than a smaller project, and therefore setting a correspondingly higher bar for the AGI thus created to reach to be able to successfully self-modify itself exponentially and also break the security procedures.</span></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">Strategies to handle AIs in the proto-Singularity, and why they're important</strong></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">First, take a look the <a href=\"http://intelligence.org/files/ResponsesAGIRisk.pdf\">External Constraints Section of this MIRI Report</a>&nbsp;and/or this article on <a href=\"http://wiki.lesswrong.com/wiki/AI_boxing\">AI Boxing</a>. I will be talking mainly about these approaches. There are certainly others, but these are the easiest to extrapolate from current computer security.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">These AIs will provide us with the experimental knowledge to better handle the construction of even stronger AIs. If careful, we will be able to use these proto-Singularity AIs to learn about the nature of intelligence and cognition, to perform economically valuable tasks, and to test theories of friendliness (not perfectly, but well enough to start).&nbsp;</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">\"If careful\" is the key phrase. I mentioned sandboxing above. And computer security is key to any attempt to contain an AI. Monitoring the source code, and setting a threshold for too much changing too fast at which point a failsafe freezes all computation; keeping extremely strict control over copies of the source. Some architectures will be more inherently dangerous and less predictable than others. A simulation of a physical brain, for instance, will be fairly opaque (depending on how far neuroscience has gone) but could have almost no potential to self-improve to an uncontrollable degree if its access to hardware is limited (it won't be able to make itself much more efficient on fixed resources). Other architectures will have other properties. Some will be utility optimizing agents. Some will have behaviors but no clear utility. Some will be opaque, some transparent.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">All will have a theory to how they operate, which can be refined by actual experimentation. This is what we can gain! We can set up controlled scenarios like honeypots to catch malevolence. We can evaluate our ability to monitor and read the thoughts of the agi. We can develop stronger theories of how damaging self-modification actually is to imposed constraints. We can test our abilities to add constraints to even the base state. But do I really have to justify the value of experimentation?</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">I am familiar with criticisms based on <a href=\"/lw/qk/that_alien_message/\">absolutley incomprehensibly perceptive and persuasive hyperintelligences</a> being able to overcome any security, but I've tried to outline above why I don't think we'd be dealing with that case.</span></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">Political</strong><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">&nbsp;issues</strong></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">Right now AGI is really a political non-issue. Blue sky even compared to space exploration and fusion both of which actually receive funding from government in substantial volumes. I think that this will change in the period immediately leading up to my hypothesized AI Manhattan Project. The AI Manhattan Project can only happen with a lot of political will behind it, which will probably mean a spiral of scientific advancements, hype and threat of competition from external unfriendly sources. Think space race.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">So suppose that the first few AIs are built under well controlled conditions. Friendliness is still not perfected, but we think/hope we've learned some valuable basics. But now people want to use the AIs for something. So what should be done at this point?</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">I won't try to speculate what happens next (well you can probably persuade me to, but it might not be as valuable), beyond extensions of the protocols I've already laid out, hybridized with notions like Oracle AI. It certainly gets a lot harder, but hopefully experimentation on the first, highly-controlled generation of AI to get a better understanding of their architectural fundamentals, combined with more direct research on friendliness in general would provide the groundwork for this.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eJDTaBEZgCpbSdAbS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 10, "extendedScore": null, "score": 1.5988219664067013e-06, "legacy": true, "legacyId": "25712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\" id=\"TL_DR\">TL;DR</strong></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">A serious possibility is that the first AGI(s) will be developed in a Manhattan Project style setting before any sort of friendliness/safety constraints can be integrated reliably. They will also be substantially short of the intelligence required to exponentially self-improve. Within a certain range of development and intelligence, containment protocols can make them safe to interact with.&nbsp;This means they can be studied experimentally, and the architecture(s) used to create them better understood, furthering the goal of safely using AI in less constrained settings.</span></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\" id=\"Setting_the_Scene\">Setting the Scene</strong></p>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><em style=\"margin: 0px; padding: 0px; border: 0px;\">The year is 2040, and in the last decade a series of breakthroughs in neuroscience, cognitive science, machine learning, and computer hardware have put the long-held dream of a human-level artificial intelligence in our grasp. The wild commercial success of lifelike robotic pets, the integration into everyday work and leisure of AI assistants and concierges, and STUDYBOT's graduation from Harvard's Online degree program with an octuple major and full honors, DARPA, the NSF and the European Research Council have announced joint funding of an artificial intelligence program that will create a superhuman intelligence in 3 years.</em></div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><br></div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><em style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">Safety was announced as a critical element of the project, especially in light of the self-modifying LeakrVirus that catastrophically disrupted markets in 36 and 37. The planned protocols have not been made public, but it seems they will be centered in traditional computer security rather than techniques from the nascent field of Provably Safe AI, which were deemed impossible to integrate on the current project timeline.</em></div>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\" id=\"Technological_and_or_Political_issues_could_force_the_development_of_AI_without_theoretical_safety_guarantees_that_we_d_certainly_like__but_there_is_a_silver_lining\">Technological and/or Political issues could force the development of AI without theoretical safety guarantees that we'd certainly like, but there is a silver lining</strong></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"></strong><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">A lot of the discussion around LessWrong and MIRI that I've seen (and I haven't seen all of it, please send links!) seems to focus very strongly on the situation of an AI that can self-modify or construct further AIs, resulting in an exponential explosion of intelligence (FOOM/Singularity). The focus on FAI is on finding an architecture that can be explicitly constrained (and a constraint set that won't fail to do what we desire).</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">My argument is essentially that there could be a critical multi-year period preceding any possible exponentially self-improving intelligence during which a series of AGIs of varying intelligence, flexibility and architecture will be built. This period will be fast and frantic, but it will be incredibly fruitful and vital both in figuring out how to make an AI sufficiently strong to exponentially self-improve and in how to make it safe and friendly (or develop protocols to bridge the even riskier period between when we can develop FOOM-capable AIs and when we can ensure their safety).&nbsp;</span></p>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">I'll break this post into three parts.</div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><ol>\n<li>why is a substantial period of proto-singularity more likely than a straight-to-singularity situation?</li>\n<li>Second, what strategies will be critical to developing, controlling, and learning from these pre-FOOM AIs?</li>\n<li>Third, what are the political challenge that will develop immediately before and during this period?</li>\n</ol></div>\n<div style=\"margin: 0px; padding: 0px; border: 0px; line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\"><strong>Why is a proto-singularity likely?</strong></div>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">The requirement for a hard singularity, an exponentially self-improving AI, is that the AI can substantially improve itself in a way that enhances its ability to further improve itself, which requires the ability to modify its own code; access to resources like time, data, and hardware to facilitate these modifications; and the intelligence to execute a fruitful self-modification strategy.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">The first two conditions can (and should) be directly restricted. I'll elaborate more on that later, but basically any AI should be very carefully sandboxed (unable to affect its software environment), and should have access to resources strictly controlled. Perhaps no data goes in without human approval or while the AI is running. Perhaps nothing comes out either. Even a hyperpersuasive hyperintelligence will be slowed down (at least) if it can only interact with prespecified tests (how do you test AGI? No idea but it shouldn't be harder than friendliness).&nbsp;This isn't a perfect situation. Eliezer Yudkowsky presents several arguments for why an intelligence explosion could happen even when resources are constrained, (see <a href=\"https://intelligence.org/files/IEM.pdf\">Section 3 of Intelligence Explosion Microeconomics</a>) not to mention ways that those constraints could be defied even if engineered perfectly (by the way, I would happily run the <a href=\"http://yudkowsky.net/singularity/aibox/\">AI box experiment</a> with anybody, I think it is absurd that anyone would fail it! [I've read <a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">Tuxedage's accounts</a>, and I think I actually do understand how a gatekeeper could fail, but I also believe I understand how one could be trained to succeed even against a much stronger foe than any person who has played the part of the AI]).</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">But the third emerges from the way technology typically develops.&nbsp;</span><em style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">I believe it is incredibly unlikely that an AGI will develop in somebody's basement, or even in a small national lab or top corporate lab.&nbsp;</em><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">When there is no clear notion of what a technology will look like, it is usually not developed. Positive, productive accidents are somewhat rare in science, but they are remarkably rare in engineering (please, give counterexamples!). The creation of an AGI will likely not happen by accident; there will be a well-funded, concrete research and development plan that leads up to it. An AI Manhattan Project described above. But even when there is a good plan successfully executed, prototypes are slow, fragile, and poor-quality compared to what is possible even with approaches using the same underlying technology. It seems very likely to me that the first AGI will be a Chicago Pile, not a Trinity; recognizably a breakthrough but with proper consideration not immediately dangerous or unmanageable. [Note, you don't have to believe this to read the rest of this. If you disagree, consider the virtues of redundancy and the question of what safety an AI development effort should implement if they can't be persuaded to delay long enough for theoretically sound methods to become available].</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">A Manhattan Project style effort makes a relatively weak, controllable AI even more likely, because not only can such a project implement substantial safety protocols that are explicitly researched in parallel with primary development, but also because the total resources, in hardware and brainpower, devoted to the AI will be much greater than a smaller project, and therefore setting a correspondingly higher bar for the AGI thus created to reach to be able to successfully self-modify itself exponentially and also break the security procedures.</span></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\" id=\"Strategies_to_handle_AIs_in_the_proto_Singularity__and_why_they_re_important\">Strategies to handle AIs in the proto-Singularity, and why they're important</strong></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">First, take a look the <a href=\"http://intelligence.org/files/ResponsesAGIRisk.pdf\">External Constraints Section of this MIRI Report</a>&nbsp;and/or this article on <a href=\"http://wiki.lesswrong.com/wiki/AI_boxing\">AI Boxing</a>. I will be talking mainly about these approaches. There are certainly others, but these are the easiest to extrapolate from current computer security.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">These AIs will provide us with the experimental knowledge to better handle the construction of even stronger AIs. If careful, we will be able to use these proto-Singularity AIs to learn about the nature of intelligence and cognition, to perform economically valuable tasks, and to test theories of friendliness (not perfectly, but well enough to start).&nbsp;</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">\"If careful\" is the key phrase. I mentioned sandboxing above. And computer security is key to any attempt to contain an AI. Monitoring the source code, and setting a threshold for too much changing too fast at which point a failsafe freezes all computation; keeping extremely strict control over copies of the source. Some architectures will be more inherently dangerous and less predictable than others. A simulation of a physical brain, for instance, will be fairly opaque (depending on how far neuroscience has gone) but could have almost no potential to self-improve to an uncontrollable degree if its access to hardware is limited (it won't be able to make itself much more efficient on fixed resources). Other architectures will have other properties. Some will be utility optimizing agents. Some will have behaviors but no clear utility. Some will be opaque, some transparent.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">All will have a theory to how they operate, which can be refined by actual experimentation. This is what we can gain! We can set up controlled scenarios like honeypots to catch malevolence. We can evaluate our ability to monitor and read the thoughts of the agi. We can develop stronger theories of how damaging self-modification actually is to imposed constraints. We can test our abilities to add constraints to even the base state. But do I really have to justify the value of experimentation?</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">I am familiar with criticisms based on <a href=\"/lw/qk/that_alien_message/\">absolutley incomprehensibly perceptive and persuasive hyperintelligences</a> being able to overcome any security, but I've tried to outline above why I don't think we'd be dealing with that case.</span></p>\n<p><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">Political</strong><strong style=\"line-height: 19.9999942779541px; font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px;\">&nbsp;issues</strong></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">Right now AGI is really a political non-issue. Blue sky even compared to space exploration and fusion both of which actually receive funding from government in substantial volumes. I think that this will change in the period immediately leading up to my hypothesized AI Manhattan Project. The AI Manhattan Project can only happen with a lot of political will behind it, which will probably mean a spiral of scientific advancements, hype and threat of competition from external unfriendly sources. Think space race.</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">So suppose that the first few AIs are built under well controlled conditions. Friendliness is still not perfected, but we think/hope we've learned some valuable basics. But now people want to use the AIs for something. So what should be done at this point?</span></p>\n<p><span style=\"font-family: Helvetica, Arial, 'Droid Sans', sans-serif; font-size: 14px; line-height: 19.9999942779541px;\">I won't try to speculate what happens next (well you can probably persuade me to, but it might not be as valuable), beyond extensions of the protocols I've already laid out, hybridized with notions like Oracle AI. It certainly gets a lot harder, but hopefully experimentation on the first, highly-controlled generation of AI to get a better understanding of their architectural fundamentals, combined with more direct research on friendliness in general would provide the groundwork for this.</span></p>", "sections": [{"title": "TL;DR", "anchor": "TL_DR", "level": 1}, {"title": "Setting the Scene", "anchor": "Setting_the_Scene", "level": 1}, {"title": "Technological and/or Political issues could force the development of AI without theoretical safety guarantees that we'd certainly like, but there is a silver lining", "anchor": "Technological_and_or_Political_issues_could_force_the_development_of_AI_without_theoretical_safety_guarantees_that_we_d_certainly_like__but_there_is_a_silver_lining", "level": 1}, {"title": "Strategies to handle AIs in the proto-Singularity, and why they're important", "anchor": "Strategies_to_handle_AIs_in_the_proto_Singularity__and_why_they_re_important", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FmxhoWxvBqSxhFeJn", "5wMcKNAwB6X4mp9og"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-07T10:10:14.961Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014Expert At Vs. Expert On", "slug": "meetup-west-la-expert-at-vs-expert-on", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E46brFnsydp7Xxz2R/meetup-west-la-expert-at-vs-expert-on", "pageUrlRelative": "/posts/E46brFnsydp7Xxz2R/meetup-west-la-expert-at-vs-expert-on", "linkUrl": "https://www.lesswrong.com/posts/E46brFnsydp7Xxz2R/meetup-west-la-expert-at-vs-expert-on", "postedAtFormatted": "Friday, March 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94Expert%20At%20Vs.%20Expert%20On&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94Expert%20At%20Vs.%20Expert%20On%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE46brFnsydp7Xxz2R%2Fmeetup-west-la-expert-at-vs-expert-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94Expert%20At%20Vs.%20Expert%20On%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE46brFnsydp7Xxz2R%2Fmeetup-west-la-expert-at-vs-expert-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE46brFnsydp7Xxz2R%2Fmeetup-west-la-expert-at-vs-expert-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xr'>West LA\u2014Expert At Vs. Expert On</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 March 2014 06:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://maps.google.com/maps?q=Del+Taco,+11066+Santa+Monica+Boulevard,+Los+Angeles,+CA+90025&amp;hl=en&amp;sll=34.047335,-118.443432&amp;sspn=0.006418,0.012392&amp;t=h&amp;hq=Del+Taco,&amp;hnear=11066+Santa+Monica+Blvd,+Los+Angeles,+California+90025&amp;z=17\" rel=\"nofollow\">this</a> Del Taco. I will bring a Rubik's Cube. The presence of a Rubik's Cube will be strong Bayesian evidence of the presence of a Less Wrong meetup.</p>\n\n<p><strong>Parking</strong> is completely free. There is a sign that claims there is a 45-minute time limit, but it is a <em>lie</em>.</p>\n\n<p><strong>Discussion</strong>: <a href=\"http://www.overcomingbias.com/2007/04/expert_at_versu.html\">Expert at vs. expert on</a> is a fairly important distinction. It's also a really simple one, which makes it conceptual low-hanging fruit. It's not totally without nuance; for example the terminology implies either total mastery or encyclopedic knowledge, but it applies just as well at any level of competence.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2007/04/expert_at_versu.html\">Expert At Versus Expert On</a>. I know of no other writing that is explicitly on this topic. Robin Hanson emphasizes the signaling aspect (of course he does), but I do not.</li>\n<li>It is well-known that you <a href=\"http://lesswrong.com/lw/9hb/position_design_and_write_rationality_curriculum/\">learn to play baseball by playing baseball, not by reading essays about baseball</a>. However, it is not usually made explicit that the former makes you an expert at baseball, and the latter makes you an expert on baseball.</li>\n<li>Another nuance: Being an expert at something helps you become an expert on it; the vice versa may be true also. For example, you are probably a better linguist if you speak many languages.</li>\n</ul>\n\n<p>NB: <em>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</em> Also, we may or may not play a card game.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xr'>West LA\u2014Expert At Vs. Expert On</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E46brFnsydp7Xxz2R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.5990214037236153e-06, "legacy": true, "legacyId": "25713", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Expert_At_Vs__Expert_On\">Discussion article for the meetup : <a href=\"/meetups/xr\">West LA\u2014Expert At Vs. Expert On</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 March 2014 06:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">11066 Santa Monica Blvd, Los Angeles, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to Find Us</strong>: Go into <a href=\"https://maps.google.com/maps?q=Del+Taco,+11066+Santa+Monica+Boulevard,+Los+Angeles,+CA+90025&amp;hl=en&amp;sll=34.047335,-118.443432&amp;sspn=0.006418,0.012392&amp;t=h&amp;hq=Del+Taco,&amp;hnear=11066+Santa+Monica+Blvd,+Los+Angeles,+California+90025&amp;z=17\" rel=\"nofollow\">this</a> Del Taco. I will bring a Rubik's Cube. The presence of a Rubik's Cube will be strong Bayesian evidence of the presence of a Less Wrong meetup.</p>\n\n<p><strong>Parking</strong> is completely free. There is a sign that claims there is a 45-minute time limit, but it is a <em>lie</em>.</p>\n\n<p><strong>Discussion</strong>: <a href=\"http://www.overcomingbias.com/2007/04/expert_at_versu.html\">Expert at vs. expert on</a> is a fairly important distinction. It's also a really simple one, which makes it conceptual low-hanging fruit. It's not totally without nuance; for example the terminology implies either total mastery or encyclopedic knowledge, but it applies just as well at any level of competence.</p>\n\n<p><strong>Recommended Reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2007/04/expert_at_versu.html\">Expert At Versus Expert On</a>. I know of no other writing that is explicitly on this topic. Robin Hanson emphasizes the signaling aspect (of course he does), but I do not.</li>\n<li>It is well-known that you <a href=\"http://lesswrong.com/lw/9hb/position_design_and_write_rationality_curriculum/\">learn to play baseball by playing baseball, not by reading essays about baseball</a>. However, it is not usually made explicit that the former makes you an expert at baseball, and the latter makes you an expert on baseball.</li>\n<li>Another nuance: Being an expert at something helps you become an expert on it; the vice versa may be true also. For example, you are probably a better linguist if you speak many languages.</li>\n</ul>\n\n<p>NB: <em>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</em> Also, we may or may not play a card game.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Expert_At_Vs__Expert_On1\">Discussion article for the meetup : <a href=\"/meetups/xr\">West LA\u2014Expert At Vs. Expert On</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014Expert At Vs. Expert On", "anchor": "Discussion_article_for_the_meetup___West_LA_Expert_At_Vs__Expert_On", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014Expert At Vs. Expert On", "anchor": "Discussion_article_for_the_meetup___West_LA_Expert_At_Vs__Expert_On1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ifL8f4Xzy2D9Bb6zs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-07T18:14:15.291Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-31", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.778Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qkWasTATELXccSMdH/weekly-lw-meetups-31", "pageUrlRelative": "/posts/qkWasTATELXccSMdH/weekly-lw-meetups-31", "linkUrl": "https://www.lesswrong.com/posts/qkWasTATELXccSMdH/weekly-lw-meetups-31", "postedAtFormatted": "Friday, March 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkWasTATELXccSMdH%2Fweekly-lw-meetups-31%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkWasTATELXccSMdH%2Fweekly-lw-meetups-31", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqkWasTATELXccSMdH%2Fweekly-lw-meetups-31", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 535, "htmlBody": "<p><strong>This summary was posted to LW main on February 28th. The following week's summary is <a href=\"/lw/jua/new_lw_meetup_auckland/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/x8\">Boston - Optimizing Empathy Levels:&nbsp;<span class=\"date\">02 March 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/x4\">Hamburg - Structure:&nbsp;<span class=\"date\">04 March 2014 05:00PM</span></a></li>\n<li><a href=\"/meetups/x9\">Munich Meetup:&nbsp;<span class=\"date\">08 March 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/xb\">Saint Petersburg sunday meetup:&nbsp;<span class=\"date\">01 March 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/xd\">Sydney Meetup - March:&nbsp;<span class=\"date\">26 March 2014 06:30PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/xa\">Berkeley: Implementation Intentions:&nbsp;<span class=\"date\">05 March 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/w6\">[Berlin] Community Weekend in Berlin:&nbsp;<span class=\"date\">11 April 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/x7\">Brussels - Calibration and other games:&nbsp;<span class=\"date\">08 March 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/xc\">London Games Meetup 09/03, + Socials 02/03 and 16/02 :&nbsp;<span class=\"date\">09 March 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/x3\">NYC Rationality Megameetup and Unconference: April 5-6:&nbsp;<span class=\"date\">05 April 2014 11:00AM</span></a></li>\n<li><a href=\"/meetups/x6\">Salt Lake City UT &mdash; Open Possibilities and Improv Skills:&nbsp;<span class=\"date\">09 March 2014 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qkWasTATELXccSMdH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.5996049337229787e-06, "legacy": true, "legacyId": "25646", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vgD2AuH89QtXWM7Qu", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-08T00:25:59.903Z", "modifiedAt": null, "url": null, "title": "Updateless Intelligence Metrics in the Multiverse", "slug": "updateless-intelligence-metrics-in-the-multiverse", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:27.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Squark", "createdAt": "2013-02-04T19:29:04.489Z", "isAdmin": false, "displayName": "Squark"}, "userId": "k4QpNYXcigqfG85t6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6GEkaQpj4WmNLhMaM/updateless-intelligence-metrics-in-the-multiverse", "pageUrlRelative": "/posts/6GEkaQpj4WmNLhMaM/updateless-intelligence-metrics-in-the-multiverse", "linkUrl": "https://www.lesswrong.com/posts/6GEkaQpj4WmNLhMaM/updateless-intelligence-metrics-in-the-multiverse", "postedAtFormatted": "Saturday, March 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Updateless%20Intelligence%20Metrics%20in%20the%20Multiverse&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUpdateless%20Intelligence%20Metrics%20in%20the%20Multiverse%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6GEkaQpj4WmNLhMaM%2Fupdateless-intelligence-metrics-in-the-multiverse%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Updateless%20Intelligence%20Metrics%20in%20the%20Multiverse%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6GEkaQpj4WmNLhMaM%2Fupdateless-intelligence-metrics-in-the-multiverse", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6GEkaQpj4WmNLhMaM%2Fupdateless-intelligence-metrics-in-the-multiverse", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1810, "htmlBody": "<p>Followup to:&nbsp;<a title=\"Intelligence Metrics with Naturalized Induction\" href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">Intelligence Metrics with Naturalized Induction using UDT</a></p>\n<p>In the previous post I have defined an <a href=\"http://wiki.lesswrong.com/wiki/Intelligence\">intelligence metric</a> solving the <a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">duality</a>&nbsp;(aka <a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalized induction</a>)&nbsp;and <a href=\"/lw/cze/reply_to_holden_on_tool_ai/70rt\">ontology</a> problems in <a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>. This model used a formalization of <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a> using Benja's <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">model</a> of logical uncertainty. In the current post I am going to:</p>\n<ul>\n<li>Explain some problems with my previous model (<em>that section can be skipped if you don't care about the previous model and only want to understand the new one</em>).</li>\n<li>Formulate a new model solving these problems. Incidentally, the new model is much closer to the usual way UDT is represented. It is also based on a <a href=\"/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">different model</a> of logical uncertainty.</li>\n<li>Show how to define intelligence without specifying the <a href=\"http://wiki.lesswrong.com/wiki/Utility_function\">utility function</a> a priori.</li>\n<li>Since the new model requires utility functions formulated with abstract ontology i.e. well-defined on the entire Tegmark <a href=\"http://en.wikipedia.org/wiki/Multiverse#Level_IV:_Ultimate_Ensemble\">level IV</a> <a href=\"http://en.wikipedia.org/wiki/Mathematical_universe_hypothesis\">multiverse</a>. These are generally difficult to construct (i.e. the ontology problem resurfaces in a different form). I outline a method for constructing such utility functions.</li>\n</ul>\n<h1>Problems with&nbsp;<a href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">UIM 1.0</a></h1>\n<p>The previous model postulated that naturalized induction uses a version of <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff induction</a>&nbsp;updated in the direction of an innate model <strong>N</strong> with a temporal confidence parameter&nbsp;<strong>t</strong>. This entails several problems:</p>\n<ul>\n<li>The dependence on the parameter <strong>t</strong> whose relevant value is not easy to determine.</li>\n<li>Conceptual divergence from the UDT philosophy that we should not update <em>at all</em>.</li>\n<li>Difficulties with <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a> and <a href=\"http://wiki.lesswrong.com/wiki/Acausal_trade\">acausal trade</a> scenarios in which <strong>G</strong> doesn't exist in the \"other universe\".</li>\n<li>Once <strong>G</strong> discovers even a small violation of <strong>N</strong> at a very early time, it loses all ground for trusting its own mind. Effectively, <strong>G</strong> would find itself in the position of a <a href=\"http://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brain</a>. This is especially dangerous when <strong>N</strong> over-specifies the hardware running <strong>G</strong>'s mind. For example assume <strong>N</strong> specifies <strong>G</strong> to be a human brain modeled on the level of <a href=\"http://en.wikipedia.org/wiki/Quantum_field_theory\">quantum field theory</a> (<a href=\"http://en.wikipedia.org/wiki/Particle_physics\">particle physics</a>). If <strong>G</strong> discovers that in truth it is a computer simulation on the merely molecular level, it loses its epistemic footing completely.</li>\n</ul>\n<h1>UIM 2.0</h1>\n<p>I now propose the following intelligence metric (the formula goes first and then I explain the notation):</p>\n<p><strong>I<sub>U</sub></strong>(<strong>q</strong>) := E<sub><strong>T</strong></sub>[E<sub><strong>D</strong></sub>[E<sub><strong>L</strong></sub>[<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) | <strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) = <strong>q</strong>]] | <strong>N</strong>]</p>\n<ul>\n<li><strong>N </strong>is the \"ideal\"&nbsp;model&nbsp;of the mind of the agent <strong>G</strong>. For example, it can be a <a href=\"http://en.wikipedia.org/wiki/Universal_Turing_machine\">universal Turing machine</a> <strong>M</strong> with special \"sensory\" registers <strong>e</strong> whose values can change arbitrarily after each step of <strong>M</strong>. <strong>N</strong>&nbsp;is specified as a system of constraints on an infinite sequence of natural numbers <strong>X</strong>, which should be thought of as the \"<a href=\"http://en.wikipedia.org/wiki/Platonic_idealism\">Platonic ideal</a>\" realization of <strong>G</strong>, i.e. an imagery realization which cannot be tempered with by external forces such as <a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">anvils</a>. As we shall see, this \"ideal\" serves as a template for \"physical\" realizations of G which <em>are</em>&nbsp;prone to violations of <strong>N</strong>.</li>\n<li><strong>Q</strong> is a function that decodes&nbsp;<strong>G</strong>'s code from <strong>X</strong> e.g. the program loaded in <strong>M</strong> at time 0. <strong>q</strong>&nbsp;is a particular value of this code whose (utility specific) intelligence&nbsp;<strong>I<sub>U</sub></strong>(<strong>q</strong>)&nbsp;we are evaluating.</li>\n<li><strong>T</strong> is a random (as in <a href=\"http://en.wikipedia.org/wiki/Random_variable\">random variable</a>) computable hypothesis about the \"physics\" of <strong>X</strong>, i.e a program computing <strong>X</strong> implemented on some fixed universal computing model (e.g. universal Turing machine) <strong>C</strong>. <strong>T</strong> is distributed according to the <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff measure</a>&nbsp;however the expectation value in the definition of&nbsp;<strong>I<sub>U</sub></strong>(<strong>q</strong>)&nbsp;is conditional on <strong>N</strong>, i.e. we restrict to programs which are compatible with <strong>N</strong>. From the UDT standpoint, <strong>T</strong>&nbsp;is the decision algorithm itself and the uncertainty in&nbsp;<strong>T</strong> is \"introspective\" uncertainty i.e. the uncertainty of the putative precursor agent <strong>PG</strong> (the agent creating <strong>G</strong> e.g. an AI programmer) regarding her own decision algorithm. Note that we don't actually <em>need</em> to postulate a <strong>PG</strong> which is \"agenty\" (i.e. use for <strong>N</strong> a model of AI hardware together with a model of the AI programmer programming this hardware), we can be content to remain in a more abstract framework.</li>\n<li><strong>D</strong> is a random computable hypothesis about the physics of <strong>Y</strong>, where <strong>Y</strong> is an infinite sequence of natural numbers representing the physical (as opposed to \"ideal\") universe. <strong>D</strong> is distributed according to the Solomonoff measure and the respective expectation value is unconditional (i.e. we use the raw Solomonoff prior for <strong>Y</strong> which makes the model truly updateless). In UDT terms, <strong>D</strong> is indexical uncertainty.</li>\n<li><strong>U</strong> is a <a href=\"http://en.wikipedia.org/wiki/Computable_number\">computable function</a> from infinite sequences of natural numbers to [0, 1] representing&nbsp;<strong>G</strong>'s utility function.&nbsp;</li>\n<li><strong>L</strong> represents logical uncertainty. It can be defined by the model explained by cousin_it&nbsp;<a href=\"/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">here</a>, together with my <a href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">previous</a> construction for computing logical expectation values of random variables in [0, 1]. That is, we define E<sub><strong>L</strong></sub>(<strong>d</strong><sub>k</sub>)&nbsp;to be the probability that a random string of bits <strong>p</strong> encodes a proof of the sentence \"<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>&nbsp;implies that the k-th digit of&nbsp;<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) is 1\" in some prefix-free encoding of proofs&nbsp;<em>conditional</em> on <strong>p</strong> encoding the proof of either that sentence or the sentence \"<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>&nbsp;implies that the k-th digit of&nbsp;<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) is 0\". We then define&nbsp;<br />E<sub><strong>L</strong></sub>[<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) |&nbsp;<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>] :=&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&Sigma;</span><sub style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify;\">k</sub><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;2</span><sup style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify;\">-k</sup><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;</span>E<sub><strong>L</strong></sub>(<strong>d</strong><sub>k</sub>). Here, the sentences and the proofs belong to some fixed formal logic <strong>F</strong>, e.g. <a href=\"http://en.wikipedia.org/wiki/Peano_axioms\">Peano arthimetics</a>&nbsp;or <a href=\"http://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory\">ZFC</a>.&nbsp;&nbsp;</li>\n</ul>\n<h2>Discussion</h2>\n<ul>\n<li><strong>G</strong>'s mental architecture <strong>N</strong> is defined in the \"ideal\" universe <strong>X</strong> where it is inviolable. However, <strong>G</strong>'s utility function <strong>U</strong>&nbsp;inhabits the physical universe <strong>Y</strong>. This means that a highly intelligent&nbsp;<strong>q </strong>is designed so that imperfect realizations of <strong>G</strong> inside <strong>Y</strong>&nbsp;generate as many&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Utility\">utilons</a>&nbsp;as possible. A typical <strong>T</strong> is a low <a href=\"http://wiki.lesswrong.com/wiki/Algorithmic_complexity\">Kolmogorov complexity</a> universe which contains a perfect realization of <strong>G</strong>. <strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) is <strong>L</strong>-correlated to the programming of imperfect realizations of <strong>G</strong> inside <strong>Y</strong>&nbsp;because <strong>T</strong> serves as an effective (approximate) model of the formation of these&nbsp;realizations. For abstract <strong>N</strong>, this means <strong>q</strong> is highly intelligent when a Solomonoff-random \"<strong>M</strong>-programming process\" producing <strong>q</strong> entails a high expected value of <strong>U</strong>.</li>\n<li>Solving the <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Loebian obstacle</a> requires a more sophisticated model of logical uncertainty. <em>I think I can formulate such a model. I will explain it in another post after more contemplation.</em></li>\n<li>It is desirable that the encoding of proofs <strong>p</strong> satisfies a universality property so that the length of the encoding can only change by an additive constant, analogically to the weak dependence of Kolmogorov complexity on <strong>C</strong>. It is in fact not difficult to formulate this property and show the existence of appropriate encodings. I will discuss this point in more detail in another post.</li>\n</ul>\n<h1>Generic Intelligence</h1>\n<p>It seems conceptually desirable to have a notion of intelligence independent of the specifics of the utility function. Such an intelligence metric is possible to construct in a way analogical to what I've done in <a href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">UIM 1.0</a>, however it is no longer a special case of the utility-specific metric.</p>\n<p>Assume&nbsp;<strong>N</strong> to consist of a machine <strong>M</strong> connected to a special storage device <strong>E</strong>. Assume further that at <strong>X</strong>-time 0,&nbsp;<strong>E</strong> contains a valid <strong>C</strong>-program <strong>u</strong> realizing a utility function&nbsp;<strong>U</strong>, but that this is the only constraint on the initial content of&nbsp;<strong>E</strong> imposed by <strong>N</strong>. Define</p>\n<p><strong>I</strong>(<strong>q</strong>) := E<sub><strong>T</strong></sub>[E<sub><strong>D</strong></sub>[E<sub><strong>L</strong></sub>[<strong>u</strong>(<strong>Y</strong>(<strong>D</strong>); <strong>X</strong>(<strong>T</strong>)) |&nbsp;<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>]] |&nbsp;<strong>N</strong>]</p>\n<p>Here,&nbsp;<strong>u</strong>(<strong>Y</strong>(<strong>D</strong>);&nbsp;<strong>X</strong>(<strong>T</strong>)) means that we decode <strong>u</strong> from <strong>X</strong>(<strong>T</strong>) and evaluate it on <strong>Y</strong>(<strong>D</strong>). Thus utility depends both on the physical universe <strong>Y</strong> and the ideal universe <strong>X</strong>. This means <strong>G</strong> is not precisely a UDT agent but rather a \"proto-agent\": only when a realization of <strong>G</strong> reads <strong>u</strong> from <strong>E</strong> it knows which other realizations of <strong>G</strong> in the multiverse (the Solomonoff ensemble from which&nbsp;<strong>Y </strong>is selected) should be considered as the \"same\" agent UDT-wise.</p>\n<p>Incidentally, this can be used as a formalism for reasoning about agents that don't know their utility functions. I believe this has important applications in metaethics I will discuss in another post.</p>\n<h1>Utility Functions in the Multiverse</h1>\n<p>UIM 2.0 is a formalism that solves the diseases of UIM 1.0 at the price of losing <strong>N</strong> in the capacity of the ontology for utility functions. We need the utility function to be defined on the entire multiverse i.e. on any sequence of natural numbers. I will outline a way to extend \"ontology-specific\" utility functions to the multiverse through a simple example.</p>\n<p>Suppose <strong>G</strong> is an agent that cares about universes realizing the <a href=\"http://en.wikipedia.org/wiki/Conway's_Game_of_Life\">Game of Life</a>, its utility function <strong>U</strong> corresponding to e.g. some sort of <a href=\"http://en.wikipedia.org/wiki/Glider_(Conway's_Life)\">glider</a> maximization with exponential temporal discount. Fix a specific way <strong>DC</strong>&nbsp;to decode any <strong>Y</strong> into a history of a 2D cellular automaton with two cell states (\"dead\" and \"alive\"). Our multiversal utility function&nbsp;<strong>U*</strong> assigns <strong>Y</strong>s for which <strong>DC</strong>(<strong>Y</strong>) is a legal Game of Life the value <strong>U</strong>(<strong>DC</strong>(<strong>Y</strong>)). All other <strong>Y</strong>s are treated by dividing the cells into cells&nbsp;<strong>O</strong> obeying the rules of Life and cells <strong>V</strong> violating the rules of Life. We can then evaluate <strong>U</strong> on <strong>O</strong> only (assuming it has some sort of locality) and assign <strong>V</strong> utility by some other rule, e.g.:</p>\n<ul>\n<li>zero utility</li>\n<li>constant utility&nbsp;per&nbsp;<strong>V</strong>&nbsp;cell&nbsp;with temporal discount</li>\n<li>constant utility&nbsp;per unit of surface area of the boundary between&nbsp;<strong>O</strong>&nbsp;and&nbsp;<strong>V&nbsp;</strong>with temporal discount&nbsp;</li>\n</ul>\n<div><strong>U*(Y)</strong> is then defined to be the sum of the values assigned to <strong>O(Y)</strong> and <strong>V(Y)</strong>.</div>\n<div><br /></div>\n<h2>Discussion</h2>\n<div>\n<ul>\n<li>The construction of <strong>U*</strong> depends on the choice of <strong>DC</strong>. However,&nbsp;<strong>U*</strong>&nbsp;only depends on&nbsp;<strong>DC</strong>&nbsp;weakly since given a hypothesis <strong>D</strong> which produces a Game of Life wrt some other low complexity encoding, there is a corresponding hypothesis <strong>D'</strong> producing a Game of Life wrt <strong>DC</strong>. <strong>D'</strong> is obtained from <strong>D</strong> by appending a corresponding \"transcoder\" and thus it is only less Solomonoff-likely than <strong>D</strong> by an O(1) factor.</li>\n<li>Since the accumulation between <strong>O</strong> and <strong>V</strong> is additive rather than e.g. multiplicative, a&nbsp;<strong>U*</strong>-agent doesn't behave as if it a priori&nbsp;<em>expects</em>&nbsp;the universe the follow the rules of Life but may have strong preferences about the universe actually doing it.</li>\n<li>This construction is reminiscent of Egan's <a href=\"http://gregegan.customer.netspace.net.au/PERMUTATION/FAQ/FAQ.html\">dust theory</a>&nbsp;in the sense that all possible encodings contribute. However, here they are weighted by the Solomonoff measure.</li>\n</ul>\n</div>\n<h1>TLDR</h1>\n<p>The intelligence of a physicalist agent is defined to be the UDT-value of the \"decision\" to create the agent by the process creating the agent. The process is selected randomly from a Solomonoff measure conditional on obeying the laws of the hardware on which the agent is implemented. The \"decision\" is made in an \"ideal\" universe in which the agent is Cartesian, but the utility function is evaluated on the real universe (raw Solomonoff measure). The interaction between the two \"universes\" is purely via logical conditional probabilities (acausal).</p>\n<p>If we want to discuss intelligence without specifying a utility function up front, we allow the \"ideal\" agent to read a program describing the utility function from a special storage immediately after \"booting up\".</p>\n<p>Utility functions in the Tegmark level IV multiverse are defined by specifying a \"reference universe\", specifying an encoding of the reference universe and extending a utility function defined on the reference universe to encodings which violate the reference laws by summing the utility of the portion of the universe which obeys the reference laws with some function of the space-time shape of the violation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6GEkaQpj4WmNLhMaM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 1.6000533735405323e-06, "legacy": true, "legacyId": "25715", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Followup to:&nbsp;<a title=\"Intelligence Metrics with Naturalized Induction\" href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">Intelligence Metrics with Naturalized Induction using UDT</a></p>\n<p>In the previous post I have defined an <a href=\"http://wiki.lesswrong.com/wiki/Intelligence\">intelligence metric</a> solving the <a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">duality</a>&nbsp;(aka <a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">naturalized induction</a>)&nbsp;and <a href=\"/lw/cze/reply_to_holden_on_tool_ai/70rt\">ontology</a> problems in <a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>. This model used a formalization of <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a> using Benja's <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">model</a> of logical uncertainty. In the current post I am going to:</p>\n<ul>\n<li>Explain some problems with my previous model (<em>that section can be skipped if you don't care about the previous model and only want to understand the new one</em>).</li>\n<li>Formulate a new model solving these problems. Incidentally, the new model is much closer to the usual way UDT is represented. It is also based on a <a href=\"/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">different model</a> of logical uncertainty.</li>\n<li>Show how to define intelligence without specifying the <a href=\"http://wiki.lesswrong.com/wiki/Utility_function\">utility function</a> a priori.</li>\n<li>Since the new model requires utility functions formulated with abstract ontology i.e. well-defined on the entire Tegmark <a href=\"http://en.wikipedia.org/wiki/Multiverse#Level_IV:_Ultimate_Ensemble\">level IV</a> <a href=\"http://en.wikipedia.org/wiki/Mathematical_universe_hypothesis\">multiverse</a>. These are generally difficult to construct (i.e. the ontology problem resurfaces in a different form). I outline a method for constructing such utility functions.</li>\n</ul>\n<h1 id=\"Problems_with_UIM_1_0\">Problems with&nbsp;<a href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">UIM 1.0</a></h1>\n<p>The previous model postulated that naturalized induction uses a version of <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff induction</a>&nbsp;updated in the direction of an innate model <strong>N</strong> with a temporal confidence parameter&nbsp;<strong>t</strong>. This entails several problems:</p>\n<ul>\n<li>The dependence on the parameter <strong>t</strong> whose relevant value is not easy to determine.</li>\n<li>Conceptual divergence from the UDT philosophy that we should not update <em>at all</em>.</li>\n<li>Difficulties with <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a> and <a href=\"http://wiki.lesswrong.com/wiki/Acausal_trade\">acausal trade</a> scenarios in which <strong>G</strong> doesn't exist in the \"other universe\".</li>\n<li>Once <strong>G</strong> discovers even a small violation of <strong>N</strong> at a very early time, it loses all ground for trusting its own mind. Effectively, <strong>G</strong> would find itself in the position of a <a href=\"http://en.wikipedia.org/wiki/Boltzmann_brain\">Boltzmann brain</a>. This is especially dangerous when <strong>N</strong> over-specifies the hardware running <strong>G</strong>'s mind. For example assume <strong>N</strong> specifies <strong>G</strong> to be a human brain modeled on the level of <a href=\"http://en.wikipedia.org/wiki/Quantum_field_theory\">quantum field theory</a> (<a href=\"http://en.wikipedia.org/wiki/Particle_physics\">particle physics</a>). If <strong>G</strong> discovers that in truth it is a computer simulation on the merely molecular level, it loses its epistemic footing completely.</li>\n</ul>\n<h1 id=\"UIM_2_0\">UIM 2.0</h1>\n<p>I now propose the following intelligence metric (the formula goes first and then I explain the notation):</p>\n<p><strong>I<sub>U</sub></strong>(<strong>q</strong>) := E<sub><strong>T</strong></sub>[E<sub><strong>D</strong></sub>[E<sub><strong>L</strong></sub>[<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) | <strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) = <strong>q</strong>]] | <strong>N</strong>]</p>\n<ul>\n<li><strong>N </strong>is the \"ideal\"&nbsp;model&nbsp;of the mind of the agent <strong>G</strong>. For example, it can be a <a href=\"http://en.wikipedia.org/wiki/Universal_Turing_machine\">universal Turing machine</a> <strong>M</strong> with special \"sensory\" registers <strong>e</strong> whose values can change arbitrarily after each step of <strong>M</strong>. <strong>N</strong>&nbsp;is specified as a system of constraints on an infinite sequence of natural numbers <strong>X</strong>, which should be thought of as the \"<a href=\"http://en.wikipedia.org/wiki/Platonic_idealism\">Platonic ideal</a>\" realization of <strong>G</strong>, i.e. an imagery realization which cannot be tempered with by external forces such as <a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">anvils</a>. As we shall see, this \"ideal\" serves as a template for \"physical\" realizations of G which <em>are</em>&nbsp;prone to violations of <strong>N</strong>.</li>\n<li><strong>Q</strong> is a function that decodes&nbsp;<strong>G</strong>'s code from <strong>X</strong> e.g. the program loaded in <strong>M</strong> at time 0. <strong>q</strong>&nbsp;is a particular value of this code whose (utility specific) intelligence&nbsp;<strong>I<sub>U</sub></strong>(<strong>q</strong>)&nbsp;we are evaluating.</li>\n<li><strong>T</strong> is a random (as in <a href=\"http://en.wikipedia.org/wiki/Random_variable\">random variable</a>) computable hypothesis about the \"physics\" of <strong>X</strong>, i.e a program computing <strong>X</strong> implemented on some fixed universal computing model (e.g. universal Turing machine) <strong>C</strong>. <strong>T</strong> is distributed according to the <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff measure</a>&nbsp;however the expectation value in the definition of&nbsp;<strong>I<sub>U</sub></strong>(<strong>q</strong>)&nbsp;is conditional on <strong>N</strong>, i.e. we restrict to programs which are compatible with <strong>N</strong>. From the UDT standpoint, <strong>T</strong>&nbsp;is the decision algorithm itself and the uncertainty in&nbsp;<strong>T</strong> is \"introspective\" uncertainty i.e. the uncertainty of the putative precursor agent <strong>PG</strong> (the agent creating <strong>G</strong> e.g. an AI programmer) regarding her own decision algorithm. Note that we don't actually <em>need</em> to postulate a <strong>PG</strong> which is \"agenty\" (i.e. use for <strong>N</strong> a model of AI hardware together with a model of the AI programmer programming this hardware), we can be content to remain in a more abstract framework.</li>\n<li><strong>D</strong> is a random computable hypothesis about the physics of <strong>Y</strong>, where <strong>Y</strong> is an infinite sequence of natural numbers representing the physical (as opposed to \"ideal\") universe. <strong>D</strong> is distributed according to the Solomonoff measure and the respective expectation value is unconditional (i.e. we use the raw Solomonoff prior for <strong>Y</strong> which makes the model truly updateless). In UDT terms, <strong>D</strong> is indexical uncertainty.</li>\n<li><strong>U</strong> is a <a href=\"http://en.wikipedia.org/wiki/Computable_number\">computable function</a> from infinite sequences of natural numbers to [0, 1] representing&nbsp;<strong>G</strong>'s utility function.&nbsp;</li>\n<li><strong>L</strong> represents logical uncertainty. It can be defined by the model explained by cousin_it&nbsp;<a href=\"/lw/imz/notes_on_logical_priors_from_the_miri_workshop/\">here</a>, together with my <a href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">previous</a> construction for computing logical expectation values of random variables in [0, 1]. That is, we define E<sub><strong>L</strong></sub>(<strong>d</strong><sub>k</sub>)&nbsp;to be the probability that a random string of bits <strong>p</strong> encodes a proof of the sentence \"<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>&nbsp;implies that the k-th digit of&nbsp;<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) is 1\" in some prefix-free encoding of proofs&nbsp;<em>conditional</em> on <strong>p</strong> encoding the proof of either that sentence or the sentence \"<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>&nbsp;implies that the k-th digit of&nbsp;<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) is 0\". We then define&nbsp;<br>E<sub><strong>L</strong></sub>[<strong>U</strong>(<strong>Y</strong>(<strong>D</strong>)) |&nbsp;<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>] :=&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">\u03a3</span><sub style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify;\">k</sub><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;2</span><sup style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify;\">-k</sup><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 15.600000381469727px; text-align: justify;\">&nbsp;</span>E<sub><strong>L</strong></sub>(<strong>d</strong><sub>k</sub>). Here, the sentences and the proofs belong to some fixed formal logic <strong>F</strong>, e.g. <a href=\"http://en.wikipedia.org/wiki/Peano_axioms\">Peano arthimetics</a>&nbsp;or <a href=\"http://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory\">ZFC</a>.&nbsp;&nbsp;</li>\n</ul>\n<h2 id=\"Discussion\">Discussion</h2>\n<ul>\n<li><strong>G</strong>'s mental architecture <strong>N</strong> is defined in the \"ideal\" universe <strong>X</strong> where it is inviolable. However, <strong>G</strong>'s utility function <strong>U</strong>&nbsp;inhabits the physical universe <strong>Y</strong>. This means that a highly intelligent&nbsp;<strong>q </strong>is designed so that imperfect realizations of <strong>G</strong> inside <strong>Y</strong>&nbsp;generate as many&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Utility\">utilons</a>&nbsp;as possible. A typical <strong>T</strong> is a low <a href=\"http://wiki.lesswrong.com/wiki/Algorithmic_complexity\">Kolmogorov complexity</a> universe which contains a perfect realization of <strong>G</strong>. <strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) is <strong>L</strong>-correlated to the programming of imperfect realizations of <strong>G</strong> inside <strong>Y</strong>&nbsp;because <strong>T</strong> serves as an effective (approximate) model of the formation of these&nbsp;realizations. For abstract <strong>N</strong>, this means <strong>q</strong> is highly intelligent when a Solomonoff-random \"<strong>M</strong>-programming process\" producing <strong>q</strong> entails a high expected value of <strong>U</strong>.</li>\n<li>Solving the <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Loebian obstacle</a> requires a more sophisticated model of logical uncertainty. <em>I think I can formulate such a model. I will explain it in another post after more contemplation.</em></li>\n<li>It is desirable that the encoding of proofs <strong>p</strong> satisfies a universality property so that the length of the encoding can only change by an additive constant, analogically to the weak dependence of Kolmogorov complexity on <strong>C</strong>. It is in fact not difficult to formulate this property and show the existence of appropriate encodings. I will discuss this point in more detail in another post.</li>\n</ul>\n<h1 id=\"Generic_Intelligence\">Generic Intelligence</h1>\n<p>It seems conceptually desirable to have a notion of intelligence independent of the specifics of the utility function. Such an intelligence metric is possible to construct in a way analogical to what I've done in <a href=\"/lw/jq9/intelligence_metrics_with_naturalized_induction/\">UIM 1.0</a>, however it is no longer a special case of the utility-specific metric.</p>\n<p>Assume&nbsp;<strong>N</strong> to consist of a machine <strong>M</strong> connected to a special storage device <strong>E</strong>. Assume further that at <strong>X</strong>-time 0,&nbsp;<strong>E</strong> contains a valid <strong>C</strong>-program <strong>u</strong> realizing a utility function&nbsp;<strong>U</strong>, but that this is the only constraint on the initial content of&nbsp;<strong>E</strong> imposed by <strong>N</strong>. Define</p>\n<p><strong>I</strong>(<strong>q</strong>) := E<sub><strong>T</strong></sub>[E<sub><strong>D</strong></sub>[E<sub><strong>L</strong></sub>[<strong>u</strong>(<strong>Y</strong>(<strong>D</strong>); <strong>X</strong>(<strong>T</strong>)) |&nbsp;<strong>Q</strong>(<strong>X</strong>(<strong>T</strong>)) =&nbsp;<strong>q</strong>]] |&nbsp;<strong>N</strong>]</p>\n<p>Here,&nbsp;<strong>u</strong>(<strong>Y</strong>(<strong>D</strong>);&nbsp;<strong>X</strong>(<strong>T</strong>)) means that we decode <strong>u</strong> from <strong>X</strong>(<strong>T</strong>) and evaluate it on <strong>Y</strong>(<strong>D</strong>). Thus utility depends both on the physical universe <strong>Y</strong> and the ideal universe <strong>X</strong>. This means <strong>G</strong> is not precisely a UDT agent but rather a \"proto-agent\": only when a realization of <strong>G</strong> reads <strong>u</strong> from <strong>E</strong> it knows which other realizations of <strong>G</strong> in the multiverse (the Solomonoff ensemble from which&nbsp;<strong>Y </strong>is selected) should be considered as the \"same\" agent UDT-wise.</p>\n<p>Incidentally, this can be used as a formalism for reasoning about agents that don't know their utility functions. I believe this has important applications in metaethics I will discuss in another post.</p>\n<h1 id=\"Utility_Functions_in_the_Multiverse\">Utility Functions in the Multiverse</h1>\n<p>UIM 2.0 is a formalism that solves the diseases of UIM 1.0 at the price of losing <strong>N</strong> in the capacity of the ontology for utility functions. We need the utility function to be defined on the entire multiverse i.e. on any sequence of natural numbers. I will outline a way to extend \"ontology-specific\" utility functions to the multiverse through a simple example.</p>\n<p>Suppose <strong>G</strong> is an agent that cares about universes realizing the <a href=\"http://en.wikipedia.org/wiki/Conway's_Game_of_Life\">Game of Life</a>, its utility function <strong>U</strong> corresponding to e.g. some sort of <a href=\"http://en.wikipedia.org/wiki/Glider_(Conway's_Life)\">glider</a> maximization with exponential temporal discount. Fix a specific way <strong>DC</strong>&nbsp;to decode any <strong>Y</strong> into a history of a 2D cellular automaton with two cell states (\"dead\" and \"alive\"). Our multiversal utility function&nbsp;<strong>U*</strong> assigns <strong>Y</strong>s for which <strong>DC</strong>(<strong>Y</strong>) is a legal Game of Life the value <strong>U</strong>(<strong>DC</strong>(<strong>Y</strong>)). All other <strong>Y</strong>s are treated by dividing the cells into cells&nbsp;<strong>O</strong> obeying the rules of Life and cells <strong>V</strong> violating the rules of Life. We can then evaluate <strong>U</strong> on <strong>O</strong> only (assuming it has some sort of locality) and assign <strong>V</strong> utility by some other rule, e.g.:</p>\n<ul>\n<li>zero utility</li>\n<li>constant utility&nbsp;per&nbsp;<strong>V</strong>&nbsp;cell&nbsp;with temporal discount</li>\n<li>constant utility&nbsp;per unit of surface area of the boundary between&nbsp;<strong>O</strong>&nbsp;and&nbsp;<strong>V&nbsp;</strong>with temporal discount&nbsp;</li>\n</ul>\n<div><strong>U*(Y)</strong> is then defined to be the sum of the values assigned to <strong>O(Y)</strong> and <strong>V(Y)</strong>.</div>\n<div><br></div>\n<h2 id=\"Discussion1\">Discussion</h2>\n<div>\n<ul>\n<li>The construction of <strong>U*</strong> depends on the choice of <strong>DC</strong>. However,&nbsp;<strong>U*</strong>&nbsp;only depends on&nbsp;<strong>DC</strong>&nbsp;weakly since given a hypothesis <strong>D</strong> which produces a Game of Life wrt some other low complexity encoding, there is a corresponding hypothesis <strong>D'</strong> producing a Game of Life wrt <strong>DC</strong>. <strong>D'</strong> is obtained from <strong>D</strong> by appending a corresponding \"transcoder\" and thus it is only less Solomonoff-likely than <strong>D</strong> by an O(1) factor.</li>\n<li>Since the accumulation between <strong>O</strong> and <strong>V</strong> is additive rather than e.g. multiplicative, a&nbsp;<strong>U*</strong>-agent doesn't behave as if it a priori&nbsp;<em>expects</em>&nbsp;the universe the follow the rules of Life but may have strong preferences about the universe actually doing it.</li>\n<li>This construction is reminiscent of Egan's <a href=\"http://gregegan.customer.netspace.net.au/PERMUTATION/FAQ/FAQ.html\">dust theory</a>&nbsp;in the sense that all possible encodings contribute. However, here they are weighted by the Solomonoff measure.</li>\n</ul>\n</div>\n<h1 id=\"TLDR\">TLDR</h1>\n<p>The intelligence of a physicalist agent is defined to be the UDT-value of the \"decision\" to create the agent by the process creating the agent. The process is selected randomly from a Solomonoff measure conditional on obeying the laws of the hardware on which the agent is implemented. The \"decision\" is made in an \"ideal\" universe in which the agent is Cartesian, but the utility function is evaluated on the real universe (raw Solomonoff measure). The interaction between the two \"universes\" is purely via logical conditional probabilities (acausal).</p>\n<p>If we want to discuss intelligence without specifying a utility function up front, we allow the \"ideal\" agent to read a program describing the utility function from a special storage immediately after \"booting up\".</p>\n<p>Utility functions in the Tegmark level IV multiverse are defined by specifying a \"reference universe\", specifying an encoding of the reference universe and extending a utility function defined on the reference universe to encodings which violate the reference laws by summing the utility of the portion of the universe which obeys the reference laws with some function of the space-time shape of the violation.</p>", "sections": [{"title": "Problems with\u00a0UIM 1.0", "anchor": "Problems_with_UIM_1_0", "level": 1}, {"title": "UIM 2.0", "anchor": "UIM_2_0", "level": 1}, {"title": "Discussion", "anchor": "Discussion", "level": 2}, {"title": "Generic Intelligence", "anchor": "Generic_Intelligence", "level": 1}, {"title": "Utility Functions in the Multiverse", "anchor": "Utility_Functions_in_the_Multiverse", "level": 1}, {"title": "Discussion", "anchor": "Discussion1", "level": 2}, {"title": "TLDR", "anchor": "TLDR", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2ZGCq44Wv8p4btyL7", "PgKADaJE4ERjtMtP9", "wXxPmc9W6kPb6i7vj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-08T06:43:15.146Z", "modifiedAt": null, "url": null, "title": "Meetup: Philadelphia, March 9, 2014", "slug": "meetup-philadelphia-march-9-2014", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8uNxnZhwpTT39sbPp/meetup-philadelphia-march-9-2014", "pageUrlRelative": "/posts/8uNxnZhwpTT39sbPp/meetup-philadelphia-march-9-2014", "linkUrl": "https://www.lesswrong.com/posts/8uNxnZhwpTT39sbPp/meetup-philadelphia-march-9-2014", "postedAtFormatted": "Saturday, March 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Philadelphia%2C%20March%209%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Philadelphia%2C%20March%209%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8uNxnZhwpTT39sbPp%2Fmeetup-philadelphia-march-9-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Philadelphia%2C%20March%209%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8uNxnZhwpTT39sbPp%2Fmeetup-philadelphia-march-9-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8uNxnZhwpTT39sbPp%2Fmeetup-philadelphia-march-9-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p><span style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">Discussion prompt: Nick Szabo's essay on judging tradition, \"Objective Versus Intersubjective Truth\".&nbsp;</span><br style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\" /><a style=\"margin: 0px; padding: 0px; border: 0px; vertical-align: baseline; text-decoration: none; color: #6611cc; cursor: pointer; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\" href=\"http://szabo.best.vwh.net/tradition.html\" target=\"_blank\">http://szabo.best.vwh.net/tradition.html</a><span style=\"color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: 13px;\">&nbsp;</span></p>\n<p>1 PM &nbsp; (remember daylight saving time!)</p>\n<p><a href=\"/namphuongphilly.com\">Nam Phuong</a> at 11th and Broad St. This is a Vietnamese restaurant which is good, cheap, quiet, and on mass transit.</p>\n<p><a href=\"https://groups.google.com/forum/#!topic/lesswrong-philadelphia/k0Ui2Zot2TY\">Discussion group/mailing list</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8uNxnZhwpTT39sbPp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.6005086882410668e-06, "legacy": true, "legacyId": "25717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-08T16:27:22.728Z", "modifiedAt": null, "url": null, "title": "Strategic choice of identity", "slug": "strategic-choice-of-identity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.025Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uR8c2NPp4bWHQ5u45/strategic-choice-of-identity", "pageUrlRelative": "/posts/uR8c2NPp4bWHQ5u45/strategic-choice-of-identity", "linkUrl": "https://www.lesswrong.com/posts/uR8c2NPp4bWHQ5u45/strategic-choice-of-identity", "postedAtFormatted": "Saturday, March 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strategic%20choice%20of%20identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrategic%20choice%20of%20identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuR8c2NPp4bWHQ5u45%2Fstrategic-choice-of-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strategic%20choice%20of%20identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuR8c2NPp4bWHQ5u45%2Fstrategic-choice-of-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuR8c2NPp4bWHQ5u45%2Fstrategic-choice-of-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 644, "htmlBody": "<p>Identity is mostly discussed on LW in a cautionary manner: keep your identity small, be aware of the identities you are attached to. As benlandautaylor <a href=\"/lw/idj/use_your_identity_carefully/\">points out</a>, identities are very powerful, and while being rightfully cautious about them, we can also cultivate them deliberately to help us achieve our goals.</p>\n<p>Some helpful identities that I have that seem generally applicable:</p>\n<ul>\n<li>growth mindset</li>\n<li>low-hanging fruit picker</li>\n<li>truth-seeker</li>\n<li>jack-of-all trades (someone who is good at a variety of skills)</li>\n<li>someone who tries new things</li>\n<li>universal curiosity</li>\n<li>mirror (someone who learns other people's skills)</li>\n</ul>\n<p>Out of the above, the most useful is probably growth mindset, since it's effectively a meta-identity that allows the other parts of my identity to be fluid. The low-hanging fruit identity helps me be on the lookout for easy optimizations. The universal curiosity identity motivates me to try to understand various systems and fields of knowledge, besides the domains I'm already familiar with. It helps to give these playful or creative names, for example, \"champion of low-hanging fruit\". Some of these work well together, for example the \"trying new things\" identity contributes to the \"jack of all trades\" identity.</p>\n<p>It's also important to identify unhelpful identities that get in your way. Negative identities can be vague like \"lazy person\" or specific like \"someone who can't finish a project\". With identities, just like with habits, the easiest way to reduce or eliminate a bad one seems to be to install a new one that is incompatible with it. For example, if you have a \"shy person\" identity, then going to parties or starting conversations with strangers can generate counterexamples for that identity, and help to displace it with a new one of \"sociable person\". Costly signaling can be used to achieve this - for example, joining a public speaking club. The old identity will not necessarily go away entirely, but the competing identity will create cognitive dissonance, which it can be useful to deliberately focus on. More specific identities require more specific counterexamples. Since the original negative identity makes it difficult to perform the actions that generate counterexamples, there needs to be some form of success spiral that starts with small steps.</p>\n<p>Some examples of unhelpful identities I've had in the past were \"person who doesn't waste things\" and \"person with poor intuition\". The aversion to wasting money and material things predictably led to wasting time and attention instead. I found it useful to try \"thinking like a trader\" to counteract this \"stingy person\" identity, and get comfortable with the idea of trading money for time. Now I no longer obsess about recycling or buy the cheapest version of everything. Underconfidence in my intuition was likely responsible for my tendency to miss the forest for the trees when studying math or statistics, where I focused on details and missed the big picture ideas that are essential to actual understanding. My main objection to intuitions was that they feel imprecise, and I am trying to develop an identity of an \"intuition wizard\" who can manipulate concepts from a distance without zooming in. That is a cooler name than \"someone who thinks about things without really understanding them\", and brings to mind some people I know who have amazing intuition for math, which should help the identity stick.</p>\n<p>There can also be ambiguously useful identities, for example I have a \"tough person\" identity, which motivates me to challenge myself and expand my comfort zone, but also increases self-criticism and self-neglect. Given the mixed effects, I'm not yet sure what to do about this one - maybe I can come up with an identity that only has the positive effects.</p>\n<p>Which identities hold you back, and which ones propel you forward? If you managed to diminish negative identities, how did you do it and how far did you get?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 4, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uR8c2NPp4bWHQ5u45", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 92, "baseScore": 117, "extendedScore": null, "score": 0.000324, "legacy": true, "legacyId": "25649", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 117, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Zupr296Zy74wpihXT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-08T18:01:06.447Z", "modifiedAt": null, "url": null, "title": "In favour of terseness", "slug": "in-favour-of-terseness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sophronius", "createdAt": "2011-06-13T16:06:52.555Z", "isAdmin": false, "displayName": "Sophronius"}, "userId": "WswxjHqPo9W6K55NA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YgKFahh3d3m4MSnD9/in-favour-of-terseness", "pageUrlRelative": "/posts/YgKFahh3d3m4MSnD9/in-favour-of-terseness", "linkUrl": "https://www.lesswrong.com/posts/YgKFahh3d3m4MSnD9/in-favour-of-terseness", "postedAtFormatted": "Saturday, March 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20favour%20of%20terseness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20favour%20of%20terseness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgKFahh3d3m4MSnD9%2Fin-favour-of-terseness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20favour%20of%20terseness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgKFahh3d3m4MSnD9%2Fin-favour-of-terseness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgKFahh3d3m4MSnD9%2Fin-favour-of-terseness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 379, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-GB</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi; mso-fareast-language:EN-US;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\" style=\"text-align:justify\">I like posts that are concise and to the point. Posts like that maximize my information/effort ratio. I would really like to see experienced rationalists simply post a list of things they believe on any given subject with a short explanation for why they believe each of those things. Then I could go ahead and adjust my beliefs based on those lists as necessary.</p>\n<p class=\"MsoNormal\" style=\"text-align:justify\">Sadly I don&rsquo;t see any posts like this. Presumably this is because of the social convention where you&rsquo;re expected to back up any public belief with arguments, so that other people can attempt to poke holes in them. I find this strange because the arguments people present rarely have anything to do with why they believe those things, which makes the whole exercise a giant distraction from the main point that the author is trying to bring across. In order to prevent this kind of derailment, posters tend to cover their arguments with endless qualifications so that their sentences read like this: &ldquo;I personally believe that, in cases X Y Z and under circumstances B and C, ceteris paribus and barring obvious exceptions, it seems safe to say that murder is wrong, though of course I could be mistaken.&rdquo; The problems with such excessive argumentation and qualification are threefold:</p>\n<ol>\n<li>The post becomes less readable: The information/effort ratio is lowered.</li>\n<li>It becomes much more difficult to tell what the author genuinely believes: Are they really unsure or just trying to appear humble? Is that their true objection, or just an argument?</li>\n<li>Despite everything, someone is STILL going to miss the point and reply that sometimes killing people is ok in certain situations, and then the next 100 comments will be about that.</li>\n</ol>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-GB</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi; mso-fareast-language:EN-US;} --> <!--[endif] --></p>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-GB</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--></p>\n<p class=\"MsoNormal\" style=\"text-align:justify\"><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi; mso-fareast-language:EN-US;} --> By contrast, terseness makes posts more readable and makes it less likely that the main point is misunderstood. So if we as a community could just relax the demand for argumentation and qualification somewhat, and we all focussed on debating the main points of posts instead of getting sidetracked, then perhaps experienced rationalists here could write nice and concise posts that give clear and direct answers to complicated questions. Instead, some of the sequences are so long and involve so many arguments, counter-arguments and disclaimers that I feel the point is lost entirely.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YgKFahh3d3m4MSnD9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 20, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "25719", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-10T03:37:37.761Z", "modifiedAt": null, "url": null, "title": "Optimal Exercise", "slug": "optimal-exercise", "viewCount": null, "lastCommentedAt": "2021-12-26T00:53:03.132Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bZ2w99pEAeAbKnKqo/optimal-exercise", "pageUrlRelative": "/posts/bZ2w99pEAeAbKnKqo/optimal-exercise", "linkUrl": "https://www.lesswrong.com/posts/bZ2w99pEAeAbKnKqo/optimal-exercise", "postedAtFormatted": "Monday, March 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimal%20Exercise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimal%20Exercise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbZ2w99pEAeAbKnKqo%2Foptimal-exercise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimal%20Exercise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbZ2w99pEAeAbKnKqo%2Foptimal-exercise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbZ2w99pEAeAbKnKqo%2Foptimal-exercise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2799, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/\">Lifestyle interventions to increase longevity</a>.</p>\n<p>What does it mean for exercise to be optimal?</p>\n<ul>\n<li>Optimal for looks</li>\n<li>Optimal for time</li>\n<li>Optimal for effort</li>\n<li>Optimal for performance</li>\n<li>Optimal for longevity</li>\n</ul>\n<p>There may be even more criteria.</p>\n<p>We're all likely going for a mix of outcomes, and optimal exercise is going to change depending on your weighting of different factors. So I'm going to discuss something close to a minimum viable routine based on meta-analyses of exercise studies.</p>\n<p>Not knowing which sort of exercise yields the best results gives our brains an excuse to stop thinking about it. The intent of this post is to go over the dose responses to various types of exercise. We&rsquo;re going to break through vague notions like &ldquo;exercise is good&rdquo; and &ldquo;I should probably exercise more&rdquo; with a concrete plan where you understand the relevant parameters that will cause dramatic improvements.</p>\n<p><a id=\"more\"></a></p>\n<h3>How much exercise?</h3>\n<p>Optimality aside, I recommend starting with a very minimal routine for 6-ish weeks to build the habit of exercise in to your life. You'll want a program that causes you little mental stress that you can actually stick with. You've got a few options for achieving this. The gains from weightlifting can be surprisingly quick&mdash;you'll see dramatic changes in your appearance in 4 months&mdash;and seeing yourself lift more weight every session can be a great motivator. Couch to 5k is a basic running progression designed for sedentary people. A daily bodyweight routine is a good way to achieve habit formation through consistency. I recommend making a firm choice and sticking with it until it becomes easy.</p>\n<p>Once you've made exercise a habit, you'll want to gradually nudge yourself towards the level that's optimal. So what is that level? Most of the rest of the claims in this post are supported by <a href=\"http://ije.oxfordjournals.org/content/40/5/1382.long\">this review by Swiss researchers</a>. As far as I know, this is the largest systematic review of exercise studies ever undertaken, reviewing 7000 studies with 80 meeting inclusion criteria covering over 1.3 million subjects. Sheer size, however, is not the only reason to take this study very seriously. As someone who has read hundreds of exercise studies, I can say that the methodology of the meta-analysis done to determine dose-response to exercise is excellent. What is most encouraging is that the study authors repeatedly point out shortcomings, and ways their findings should <em>not</em> be interpreted because the underlying data does not warrant it. They also check for publication bias. One potential caveat is that this a review of cohort studies, not RCTs. But the authors note that RCTs of exercise almost always show <em>greater</em> effect sizes, not smaller. This is likely because people over-report how much exercise they do in observational studies.</p>\n<p>In order to compare the intensity of different activities, exercise researchers use a unit called a MET, or metabolic equivalent. The MET is defined so that your weight (in kg) * METs = Calories you're burning per hour. An example MET table can be found <a href=\"http://www.health.harvard.edu/newsletters/Harvard_Womens_Health_Watch/2009/December/met-hour-equivalents-of-various-physical-activities\">here</a>. For the purposes of exercise studies, activities are typically classified as low-intensity, moderate-intensity, and vigorous-intensity. These roughly correspond to 1-3, 4-6, and 7+ METs per hour. For typical individuals, this will translate to approximately 200, 400, and 600+ Calories burned per hour.</p>\n<p>On the low end, some studies have found dramatic benefits from just the first 15 minutes of moderate-intensity exercise per week. These studies indicate that you gain about as much going from no exercise to some exercise as you do going from some exercise to optimal exercise.</p>\n<p>The Swiss review finds that the first hour per week of vigorous-intensity activity gets you 2/3rds the benefit of 10 hours per week, but the study authors make sure to point out that this is an implausible effect size and that there are almost certainly some confounding and reverse causality issues going on. Which is to say that people who have better health are simply going to be capable of more exercise.</p>\n<p>How about on the high end? Studies differ on where the point of diminishing returns is. Some <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3491006/\">put it</a> at 1000-1500 Calories; others as high as 3500 Calories. (Remember, a typical individual burns ~400 Calories per hour of moderate-intensity exercise.) I'll shoot for 1500 Calories in my recommendations; 3500 Calories is pretty hard to reach without exercising like a pro athlete.</p>\n<p>Estimates indicate that each minute of exercise gets you 3-7 minutes of extra life on average, with higher returns for more intense exercise. So every week, you have the opportunity to get a 3-7x ROI on time spent exercising up to the point of diminishing returns. I recommend high-intensity exercise&mdash;not only does it save time, it's also been shown to improve health more on a per-Calorie basis.</p>\n<p>&nbsp;</p>\n<h3><strong>Which exercises?</strong></h3>\n<h4>Weight training programs</h4>\n<p>You may have shied away from weight training in the past because you thought you would turn into some huge gross bodybuilder. But bodybuilders and fitness models take drugs and spend years training intensively to look the way they do. You are not going to gain 20lbs of muscle overnight magically. This goes double if you&rsquo;re a woman. You do not have testosterone; you are not going to be building huge muscles no matter what you do.</p>\n<p>Of the forms of exercise I cover, weight training has the most rigorous evidence separating what works and what doesn&rsquo;t. <a href=\"http://saudeemovimento.net.br/wp-content/uploads/bsk-pdf-manager/336_A_META-ANALYSIS_TO_DETERMINE_THE_DOSE_-_RESPONSE_FOR_STRENGTH_DEVELOPMENT.PDF\">This study</a> (pdf warning) examines what sort of resistance training results in the most rapid improvements.</p>\n<p>In weight training lingo, AxB means A sets of B repetitions. So 4x10 would mean 40 reps with rest periods every 10 reps. Our study recommends starting with 4x10 3 times a week, and transitioning to 4x4 2 times a week as you become stronger. Aim for a weight you can barely complete all the reps with.</p>\n<p>For an efficient full-body workout, select one exercise from each movement pattern:</p>\n<p>Upper push: bench press, incline press, overhead press, dips.</p>\n<p>Upper pull: cable rows, barbell rows, dumbbell rows, chin-ups, face pulls.</p>\n<p>Lower push: squats, lunges, leg press.</p>\n<p>Lower pull: deadlifts, power cleans, hyperextensions, romanian deadlifts, reverse hyperextensions, glute-ham raises.</p>\n<p>So a good starting routine would be</p>\n<p>A: 4x10 each of squat, bench press, lat pulldown, hyperextension</p>\n<p>B: 4x10 each of squat, overhead press, cable row, hyperextension</p>\n<p>alternating A and B workouts on different days of the week e.g. AxBxAxx, BxAxBxx.</p>\n<p>You'll try to increase the weight by 5lbs each session. As you improve, you want to decrease the reps and increase the intensity so you can keep advancing. For example, if you stall a couple times doing 4x10 at 125lbs on your squat, switch to 4x8 and keep increasing the weight, then 4x6, etc. until you get to something close to an optimal trained routine:</p>\n<p>A: 4x4 each of squats, bench, weighted chins, deadlifts</p>\n<p>B: 4x4 each of squats, overhead press, barbell row, power cleans</p>\n<p>At this point, you're going to the gym only twice per week to give yourself more recovery time.</p>\n<p>For learning exercises, there are many tutorials available online and I recommend checking some out if you are confused about form. You can always search for \"&lt;name of exercise&gt; tutorial\" and get articles and Youtube videos. Many people feel silly practicing their form with extremely light weights (often just the empty bar). But many world record holders start EVERY session this way to warm up and cement muscle memory. Others are silly NOT to do this. Also keep an eye on your ego. It's easy when setting goals for yourself to try to lift a weight that you can't really lift with proper form, because you want to set that personal best. But you'll feel pretty stupid when you are forced to miss the gym for a month because you hurt yourself.</p>\n<p>On exercise selection: I'm not a big fan of deadlifts for absolute newbies, unlike say Mark Rippetoe in Starting Strength. Maybe add deadlifts in after you've gained some muscle and you have better awareness of form. I also differ from Rippetoe in recommending that newbies high bar squat (the distinction being that low bar squats place the bar across the shoulders and high bar squats place the bar on the trapezius). I have taught newbies both forms and most find high bar squatting easier to figure out how to do properly. I spent months learning to low bar squat and still injured myself; high bar squatting can be taught in a couple sessions in my experience. Pay attention to whether a tutorial video is trying to teach you low bar squatting; the cues for each exercise are different.</p>\n<p>Free weights are generally better than machine exercises, but I recommend cable rows and lat pulldowns to newbies. The goal is to move from cable rows to barbell rows, and from lat pulldowns to actual chin-ups. The issue here is that a beginner won't be able to do the requisite sets and reps of chin-ups and rows with good form.</p>\n<p>A note about equipment: Weightlifting shoes have an incredibly high return on investment. They make back injuries less likely, and drastically improve subjective experience of squatting. You can get Rogue weightlifting shoes (use your size in men's dress shoes to size them regardless of gender) for around $120; there are cheaper options available but good shoes will last years so the amortized cost is low. <a href=\"http://wlshoes.com/purpose/olympic-lifting/\">Here's a full list of options</a>; note that even the cheapest weightlifting shoes are miles better than lifting in tennis shoes. I don't have any personal experience with the Reebok CrossFit lifter, but they seem like a good option under $100 for a shoe with the desired .75-inch rigid heel. I recommend a <a href=\"http://www.amazon.com/Harbinger-4-Inch-Nylon-3-Inch-Medium/dp/B00074H7WI/ref=sr_1_11?ie=UTF8&amp;qid=1394521980&amp;sr=8-11&amp;keywords=weightlifting+belt\">cheap belt</a> (expensive ones aren't any better) in order to improve your execution of the <a href=\"http://en.wikipedia.org/wiki/Valsalva_maneuver\">valsalva maneuver</a>&nbsp;during squats and deadlifts which further protects the spine from flexion under load.</p>\n<p>&nbsp;</p>\n<p><strong>Bodyweight routines</strong></p>\n<p>For a beginner, something like <a href=\"http://well.blogs.nytimes.com/2013/05/09/the-scientific-7-minute-workout/?_php=true&amp;_type=blogs&amp;_r=0\">this</a> is reasonable. Of course such a program will max out in fitness gains fairly quickly, even if you start doing several cycles of it. But this isn&rsquo;t our worry as a beginner. For someone serious about progressing with body weight exercises past this stage, I recommend a program like Overcoming Gravity or Building the Gymnastic Body. There is not really formal support for the efficacy of these programs, but they are endorsed by coaches who train many people successfully, and are consistent with the general principles of weight lifting (progressive overload, training frequency, etc.).</p>\n<p>&nbsp;</p>\n<p><strong>Cardio routines</strong></p>\n<p>For cardio, I recommend against high-intensity intervals when starting out. High-intensity intervals carry a greater risk of injury, especially if you're not used to them. They're also unpleasant and not conducive to building habits. For starting out, I recommend something that is based more on psychological results rather than performance optimality, like <a href=\"http://www.coolrunning.com/engine/2/2_3/181.shtml\">Couch to 5k</a>. As you progress, start adding in short bursts of more intense effort. The idea is to tire yourself out quickly. If your cardio routine lasts more than 30 minutes you&rsquo;re probably going too easy.</p>\n<p>What type of cardio should you do? Cardio that is amenable to high intensity is probably one of: running (especially up hills), swimming, rowing, biking, burpees, or jump rope. But you might be able to adapt others. I'm a huge fan of rowing for a few reasons. One, it works more than just the legs. Two, you can have a rowing machine in your house, which drastically lowers activation cost. Three, I just find it less aversive subjectively. You can keep stationary bicycle mounts in your house as well, and they have the advantages of being compact and&nbsp;<a href=\"http://www.amazon.com/Magnet-Bicycle-Indoor-Exercise-Trainer/dp/B004I576SM/ref=sr_1_1?ie=UTF8&amp;qid=1394232108&amp;sr=8-1&amp;keywords=stationary+bike+mount\">very cheap</a>&nbsp;if you already own a bike. Burpees require no equipment, but they bothered my knees. They work great for some people though. Jumping rope is also very space/time efficient but the skill required acts as something of a barrier. If you find learning the skill enjoyable, it's a great option. It is worth noting that runners, bikers, and rowers have among the best VO2 max scores of any athletes.</p>\n<p>What does the optimal high-intensity cardio routine look like? Data on this comes from <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3774727/\">this Meta-analysis</a> of VO2 max trainability. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/19454641\">VO2 max has been shown to be a robust predictor of mortality</a>. This relation has held across elite athletes, to average individuals, to the overweight (see Figure 2 from <a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073182\">this meta-analysis</a> of vo2 max trainability). Unfortunately, it appears that the protocols eliciting the greatest increases in VO2 max are so arduous as to have high attrition rates. 6 days/week is not a schedule of training I expect anyone but professional athletes to maintain. What sort of realistic routine can still achieve most of these gains? The meta-analysis supports a mixture of 3-5 minute intervals, and longer duration but still intense intervals (30-40 minutes of continuous training). The authors also note that they did not include analysis of the evidence that very high intensity exercise (1 minute or less of max effort) shows unique health benefits. We could simply conclude that a mixture of interval times is good, and that every increment of cardio up to very high levels is likely good for us, but that doesn't feel very motivating. What we want is a clear goal. I'm going to combine data from the VO2 max study and the Swiss review to get a rough estimate. If we want to do resistance training twice per week and cardio at least twice per week can we realistically burn the 1500 calories we want? Let's see. Interval training sessions can vary widely in number of calories burned, but a sprinting session, a 4x4 protocol (one of the more popular protocols in the VO2 max meta analysis, consisting of four 4-minute intervals), and a 30 minute run can burn between 200-450 calories as a first approximation. Twice weekly and we have 400-900 calories. This leaves 600-1100 calories for 2 weightlifting sessions. Estimates for calories burned weightlifting vary extremely widely, most likely due to the huge number of exercises considered \"weightlifting\", but even the lower estimates put it over 300 calories per hour. This puts 2-4 hours of weightlifting per week at 600-1200 calories expended. How convenient!</p>\n<p>All that remains is to suggest specific cardio routines. I don't have the evidence to say with any confidence what a truly optimal routine would look like here, but I can at least give well studied examples of each.</p>\n<p>Very high intensity routines follow a pattern of a short warmup (5 minutes at a slow pace) followed by several bursts of 10-60 seconds all out intensity. (30 on 30 off for 10 intervals is popular and close to maximizing vV02max)</p>\n<p>VO2 max interval training consists of four 3-5 minute intervals at 85%-95% your max heart rate interspersed with slower jogging for the same interval.</p>\n<p>Longer interval training consists of 20-40 minute runs at a consistent pace such that you are exhausted by the end.</p>\n<p>I wouldn't worry about the optimal frequency for each one. Don't forget that even training populations just consistently doing one type shows very dramatic improvements in health. I'd suggest freely mixing them up and trying to have fun with it.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Summary of my recommended routine</strong></p>\n<p><strong></strong></p>\n<p>This is what I recommend gradually working towards once you've made exercise a habit:</p>\n<ul>\n<li>~1-2 hour weightlifting sessions 2-3x a week. (A third weightlifting session is recommended for the first several months, for both gaining strength and building habits.)</li>\n<li>~15-40 minutes of vigorous cardio 2-3x a week.&nbsp;</li>\n</ul>\n<p>Don't do vigorous cardio on the same day as lifting weights! It's a good way to injure yourself, especially your lower back. Exercise doesn't make you stronger; it makes you weaker. It's the recovery from exercise that makes you stronger; give your body time to recover.</p>\n<p>&nbsp;</p>\n<p><strong>Nutrition</strong></p>\n<p>Don't try to implement a new diet and a new exercise plan at the same time. If you're trying to choose, do an exercise plan first&mdash;effects on health are much larger.</p>\n<p>If you are underweight or normal weight, you'll need to eat more when you start exercising. Celebrate after your workouts by eating to reinforce the exercise habit. You may think eating pizza is bad for you, but not exercising is worse, so reward yourself however you want. Or drink my <a href=\"/lw/h2h/soylent_orange_whole_food_open_source_soylent/\">nutrient dense shake</a>, designed to be consumed after workouts. (John_Maxwell_IV and I are planning to commercialize it after we roll out <a href=\"http://www.mealsquares.com/\">our first nutritionally complete food</a>.)</p>\n<p>If you're overweight: I agree with Gary Taubes that exercise is NOT a good way to lose weight. But exercise has bigger effects on health than weight loss, so I actually recommend prioritizing exercise over changing your diet. (Like I said, don't try to do both at once.)</p>\n<p>Note that you don't need to stuff yourself with massive amounts of protein to build muscle. Studies have never shown a measured benefit to consumption above .64g/lb of bodyweight, which translates to around 100g for a 150-160lb person. A single serving (3oz) of chicken, for example, contains about 21g of protein.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>If you've made it this far, congratulations; you are now as knowledgeable as any personal trainer I've spoken with.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8nAXyYLu8eT72Hwuh": 5, "rfZ6DY88ApBDXFpyW": 5, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bZ2w99pEAeAbKnKqo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 86, "baseScore": 109, "extendedScore": null, "score": 0.00029, "legacy": true, "legacyId": "25716", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 109, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to: </strong><a href=\"/lw/jrt/lifestyle_interventions_to_increase_longevity/\">Lifestyle interventions to increase longevity</a>.</p>\n<p>What does it mean for exercise to be optimal?</p>\n<ul>\n<li>Optimal for looks</li>\n<li>Optimal for time</li>\n<li>Optimal for effort</li>\n<li>Optimal for performance</li>\n<li>Optimal for longevity</li>\n</ul>\n<p>There may be even more criteria.</p>\n<p>We're all likely going for a mix of outcomes, and optimal exercise is going to change depending on your weighting of different factors. So I'm going to discuss something close to a minimum viable routine based on meta-analyses of exercise studies.</p>\n<p>Not knowing which sort of exercise yields the best results gives our brains an excuse to stop thinking about it. The intent of this post is to go over the dose responses to various types of exercise. We\u2019re going to break through vague notions like \u201cexercise is good\u201d and \u201cI should probably exercise more\u201d with a concrete plan where you understand the relevant parameters that will cause dramatic improvements.</p>\n<p><a id=\"more\"></a></p>\n<h3 id=\"How_much_exercise_\">How much exercise?</h3>\n<p>Optimality aside, I recommend starting with a very minimal routine for 6-ish weeks to build the habit of exercise in to your life. You'll want a program that causes you little mental stress that you can actually stick with. You've got a few options for achieving this. The gains from weightlifting can be surprisingly quick\u2014you'll see dramatic changes in your appearance in 4 months\u2014and seeing yourself lift more weight every session can be a great motivator. Couch to 5k is a basic running progression designed for sedentary people. A daily bodyweight routine is a good way to achieve habit formation through consistency. I recommend making a firm choice and sticking with it until it becomes easy.</p>\n<p>Once you've made exercise a habit, you'll want to gradually nudge yourself towards the level that's optimal. So what is that level? Most of the rest of the claims in this post are supported by <a href=\"http://ije.oxfordjournals.org/content/40/5/1382.long\">this review by Swiss researchers</a>. As far as I know, this is the largest systematic review of exercise studies ever undertaken, reviewing 7000 studies with 80 meeting inclusion criteria covering over 1.3 million subjects. Sheer size, however, is not the only reason to take this study very seriously. As someone who has read hundreds of exercise studies, I can say that the methodology of the meta-analysis done to determine dose-response to exercise is excellent. What is most encouraging is that the study authors repeatedly point out shortcomings, and ways their findings should <em>not</em> be interpreted because the underlying data does not warrant it. They also check for publication bias. One potential caveat is that this a review of cohort studies, not RCTs. But the authors note that RCTs of exercise almost always show <em>greater</em> effect sizes, not smaller. This is likely because people over-report how much exercise they do in observational studies.</p>\n<p>In order to compare the intensity of different activities, exercise researchers use a unit called a MET, or metabolic equivalent. The MET is defined so that your weight (in kg) * METs = Calories you're burning per hour. An example MET table can be found <a href=\"http://www.health.harvard.edu/newsletters/Harvard_Womens_Health_Watch/2009/December/met-hour-equivalents-of-various-physical-activities\">here</a>. For the purposes of exercise studies, activities are typically classified as low-intensity, moderate-intensity, and vigorous-intensity. These roughly correspond to 1-3, 4-6, and 7+ METs per hour. For typical individuals, this will translate to approximately 200, 400, and 600+ Calories burned per hour.</p>\n<p>On the low end, some studies have found dramatic benefits from just the first 15 minutes of moderate-intensity exercise per week. These studies indicate that you gain about as much going from no exercise to some exercise as you do going from some exercise to optimal exercise.</p>\n<p>The Swiss review finds that the first hour per week of vigorous-intensity activity gets you 2/3rds the benefit of 10 hours per week, but the study authors make sure to point out that this is an implausible effect size and that there are almost certainly some confounding and reverse causality issues going on. Which is to say that people who have better health are simply going to be capable of more exercise.</p>\n<p>How about on the high end? Studies differ on where the point of diminishing returns is. Some <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3491006/\">put it</a> at 1000-1500 Calories; others as high as 3500 Calories. (Remember, a typical individual burns ~400 Calories per hour of moderate-intensity exercise.) I'll shoot for 1500 Calories in my recommendations; 3500 Calories is pretty hard to reach without exercising like a pro athlete.</p>\n<p>Estimates indicate that each minute of exercise gets you 3-7 minutes of extra life on average, with higher returns for more intense exercise. So every week, you have the opportunity to get a 3-7x ROI on time spent exercising up to the point of diminishing returns. I recommend high-intensity exercise\u2014not only does it save time, it's also been shown to improve health more on a per-Calorie basis.</p>\n<p>&nbsp;</p>\n<h3 id=\"Which_exercises_\"><strong>Which exercises?</strong></h3>\n<h4 id=\"Weight_training_programs\">Weight training programs</h4>\n<p>You may have shied away from weight training in the past because you thought you would turn into some huge gross bodybuilder. But bodybuilders and fitness models take drugs and spend years training intensively to look the way they do. You are not going to gain 20lbs of muscle overnight magically. This goes double if you\u2019re a woman. You do not have testosterone; you are not going to be building huge muscles no matter what you do.</p>\n<p>Of the forms of exercise I cover, weight training has the most rigorous evidence separating what works and what doesn\u2019t. <a href=\"http://saudeemovimento.net.br/wp-content/uploads/bsk-pdf-manager/336_A_META-ANALYSIS_TO_DETERMINE_THE_DOSE_-_RESPONSE_FOR_STRENGTH_DEVELOPMENT.PDF\">This study</a> (pdf warning) examines what sort of resistance training results in the most rapid improvements.</p>\n<p>In weight training lingo, AxB means A sets of B repetitions. So 4x10 would mean 40 reps with rest periods every 10 reps. Our study recommends starting with 4x10 3 times a week, and transitioning to 4x4 2 times a week as you become stronger. Aim for a weight you can barely complete all the reps with.</p>\n<p>For an efficient full-body workout, select one exercise from each movement pattern:</p>\n<p>Upper push: bench press, incline press, overhead press, dips.</p>\n<p>Upper pull: cable rows, barbell rows, dumbbell rows, chin-ups, face pulls.</p>\n<p>Lower push: squats, lunges, leg press.</p>\n<p>Lower pull: deadlifts, power cleans, hyperextensions, romanian deadlifts, reverse hyperextensions, glute-ham raises.</p>\n<p>So a good starting routine would be</p>\n<p>A: 4x10 each of squat, bench press, lat pulldown, hyperextension</p>\n<p>B: 4x10 each of squat, overhead press, cable row, hyperextension</p>\n<p>alternating A and B workouts on different days of the week e.g. AxBxAxx, BxAxBxx.</p>\n<p>You'll try to increase the weight by 5lbs each session. As you improve, you want to decrease the reps and increase the intensity so you can keep advancing. For example, if you stall a couple times doing 4x10 at 125lbs on your squat, switch to 4x8 and keep increasing the weight, then 4x6, etc. until you get to something close to an optimal trained routine:</p>\n<p>A: 4x4 each of squats, bench, weighted chins, deadlifts</p>\n<p>B: 4x4 each of squats, overhead press, barbell row, power cleans</p>\n<p>At this point, you're going to the gym only twice per week to give yourself more recovery time.</p>\n<p>For learning exercises, there are many tutorials available online and I recommend checking some out if you are confused about form. You can always search for \"&lt;name of exercise&gt; tutorial\" and get articles and Youtube videos. Many people feel silly practicing their form with extremely light weights (often just the empty bar). But many world record holders start EVERY session this way to warm up and cement muscle memory. Others are silly NOT to do this. Also keep an eye on your ego. It's easy when setting goals for yourself to try to lift a weight that you can't really lift with proper form, because you want to set that personal best. But you'll feel pretty stupid when you are forced to miss the gym for a month because you hurt yourself.</p>\n<p>On exercise selection: I'm not a big fan of deadlifts for absolute newbies, unlike say Mark Rippetoe in Starting Strength. Maybe add deadlifts in after you've gained some muscle and you have better awareness of form. I also differ from Rippetoe in recommending that newbies high bar squat (the distinction being that low bar squats place the bar across the shoulders and high bar squats place the bar on the trapezius). I have taught newbies both forms and most find high bar squatting easier to figure out how to do properly. I spent months learning to low bar squat and still injured myself; high bar squatting can be taught in a couple sessions in my experience. Pay attention to whether a tutorial video is trying to teach you low bar squatting; the cues for each exercise are different.</p>\n<p>Free weights are generally better than machine exercises, but I recommend cable rows and lat pulldowns to newbies. The goal is to move from cable rows to barbell rows, and from lat pulldowns to actual chin-ups. The issue here is that a beginner won't be able to do the requisite sets and reps of chin-ups and rows with good form.</p>\n<p>A note about equipment: Weightlifting shoes have an incredibly high return on investment. They make back injuries less likely, and drastically improve subjective experience of squatting. You can get Rogue weightlifting shoes (use your size in men's dress shoes to size them regardless of gender) for around $120; there are cheaper options available but good shoes will last years so the amortized cost is low. <a href=\"http://wlshoes.com/purpose/olympic-lifting/\">Here's a full list of options</a>; note that even the cheapest weightlifting shoes are miles better than lifting in tennis shoes. I don't have any personal experience with the Reebok CrossFit lifter, but they seem like a good option under $100 for a shoe with the desired .75-inch rigid heel. I recommend a <a href=\"http://www.amazon.com/Harbinger-4-Inch-Nylon-3-Inch-Medium/dp/B00074H7WI/ref=sr_1_11?ie=UTF8&amp;qid=1394521980&amp;sr=8-11&amp;keywords=weightlifting+belt\">cheap belt</a> (expensive ones aren't any better) in order to improve your execution of the <a href=\"http://en.wikipedia.org/wiki/Valsalva_maneuver\">valsalva maneuver</a>&nbsp;during squats and deadlifts which further protects the spine from flexion under load.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Bodyweight_routines\">Bodyweight routines</strong></p>\n<p>For a beginner, something like <a href=\"http://well.blogs.nytimes.com/2013/05/09/the-scientific-7-minute-workout/?_php=true&amp;_type=blogs&amp;_r=0\">this</a> is reasonable. Of course such a program will max out in fitness gains fairly quickly, even if you start doing several cycles of it. But this isn\u2019t our worry as a beginner. For someone serious about progressing with body weight exercises past this stage, I recommend a program like Overcoming Gravity or Building the Gymnastic Body. There is not really formal support for the efficacy of these programs, but they are endorsed by coaches who train many people successfully, and are consistent with the general principles of weight lifting (progressive overload, training frequency, etc.).</p>\n<p>&nbsp;</p>\n<p><strong id=\"Cardio_routines\">Cardio routines</strong></p>\n<p>For cardio, I recommend against high-intensity intervals when starting out. High-intensity intervals carry a greater risk of injury, especially if you're not used to them. They're also unpleasant and not conducive to building habits. For starting out, I recommend something that is based more on psychological results rather than performance optimality, like <a href=\"http://www.coolrunning.com/engine/2/2_3/181.shtml\">Couch to 5k</a>. As you progress, start adding in short bursts of more intense effort. The idea is to tire yourself out quickly. If your cardio routine lasts more than 30 minutes you\u2019re probably going too easy.</p>\n<p>What type of cardio should you do? Cardio that is amenable to high intensity is probably one of: running (especially up hills), swimming, rowing, biking, burpees, or jump rope. But you might be able to adapt others. I'm a huge fan of rowing for a few reasons. One, it works more than just the legs. Two, you can have a rowing machine in your house, which drastically lowers activation cost. Three, I just find it less aversive subjectively. You can keep stationary bicycle mounts in your house as well, and they have the advantages of being compact and&nbsp;<a href=\"http://www.amazon.com/Magnet-Bicycle-Indoor-Exercise-Trainer/dp/B004I576SM/ref=sr_1_1?ie=UTF8&amp;qid=1394232108&amp;sr=8-1&amp;keywords=stationary+bike+mount\">very cheap</a>&nbsp;if you already own a bike. Burpees require no equipment, but they bothered my knees. They work great for some people though. Jumping rope is also very space/time efficient but the skill required acts as something of a barrier. If you find learning the skill enjoyable, it's a great option. It is worth noting that runners, bikers, and rowers have among the best VO2 max scores of any athletes.</p>\n<p>What does the optimal high-intensity cardio routine look like? Data on this comes from <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3774727/\">this Meta-analysis</a> of VO2 max trainability. <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/19454641\">VO2 max has been shown to be a robust predictor of mortality</a>. This relation has held across elite athletes, to average individuals, to the overweight (see Figure 2 from <a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073182\">this meta-analysis</a> of vo2 max trainability). Unfortunately, it appears that the protocols eliciting the greatest increases in VO2 max are so arduous as to have high attrition rates. 6 days/week is not a schedule of training I expect anyone but professional athletes to maintain. What sort of realistic routine can still achieve most of these gains? The meta-analysis supports a mixture of 3-5 minute intervals, and longer duration but still intense intervals (30-40 minutes of continuous training). The authors also note that they did not include analysis of the evidence that very high intensity exercise (1 minute or less of max effort) shows unique health benefits. We could simply conclude that a mixture of interval times is good, and that every increment of cardio up to very high levels is likely good for us, but that doesn't feel very motivating. What we want is a clear goal. I'm going to combine data from the VO2 max study and the Swiss review to get a rough estimate. If we want to do resistance training twice per week and cardio at least twice per week can we realistically burn the 1500 calories we want? Let's see. Interval training sessions can vary widely in number of calories burned, but a sprinting session, a 4x4 protocol (one of the more popular protocols in the VO2 max meta analysis, consisting of four 4-minute intervals), and a 30 minute run can burn between 200-450 calories as a first approximation. Twice weekly and we have 400-900 calories. This leaves 600-1100 calories for 2 weightlifting sessions. Estimates for calories burned weightlifting vary extremely widely, most likely due to the huge number of exercises considered \"weightlifting\", but even the lower estimates put it over 300 calories per hour. This puts 2-4 hours of weightlifting per week at 600-1200 calories expended. How convenient!</p>\n<p>All that remains is to suggest specific cardio routines. I don't have the evidence to say with any confidence what a truly optimal routine would look like here, but I can at least give well studied examples of each.</p>\n<p>Very high intensity routines follow a pattern of a short warmup (5 minutes at a slow pace) followed by several bursts of 10-60 seconds all out intensity. (30 on 30 off for 10 intervals is popular and close to maximizing vV02max)</p>\n<p>VO2 max interval training consists of four 3-5 minute intervals at 85%-95% your max heart rate interspersed with slower jogging for the same interval.</p>\n<p>Longer interval training consists of 20-40 minute runs at a consistent pace such that you are exhausted by the end.</p>\n<p>I wouldn't worry about the optimal frequency for each one. Don't forget that even training populations just consistently doing one type shows very dramatic improvements in health. I'd suggest freely mixing them up and trying to have fun with it.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Summary_of_my_recommended_routine\">Summary of my recommended routine</strong></p>\n<p><strong></strong></p>\n<p>This is what I recommend gradually working towards once you've made exercise a habit:</p>\n<ul>\n<li>~1-2 hour weightlifting sessions 2-3x a week. (A third weightlifting session is recommended for the first several months, for both gaining strength and building habits.)</li>\n<li>~15-40 minutes of vigorous cardio 2-3x a week.&nbsp;</li>\n</ul>\n<p>Don't do vigorous cardio on the same day as lifting weights! It's a good way to injure yourself, especially your lower back. Exercise doesn't make you stronger; it makes you weaker. It's the recovery from exercise that makes you stronger; give your body time to recover.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Nutrition\">Nutrition</strong></p>\n<p>Don't try to implement a new diet and a new exercise plan at the same time. If you're trying to choose, do an exercise plan first\u2014effects on health are much larger.</p>\n<p>If you are underweight or normal weight, you'll need to eat more when you start exercising. Celebrate after your workouts by eating to reinforce the exercise habit. You may think eating pizza is bad for you, but not exercising is worse, so reward yourself however you want. Or drink my <a href=\"/lw/h2h/soylent_orange_whole_food_open_source_soylent/\">nutrient dense shake</a>, designed to be consumed after workouts. (John_Maxwell_IV and I are planning to commercialize it after we roll out <a href=\"http://www.mealsquares.com/\">our first nutritionally complete food</a>.)</p>\n<p>If you're overweight: I agree with Gary Taubes that exercise is NOT a good way to lose weight. But exercise has bigger effects on health than weight loss, so I actually recommend prioritizing exercise over changing your diet. (Like I said, don't try to do both at once.)</p>\n<p>Note that you don't need to stuff yourself with massive amounts of protein to build muscle. Studies have never shown a measured benefit to consumption above .64g/lb of bodyweight, which translates to around 100g for a 150-160lb person. A single serving (3oz) of chicken, for example, contains about 21g of protein.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>If you've made it this far, congratulations; you are now as knowledgeable as any personal trainer I've spoken with.</p>", "sections": [{"title": "How much exercise?", "anchor": "How_much_exercise_", "level": 1}, {"title": "Which exercises?", "anchor": "Which_exercises_", "level": 1}, {"title": "Weight training programs", "anchor": "Weight_training_programs", "level": 2}, {"title": "Bodyweight routines", "anchor": "Bodyweight_routines", "level": 3}, {"title": "Cardio routines", "anchor": "Cardio_routines", "level": 3}, {"title": "Summary of my recommended routine", "anchor": "Summary_of_my_recommended_routine", "level": 3}, {"title": "Nutrition", "anchor": "Nutrition", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "146 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 146, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PhXENjdXiHhsWGfQo", "X6APQeHhXH9mbredM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-10T12:38:14.723Z", "modifiedAt": null, "url": null, "title": "Meetup : Salt Lake City, UT: Schelling Day", "slug": "meetup-salt-lake-city-ut-schelling-day", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AH8g65pRhcRfgWTeX/meetup-salt-lake-city-ut-schelling-day", "pageUrlRelative": "/posts/AH8g65pRhcRfgWTeX/meetup-salt-lake-city-ut-schelling-day", "linkUrl": "https://www.lesswrong.com/posts/AH8g65pRhcRfgWTeX/meetup-salt-lake-city-ut-schelling-day", "postedAtFormatted": "Monday, March 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Salt%20Lake%20City%2C%20UT%3A%20Schelling%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Salt%20Lake%20City%2C%20UT%3A%20Schelling%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAH8g65pRhcRfgWTeX%2Fmeetup-salt-lake-city-ut-schelling-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Salt%20Lake%20City%2C%20UT%3A%20Schelling%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAH8g65pRhcRfgWTeX%2Fmeetup-salt-lake-city-ut-schelling-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAH8g65pRhcRfgWTeX%2Fmeetup-salt-lake-city-ut-schelling-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 682, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/xs\">Salt Lake City, UT: Schelling Day</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 April 2014 02:45:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">9771 S 170 E, Sandy UT</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Every person reading this (ESPECIALLY YOU!) is challenged to leave ONE piece of feedback regarding what you think about this event. Before, during, or after. One sentence. Doable?</p>\n<p>Why should I care?</p>\n<ul>\n<li>There will be dinner! Eat food!</li>\n<li>Hang out with and befriend generally awesome people!</li>\n<li>Have friends or family you want to introduce to rationality? This makes a great appeal for emotional thinkers!</li>\n<li>Generate hedons and warm fuzzies!</li>\n<li>Generate fond memories to be nostalgic about!</li>\n<li>Receive empathy for what's going on in your life!</li>\n<li>Gain arbitrary Kudo points for having come!</li>\n</ul>\n<p>I like hanging out with just generally awesome people. It's why I joined this group in the first place!</p>\n<p>Spending time with people is fun. But getting to know people&mdash;really, truly getting to know people&mdash;is hard. Rewarding, but hard.</p>\n<p>Sharing your fondest hopes and deepest fears is a powerful way to make connections, but exposing your soul like that terrifying. Worse, it&rsquo;s awkward. There are few socially appropriate times to bring up stuff like that. Even when everything works out beautifully, getting it started feels stressful and not-fun.</p>\n<p>As soon as people are in a context where everyone agrees that sharing is normal (e.g. an Alcoholics Anonymous meeting, or a conversation with a therapist, or Truth or Dare), the stigma and self-consciousness don&rsquo;t hold people back nearly as much.</p>\n<p>This is our version of Truth or Dare: optimized for more plausible deniability, more warm fuzzy feelings, less debasement&dagger;, and more genuine connection with other people.</p>\n<ul>\n<li>Five different flavors of Truth: Struggles, Joys, Confessions, Hopes, and Miscellaneous.</li>\n<li>To provide plausible deniability, everyone rolls a die before speaking. A one means you cannot speak your turn, a 6 means you MUST. The result is not shared.</li>\n<li>What happens in the Schelling Game stays in the Schelling Game.</li>\n</ul>\n<p>This game is traditionally meant to played every April 14th, the birthday of Thomas Schelling, for the obvious reason&Dagger;, followed by dinner and socializing. I moved it around a bit to get it on the weekend instead of a Monday.</p>\n<p>Any of that interest you? All you have to do is:</p>\n<ul>\n<li>RSVP now. I won't hold it against you if you back out later.</li>\n<li>Show up at 9771 S 170 E, Sandy UT<br /> 2:45pm on April 20th. If you come in after we start the game at 3:00, please wait for the lull between speakers to announce yourself.</li>\n<li>Find us near the Trax station on 9800, just north of Dewey Bluth Park. There will be a small boat in the driveway.</li>\n<li>Bring your observations on what you've been up to since last meetup, if you've got any to share.</li>\n<li>Children are welcome, provided they are mature enough to either handle the adult themes in the game, or entertain themselves mostly unsupervised while we play.</li>\n<li>Anyone who brings Potluck contributions will get Two Rationality Points. If you'd like to show some non-food support for this and other events, I have a PayItSquare set up: <a rel=\"nofollow\" href=\"http://www.payitsquare.com/collect-page/edit/24123\">http://www.payitsquare.com/collect-page/edit/24123</a></li>\n</ul>\n<p>Thank you for your time, and thank you for being part of the most fun, engaging, and all-around rewarding social group I know.</p>\n<p>(&dagger;) Don't worry about missing the discomfort and humiliation of dares. That will be the focus of a different game, later in the year &gt;:-D Muahahahahahaha! (&Dagger;) A Schelling point is is a solution that people will tend to use in the absence of communication, because it seems natural, special or relevant to them. This is an arbitrary consensus point for changing social rules. It fits.</p>\n<p>PS: Be warned--fuzzy feelings are fun, but they do run a risk of skewing your view of people. It is up to you to find the appropriate benefit/cost balance. PPS: This is plagiarized heavily from the original Schelling Day post. Don't sue me.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/xs\">Salt Lake City, UT: Schelling Day</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AH8g65pRhcRfgWTeX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.604422488678176e-06, "legacy": true, "legacyId": "25728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City__UT__Schelling_Day\">Discussion article for the meetup : <a href=\"/meetups/xs\">Salt Lake City, UT: Schelling Day</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">20 April 2014 02:45:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">9771 S 170 E, Sandy UT</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Every person reading this (ESPECIALLY YOU!) is challenged to leave ONE piece of feedback regarding what you think about this event. Before, during, or after. One sentence. Doable?</p>\n<p>Why should I care?</p>\n<ul>\n<li>There will be dinner! Eat food!</li>\n<li>Hang out with and befriend generally awesome people!</li>\n<li>Have friends or family you want to introduce to rationality? This makes a great appeal for emotional thinkers!</li>\n<li>Generate hedons and warm fuzzies!</li>\n<li>Generate fond memories to be nostalgic about!</li>\n<li>Receive empathy for what's going on in your life!</li>\n<li>Gain arbitrary Kudo points for having come!</li>\n</ul>\n<p>I like hanging out with just generally awesome people. It's why I joined this group in the first place!</p>\n<p>Spending time with people is fun. But getting to know people\u2014really, truly getting to know people\u2014is hard. Rewarding, but hard.</p>\n<p>Sharing your fondest hopes and deepest fears is a powerful way to make connections, but exposing your soul like that terrifying. Worse, it\u2019s awkward. There are few socially appropriate times to bring up stuff like that. Even when everything works out beautifully, getting it started feels stressful and not-fun.</p>\n<p>As soon as people are in a context where everyone agrees that sharing is normal (e.g. an Alcoholics Anonymous meeting, or a conversation with a therapist, or Truth or Dare), the stigma and self-consciousness don\u2019t hold people back nearly as much.</p>\n<p>This is our version of Truth or Dare: optimized for more plausible deniability, more warm fuzzy feelings, less debasement\u2020, and more genuine connection with other people.</p>\n<ul>\n<li>Five different flavors of Truth: Struggles, Joys, Confessions, Hopes, and Miscellaneous.</li>\n<li>To provide plausible deniability, everyone rolls a die before speaking. A one means you cannot speak your turn, a 6 means you MUST. The result is not shared.</li>\n<li>What happens in the Schelling Game stays in the Schelling Game.</li>\n</ul>\n<p>This game is traditionally meant to played every April 14th, the birthday of Thomas Schelling, for the obvious reason\u2021, followed by dinner and socializing. I moved it around a bit to get it on the weekend instead of a Monday.</p>\n<p>Any of that interest you? All you have to do is:</p>\n<ul>\n<li>RSVP now. I won't hold it against you if you back out later.</li>\n<li>Show up at 9771 S 170 E, Sandy UT<br> 2:45pm on April 20th. If you come in after we start the game at 3:00, please wait for the lull between speakers to announce yourself.</li>\n<li>Find us near the Trax station on 9800, just north of Dewey Bluth Park. There will be a small boat in the driveway.</li>\n<li>Bring your observations on what you've been up to since last meetup, if you've got any to share.</li>\n<li>Children are welcome, provided they are mature enough to either handle the adult themes in the game, or entertain themselves mostly unsupervised while we play.</li>\n<li>Anyone who brings Potluck contributions will get Two Rationality Points. If you'd like to show some non-food support for this and other events, I have a PayItSquare set up: <a rel=\"nofollow\" href=\"http://www.payitsquare.com/collect-page/edit/24123\">http://www.payitsquare.com/collect-page/edit/24123</a></li>\n</ul>\n<p>Thank you for your time, and thank you for being part of the most fun, engaging, and all-around rewarding social group I know.</p>\n<p>(\u2020) Don't worry about missing the discomfort and humiliation of dares. That will be the focus of a different game, later in the year &gt;:-D Muahahahahahaha! (\u2021) A Schelling point is is a solution that people will tend to use in the absence of communication, because it seems natural, special or relevant to them. This is an arbitrary consensus point for changing social rules. It fits.</p>\n<p>PS: Be warned--fuzzy feelings are fun, but they do run a risk of skewing your view of people. It is up to you to find the appropriate benefit/cost balance. PPS: This is plagiarized heavily from the original Schelling Day post. Don't sue me.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City__UT__Schelling_Day1\">Discussion article for the meetup : <a href=\"/meetups/xs\">Salt Lake City, UT: Schelling Day</a></h2>", "sections": [{"title": "Discussion article for the meetup : Salt Lake City, UT: Schelling Day", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City__UT__Schelling_Day", "level": 1}, {"title": "Discussion article for the meetup : Salt Lake City, UT: Schelling Day", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City__UT__Schelling_Day1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-10T18:32:46.462Z", "modifiedAt": null, "url": null, "title": "Academia as a career option, its social value, and alternatives", "slug": "academia-as-a-career-option-its-social-value-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:34.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x2fkoZMuMNhBashH4/academia-as-a-career-option-its-social-value-and", "pageUrlRelative": "/posts/x2fkoZMuMNhBashH4/academia-as-a-career-option-its-social-value-and", "linkUrl": "https://www.lesswrong.com/posts/x2fkoZMuMNhBashH4/academia-as-a-career-option-its-social-value-and", "postedAtFormatted": "Monday, March 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Academia%20as%20a%20career%20option%2C%20its%20social%20value%2C%20and%20alternatives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAcademia%20as%20a%20career%20option%2C%20its%20social%20value%2C%20and%20alternatives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx2fkoZMuMNhBashH4%2Facademia-as-a-career-option-its-social-value-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Academia%20as%20a%20career%20option%2C%20its%20social%20value%2C%20and%20alternatives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx2fkoZMuMNhBashH4%2Facademia-as-a-career-option-its-social-value-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx2fkoZMuMNhBashH4%2Facademia-as-a-career-option-its-social-value-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 512, "htmlBody": "<p>Many of the high school and college students who contacted us at <a href=\"http://cognitomentoring.org\">Cognito Mentoring</a> were looking for advice were considering going into academia. The main draw to them was the desire to learn specific subjects and explore ideas in greater depth. As a result, we've been investigating academia as a career option and also considering what alternatives there may be to academia that fulfill the same needs but provide better pay and/or generate more social value. The love of ideas and epistemic exploration is shared by many of the people at <em>Less Wrong</em>, including those who are not in academia. So I'm hoping that people will share their own perspectives in the comments. That'll help us as well as the many <em>LessWrong</em> <a href=\"/lw/jtz/what_we_learned_about_less_wrong_from_cognito/\">lurkers</a> interested in academia.</p>\n<p>I'm eager to hear about what considerations you used when weighing academia against other career options, and how you came to your decision. Incidentally, there are a number of great answers to the Quora question <a href=\"http://www.quora.com/Academia/Why-did-you-leave-academia\">Why did you leave academia?</a>, but there's probably many thoughts people have here that aren't reflected in the Quora answers. I've also written up a detailed review of academia as a career option on the info wiki for Cognito Mentoring <a href=\"http://info.cognitomentoring.org/wiki/Academia_as_a_career_option\">here (long read)</a>, and I'd also love feedback on the validity of the points I make there.</p>\n<p>Many of our advisees as well as the <em>LessWrong</em> readership at large are interested in choosing careers based on the social value generated by these careers. (This is evidenced in the strong connection between the <em>LessWrong</em> and effective altruism communities). What are your thoughts on that front? Jonah and I have collaboratively written a page on the <a href=\"http://info.cognitomentoring.org/wiki/Social_value_of_academia\">social value of academia</a>. Our key point is that research academia is higher value than alternative careers only in cases where either the person has a chance of making big breakthroughs in the area, or if the area of research itself is high-value. Examples of the latter may include machine learning (we're <a href=\"http://info.cognitomentoring.org/wiki/Social_value_of_working_in_artificial_intelligence_and_machine_learning\">just starting on investigating this</a>) and (arguably) biomedical research (we've collected <a href=\"http://info.cognitomentoring.org/wiki/Social_value_of_biomedical_research\">some links on this</a>, but haven't investigated this in depth).</p>\n<p>For those who are or were attracted to academia, what other career options did you consider? If you decided not to join, or chose to quit, academia, what alternative career are you now pursuing? We've identified a few possibilities at our <a href=\"http://info.cognitomentoring.org/wiki/Alternatives_to_academia\">alternatives to academia</a> page, but we're largely shooting in the dark here. Based on anecdotal evidence from people working in venture capital, it seems like venture capital is a great place for polymath-types who are interested in researching a wide range of subjects shallowly, so it's ideal for people who like shallow intellectual exploration rather than sticking to a single subject for an inordinate amount of time. But there are very few jobs in venture capital. On paper, jobs at consulting firms should be similar to venture capital in requiring a lot of shallow research. But we don't have an inside view of consulting jobs -- are they a good venue for intellectually curious people? Are there other job categories we missed?</p>\n<p>All thoughts are greatly appreciated!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x2fkoZMuMNhBashH4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 1.6048524391051503e-06, "legacy": true, "legacyId": "25729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xqEuAD3dXHPSFeerd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-10T19:27:04.492Z", "modifiedAt": "2020-01-20T20:40:06.304Z", "url": null, "title": "A vote against spaced repetition", "slug": "a-vote-against-spaced-repetition", "viewCount": null, "lastCommentedAt": "2021-06-05T19:27:28.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "ancientcampus", "user": {"username": "ancientcampus", "createdAt": "2011-11-09T20:20:29.779Z", "isAdmin": false, "displayName": "ancientcampus"}, "userId": "BGcNEeTWb7Qxcf5Pc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/As9E3HfgED2zkTAfB/a-vote-against-spaced-repetition", "pageUrlRelative": "/posts/As9E3HfgED2zkTAfB/a-vote-against-spaced-repetition", "linkUrl": "https://www.lesswrong.com/posts/As9E3HfgED2zkTAfB/a-vote-against-spaced-repetition", "postedAtFormatted": "Monday, March 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20vote%20against%20spaced%20repetition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20vote%20against%20spaced%20repetition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAs9E3HfgED2zkTAfB%2Fa-vote-against-spaced-repetition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20vote%20against%20spaced%20repetition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAs9E3HfgED2zkTAfB%2Fa-vote-against-spaced-repetition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAs9E3HfgED2zkTAfB%2Fa-vote-against-spaced-repetition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 681, "htmlBody": "<html><head></head><body><p>LessWrong seems to be a big fan of spaced-repetition flashcard programs like Anki, Supermemo, or Mnemosyne. I used to be. After using them religiously for 3 years in medical school, I now categorically advise against using them for large volumes of memorization.</p><p>[A caveat before people get upset: I think they appropriate in certain situations, and I have not tried to use them to learn a language, which seems its most popular use. More at the bottom.]</p><p>A bit more history: I and 30 other students tried using Mnemosyne (and some used Anki) for multiple tests. At my school, we have a test approximately every 3 weeks, and each test covers about 75 pages of high-density outline-format notes. Many stopped after 5 or so such tests, citing that they simply did not get enough returns from their time. I stuck with it longer and used them more than anyone else, using them for 3 years.</p><p>Incidentally, I failed my first year and had to repeat.</p><p>By the end of that third year (and studying for my Step 1 boards, a several-month process), I lost faith in spaced-repetition cards as an effective tool for my memorization demands. I later met with a learning-skills specialist, who felt the same way, and had better reasons than my intuition/trial-and-error:</p><ul><li>Flashcards are less useful to learning the \u201cbig picture\u201d</li><li>Specifically, if you are memorizing a large amount of information, there is often a hierarchy, organization, etc that can make leaning the whole thing easier, and you loose the constant visual reminder of the larger context when using flashcards.</li><li>Flashcards do not take advantage of spatial, mapping, or visual memory, all of which the human mind is much better optimized for. It is not so well built to memorize pairs between seemingly arbitrary concepts with few to no intuitive links. My preferred methods are, in essence, hacks that use your visual and spatial memory rather than rote.</li></ul><p>Here are examples of the typical kind of things I memorize every day and have found flashcards to be surprisingly worthless for:</p><ul><li>The definition of Sj\u00f6gren's syndrome</li><li>The contraindications of Metronidazole</li><li>The significance of a rise in serum \u03b1FP</li></ul><p>Here is what I now use in place of flashcards:</p><ol><li>Ven diagrams/etc, to compare and contrast similar lists. (This is more specific to medical school, when you learn subtly different diseases.)</li><li>Mnemonic pictures. I have used this myself for years to great effect, and later learned it was taught by my study-skills expert, though I'm surprised I haven't found them formally named and taught anywhere else. The basic concept is to make a large picture, where each detail on the picture corresponds to a detail you want to memorize.</li><li>Memory palaces. I recently learned how to properly use these, and I'm a true believer. When I only had the general idea to \u201cpair things you want to memorize with places in your room\u201d I found it worthless, but after I was taught a lot of do's and don'ts, they're now my favorite way to memorize any list of 5+ items. If there's enough demand on LW I can write up a summary.</li></ol><p>Spaced repetition is still good for knowledge you need to retrieve <i>immediately</i>, when a 2-second delay would make it useless. I would still consider spaced-repetition to memorize some of the more rarely-used notes on the treble and bass clef, if I ever decide to learn to sight-read music properly. I make no comment on it's usefulness to learn a foreign language, as I haven't tried it, but if I were to pick one up I personally would start with a rosetta-stone-esque program.</p><p>Your mileage may vary, but after seeing so many people try and reject them, I figured it was enough data to share. Mnemonic pictures and memory palaces are slightly time consuming when you're learning them. However, if someone has the motivation and discipline to make a stack of flashcards and study them every day indefinitely, then I believe learning and using those skills is a far better use of time.</p></body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "As9E3HfgED2zkTAfB", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 80, "extendedScore": null, "score": 0.000208, "legacy": true, "legacyId": "25730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 80, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2014-03-10T19:27:04.492Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-11T01:48:26.369Z", "modifiedAt": null, "url": null, "title": "Should one be sad when an opportunity is lost?", "slug": "should-one-be-sad-when-an-opportunity-is-lost", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:08.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FxJ2gSQTBKdi6W9bQ/should-one-be-sad-when-an-opportunity-is-lost", "pageUrlRelative": "/posts/FxJ2gSQTBKdi6W9bQ/should-one-be-sad-when-an-opportunity-is-lost", "linkUrl": "https://www.lesswrong.com/posts/FxJ2gSQTBKdi6W9bQ/should-one-be-sad-when-an-opportunity-is-lost", "postedAtFormatted": "Tuesday, March 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20one%20be%20sad%20when%20an%20opportunity%20is%20lost%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20one%20be%20sad%20when%20an%20opportunity%20is%20lost%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxJ2gSQTBKdi6W9bQ%2Fshould-one-be-sad-when-an-opportunity-is-lost%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20one%20be%20sad%20when%20an%20opportunity%20is%20lost%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxJ2gSQTBKdi6W9bQ%2Fshould-one-be-sad-when-an-opportunity-is-lost", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxJ2gSQTBKdi6W9bQ%2Fshould-one-be-sad-when-an-opportunity-is-lost", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 501, "htmlBody": "<p>There are many ways to tackle this question, but I mean this in a homo economicus, not biased perspective. If we were great optimizers of some things (experiences, states of the world, utility in the emotional sense), should we be sad upon hearing we lost an opportunity?</p>\n<p>The intuitive answer, to me, is yes. But for many things, for most things I have begun to believe otherwise. This is because we combine two distinct meanings of opportunity</p>\n<p>Opportunity<sub>1</sub>&nbsp;= Something good in the future that is uncertain at the moment and could happen to you, frequently depending on environmental factors outside of your control and some factors within your control in the time between now and the opportunity taking place. Ex:&nbsp;</p>\n<p>&nbsp;</p>\n<ul>\n<li>Getting a promotion</li>\n<li>Finding a romantic partner</li>\n<li>Having a really good friendship</li>\n<li>Having a large H index (for scientific publications)</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Opportunity<sub>2 </sub>= Something good in the future that is uncertain at the moment and could happen to you, but all the actions you could have personally taken that could influence this are in the past, and now only time and chance will determine if it will be the case. Ex:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Being approved at Google after the entire interview process has happened</li>\n<li>Being accepted at Harvard</li>\n<li>Avoiding wine in your clothing after it has been dropped</li>\n<li>Being accepted to work with CEA after filling in the entire application.&nbsp;</li>\n</ul>\n<p>&nbsp;</p>\n<p><br />I think it is very reasonable to be sad when you lose opportunites<sub>1</sub> but completely pointless to be sad over the loss of the second kind, opporunities<sub>2</sub>. It feels obvious to me, but in case it isn't I'll try to make it explicit:</p>\n<p>When you lose opportunities<sub>1</sub>, you change the course of your future actions, each of your actions, your time and your effort has become less valuable, since you have to do more to get the same odds or even less.&nbsp;</p>\n<p>When you lose opportunities<sub>2 </sub>you are only being notified of an indexical property, you learn in which of the possible universes you could be you happen to be. You have gained knowledge, you can tailor your future actions regarding other things accordingly. Nothing has become pricier for your efforts, in fact, now you have a better map, and can navigate with ease.&nbsp;</p>\n<p>So let us be neutral or happy with the loss of oportunities<sub>2</sub>, and gain strenght from the loss of opportunities<sub>1</sub>. It seems right to allocate emotional and psychological resources to things you can act on, when you are not in flow. Otherwise, you may end up in the hardest death spiral to overcome, learned helplessness.</p>\n<p><br />For political reasons related to my prospective adviser's academic history, all applicants who wanted to study with him didn't make it to Berkeley University. &nbsp;But hey, I didn't care... That just means I'm in the fun universe in which I actually have to do all the crazy stuff like moving into the unknown, that is a universe of adventure right?</p>\n<p>Loss aversion be damned!&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FxJ2gSQTBKdi6W9bQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.6053810654829548e-06, "legacy": true, "legacyId": "25733", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-11T02:44:39.938Z", "modifiedAt": null, "url": null, "title": "On not getting a job as an option", "slug": "on-not-getting-a-job-as-an-option", "viewCount": null, "lastCommentedAt": "2018-02-01T02:50:20.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aKQtuNvawj3Fzmw3X/on-not-getting-a-job-as-an-option", "pageUrlRelative": "/posts/aKQtuNvawj3Fzmw3X/on-not-getting-a-job-as-an-option", "linkUrl": "https://www.lesswrong.com/posts/aKQtuNvawj3Fzmw3X/on-not-getting-a-job-as-an-option", "postedAtFormatted": "Tuesday, March 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20not%20getting%20a%20job%20as%20an%20option&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20not%20getting%20a%20job%20as%20an%20option%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKQtuNvawj3Fzmw3X%2Fon-not-getting-a-job-as-an-option%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20not%20getting%20a%20job%20as%20an%20option%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKQtuNvawj3Fzmw3X%2Fon-not-getting-a-job-as-an-option", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKQtuNvawj3Fzmw3X%2Fon-not-getting-a-job-as-an-option", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 714, "htmlBody": "<p>This was originally a comment to VipulNaik's recent indagations about the academic lifestyle versus the job lifestyle. Instead of calling it lifestyle he called them career options, but I'm taking a different emphasis here on purpose.</p>\n<p>Due to <a href=\"http://www.nickbostrom.com/information-hazards.pdf&lrm;\">information hazards</a> risks, I recommend that Effective Altruists who are still wavering back and forth do not read this. Spoiler EA alert.&nbsp;</p>\n<p>I'd just like to provide a cultural difference information that I have consistently noted between Americans and Brazilians which seems relevant here.&nbsp;</p>\n<p>To have a job and work in the US is taken as a *de facto* biological need. It is as abnormal for an American, in my experience, to consider not working, as it is to consider not breathing, or not eating. &nbsp;It just doesn't cross people's minds.&nbsp;</p>\n<p>If anyone has insight above and beyond \"Protestant ethics and the spirit of capitalism\" let me know about it, I've been waiting for the \"why?\" for years.&nbsp;</p>\n<p>So yeah, let me remind people that you can spend years and years not working. that not getting a job isn't going to kill you or make you less healthy, that ultravagabonding is possible and feasible and many do it for over six months a year, that I have a friend who lives as the boyfriend of his sponsor's wife in a triad and somehow never worked a day in his life (the husband of the triad pays it all, both men are straight). That I've hosted an Argentinian who left graduate economics for two years to randomly travel the world, ended up in Rome and passed by here in his way back, through couchsurfing. &nbsp;That Puneet Sahani has been well over two years travelling the world with no money and an Indian passport now. I've also hosted a lovely estonian gentleman who works on computers 4 months a year in London to earn pounds, and spends eight months a year getting to know countries while learning their culture etc... Brazil was his third country.&nbsp;</p>\n<p>Oh, and never forget the Uruguay couple I just met at a dance festival who have been travelling as hippies around and around South America for 5 years now, and showed no sign of owning more than 500 dollars worth of stuff.&nbsp;</p>\n<p>Also in case you'd like to live in a paradise valley taking Santo Daime (a religious ritual with DMT) about twice a week, you can do it with a salary of aproximatelly 500 dollars per month in Vale do Gamarra, where I just spent carnival, that is what the guy who drove us back did. &nbsp;Given Brazilian or Turkish returns on investment, that would cost you 50 000 bucks in case you refused to work within the land itself for the 500.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Oh, I forgot to mention that though it certainly makes you unable to do expensive stuff, thus removing the paradox of choice and part of your existential angst from you (uhuu less choices!), there is nearly no detraction in status from not having a job. In fact, during these years in which I was either being an EA and directing an NGO, or studying on my own, or doing a Masters (which, let's agree is not very time consuming) my status has increased steadily, and many opportunities would have been lost if I had a job that wouldn't let me move freely. Things like being invited as Visiting Scholar to Singularity Institute, like giving a TED talk, like directing IERFH, and like spending a month working at FHI with Bostrom, Sandberg, and the classic Lesswrong poster Stuart Armstrong.&nbsp;</p>\n<p>So when thinking about what to do with you future my dear fellow Americans, please, at least consider not getting a job. At least admit what everyone knows from the bottom of their hearts, that jobs are abundant for high IQ people (specially you my programmer lurker readers.... I know you are there...and you native English speakers, I can see you there, unnecessarily worrying about your earning potential).&nbsp;</p>\n<p>A job is truly an instrumental goal, and your terminal goals certainly do have chains of causation leading to them that do not contain a job for 330 days a year. &nbsp;Unless you are a workaholic who experiences flow in virtue of pursuing instrumental goals. Then please, work all day long, donate as much as you can, and may your life be awesome!&nbsp;</p>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "Jzm2mYuuDBCNWq8hi": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aKQtuNvawj3Fzmw3X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 50, "extendedScore": null, "score": 0.000132, "legacy": true, "legacyId": "25735", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 190, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-11T09:31:16.868Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt", "slug": "meetup-frankfurt-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6LPjoQYcuWpPYiBrw/meetup-frankfurt-1", "pageUrlRelative": "/posts/6LPjoQYcuWpPYiBrw/meetup-frankfurt-1", "linkUrl": "https://www.lesswrong.com/posts/6LPjoQYcuWpPYiBrw/meetup-frankfurt-1", "postedAtFormatted": "Tuesday, March 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6LPjoQYcuWpPYiBrw%2Fmeetup-frankfurt-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6LPjoQYcuWpPYiBrw%2Fmeetup-frankfurt-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6LPjoQYcuWpPYiBrw%2Fmeetup-frankfurt-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xt'>Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 March 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location will be published in time. Contact me under 0176 34 095 760. I prefer texting to calling. If you have any special requirements and need help to attend (whether a disability, social anxiety, whatever), please tell us in advance!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xt'>Frankfurt</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6LPjoQYcuWpPYiBrw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.6059430047526871e-06, "legacy": true, "legacyId": "25740", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt\">Discussion article for the meetup : <a href=\"/meetups/xt\">Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 March 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location will be published in time. Contact me under 0176 34 095 760. I prefer texting to calling. If you have any special requirements and need help to attend (whether a disability, social anxiety, whatever), please tell us in advance!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt1\">Discussion article for the meetup : <a href=\"/meetups/xt\">Frankfurt</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-11T15:01:10.003Z", "modifiedAt": null, "url": null, "title": "[Link] First talk by CSER", "slug": "link-first-talk-by-cser", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:02.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NoSuchPlace", "createdAt": "2013-02-28T22:27:28.767Z", "isAdmin": false, "displayName": "NoSuchPlace"}, "userId": "CoEcoZoBfoZtWb2NZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yvWWpe5Nihumty57J/link-first-talk-by-cser", "pageUrlRelative": "/posts/yvWWpe5Nihumty57J/link-first-talk-by-cser", "linkUrl": "https://www.lesswrong.com/posts/yvWWpe5Nihumty57J/link-first-talk-by-cser", "postedAtFormatted": "Tuesday, March 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20First%20talk%20by%20CSER&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20First%20talk%20by%20CSER%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvWWpe5Nihumty57J%2Flink-first-talk-by-cser%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20First%20talk%20by%20CSER%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvWWpe5Nihumty57J%2Flink-first-talk-by-cser", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvWWpe5Nihumty57J%2Flink-first-talk-by-cser", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>The Centre for the Study of Existential Risk (CSER) has recently held its first public lecture which can be found here:</p>\n<p>&nbsp;</p>\n<p><strong><a href=\"http://cser.org/hundreds-attend-cser-lecture/\">Existential Risk: Surviving the 21st Century</a></strong></p>\n<p>&nbsp;</p>\n<p>The talk's blurb:</p>\n<p style=\"padding-left: 30px;\">\"In the coming century, the greatest threats to human survival may come from our own technological developments. However, if we can safely navigate the pitfalls, the benefits that technology promises are enormous. A philosopher, an astronomer, and an entrepreneur have come together to form the Centre for the Study of Existential Risk. The goal: to bring a fraction of humanity&rsquo;s talents to bear on the task of ensuring our long-term survival. In this lecture, Huw Price, Martin Rees and Jaan Tallinn will outline humanity&rsquo;s greatest challenge: surviving the 21st century.\"</p>\n<p>From CSER's about page:</p>\n<p style=\"padding-left: 30px;\" dir=\"ltr\">\"An existential risk is one that threatens the existence of our entire species. &nbsp;The Cambridge Centre for the Study of Existential Risk (CSER) &mdash; a joint initiative between a philosopher, a scientist, and a software entrepreneur &mdash; was founded on the conviction that these risks require a great deal more scientific investigation than they presently receive. &nbsp;CSER is a multidisciplinary research centre dedicated to the study and mitigation of risks that could lead to human extinction.</p>\n<p style=\"padding-left: 30px;\" dir=\"ltr\">Our goal is to steer a small fraction of Cambridge&rsquo;s great intellectual resources, and of the reputation built on its past and present scientific pre-eminence, to the task of ensuring that our own species has a long-term future.\"</p>\n<p>The philosopher, scientist and entrepreneur in question being Huw Price, Martin Rees and Jaan Tallinn respectively.</p>\n<p>&nbsp;</p>\n<p>Incase you are looking for the talk that Jaan Tallinn referred to, I think that it is <a href=\"https://www.youtube.com/watch?v=GbG5VbtI-5A\">this</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yvWWpe5Nihumty57J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.6063437336903288e-06, "legacy": true, "legacyId": "25741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-11T17:42:49.788Z", "modifiedAt": null, "url": null, "title": "Is my view contrarian?", "slug": "is-my-view-contrarian", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:33.021Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kFiz7Etau5HNMdKdx/is-my-view-contrarian", "pageUrlRelative": "/posts/kFiz7Etau5HNMdKdx/is-my-view-contrarian", "linkUrl": "https://www.lesswrong.com/posts/kFiz7Etau5HNMdKdx/is-my-view-contrarian", "postedAtFormatted": "Tuesday, March 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20my%20view%20contrarian%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20my%20view%20contrarian%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFiz7Etau5HNMdKdx%2Fis-my-view-contrarian%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20my%20view%20contrarian%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFiz7Etau5HNMdKdx%2Fis-my-view-contrarian", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFiz7Etau5HNMdKdx%2Fis-my-view-contrarian", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2611, "htmlBody": "<p><small>Previously: <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">Contrarian Excuses</a>, <a href=\"/lw/1kh/the_correct_contrarian_cluster/\">The Correct Contrarian Cluster</a>, <a href=\"/lw/28i/what_is_bunk/\">What is bunk?</a>, <a href=\"/lw/iao/common_sense_as_a_prior/\">Common Sense as a Prior</a>, <a href=\"/lw/iu0/trusting_expert_consensus/\">Trusting Expert Consensus</a>, <a href=\"http://www.overcomingbias.com/2014/03/prefer-contrarian-questions-vs-answers.html\">Prefer Contrarian Questions</a>.</small></p>\n<p>Robin Hanson once <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">wrote</a>:</p>\n<blockquote>\n<p>On average, contrarian views are less accurate than standard views. Honest contrarians should admit this, that neutral outsiders should assign most contrarian views a lower probability than standard views, though perhaps a high enough probability to warrant further investigation. Honest contrarians who expect reasonable outsiders to give their contrarian view more than normal credence should point to strong outside indicators that correlate enough with contrarians tending more to be right.</p>\n</blockquote>\n<p>I tend to think through the issue in three stages:</p>\n<ol>\n<li>When should I consider myself to be holding a contrarian<sup><a id=\"fnref:1\" class=\"footnote\" title=\"see footnote\" href=\"#fn:1\">[1]</a></sup> view? What is the relevant expert community?</li>\n<li>If I seem to hold a contrarian view, when do <em>I</em> have enough reason to think I&rsquo;m correct?</li>\n<li>If I seem to hold a <em>correct</em> contrarian view, what can I do to give <em>other</em> people good reasons to accept my view, or at least to take it seriously enough to examine it at length?</li>\n</ol>\n<p>I don&rsquo;t yet feel that I have &ldquo;answers&rdquo; to these questions, but in this post (and hopefully some future posts) I&rsquo;d like to organize some of what has been said before,<sup><a id=\"fnref:2\" class=\"footnote\" title=\"see footnote\" href=\"#fn:2\">[2]</a></sup> and push things a bit further along, in the hope that further discussion and inquiry will contribute toward significant progress in <a href=\"http://plato.stanford.edu/entries/epistemology-social/\">social epistemology</a>.<sup><a id=\"fnref:3\" class=\"footnote\" title=\"see footnote\" href=\"#fn:3\">[3]</a></sup> Basically, I hope to say a bunch of obvious things, in a relatively well-organized fashion, so that less obvious things can be said from there.<sup><a id=\"fnref:4\" class=\"footnote\" title=\"see footnote\" href=\"#fn:4\">[4]</a></sup></p>\n<p>In this post, I&rsquo;ll just address stage 1. Hopefully I&rsquo;ll have time to revisit stages 2 and 3 in future posts.</p>\n<p>&nbsp;</p>\n<h4>Is my view contrarian?</h4>\n<h5>World model differences vs. value differences</h5>\n<p>Is my <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">effective altruism</a> a contrarian view? It seems to be more of a contrarian <em>value judgment</em> than a contrarian <em>world model</em>,<sup><a id=\"fnref:5\" class=\"footnote\" title=\"see footnote\" href=\"#fn:5\">[5]</a></sup> and by &ldquo;contrarian view&rdquo; I tend to mean &ldquo;contrarian world model.&rdquo; Some apparently contrarian views are probably actually contrarian <em>values</em>.</p>\n<p>&nbsp;</p>\n<h5>Expert consensus</h5>\n<p>Is my <a href=\"http://commonsenseatheism.com/\">atheism</a> a contrarian view? It&rsquo;s definitely a world model, not a value judgment, and <a href=\"https://www.cia.gov/library/publications/the-world-factbook/fields/2122.html#xx\">only 2% of people are atheists</a>.</p>\n<p>But what&rsquo;s the relevant <em>expert</em> population, here? Suppose it&rsquo;s &ldquo;academics who specialize in the arguments and evidence concerning whether a god or gods exist.&rdquo; If so, then the expert population is probably dominated by academic theologians and religious philosophers, and my atheism is a contrarian view.</p>\n<p>We need <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">some heuristics</a> for evaluating the soundness of the academic consensus in different fields. <sup><a id=\"fnref:6\" class=\"footnote\" title=\"see footnote\" href=\"#fn:6\">[6]</a></sup></p>\n<p>For example, we should consider the selection effects operating on communities of experts. If someone doesn&rsquo;t believe in God, they&rsquo;re unlikely to spend their career studying arcane arguments for and against God&rsquo;s existence. So most people who specialize in this topic are theists, but nearly all of them were theists <em>before</em> they knew the arguments.</p>\n<p>Perhaps instead the relevant expert community is &ldquo;scholars who study the fundamental nature of the universe&rdquo; &mdash; maybe, philosophers and physicists? They&rsquo;re mostly atheists. <sup><a id=\"fnref:7\" class=\"footnote\" title=\"see footnote\" href=\"#fn:7\">[7]</a></sup> This is starting to get pretty ad-hoc, but maybe that&rsquo;s unavoidable.</p>\n<p>What about my view that the overall long-term impact of <a href=\"http://intelligence.org/2013/08/11/what-is-agi/\">AGI</a> will be, most likely, extremely bad? A recent survey of the top 100 authors in artificial intelligence (by citation index)<sup><a id=\"fnref:8\" class=\"footnote\" title=\"see footnote\" href=\"#fn:8\">[8]</a></sup> suggests that my view is somewhat out of sync with the views of those researchers.<sup><a id=\"fnref:9\" class=\"footnote\" title=\"see footnote\" href=\"#fn:9\">[9]</a></sup> But is that the relevant expert population? My impression is that AI experts know a lot about contemporary AI methods, especially within their subfield, but usually haven&rsquo;t thought much about, or read much about, long-term AI impacts.</p>\n<p>Instead, perhaps I&rsquo;d need to survey &ldquo;<a href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI impact experts</a>&rdquo; to tell whether my view is contrarian. But who is that, exactly? There&rsquo;s no standard credential.</p>\n<p>Moreover, the most plausible candidates around today for &ldquo;AGI impact experts&rdquo; are &mdash; like the &ldquo;experts&rdquo; of many other fields &mdash; mere &ldquo;scholastic experts,&rdquo; in that they<sup><a id=\"fnref:10\" class=\"footnote\" title=\"see footnote\" href=\"#fn:10\">[10]</a></sup> know a lot about the arguments and evidence typically brought to bear on questions of long-term AI outcomes.<sup><a id=\"fnref:11\" class=\"footnote\" title=\"see footnote\" href=\"#fn:11\">[11]</a></sup> They generally are <em>not</em> experts in the sense of &ldquo;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Ericsson-An-Introduction-to-Cambridge-Handbook-of-Expertise-and-Expert-Performance.pdf\">Reliably superior performance on representative tasks</a>&rdquo; &mdash; they don&rsquo;t have uniquely good track records on predicting long-term AI outcomes, for example. As far as I know, they don&rsquo;t even have uniquely good track records on predicting short-term geopolitical or sci-tech outcomes &mdash; e.g. they aren&rsquo;t among the &ldquo;<a href=\"http://www.edge.org/conversation/how-to-win-at-forecasting\">super forecasters</a>&rdquo; discovered in <a href=\"http://www.iarpa.gov/Programs/ia/ACE/ace.html\">IARPA&rsquo;s forecasting tournaments</a>.</p>\n<p>Furthermore, we might start to worry about selection effects, again. E.g. if we ask AGI experts when they think AGI will be built, they may be overly optimistic about the timeline: after all, if they didn&rsquo;t think AGI was feasible soon, they probably wouldn&rsquo;t be focusing their careers on it.</p>\n<p>Perhaps we can salvage this approach for determining whether one has a contrarian view, but for now, let&rsquo;s consider another proposal.</p>\n<p>&nbsp;</p>\n<h5>Mildly extrapolated elite opinion</h5>\n<p>Nick Beckstead instead <a href=\"/lw/iao/common_sense_as_a_prior/\">suggests </a> that, at least as a strong prior, one should believe what one thinks &ldquo;a broad coalition of trustworthy people would believe if they were trying to have accurate views and they had access to [one&rsquo;s own] evidence.&rdquo;<sup><a id=\"fnref:12\" class=\"footnote\" title=\"see footnote\" href=\"#fn:12\">[12]</a></sup> Below, I&rsquo;ll propose a modification of Beckstead&rsquo;s approach which aims to address the &ldquo;Is my view contrarian?&rdquo; question, and I&rsquo;ll call it the &ldquo;mildly extrapolated elite opinion&rdquo; (MEEO) method for determining the relevant expert population. <sup><a id=\"fnref:13\" class=\"footnote\" title=\"see footnote\" href=\"#fn:13\">[13]</a></sup></p>\n<p>First: which people are &ldquo;trustworthy&rdquo;? With Beckstead, I favor &ldquo;giving more weight to the opinions of people who can be shown to be trustworthy by clear indicators that many people would accept, rather than people that seem trustworthy to you personally.&rdquo; (This guideline aims to avoid parochialism and self-serving cognitive biases.)</p>\n<p>What are some &ldquo;clear indicators that many people would accept&rdquo;? Beckstead suggests:</p>\n<blockquote>\n<p>IQ, business success, academic success, generally respected scientific or other intellectual achievements, wide acceptance as an intellectual authority by certain groups of people, or success in any area where there is intense competition and success is a function of ability to make accurate predictions and good decisions&hellip;</p>\n</blockquote>\n<blockquote>\n<p>Of course, trustworthiness can also be domain-specific. Very often, elite common sense would recommend deferring to the opinions of experts (e.g., listening to what physicists say about physics, what biologists say about biology, and what doctors say about medicine). In other cases, elite common sense may give partial weight to what putative experts say without accepting it all (e.g. economics and psychology). In other cases, they may give less weight to what putative experts say (e.g. sociology and philosophy).</p>\n</blockquote>\n<p>Hence MEEO outsources the challenge of evaluating academic consensus in different fields to the &ldquo;generally trustworthy people.&rdquo; But in doing so, it raises several new challenges. How do we determine which people are trustworthy? How do we &ldquo;mildly extrapolate&rdquo; their opinions? How do we weight those mildly extrapolated opinions in combination?</p>\n<p>This approach might also be promising, or it might be even harder to use than the &ldquo;expert consensus&rdquo; method.</p>\n<p>&nbsp;</p>\n<h4>My approach</h4>\n<p>In practice, I tend to do something like this:</p>\n<ul>\n<li>To determine whether my view is contrarian, I ask whether there&rsquo;s a fairly obvious, relatively trustworthy expert population on the issue. If there is, I try to figure out what their consensus on the matter is. If it&rsquo;s different than my view, I conclude I have a contrarian view.</li>\n<li>If there <em>isn&rsquo;t</em> an obvious trustworthy expert population on the issue from which to extract a consensus view, then I basically give up on step 1 (&ldquo;Is my view contrarian?&rdquo;) and just move to the model combination in step 2 (see below), retaining pretty large uncertainty about how contrarian my view might be.</li>\n</ul>\n<h4><br /></h4>\n<h4>When do I have good reason to think I&rsquo;m correct?</h4>\n<p>Suppose I conclude I have a contrarian view, as I plausibly have about long-term AGI outcomes,<sup><a id=\"fnref:14\" class=\"footnote\" title=\"see footnote\" href=\"#fn:14\">[14]</a></sup> and as I might have about the technological feasibility of preserving myself via cryonics.<sup><a id=\"fnref:15\" class=\"footnote\" title=\"see footnote\" href=\"#fn:15\">[15]</a></sup> How much evidence do I need to conclude that my view is justified despite the informed disagreement of others?</p>\n<p>I&rsquo;ll try to tackle that question in a future post. Not surprisingly, my approach is a kind of <a href=\"/lw/hzu/model_combination_and_adjustment/\">model combination and adjustment</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p><small> <ol>\n<li id=\"fn:1\">\n<p>I don&rsquo;t have a concise definition for what counts as a &ldquo;contrarian view.&rdquo; In any case, I don&rsquo;t think that searching for an exact definition of &ldquo;contrarian view&rdquo; is what matters. In an email conversation with me, Holden Karnofsky concurred, making the point this way: &ldquo;I agree with you that the idea of &lsquo;contrarianism&rsquo; is tricky to define. I think things get a bit easier when you start looking for patterns that should worry you rather than trying to Platonically define contrarianism&hellip; I find &lsquo;Most smart people think I&rsquo;m bonkers about X&rsquo; and &lsquo;Most people who have studied X more than I have <em>plus seem to generally think like I do</em> think I&rsquo;m wrong about X&rsquo; both worrying; I find &lsquo;Most smart people think I&rsquo;m wrong about X&rsquo; and &lsquo;Most people who spend their lives studying X within a system that seems to be clearly dysfunctional and to have a bad track record think I&rsquo;m bonkers about X&rsquo; to be less worrying.&rdquo; <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:1\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:2\">\n<p>For a diverse set of perspectives on the social epistemology of disagreement and contrarianism not influenced (as far as I know) by the Overcoming Bias and Less Wrong conversations about the topic, see <a href=\"http://fitelson.org/seminar/christensen.pdf\">Christensen (2009)</a>; <a href=\"http://www.amazon.com/Cambridge-Expertise-Performance-Handbooks-Psychology-ebook/dp/B00AKE1Z1Y/\">Ericsson et al. (2006)</a>; <a href=\"http://www.iel.carloalberto.org/public/pavel.k/pkuchar-consensus.pdf\">Kuchar (forthcoming)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Miller-When-is-consensus-knowledge-based.pdf\">Miller (2013)</a>; <a href=\"http://andrewgelman.com/2009/12/14/how_do_i_form_m/\">Gelman (2009)</a>; <a href=\"http://www.bmartin.cc/pubs/95handbook2.pdf\">Martin &amp; Richards (1995)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Shwed-Bearman-The-temporal-structure-of-scientific-consensus-formation.pdf\">Schwed &amp; Bearman (2010)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Intemann-de-Melo-Martin-Are-there-limits-to-scientists-obligations-to-seek-and-engage-dissenters.pdf\">Intemann &amp; de Melo-Martin (2013)</a>. Also see Wikipedia&rsquo;s article on <a href=\"http://en.wikipedia.org/wiki/Scientific_consensus\">scientific consensus</a>. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:2\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:3\">\n<p>I suppose I should mention that my entire inquiry here is, <em>ala</em> <a href=\"http://www.amazon.com/Knowledge-Social-World-Alvin-Goldman-ebook/dp/B002B557GE/\">Goldman (1998)</a>, premised on the assumptions that (1) the point of epistemology is the pursuit of correspondence-theory truth, and (2) the point of <em>social</em> epistemology is to evaluate which social institutions and practices have instrumental value for producing true or well-calibrated beliefs. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:3\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:4\">\n<p>I borrow this line from <a href=\"http://consc.net/papers/progress.pdf\">Chalmers (2014)</a>: &ldquo;For much of the paper I am largely saying the obvious, but sometimes the obvious is worth saying so that less obvious things can be said from there.&rdquo; <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:4\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:5\">\n<p>Holden Karnofsky <a href=\"http://intelligence.org/2013/08/25/holden-karnofsky-interview/\">seems to agree</a>: &ldquo;I think effective altruism falls somewhere on the spectrum between &lsquo;contrarian view&rsquo; and &lsquo;unusual taste.&rsquo; My commitment to effective altruism is probably better characterized as &lsquo;wanting/choosing to be an effective altruist&rsquo; than as &lsquo;believing that effective altruism is correct.&rsquo;&rdquo; <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:5\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:6\">\n<p>Without such heuristics, we can also rather quickly arrive at contradictions. For example, the majority of scholars who specialize in Allah&rsquo;s existence believe that Allah is the One True God, and the majority of scholars who specialize in Yahweh&rsquo;s existence believe that Yahweh is the One True God. Consistency isn&rsquo;t everything, but contradictions like this should still be a warning sign. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:6\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:7\">\n<p>According to the <a href=\"http://philpapers.org/surveys/\">PhilPapers Surveys</a>, 72.8% of philosophers are atheists, 14.6% are theists, and 12.6% categorized themselves as &ldquo;other.&rdquo; If we look only at metaphysicians, atheism remains dominant at 73.7%. If we look only at analytic philosophers, we again see atheism at 76.3%. As for physicists: <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Larson-Witham-Scientists-are-still-keeping-the-faith.pdf\">Larson &amp; Witham (1997)</a> found that 77.9% of physicists and astronomers are disbelievers, and <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Pew-Research-Center-Scientific-achievements-less-prominent-than-a-decade-ago.pdf\">Pew Research Center (2009)</a> found that 71% of physicists and astronomers did not believe in a god. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:7\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:8\">\n<p>Muller &amp; Bostrom (forthcoming). &ldquo;Future Progress in Artificial Intelligence: A Poll Among Experts.&rdquo; <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:8\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:9\">\n<p>But, this is unclear. First, I haven&rsquo;t read the forthcoming paper, so I don&rsquo;t yet have the full results of the survey, along with all its important caveats. Second, distributions of expert opinion can vary widely between polls. For example, <a href=\"http://arxiv.org/pdf/1301.1069v1.pdf\">Schlosshauer et al. (2013)</a> reports the results of a poll given to participants in a 2011 quantum foundations conference (mostly physicists). When asked &ldquo;When will we have a working and useful quantum computer?&rdquo;, 9% said &ldquo;within 10 years,&rdquo; 42% said &ldquo;10&ndash;25 years,&rdquo; 30% said &ldquo;25&ndash;50 years,&rdquo; 0% said &ldquo;50&ndash;100 years,&rdquo; and 15% said &ldquo;never.&rdquo; But when the exact same questions were asked of participants at another quantum foundations conference just two years later, <a href=\"http://arxiv.org/pdf/1306.4646.pdf\">Norsen &amp; Nelson (2013)</a> report, the distribution of opinion was substantially different: 9% said &ldquo;within 10 years,&rdquo; 22% said &ldquo;10&ndash;25 years,&rdquo; 20% said &ldquo;25&ndash;50 years,&rdquo; 21% said &ldquo;50&ndash;100 years,&rdquo; and 12% said &ldquo;never.&rdquo; <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:9\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:10\">\n<p>I say &ldquo;they&rdquo; in this paragraph, but I consider myself to be a plausible candidate for an &ldquo;AGI impact expert,&rdquo; in that I&rsquo;m unusually familiar with the arguments and evidence typically brought to bear on questions of long-term AI outcomes. I <em>also</em> don&rsquo;t have a uniquely good track record on predicting long-term AI outcomes, nor am I among the discovered &ldquo;super forecasters.&rdquo; I haven&rsquo;t participated in IARPA&rsquo;s forecasting tournaments myself because it would just be too time consuming. I would, however, very much like to see these super forecasters grouped into teams and tasked with forecasting longer-term outcomes, so that we can begin to gather scientific data on which psychological and computational methods result in the best predictive outcomes when considering long-term questions. Given how long it takes to acquire these data, we should start as soon as possible. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:10\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:11\">\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Weiss-Shanteau-Decloaking-the-privileged-expert.pdf\">Weiss &amp; Shanteau (2012)</a> would call them &ldquo;privileged experts.&rdquo; <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:11\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:12\">\n<p>Beckstead&rsquo;s &ldquo;elite common sense&rdquo; prior and my &ldquo;mildly extrapolated elite opinion&rdquo; method are epistemic notions that involve some kind idealization or extrapolation of opinion. One earlier such proposal in social epistemology was Habermas&rsquo; &ldquo;ideal speech situation,&rdquo; a situation of unlimited discussion between free and equal humans. See Habermas&rsquo; &ldquo;Wahrheitstheorien&rdquo; in <a href=\"http://www.worldcat.org/title/wirklichkeit-und-reflexion-festschrift-walter-schulz-zum-60-geburtstag/oclc/462814955\">Schulz &amp; Fahrenbach (1973)</a> or, for an English description, <a href=\"http://www.amazon.com/The-Idea-Critical-Theory-Philosophy/dp/0521284228/\">Geuss (1981)</a>, pp. 65&ndash;66. See also the discussion in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Tucker-The-epistemic-significance-of-consensus.pdf\">Tucker (2003)</a>, pp. 502&ndash;504. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:12\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:13\">\n<p>Beckstead calls his method the &ldquo;elite common sense&rdquo; prior. I&rsquo;ve named my method differently for two reasons. First, I want to distinguish MEEO from Beckstead&rsquo;s prior, since I&rsquo;m using the method for a slightly different purpose. Second, I think &ldquo;elite common sense&rdquo; is a confusing term even for Beckstead&rsquo;s prior, since there&rsquo;s some extrapolation of views going on. But also, it&rsquo;s only a &ldquo;mild&rdquo; extrapolation &mdash; e.g. we aren&rsquo;t asking what elites would think if they knew <em>everything</em>, or if they could rewrite their cognitive software for better reasoning accuracy. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:13\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:14\">\n<p>My rough impression is that among the people who seem to have thought long and hard about AGI outcomes, and seem to me to exhibit fairly good epistemic practices on most issues, my view on AGI outcomes is still an outlier in its pessimism about the likelihood of desirable outcomes. But it&rsquo;s hard to tell: there haven&rsquo;t been systematic surveys of the important-to-me experts on the issue. I also wonder whether my views about long-term AGI outcomes are more a matter of seriously tackling a contrarian <em>question</em> rather than being a matter of having a particularly contrarian <em>view</em>. On this latter point, see <a href=\"https://www.facebook.com/lukeprog/posts/10104316308390320?stream_ref=5\">this Facebook discussion</a>. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:14\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:15\">\n<p>I haven&rsquo;t seen a poll of cryobiologists on the likely future technological feasibility of cryonics. Even if there were such polls, I&rsquo;d wonder whether cryobiologists <em>also</em> had the relevant philosophical and neuroscientific expertise. I should mention that I&rsquo;m not personally signed up for cryonics, for <a href=\"/lw/guy/open_thread_march_115_2013/8kin\">these reasons</a>. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:15\">&nbsp;\u21a9</a></p>\n</li>\n</ol> </small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kFiz7Etau5HNMdKdx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 33, "extendedScore": null, "score": 1.6065401787259293e-06, "legacy": true, "legacyId": "25742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Previously: <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">Contrarian Excuses</a>, <a href=\"/lw/1kh/the_correct_contrarian_cluster/\">The Correct Contrarian Cluster</a>, <a href=\"/lw/28i/what_is_bunk/\">What is bunk?</a>, <a href=\"/lw/iao/common_sense_as_a_prior/\">Common Sense as a Prior</a>, <a href=\"/lw/iu0/trusting_expert_consensus/\">Trusting Expert Consensus</a>, <a href=\"http://www.overcomingbias.com/2014/03/prefer-contrarian-questions-vs-answers.html\">Prefer Contrarian Questions</a>.</small></p>\n<p>Robin Hanson once <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">wrote</a>:</p>\n<blockquote>\n<p>On average, contrarian views are less accurate than standard views. Honest contrarians should admit this, that neutral outsiders should assign most contrarian views a lower probability than standard views, though perhaps a high enough probability to warrant further investigation. Honest contrarians who expect reasonable outsiders to give their contrarian view more than normal credence should point to strong outside indicators that correlate enough with contrarians tending more to be right.</p>\n</blockquote>\n<p>I tend to think through the issue in three stages:</p>\n<ol>\n<li>When should I consider myself to be holding a contrarian<sup><a id=\"fnref:1\" class=\"footnote\" title=\"see footnote\" href=\"#fn:1\">[1]</a></sup> view? What is the relevant expert community?</li>\n<li>If I seem to hold a contrarian view, when do <em>I</em> have enough reason to think I\u2019m correct?</li>\n<li>If I seem to hold a <em>correct</em> contrarian view, what can I do to give <em>other</em> people good reasons to accept my view, or at least to take it seriously enough to examine it at length?</li>\n</ol>\n<p>I don\u2019t yet feel that I have \u201canswers\u201d to these questions, but in this post (and hopefully some future posts) I\u2019d like to organize some of what has been said before,<sup><a id=\"fnref:2\" class=\"footnote\" title=\"see footnote\" href=\"#fn:2\">[2]</a></sup> and push things a bit further along, in the hope that further discussion and inquiry will contribute toward significant progress in <a href=\"http://plato.stanford.edu/entries/epistemology-social/\">social epistemology</a>.<sup><a id=\"fnref:3\" class=\"footnote\" title=\"see footnote\" href=\"#fn:3\">[3]</a></sup> Basically, I hope to say a bunch of obvious things, in a relatively well-organized fashion, so that less obvious things can be said from there.<sup><a id=\"fnref:4\" class=\"footnote\" title=\"see footnote\" href=\"#fn:4\">[4]</a></sup></p>\n<p>In this post, I\u2019ll just address stage 1. Hopefully I\u2019ll have time to revisit stages 2 and 3 in future posts.</p>\n<p>&nbsp;</p>\n<h4 id=\"Is_my_view_contrarian_\">Is my view contrarian?</h4>\n<h5>World model differences vs. value differences</h5>\n<p>Is my <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">effective altruism</a> a contrarian view? It seems to be more of a contrarian <em>value judgment</em> than a contrarian <em>world model</em>,<sup><a id=\"fnref:5\" class=\"footnote\" title=\"see footnote\" href=\"#fn:5\">[5]</a></sup> and by \u201ccontrarian view\u201d I tend to mean \u201ccontrarian world model.\u201d Some apparently contrarian views are probably actually contrarian <em>values</em>.</p>\n<p>&nbsp;</p>\n<h5>Expert consensus</h5>\n<p>Is my <a href=\"http://commonsenseatheism.com/\">atheism</a> a contrarian view? It\u2019s definitely a world model, not a value judgment, and <a href=\"https://www.cia.gov/library/publications/the-world-factbook/fields/2122.html#xx\">only 2% of people are atheists</a>.</p>\n<p>But what\u2019s the relevant <em>expert</em> population, here? Suppose it\u2019s \u201cacademics who specialize in the arguments and evidence concerning whether a god or gods exist.\u201d If so, then the expert population is probably dominated by academic theologians and religious philosophers, and my atheism is a contrarian view.</p>\n<p>We need <a href=\"/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/\">some heuristics</a> for evaluating the soundness of the academic consensus in different fields. <sup><a id=\"fnref:6\" class=\"footnote\" title=\"see footnote\" href=\"#fn:6\">[6]</a></sup></p>\n<p>For example, we should consider the selection effects operating on communities of experts. If someone doesn\u2019t believe in God, they\u2019re unlikely to spend their career studying arcane arguments for and against God\u2019s existence. So most people who specialize in this topic are theists, but nearly all of them were theists <em>before</em> they knew the arguments.</p>\n<p>Perhaps instead the relevant expert community is \u201cscholars who study the fundamental nature of the universe\u201d \u2014 maybe, philosophers and physicists? They\u2019re mostly atheists. <sup><a id=\"fnref:7\" class=\"footnote\" title=\"see footnote\" href=\"#fn:7\">[7]</a></sup> This is starting to get pretty ad-hoc, but maybe that\u2019s unavoidable.</p>\n<p>What about my view that the overall long-term impact of <a href=\"http://intelligence.org/2013/08/11/what-is-agi/\">AGI</a> will be, most likely, extremely bad? A recent survey of the top 100 authors in artificial intelligence (by citation index)<sup><a id=\"fnref:8\" class=\"footnote\" title=\"see footnote\" href=\"#fn:8\">[8]</a></sup> suggests that my view is somewhat out of sync with the views of those researchers.<sup><a id=\"fnref:9\" class=\"footnote\" title=\"see footnote\" href=\"#fn:9\">[9]</a></sup> But is that the relevant expert population? My impression is that AI experts know a lot about contemporary AI methods, especially within their subfield, but usually haven\u2019t thought much about, or read much about, long-term AI impacts.</p>\n<p>Instead, perhaps I\u2019d need to survey \u201c<a href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI impact experts</a>\u201d to tell whether my view is contrarian. But who is that, exactly? There\u2019s no standard credential.</p>\n<p>Moreover, the most plausible candidates around today for \u201cAGI impact experts\u201d are \u2014 like the \u201cexperts\u201d of many other fields \u2014 mere \u201cscholastic experts,\u201d in that they<sup><a id=\"fnref:10\" class=\"footnote\" title=\"see footnote\" href=\"#fn:10\">[10]</a></sup> know a lot about the arguments and evidence typically brought to bear on questions of long-term AI outcomes.<sup><a id=\"fnref:11\" class=\"footnote\" title=\"see footnote\" href=\"#fn:11\">[11]</a></sup> They generally are <em>not</em> experts in the sense of \u201c<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/12/Ericsson-An-Introduction-to-Cambridge-Handbook-of-Expertise-and-Expert-Performance.pdf\">Reliably superior performance on representative tasks</a>\u201d \u2014 they don\u2019t have uniquely good track records on predicting long-term AI outcomes, for example. As far as I know, they don\u2019t even have uniquely good track records on predicting short-term geopolitical or sci-tech outcomes \u2014 e.g. they aren\u2019t among the \u201c<a href=\"http://www.edge.org/conversation/how-to-win-at-forecasting\">super forecasters</a>\u201d discovered in <a href=\"http://www.iarpa.gov/Programs/ia/ACE/ace.html\">IARPA\u2019s forecasting tournaments</a>.</p>\n<p>Furthermore, we might start to worry about selection effects, again. E.g. if we ask AGI experts when they think AGI will be built, they may be overly optimistic about the timeline: after all, if they didn\u2019t think AGI was feasible soon, they probably wouldn\u2019t be focusing their careers on it.</p>\n<p>Perhaps we can salvage this approach for determining whether one has a contrarian view, but for now, let\u2019s consider another proposal.</p>\n<p>&nbsp;</p>\n<h5>Mildly extrapolated elite opinion</h5>\n<p>Nick Beckstead instead <a href=\"/lw/iao/common_sense_as_a_prior/\">suggests </a> that, at least as a strong prior, one should believe what one thinks \u201ca broad coalition of trustworthy people would believe if they were trying to have accurate views and they had access to [one\u2019s own] evidence.\u201d<sup><a id=\"fnref:12\" class=\"footnote\" title=\"see footnote\" href=\"#fn:12\">[12]</a></sup> Below, I\u2019ll propose a modification of Beckstead\u2019s approach which aims to address the \u201cIs my view contrarian?\u201d question, and I\u2019ll call it the \u201cmildly extrapolated elite opinion\u201d (MEEO) method for determining the relevant expert population. <sup><a id=\"fnref:13\" class=\"footnote\" title=\"see footnote\" href=\"#fn:13\">[13]</a></sup></p>\n<p>First: which people are \u201ctrustworthy\u201d? With Beckstead, I favor \u201cgiving more weight to the opinions of people who can be shown to be trustworthy by clear indicators that many people would accept, rather than people that seem trustworthy to you personally.\u201d (This guideline aims to avoid parochialism and self-serving cognitive biases.)</p>\n<p>What are some \u201cclear indicators that many people would accept\u201d? Beckstead suggests:</p>\n<blockquote>\n<p>IQ, business success, academic success, generally respected scientific or other intellectual achievements, wide acceptance as an intellectual authority by certain groups of people, or success in any area where there is intense competition and success is a function of ability to make accurate predictions and good decisions\u2026</p>\n</blockquote>\n<blockquote>\n<p>Of course, trustworthiness can also be domain-specific. Very often, elite common sense would recommend deferring to the opinions of experts (e.g., listening to what physicists say about physics, what biologists say about biology, and what doctors say about medicine). In other cases, elite common sense may give partial weight to what putative experts say without accepting it all (e.g. economics and psychology). In other cases, they may give less weight to what putative experts say (e.g. sociology and philosophy).</p>\n</blockquote>\n<p>Hence MEEO outsources the challenge of evaluating academic consensus in different fields to the \u201cgenerally trustworthy people.\u201d But in doing so, it raises several new challenges. How do we determine which people are trustworthy? How do we \u201cmildly extrapolate\u201d their opinions? How do we weight those mildly extrapolated opinions in combination?</p>\n<p>This approach might also be promising, or it might be even harder to use than the \u201cexpert consensus\u201d method.</p>\n<p>&nbsp;</p>\n<h4 id=\"My_approach\">My approach</h4>\n<p>In practice, I tend to do something like this:</p>\n<ul>\n<li>To determine whether my view is contrarian, I ask whether there\u2019s a fairly obvious, relatively trustworthy expert population on the issue. If there is, I try to figure out what their consensus on the matter is. If it\u2019s different than my view, I conclude I have a contrarian view.</li>\n<li>If there <em>isn\u2019t</em> an obvious trustworthy expert population on the issue from which to extract a consensus view, then I basically give up on step 1 (\u201cIs my view contrarian?\u201d) and just move to the model combination in step 2 (see below), retaining pretty large uncertainty about how contrarian my view might be.</li>\n</ul>\n<h4><br></h4>\n<h4 id=\"When_do_I_have_good_reason_to_think_I_m_correct_\">When do I have good reason to think I\u2019m correct?</h4>\n<p>Suppose I conclude I have a contrarian view, as I plausibly have about long-term AGI outcomes,<sup><a id=\"fnref:14\" class=\"footnote\" title=\"see footnote\" href=\"#fn:14\">[14]</a></sup> and as I might have about the technological feasibility of preserving myself via cryonics.<sup><a id=\"fnref:15\" class=\"footnote\" title=\"see footnote\" href=\"#fn:15\">[15]</a></sup> How much evidence do I need to conclude that my view is justified despite the informed disagreement of others?</p>\n<p>I\u2019ll try to tackle that question in a future post. Not surprisingly, my approach is a kind of <a href=\"/lw/hzu/model_combination_and_adjustment/\">model combination and adjustment</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p><small> </small></p><ol><small>\n<li id=\"fn:1\">\n<p>I don\u2019t have a concise definition for what counts as a \u201ccontrarian view.\u201d In any case, I don\u2019t think that searching for an exact definition of \u201ccontrarian view\u201d is what matters. In an email conversation with me, Holden Karnofsky concurred, making the point this way: \u201cI agree with you that the idea of \u2018contrarianism\u2019 is tricky to define. I think things get a bit easier when you start looking for patterns that should worry you rather than trying to Platonically define contrarianism\u2026 I find \u2018Most smart people think I\u2019m bonkers about X\u2019 and \u2018Most people who have studied X more than I have <em>plus seem to generally think like I do</em> think I\u2019m wrong about X\u2019 both worrying; I find \u2018Most smart people think I\u2019m wrong about X\u2019 and \u2018Most people who spend their lives studying X within a system that seems to be clearly dysfunctional and to have a bad track record think I\u2019m bonkers about X\u2019 to be less worrying.\u201d <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:1\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:2\">\n<p>For a diverse set of perspectives on the social epistemology of disagreement and contrarianism not influenced (as far as I know) by the Overcoming Bias and Less Wrong conversations about the topic, see <a href=\"http://fitelson.org/seminar/christensen.pdf\">Christensen (2009)</a>; <a href=\"http://www.amazon.com/Cambridge-Expertise-Performance-Handbooks-Psychology-ebook/dp/B00AKE1Z1Y/\">Ericsson et al. (2006)</a>; <a href=\"http://www.iel.carloalberto.org/public/pavel.k/pkuchar-consensus.pdf\">Kuchar (forthcoming)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Miller-When-is-consensus-knowledge-based.pdf\">Miller (2013)</a>; <a href=\"http://andrewgelman.com/2009/12/14/how_do_i_form_m/\">Gelman (2009)</a>; <a href=\"http://www.bmartin.cc/pubs/95handbook2.pdf\">Martin &amp; Richards (1995)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Shwed-Bearman-The-temporal-structure-of-scientific-consensus-formation.pdf\">Schwed &amp; Bearman (2010)</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Intemann-de-Melo-Martin-Are-there-limits-to-scientists-obligations-to-seek-and-engage-dissenters.pdf\">Intemann &amp; de Melo-Martin (2013)</a>. Also see Wikipedia\u2019s article on <a href=\"http://en.wikipedia.org/wiki/Scientific_consensus\">scientific consensus</a>. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:2\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:3\">\n<p>I suppose I should mention that my entire inquiry here is, <em>ala</em> <a href=\"http://www.amazon.com/Knowledge-Social-World-Alvin-Goldman-ebook/dp/B002B557GE/\">Goldman (1998)</a>, premised on the assumptions that (1) the point of epistemology is the pursuit of correspondence-theory truth, and (2) the point of <em>social</em> epistemology is to evaluate which social institutions and practices have instrumental value for producing true or well-calibrated beliefs. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:3\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:4\">\n<p>I borrow this line from <a href=\"http://consc.net/papers/progress.pdf\">Chalmers (2014)</a>: \u201cFor much of the paper I am largely saying the obvious, but sometimes the obvious is worth saying so that less obvious things can be said from there.\u201d <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:4\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:5\">\n<p>Holden Karnofsky <a href=\"http://intelligence.org/2013/08/25/holden-karnofsky-interview/\">seems to agree</a>: \u201cI think effective altruism falls somewhere on the spectrum between \u2018contrarian view\u2019 and \u2018unusual taste.\u2019 My commitment to effective altruism is probably better characterized as \u2018wanting/choosing to be an effective altruist\u2019 than as \u2018believing that effective altruism is correct.\u2019\u201d <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:5\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:6\">\n<p>Without such heuristics, we can also rather quickly arrive at contradictions. For example, the majority of scholars who specialize in Allah\u2019s existence believe that Allah is the One True God, and the majority of scholars who specialize in Yahweh\u2019s existence believe that Yahweh is the One True God. Consistency isn\u2019t everything, but contradictions like this should still be a warning sign. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:6\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:7\">\n<p>According to the <a href=\"http://philpapers.org/surveys/\">PhilPapers Surveys</a>, 72.8% of philosophers are atheists, 14.6% are theists, and 12.6% categorized themselves as \u201cother.\u201d If we look only at metaphysicians, atheism remains dominant at 73.7%. If we look only at analytic philosophers, we again see atheism at 76.3%. As for physicists: <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Larson-Witham-Scientists-are-still-keeping-the-faith.pdf\">Larson &amp; Witham (1997)</a> found that 77.9% of physicists and astronomers are disbelievers, and <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Pew-Research-Center-Scientific-achievements-less-prominent-than-a-decade-ago.pdf\">Pew Research Center (2009)</a> found that 71% of physicists and astronomers did not believe in a god. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:7\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:8\">\n<p>Muller &amp; Bostrom (forthcoming). \u201cFuture Progress in Artificial Intelligence: A Poll Among Experts.\u201d <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:8\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:9\">\n<p>But, this is unclear. First, I haven\u2019t read the forthcoming paper, so I don\u2019t yet have the full results of the survey, along with all its important caveats. Second, distributions of expert opinion can vary widely between polls. For example, <a href=\"http://arxiv.org/pdf/1301.1069v1.pdf\">Schlosshauer et al. (2013)</a> reports the results of a poll given to participants in a 2011 quantum foundations conference (mostly physicists). When asked \u201cWhen will we have a working and useful quantum computer?\u201d, 9% said \u201cwithin 10 years,\u201d 42% said \u201c10\u201325 years,\u201d 30% said \u201c25\u201350 years,\u201d 0% said \u201c50\u2013100 years,\u201d and 15% said \u201cnever.\u201d But when the exact same questions were asked of participants at another quantum foundations conference just two years later, <a href=\"http://arxiv.org/pdf/1306.4646.pdf\">Norsen &amp; Nelson (2013)</a> report, the distribution of opinion was substantially different: 9% said \u201cwithin 10 years,\u201d 22% said \u201c10\u201325 years,\u201d 20% said \u201c25\u201350 years,\u201d 21% said \u201c50\u2013100 years,\u201d and 12% said \u201cnever.\u201d <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:9\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:10\">\n<p>I say \u201cthey\u201d in this paragraph, but I consider myself to be a plausible candidate for an \u201cAGI impact expert,\u201d in that I\u2019m unusually familiar with the arguments and evidence typically brought to bear on questions of long-term AI outcomes. I <em>also</em> don\u2019t have a uniquely good track record on predicting long-term AI outcomes, nor am I among the discovered \u201csuper forecasters.\u201d I haven\u2019t participated in IARPA\u2019s forecasting tournaments myself because it would just be too time consuming. I would, however, very much like to see these super forecasters grouped into teams and tasked with forecasting longer-term outcomes, so that we can begin to gather scientific data on which psychological and computational methods result in the best predictive outcomes when considering long-term questions. Given how long it takes to acquire these data, we should start as soon as possible. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:10\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:11\">\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/02/Weiss-Shanteau-Decloaking-the-privileged-expert.pdf\">Weiss &amp; Shanteau (2012)</a> would call them \u201cprivileged experts.\u201d <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:11\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:12\">\n<p>Beckstead\u2019s \u201celite common sense\u201d prior and my \u201cmildly extrapolated elite opinion\u201d method are epistemic notions that involve some kind idealization or extrapolation of opinion. One earlier such proposal in social epistemology was Habermas\u2019 \u201cideal speech situation,\u201d a situation of unlimited discussion between free and equal humans. See Habermas\u2019 \u201cWahrheitstheorien\u201d in <a href=\"http://www.worldcat.org/title/wirklichkeit-und-reflexion-festschrift-walter-schulz-zum-60-geburtstag/oclc/462814955\">Schulz &amp; Fahrenbach (1973)</a> or, for an English description, <a href=\"http://www.amazon.com/The-Idea-Critical-Theory-Philosophy/dp/0521284228/\">Geuss (1981)</a>, pp. 65\u201366. See also the discussion in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/03/Tucker-The-epistemic-significance-of-consensus.pdf\">Tucker (2003)</a>, pp. 502\u2013504. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:12\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:13\">\n<p>Beckstead calls his method the \u201celite common sense\u201d prior. I\u2019ve named my method differently for two reasons. First, I want to distinguish MEEO from Beckstead\u2019s prior, since I\u2019m using the method for a slightly different purpose. Second, I think \u201celite common sense\u201d is a confusing term even for Beckstead\u2019s prior, since there\u2019s some extrapolation of views going on. But also, it\u2019s only a \u201cmild\u201d extrapolation \u2014 e.g. we aren\u2019t asking what elites would think if they knew <em>everything</em>, or if they could rewrite their cognitive software for better reasoning accuracy. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:13\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:14\">\n<p>My rough impression is that among the people who seem to have thought long and hard about AGI outcomes, and seem to me to exhibit fairly good epistemic practices on most issues, my view on AGI outcomes is still an outlier in its pessimism about the likelihood of desirable outcomes. But it\u2019s hard to tell: there haven\u2019t been systematic surveys of the important-to-me experts on the issue. I also wonder whether my views about long-term AGI outcomes are more a matter of seriously tackling a contrarian <em>question</em> rather than being a matter of having a particularly contrarian <em>view</em>. On this latter point, see <a href=\"https://www.facebook.com/lukeprog/posts/10104316308390320?stream_ref=5\">this Facebook discussion</a>. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:14\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:15\">\n<p>I haven\u2019t seen a poll of cryobiologists on the likely future technological feasibility of cryonics. Even if there were such polls, I\u2019d wonder whether cryobiologists <em>also</em> had the relevant philosophical and neuroscientific expertise. I should mention that I\u2019m not personally signed up for cryonics, for <a href=\"/lw/guy/open_thread_march_115_2013/8kin\">these reasons</a>. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:15\">&nbsp;\u21a9</a></p>\n</li>\n</small></ol><small> </small><p></p>", "sections": [{"title": "Is my view contrarian?", "anchor": "Is_my_view_contrarian_", "level": 1}, {"title": "My approach", "anchor": "My_approach", "level": 1}, {"title": "When do I have good reason to think I\u2019m correct?", "anchor": "When_do_I_have_good_reason_to_think_I_m_correct_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "96 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9KvefburLia7ptEE3", "XiSCHS3Xu3a6EC7e6", "wgdfxQJ2DQuju73zC", "R8YpYTq8LoD3k948L", "JmmA2Mf5GrY9D6nQD", "fyZBtNB3Ki3fM4a6Y", "iyRpsScBa6y4rduEt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-11T21:35:41.415Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels - all fun and games", "slug": "meetup-brussels-all-fun-and-games", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d2HSvDxDieKGbJctR/meetup-brussels-all-fun-and-games", "pageUrlRelative": "/posts/d2HSvDxDieKGbJctR/meetup-brussels-all-fun-and-games", "linkUrl": "https://www.lesswrong.com/posts/d2HSvDxDieKGbJctR/meetup-brussels-all-fun-and-games", "postedAtFormatted": "Tuesday, March 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20-%20all%20fun%20and%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20-%20all%20fun%20and%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd2HSvDxDieKGbJctR%2Fmeetup-brussels-all-fun-and-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20-%20all%20fun%20and%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd2HSvDxDieKGbJctR%2Fmeetup-brussels-all-fun-and-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd2HSvDxDieKGbJctR%2Fmeetup-brussels-all-fun-and-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xu'>Brussels - all fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 April 2014 12:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Because of a scheduling conflict with the Berlin community meetup, April's meetup will <em>not</em> be on the second Saturday of the month. It's one week later.)</p>\n\n<p>No serious theme for this month. We'll probably chat about Berlin, play boardgames that aren't rationalist training, and just talk about anything we want.</p>\n\n<p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xu'>Brussels - all fun and games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d2HSvDxDieKGbJctR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.6068232140268047e-06, "legacy": true, "legacyId": "25743", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels___all_fun_and_games\">Discussion article for the meetup : <a href=\"/meetups/xu\">Brussels - all fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 April 2014 12:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Because of a scheduling conflict with the Berlin community meetup, April's meetup will <em>not</em> be on the second Saturday of the month. It's one week later.)</p>\n\n<p>No serious theme for this month. We'll probably chat about Berlin, play boardgames that aren't rationalist training, and just talk about anything we want.</p>\n\n<p>We will meet at 1 pm at \"La Fleur en papier dor\u00e9\", close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">this one minute form</a> to share your contact information.</p>\n\n<p>The Brussels meetup group communicates through a <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\">Google Group</a>.</p>\n\n<p>Meetup announcements are also mirrored on <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">meetup.com</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels___all_fun_and_games1\">Discussion article for the meetup : <a href=\"/meetups/xu\">Brussels - all fun and games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels - all fun and games", "anchor": "Discussion_article_for_the_meetup___Brussels___all_fun_and_games", "level": 1}, {"title": "Discussion article for the meetup : Brussels - all fun and games", "anchor": "Discussion_article_for_the_meetup___Brussels___all_fun_and_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-11T22:45:10.121Z", "modifiedAt": null, "url": null, "title": "Open thread, 11-17 March 2014", "slug": "open-thread-11-17-march-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:32.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fE7TBHp95eQLBXdBc/open-thread-11-17-march-2014", "pageUrlRelative": "/posts/fE7TBHp95eQLBXdBc/open-thread-11-17-march-2014", "linkUrl": "https://www.lesswrong.com/posts/fE7TBHp95eQLBXdBc/open-thread-11-17-march-2014", "postedAtFormatted": "Tuesday, March 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%2011-17%20March%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%2011-17%20March%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfE7TBHp95eQLBXdBc%2Fopen-thread-11-17-march-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%2011-17%20March%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfE7TBHp95eQLBXdBc%2Fopen-thread-11-17-march-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfE7TBHp95eQLBXdBc%2Fopen-thread-11-17-march-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_jtb\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fE7TBHp95eQLBXdBc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "25744", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 227, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-12T04:52:40.026Z", "modifiedAt": null, "url": null, "title": "Channel factors", "slug": "channel-factors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:08.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "benkuhn", "createdAt": "2013-05-28T21:13:11.124Z", "isAdmin": false, "displayName": "benkuhn"}, "userId": "cdndD4NnRf6ud2hL5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DCmk7xdxW5ivaGYGx/channel-factors", "pageUrlRelative": "/posts/DCmk7xdxW5ivaGYGx/channel-factors", "linkUrl": "https://www.lesswrong.com/posts/DCmk7xdxW5ivaGYGx/channel-factors", "postedAtFormatted": "Wednesday, March 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Channel%20factors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChannel%20factors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDCmk7xdxW5ivaGYGx%2Fchannel-factors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Channel%20factors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDCmk7xdxW5ivaGYGx%2Fchannel-factors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDCmk7xdxW5ivaGYGx%2Fchannel-factors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 973, "htmlBody": "<p><em>Or, &ldquo;how not to make a fundamental attribution error on yourself;&rdquo; or, &ldquo;how to do that thing that you keep being frustrated at yourself for not doing;&rdquo; or, &ldquo;finding and solving trivial but leveraged inconveniences.&rdquo;</em></p>\n<p><em><a id=\"more\"></a></em></p>\n<p><em>Note: cross-posted from&nbsp;<a href=\"http://www.benkuhn.net/channel-factors\">my blog</a>, so some of this may be rather elementary to LessWrong readers or CFAR workshop attendees. If that's you, feel free to skip or skim to the end, where I try to crowdsource a list of interesting channel factors.</em></p>\n<hr />\n<p>One of the key insights of social psychology is that our reactions to events are hugely dependent on the fine details of the situation in question, and often pretty much independent of personality. For instance, suppose you have a bunch of people to playing the Prisoner&rsquo;s Dilemma with each other, and you want to figure out who will defect. Most people&rsquo;s theory of mind here says something like, &ldquo;people defect because they&rsquo;re self-interested or grumpy or vindictive.&rdquo; So you might ask a player&rsquo;s friends how cooperative they were, and use that to guess who will cooperate.</p>\n<p>Unfortunately, this approach is totally useless. People are equally likely to cooperate whether or not their friends think they&rsquo;re cooperative. In the Prisoners&rsquo; Dilemma, your friends&rsquo; assessment of your personality <em>does not correlate at all with your behavior.</em></p>\n<p>Fortunately, there&rsquo;s something else which is a pretty good predictor! In particular, it matters a <em>lot</em> whether the instructions of the game called it the &ldquo;Community Game&rdquo; or the &ldquo;Wall Street Game.&rdquo;<sup id=\"fnref:liberman\"><a class=\"footnote-ref\" rel=\"footnote\" href=\"#fn:liberman\">1</a></sup></p>\n<p><img src=\"https://s3.amazonaws.com/bknet/liberman_2004.png\" alt=\"A graph of Liberman's results.\" /></p>\n<p>Yep, a <em>single phrase</em> of the instructions, repeated twice, causes cooperation rates to double. If you ever like to think of yourself as some kind of agent whose decisions are controlled by a rational ego (instead of some random words you heard once upon a time), you might find that a bit worrying.</p>\n<p>On the other hand, if you like to think of yourself as the kind of person who prefers to have true beliefs, you might be excited because your beliefs just got truer! You probably sometimes fail to impose egoistic control on your own decisions, and if you understand your brain purely as a consciously-controlled ego then you will be really confused when this happens, and spit out solutions like &ldquo;try harder next time&rdquo; or &ldquo;have more willpower&rdquo; or &ldquo;don&rsquo;t be dumb&rdquo; or &ldquo;don&rsquo;t get distracted&rdquo; or something.</p>\n<p>These are fake solutions. They would totally solve your problem, except that they&rsquo;re not really actions you can take.</p>\n<p>So you should change your model. There are examples everywhere of seemingly trivial changes to circumstances that disproportionately change your behavior. Another example in the literature is that students advised to get tetanus inoculations were far more likely to do so if they were given a map to the university health center and times of operation, which the majority of the students already knew.<sup id=\"fnref:leventhal\"><a class=\"footnote-ref\" rel=\"footnote\" href=\"#fn:leventhal\">2</a></sup> (Of course, intent to get an inoculation had almost no predictive power unless students were given a map.)</p>\n<p>The psychology term for such things is a &ldquo;channel factor,&rdquo; and it&rsquo;s probably the most useful psychology concept I&rsquo;ve learned this semester. Since acquiring it, I&rsquo;ve noticed it cropping up a lot. For instance:</p>\n<ul>\n<li>\n<p>For flossing, a channel factor for me is availability of those little plastic flossy things. Obviously flossing is good enough that I should do it even without handy newfangled devices to help. But while I know this on some level, flossing is time-consuming and unpleasant enough without the plastic aids that I can never really be bothered.</p>\n</li>\n<li>\n<p>For listening to music and calling friends/family/people who put me on hold for a while: wireless headphones. I didn&rsquo;t realize how uncomfortable it was to walk around with a phone jammed up to my ear until I bought a pair of <a href=\"http://www.amazon.com/LG-Electronics-HBS-730-Bluetooth-Headset/dp/B009A5204K/ref=sr_1_2?ie=UTF8&amp;qid=1394508524&amp;sr=8-2&amp;keywords=lg+tone\">these</a> and suddenly became much more excited about phone calls.</p>\n</li>\n<li>\n<p>For doing laundry routinely, detergent packets. I used to have the problem of leaving my detergent in my dorm&rsquo;s public laundry room, where it would get removed by well-meaning people and disappear before I realized that it was gone. Then I wouldn&rsquo;t have any detergent, and I would feel bad about doing laundry and be too lazy to buy new detergent.</p>\n</li>\n</ul>\n<p>The common thread is that the inconvenience is so trivial that I didn&rsquo;t even <em>notice</em> it until I was specifically on the lookout for channel factors. My usual model of myself is too sane to do things like not floss because I didn&rsquo;t have the thing that made my fingers slightly more comfortable while I do it. Whoops.</p>\n<p>Anyway, this results in a heuristic of, when I am frustrated by the fact that I &lsquo;end up&rsquo; not doing something I &lsquo;want&rsquo; to do, looking for channel factors. So far, it&rsquo;s been pretty successful at uncovering small modifications with large effects:</p>\n<ul>\n<li>\n<p>closing browser tabs as soon as I&rsquo;m done with them</p>\n</li>\n<li>\n<p>avoiding checking email like the plague</p>\n</li>\n<li>\n<p>using <a href=\"http://mightytext.net/\">mightytext</a> to send texts from my computer<sup id=\"fnref:mighty\"><a class=\"footnote-ref\" rel=\"footnote\" href=\"#fn:mighty\">3</a></sup></p>\n</li>\n</ul>\n<p>Have you discovered any channel factors of your own? What am I missing?</p>\n<div class=\"footnote\">\n<hr />\n<ol>\n<li id=\"fn:liberman\">\n<p>Lieberman, Varda, et al. &ldquo;The Name of the Game: Predictive Power of Reputations versus Situational Labels in Determining Prisoner&rsquo;s Dilemma Game Moves.&rdquo; <em>Personality and Social Psychology Bulletin</em>, September 2004 vol. 30 no. 9: 1175-1185. doi:<a href=\"http://dx.doi.org/10.1177%2F0146167204264004\">10.1177/0146167204264004</a>.&nbsp;<a class=\"footnote-backref\" title=\"Jump back to footnote 1 in the text\" rev=\"footnote\" href=\"#fnref:liberman\">\u21a9</a></p>\n</li>\n<li id=\"fn:leventhal\">\n<p>Leventhal, Howard, et al. &ldquo;Effects of fear and specificity of recommendation upon attitudes and behavior.&rdquo; <em>Journal of Personality and Social Psychology</em>, Vol 2(1), Jul 1965: 20-29. doi:<a href=\"http://dx.doi.org/10.1037%2Fh0022089\">10.1037/h0022089</a>.&nbsp;<a class=\"footnote-backref\" title=\"Jump back to footnote 2 in the text\" rev=\"footnote\" href=\"#fnref:leventhal\">\u21a9</a></p>\n</li>\n<li id=\"fn:mighty\">\n<p>Apparently staying in touch with classmates is just not worth the extra 10 seconds per text on my phone keyboard.&nbsp;<a class=\"footnote-backref\" title=\"Jump back to footnote 3 in the text\" rev=\"footnote\" href=\"#fnref:mighty\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DCmk7xdxW5ivaGYGx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 27, "extendedScore": null, "score": 1.6073545849766653e-06, "legacy": true, "legacyId": "25751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-12T08:00:39.552Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2ZLbNWK7bTHqETor6/meetup-helsinki-meetup-3", "pageUrlRelative": "/posts/2ZLbNWK7bTHqETor6/meetup-helsinki-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/2ZLbNWK7bTHqETor6/meetup-helsinki-meetup-3", "postedAtFormatted": "Wednesday, March 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZLbNWK7bTHqETor6%2Fmeetup-helsinki-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZLbNWK7bTHqETor6%2Fmeetup-helsinki-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ZLbNWK7bTHqETor6%2Fmeetup-helsinki-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xv'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 March 2014 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019re having a social meetup in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a>. To find us there, look for someone wearing a pink elephant hat.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xv'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2ZLbNWK7bTHqETor6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.6075832820999001e-06, "legacy": true, "legacyId": "25754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/xv\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 March 2014 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We\u2019re having a social meetup in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a>. To find us there, look for someone wearing a pink elephant hat.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/xv\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-12T13:51:00.555Z", "modifiedAt": null, "url": null, "title": "Irrationality Game III", "slug": "irrationality-game-iii", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:38.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CellBioGuy", "createdAt": "2012-12-20T06:26:48.044Z", "isAdmin": false, "displayName": "CellBioGuy"}, "userId": "EGYsGxCKPSxXQACm8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RjpM7Mnfq4TAojdSv/irrationality-game-iii", "pageUrlRelative": "/posts/RjpM7Mnfq4TAojdSv/irrationality-game-iii", "linkUrl": "https://www.lesswrong.com/posts/RjpM7Mnfq4TAojdSv/irrationality-game-iii", "postedAtFormatted": "Wednesday, March 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Irrationality%20Game%20III&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIrrationality%20Game%20III%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjpM7Mnfq4TAojdSv%2Firrationality-game-iii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Irrationality%20Game%20III%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjpM7Mnfq4TAojdSv%2Firrationality-game-iii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjpM7Mnfq4TAojdSv%2Firrationality-game-iii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1185, "htmlBody": "<p>The '<a href=\"/lw/2sl/the_irrationality_game/\">Irrationality</a> <a href=\"/r/discussion/lw/df8/irrationality_game_ii_electric_boogaloo/\">Game</a>' posts in discussion came before my time here, but I had a very good time reading the bits written in the comments section.&nbsp; I also had a number of thoughts I would've liked to post and get feedback on, but I knew that being buried in such old threads not much would come of it.&nbsp; So I asked around and feedback from people has suggested that they would be open to a reboot!</p>\n<p>I hereby again quote the original rules:</p>\n<div style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<blockquote>\n<p style=\"margin: 0px 0px 1em;\"><big><strong>Please read the post before voting on the comments, as this is a game where voting works differently.</strong></big></p>\n<p style=\"margin: 0px 0px 1em;\"><em>Warning: the comments section of this post will look odd. The most reasonable comments will have lots of negative karma. Do not be alarmed, it's all part of the plan.</em><em>&nbsp;In order to participate in this game you should disable any viewing threshold for negatively voted comments.</em></p>\n<p style=\"margin: 0px 0px 1em;\">Here's an irrationalist game meant to quickly collect a pool of controversial ideas for people to debate and assess. It kinda relies on people being honest and not being nitpickers, but it might be fun.</p>\n<p style=\"margin: 0px 0px 1em;\">Write a comment reply to this post describing a belief you think has a reasonable chance of being true&nbsp;<em>relative to the the beliefs of other Less Wrong folk.</em>&nbsp;Jot down a proposition and a rough probability estimate or qualitative description, like 'fairly confident'.</p>\n<p style=\"margin: 0px 0px 1em;\">Example (not my true belief): \"The U.S. government was directly responsible for financing the September 11th terrorist attacks. Very confident. (~95%).\"</p>\n<p style=\"margin: 0px 0px 1em;\">If you post a belief, you have to vote on the beliefs of all other comments. Voting works like this:&nbsp;if you basically agree with the comment,&nbsp;<em>vote the comment down.&nbsp;</em>If you basically&nbsp;<em>disagree</em>&nbsp;with the comment,&nbsp;<em>vote the comment up.&nbsp;</em>What 'basically' means here is intuitive; instead of using a precise mathy scoring system, just make a guess. In my view, if their stated probability is 99.9% and your degree of belief is 90%, that merits an upvote: it's a pretty big difference of opinion. If they're at 99.9% and you're at 99.5%, it could go either way. If you're genuinely unsure whether or not you basically agree with them, you can pass on voting (but try not to). Vote up if you think they are either overconfident or underconfident in their belief: any disagreement is valid disagreement.</p>\n<p style=\"margin: 0px 0px 1em;\">That's the spirit of the game, but some more qualifications and rules follow.</p>\n<p style=\"margin: 0px 0px 1em;\">If the proposition in a comment isn't incredibly precise, use your best interpretation. If you&nbsp;<em>really</em>&nbsp;have to pick nits for whatever reason, say so in a comment reply.</p>\n<p style=\"margin: 0px 0px 1em;\"><em>The more upvotes you get, the more irrational Less Wrong perceives your belief to be.</em><strong>&nbsp;</strong>Which means that if you have a large amount of Less Wrong karma and can still get lots of upvotes on your crazy beliefs then you will get lots of smart people to take your weird ideas a little more seriously.</p>\n<p style=\"margin: 0px 0px 1em;\">Some poor soul is going to come along and post \"I believe in God\". Don't pick nits and say \"Well in a a Tegmark multiverse there is definitely a universe exactly like ours where some sort of god rules over us...\" and downvote it. That's cheating. You better upvote the guy. For just this post, get over your desire to upvote rationality.&nbsp;<em>For this game, we reward perceived irrationality.</em></p>\n<p style=\"margin: 0px 0px 1em;\">Try to be precise in your propositions. Saying \"I believe in God. 99% sure.\" isn't informative because we don't quite know which God you're talking about. A deist god? The Christian God? Jewish?</p>\n<p style=\"margin: 0px 0px 1em;\">Y'all know this already, but just a reminder: preferences ain't beliefs. Downvote preferences disguised as beliefs. Beliefs that include the word \"should\" are are almost always imprecise: avoid them.</p>\n<div>That means our local theists are probably gonna get a lot of upvotes. Can you beat them with your confident but perceived-by-LW-as-irrational beliefs? It's a challenge!</div>\n<p style=\"margin: 0px 0px 1em;\">Additional rules:</p>\n<ul style=\"padding: 0px;\">\n<li>Generally, no repeating an altered version of a proposition already in the comments unless it's different in an interesting and important way.<em>&nbsp;</em>Use your judgement.</li>\n<li>If you have comments about the game, please reply to my comment below about meta discussion, not to the post itself. Only propositions to be judged for the game should be direct comments to this post.&nbsp;</li>\n<li>Don't post propositions as comment replies to other comments. That'll make it disorganized.</li>\n<li><strong>You have to actually think your degree of belief is rational. &nbsp;</strong>You should already have taken the fact that most people would disagree with you into account and updated on that information. That means that &nbsp;<strong>any proposition you make is a proposition that you think you are personally more rational about than the Less Wrong average. &nbsp;</strong>This could be good or bad. Lots of upvotes means lots of people disagree with you. That's generally bad. Lots of downvotes means you're probably right. That's good, but this is a game where perceived irrationality wins you karma.&nbsp;The game is only fun if you're trying to be completely honest in your stated beliefs. Don't post something crazy and expect to get karma. Don't exaggerate your beliefs. Play fair.</li>\n<li><strong>Debate and discussion is great, but keep it civil. &nbsp;</strong>Linking to the Sequences is barely civil -- summarize arguments from specific LW posts and maybe link, but don't tell someone to go read something. If someone says they believe in God with 100% probability and you don't want to take the time to give a brief but substantive counterargument, don't comment at all. We're inviting people to share beliefs we think are irrational; don't be mean about their responses.</li>\n<li>No propositions that people are unlikely to have an opinion about, like \"Yesterday I wore black socks. ~80%\" or \"Antipope Christopher would have been a good leader in his latter days had he not been dethroned by Pope Sergius III. ~30%.\" The goal is to be controversial and&nbsp;<em>interesting.</em></li>\n<li>Multiple propositions are fine, so long as they're moderately interesting.</li>\n<li>You are encouraged to reply to comments with your own probability estimates, but &nbsp;<strong>comment voting works normally for comment replies to other comments. &nbsp;</strong>That is, upvote for good discussion, not agreement or disagreement.</li>\n<li>In general, just keep within the spirit of the game: we're celebrating LW-contrarian beliefs for a change!</li>\n</ul>\n</blockquote>\n</div>\n<p>I would suggest placing *related* propositions in the same comment, but wildly different ones might deserve separate comments for keeping threads separate.<br /><br />Make sure you put \"Irrationality Game\" as the first two words of a post containing a proposition to be voted upon in the game's format.<br /><br />Here we go!</p>\n<p><em><strong>EDIT:&nbsp; </strong>It was pointed out in the meta-thread below that this could be  done with polls rather than karma so as to discourage playing-to-win  and getting around the hiding of downvoted comments.&nbsp; If anyone  resurrects this game in the future, please do so under that system&nbsp; If  you wish to test a poll format in this thread feel free to do so, but  continue voting as normal for those that are not in poll format.&nbsp; </em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RjpM7Mnfq4TAojdSv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 15, "extendedScore": null, "score": 1.6080096455579197e-06, "legacy": true, "legacyId": "25756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 209, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wDJaQG4QSKDYxzmor", "fufh5NEjKeMRgAkcd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-12T18:33:44.433Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Discussion", "slug": "meetup-urbana-champaign-discussion-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q85K4tSYgoAf4sbCb/meetup-urbana-champaign-discussion-0", "pageUrlRelative": "/posts/Q85K4tSYgoAf4sbCb/meetup-urbana-champaign-discussion-0", "linkUrl": "https://www.lesswrong.com/posts/Q85K4tSYgoAf4sbCb/meetup-urbana-champaign-discussion-0", "postedAtFormatted": "Wednesday, March 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ85K4tSYgoAf4sbCb%2Fmeetup-urbana-champaign-discussion-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ85K4tSYgoAf4sbCb%2Fmeetup-urbana-champaign-discussion-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ85K4tSYgoAf4sbCb%2Fmeetup-urbana-champaign-discussion-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xw'>Urbana-Champaign: Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 March 2014 11:00:00AM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 S. Goodwin Ave Apt 102 Urbana IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHAT:\nStarting topic: contrarianism. When is it justified?\nRelated reading: <a href=\"http://lesswrong.com/lw/jv2/is_my_view_contrarian/\">This</a> and everything linked at the beginning.\nWHERE:\n300 S Goodwin Ave Apt 102, Urbana.\nThe door directly to my ground-floor apartment, on which you can knock and I will hear it, is at the North-West corner of the building. Do not attempt to enter through the building's main door, because that requires keycard access, and I will not be waiting there to let you in. If you have trouble getting in, call me at REDACTED.\nWHEN:\n2pm Sunday.\nALSO:\nIf you're interested in playing my video game, I will be publicly demoing it at Engineering Open House Friday and Saturday roughly from 9am-4pm and 9am-3pm respectively, in the Siebel building, room 1304.\nCross-posted, as always, on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/5qUNfO-XBdw\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xw'>Urbana-Champaign: Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q85K4tSYgoAf4sbCb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.6083538653068893e-06, "legacy": true, "legacyId": "25757", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Discussion\">Discussion article for the meetup : <a href=\"/meetups/xw\">Urbana-Champaign: Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 March 2014 11:00:00AM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 S. Goodwin Ave Apt 102 Urbana IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHAT:\nStarting topic: contrarianism. When is it justified?\nRelated reading: <a href=\"http://lesswrong.com/lw/jv2/is_my_view_contrarian/\">This</a> and everything linked at the beginning.\nWHERE:\n300 S Goodwin Ave Apt 102, Urbana.\nThe door directly to my ground-floor apartment, on which you can knock and I will hear it, is at the North-West corner of the building. Do not attempt to enter through the building's main door, because that requires keycard access, and I will not be waiting there to let you in. If you have trouble getting in, call me at REDACTED.\nWHEN:\n2pm Sunday.\nALSO:\nIf you're interested in playing my video game, I will be publicly demoing it at Engineering Open House Friday and Saturday roughly from 9am-4pm and 9am-3pm respectively, in the Siebel building, room 1304.\nCross-posted, as always, on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/5qUNfO-XBdw\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Discussion1\">Discussion article for the meetup : <a href=\"/meetups/xw\">Urbana-Champaign: Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Discussion", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kFiz7Etau5HNMdKdx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-12T21:32:57.649Z", "modifiedAt": null, "url": null, "title": "[LINK] Former Christian fundamentalist: Science robbed me of my faith", "slug": "link-former-christian-fundamentalist-science-robbed-me-of-my", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:08.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tKo62NvYwLcuid3xf/link-former-christian-fundamentalist-science-robbed-me-of-my", "pageUrlRelative": "/posts/tKo62NvYwLcuid3xf/link-former-christian-fundamentalist-science-robbed-me-of-my", "linkUrl": "https://www.lesswrong.com/posts/tKo62NvYwLcuid3xf/link-former-christian-fundamentalist-science-robbed-me-of-my", "postedAtFormatted": "Wednesday, March 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Former%20Christian%20fundamentalist%3A%20Science%20robbed%20me%20of%20my%20faith&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Former%20Christian%20fundamentalist%3A%20Science%20robbed%20me%20of%20my%20faith%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKo62NvYwLcuid3xf%2Flink-former-christian-fundamentalist-science-robbed-me-of-my%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Former%20Christian%20fundamentalist%3A%20Science%20robbed%20me%20of%20my%20faith%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKo62NvYwLcuid3xf%2Flink-former-christian-fundamentalist-science-robbed-me-of-my", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKo62NvYwLcuid3xf%2Flink-former-christian-fundamentalist-science-robbed-me-of-my", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>This person seems to have the virtue of non-compartmentalization. What rationalist skill can we learn from this? Maybe look for ways a strong belief in one domain, to another where it's more testable?</p>\n<p>http://www.salon.com/2013/09/09/i_was_a_fundamentalist_until_science_changed_my_mind_partner/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tKo62NvYwLcuid3xf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 5, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "25758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-12T22:07:26.233Z", "modifiedAt": null, "url": null, "title": "Meetup : Israel Less Wrong Meetup - Productivity Hacks", "slug": "meetup-israel-less-wrong-meetup-productivity-hacks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:02.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nDD7on3zDaZ3fpEt6/meetup-israel-less-wrong-meetup-productivity-hacks", "pageUrlRelative": "/posts/nDD7on3zDaZ3fpEt6/meetup-israel-less-wrong-meetup-productivity-hacks", "linkUrl": "https://www.lesswrong.com/posts/nDD7on3zDaZ3fpEt6/meetup-israel-less-wrong-meetup-productivity-hacks", "postedAtFormatted": "Wednesday, March 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Productivity%20Hacks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Productivity%20Hacks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnDD7on3zDaZ3fpEt6%2Fmeetup-israel-less-wrong-meetup-productivity-hacks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Israel%20Less%20Wrong%20Meetup%20-%20Productivity%20Hacks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnDD7on3zDaZ3fpEt6%2Fmeetup-israel-less-wrong-meetup-productivity-hacks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnDD7on3zDaZ3fpEt6%2Fmeetup-israel-less-wrong-meetup-productivity-hacks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 352, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xx'>Israel Less Wrong Meetup - Productivity Hacks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 March 2014 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, March 20th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time we will be discussion productivity systems and hacks. We will have multiple speakers telling us how they became and stay production and the tricks they used. Anyone is welcome to bring their own systems and tricks - because the more we learn from each other, the better we'll be (and the more winning we'll do!).</p>\n\n<p>Examples of subjects we will be talking about: Getting Things Done, Anki and spaced repetition, contingency planning, polyphasic sleep, commitment contracts and many more...\nWe will also be talking to each other about what works for each of us, and learn about how to actually implement these things in our lives.</p>\n\n<p>This meetup will not require any previous knowledge of LW or rationality, and so is a great opportunity to bring along interested friends!</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. (We've had great success with the earlier hour last meetup and so we're continuing on the trend)</p>\n\n<p>Please come on time, as we will begin the discussions and talks close to when we start. But, if you can only come later, thats totally ok!</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is). If you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>Note that we are trying a transition to a meet once every two weeks. This is to allow people who cant make it to a meetup to not have to wait a whole month to meet again, and because we'd like to have both subject based and social meetups and doing one meetup a month would make that hard.</p>\n\n<p>If you have any question feel free to email me at hochbergg@gmail.com or call me at 054-533-0678.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xx'>Israel Less Wrong Meetup - Productivity Hacks</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nDD7on3zDaZ3fpEt6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "25759", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Productivity_Hacks\">Discussion article for the meetup : <a href=\"/meetups/xx\">Israel Less Wrong Meetup - Productivity Hacks</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 March 2014 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Google Tel Aviv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, March 20th at Google Israel's offices, Electra Tower, 98 Yigal Alon st., Tel Aviv.</p>\n\n<p>This time we will be discussion productivity systems and hacks. We will have multiple speakers telling us how they became and stay production and the tricks they used. Anyone is welcome to bring their own systems and tricks - because the more we learn from each other, the better we'll be (and the more winning we'll do!).</p>\n\n<p>Examples of subjects we will be talking about: Getting Things Done, Anki and spaced repetition, contingency planning, polyphasic sleep, commitment contracts and many more...\nWe will also be talking to each other about what works for each of us, and learn about how to actually implement these things in our lives.</p>\n\n<p>This meetup will not require any previous knowledge of LW or rationality, and so is a great opportunity to bring along interested friends!</p>\n\n<p>We'll start the meetup at 19:00, and we'll go on as much as we like to. (We've had great success with the earlier hour last meetup and so we're continuing on the trend)</p>\n\n<p>Please come on time, as we will begin the discussions and talks close to when we start. But, if you can only come later, thats totally ok!</p>\n\n<p>We'll meet at the 29th floor of the building (Note: Not the 26th where Google Campus is). If you arrive and cant find your way around, call Anatoly who is graciously hosting us at 054-245-1060.</p>\n\n<p>Note that we are trying a transition to a meet once every two weeks. This is to allow people who cant make it to a meetup to not have to wait a whole month to meet again, and because we'd like to have both subject based and social meetups and doing one meetup a month would make that hard.</p>\n\n<p>If you have any question feel free to email me at hochbergg@gmail.com or call me at 054-533-0678.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Productivity_Hacks1\">Discussion article for the meetup : <a href=\"/meetups/xx\">Israel Less Wrong Meetup - Productivity Hacks</a></h2>", "sections": [{"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Productivity Hacks", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Productivity_Hacks", "level": 1}, {"title": "Discussion article for the meetup : Israel Less Wrong Meetup - Productivity Hacks", "anchor": "Discussion_article_for_the_meetup___Israel_Less_Wrong_Meetup___Productivity_Hacks1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-13T12:00:15.578Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-25", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:07.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kd7MzSGS8xh6Zj9Y3/meetup-melbourne-social-meetup-25", "pageUrlRelative": "/posts/kd7MzSGS8xh6Zj9Y3/meetup-melbourne-social-meetup-25", "linkUrl": "https://www.lesswrong.com/posts/kd7MzSGS8xh6Zj9Y3/meetup-melbourne-social-meetup-25", "postedAtFormatted": "Thursday, March 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkd7MzSGS8xh6Zj9Y3%2Fmeetup-melbourne-social-meetup-25%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkd7MzSGS8xh6Zj9Y3%2Fmeetup-melbourne-social-meetup-25", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkd7MzSGS8xh6Zj9Y3%2Fmeetup-melbourne-social-meetup-25", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/xy'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 March 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The March Social Meetup will be running on Friday 21st at our usual location in Carlton. All are welcome from 6:30pm.</p>\n\n<p>Social meetups are casual events where we gather to chat and play games. Some people play board games, often we will play a parlour game like Mafia/Werewolf or The Resistance later in the evening. We usually arrange some sort of delivered food for dinner for those that are interested.</p>\n\n<p>Just ring the number 5 when you arrive at the front gate, and we'll buzz you in. If you have any issues, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/xy'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kd7MzSGS8xh6Zj9Y3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.6096291163166622e-06, "legacy": true, "legacyId": "25768", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/xy\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 March 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The March Social Meetup will be running on Friday 21st at our usual location in Carlton. All are welcome from 6:30pm.</p>\n\n<p>Social meetups are casual events where we gather to chat and play games. Some people play board games, often we will play a parlour game like Mafia/Werewolf or The Resistance later in the evening. We usually arrange some sort of delivered food for dinner for those that are interested.</p>\n\n<p>Just ring the number 5 when you arrive at the front gate, and we'll buzz you in. If you have any issues, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/xy\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-13T17:22:15.090Z", "modifiedAt": null, "url": null, "title": "What attracts smart and curious young people to physics? Should this be encouraged?", "slug": "what-attracts-smart-and-curious-young-people-to-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:31.118Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uJDdmkFw8iFRG9ZZ7/what-attracts-smart-and-curious-young-people-to-physics", "pageUrlRelative": "/posts/uJDdmkFw8iFRG9ZZ7/what-attracts-smart-and-curious-young-people-to-physics", "linkUrl": "https://www.lesswrong.com/posts/uJDdmkFw8iFRG9ZZ7/what-attracts-smart-and-curious-young-people-to-physics", "postedAtFormatted": "Thursday, March 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20attracts%20smart%20and%20curious%20young%20people%20to%20physics%3F%20Should%20this%20be%20encouraged%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20attracts%20smart%20and%20curious%20young%20people%20to%20physics%3F%20Should%20this%20be%20encouraged%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJDdmkFw8iFRG9ZZ7%2Fwhat-attracts-smart-and-curious-young-people-to-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20attracts%20smart%20and%20curious%20young%20people%20to%20physics%3F%20Should%20this%20be%20encouraged%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJDdmkFw8iFRG9ZZ7%2Fwhat-attracts-smart-and-curious-young-people-to-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJDdmkFw8iFRG9ZZ7%2Fwhat-attracts-smart-and-curious-young-people-to-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 608, "htmlBody": "<p>Many of the high school students who sought advice from <a href=\"http://cognitomentoring.org\">Cognito Mentoring</a> were interested in mathematics, computer science, and physics. This both makes sense and is valuable. Mathematics has <a href=\"http://info.cognitomentoring.org/wiki/Mathematics_learning_benefits\">many benefits</a>: it underpins a lot of quantitative analysis, and helps us understand the world. Computer science is also quite important <a href=\"http://info.cognitomentoring.org/wiki/Computer_science_learning_benefits\">for obvious reasons</a>: programming in particular <a href=\"http://info.cognitomentoring.org/wiki/Programming_learning_benefits\">is directly and indirectly useful</a>, and a deeper understanding of algorithms and the theory of computation can help with algorithms.</p>\n<p>Physics, however, is a little different. There are some <a href=\"http://info.cognitomentoring.org/wiki/Physics_learning_benefits\">benefits of learning physics</a>. In particular, classical mechanics is often people's first exposure to using mathematical structure in a nontrivial way to understand and model situations pertaining to the real world. Nonetheless, unlike mathematics or computer science, the benefits of physics for people who are not in science or engineering careers are fairly low. I find myself using high school-level mathematical intuition on a regular basis (for instance, understanding the growth trajectories of various things, or interpreting graphs), and I find myself using basic programming-like intuition quite often. But I rarely find myself using my physics intuition in the real world. Moreover, I think physics quickly hits diminishing returns in terms of teaching people about mathematical modeling: I'd say that the returns from physics beyond classical mechanics, DC circuits, and basic thermodynamics are near-zero. For instance, I'd say it's more beneficial to learn microeconomics rather than electromagnetism, even though the latter is often considered more prestigious by smart people. Similarly, I think that behavioral economics is more valuable than quantum mechanics.</p>\n<p>It's also not clear that learning physics beyond the basics suggested above (classical mechanics, thermodynamics, DC circuits) passes a cost-benefit analysis for people in the vast majority of science-based and engineering-based careers. Even the extent to which they crucially rely on these basics is questionable, given that most people <em>don't</em> learn the basics well and still manage to go on to do decent jobs. I'd like to hear any opinions on this. On a related note, I recently asked on Quora the question <a href=\"http://www.quora.com/In-what-ways-is-knowledge-of-Newtonian-classical-mechanics-helpful-to-people-pursuing-biomedical-research\">In what ways is knowledge of Newtonian classical mechanics helpful to people pursuing biomedical research?</a> and there were a few interesting answers.</p>\n<p>So my question: what attracts smart and curious young people to physics? Are the smartest people too attracted by physics, relative to its real-world applicability? Does the intellectual stimulation provided by physics justify the attraction? Is there some sort of mood affiliation going on here, where the smartest people are pulled to physics to distinguish themselves from the crowd, insofar as physics is more difficult and repels the crowd? To the extent that people overvalue physics, does it make sense to push them at the margin away from physics and in the direction of computer science or economics or some other subject? Or should their interest in physics be encouraged?</p>\n<p>Thoughts on your personal experience, as well as thoughts on the general points about the usefulness and attractiveness of learning physics, would be appreciated.</p>\n<p>PS: In a <a href=\"http://www.youtube.com/watch?v=WwslBPj8GgI\">video</a>, Eric Mazur describes research related to the <a href=\"https://en.wikipedia.org/wiki/Force_Concept_Inventory\">Force Concept Inventory</a>: people often learn how to solve complicated mechanics problems by pattern-matching but fail to demonstrate clear understanding of Newton's Third Law. Similarly, people can predict potential differences and current flows in complicated circuits using Kirchhoff's laws, yet fail to predict that if you short a circuit, all the current will flow through the short. (The latter failure of prediction occurred in an end-of-course examination co-taught by Mazur to Harvard University first-year students, many of whom were planning to go on to medical school.</p>\n<p>PS2: My collaborator Jonah Sinick's Quora post (no login needed to view) titled <a href=\"http://cognitomentoring.quora.com/Is-math-privileged-for-gifted-children\">Is math privileged for gifted children</a> is somewhat related.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uJDdmkFw8iFRG9ZZ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 1.6100218442307107e-06, "legacy": true, "legacyId": "25731", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-03-13T17:56:29.852Z", "modifiedAt": null, "url": null, "title": "Parenting versus career choice thinking in teenagers", "slug": "parenting-versus-career-choice-thinking-in-teenagers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:09.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tGpCRMtPMeM2hk8NA/parenting-versus-career-choice-thinking-in-teenagers", "pageUrlRelative": "/posts/tGpCRMtPMeM2hk8NA/parenting-versus-career-choice-thinking-in-teenagers", "linkUrl": "https://www.lesswrong.com/posts/tGpCRMtPMeM2hk8NA/parenting-versus-career-choice-thinking-in-teenagers", "postedAtFormatted": "Thursday, March 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Parenting%20versus%20career%20choice%20thinking%20in%20teenagers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParenting%20versus%20career%20choice%20thinking%20in%20teenagers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGpCRMtPMeM2hk8NA%2Fparenting-versus-career-choice-thinking-in-teenagers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Parenting%20versus%20career%20choice%20thinking%20in%20teenagers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGpCRMtPMeM2hk8NA%2Fparenting-versus-career-choice-thinking-in-teenagers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGpCRMtPMeM2hk8NA%2Fparenting-versus-career-choice-thinking-in-teenagers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 815, "htmlBody": "<p>This is a somewhat modified version of a <a href=\"https://www.facebook.com/vipulnaik.r/posts/10202683502424636?stream_ref=10\">Facebook post</a> I made a few days ago, incorporating some of the comments there. I think the <em>Less Wrong</em> readership may have interesting thoughts on the subject.</p>\n<p>In recent times, especially in the developed world and among higher socio-economic status families everywhere in the world, <span class=\"userContent\"> it's common for teenagers (and even younger children) to be encouraged to think in systematic ways about their career choice, but it's relatively rare for them to be encouraged to think in systematic ways about how many children they'll have or how they'll raise their children. A lot of teenagers <em>do</em> have views on the subject of children, but they're not encouraged to have views, and they're not encouraged to refine those views. With career choice, although there's still probably a lot of room for improvement in the quality of advice and guidance offered, people at least <em>in principle</em> acknowledge its importance.</span></p>\n<p>What do you think explains the disparity? Here are some explanations with my thoughts on them:</p>\n<ol>\n<li><span class=\"userContent\"><span class=\"text_exposed_show\"><strong>Career choice is (believed to be) more important than the choice of how many children to have and how to raise them</strong>: For people who expect to generate a huge amount of value in their careers, this is probably true, because if they do have children, the children are likely to be less exceptional (regression to the mean). However, for most people, this probably isn't the case: having children could be one of their main forms of contribution to society.<br /></span></span></li>\n<li><span class=\"userContent\"><span class=\"text_exposed_show\"><strong>Career choice requires planning from a younger age, because it requires selection of subjects to study in school and college</strong>: There's more lead time needed for career choice, whereas it takes only nine months to have a child. This seems to work as a reasonable explanation for people who are inclined to have very few kids, but it doesn't work for people who are interested in keeping open the option of having a large number of kids. Also, as they say, it takes two to have a baby, so one does need to plan somewhat in advance. Finally, the choice of when to have kids can affect one's selection of career as well.</span></span></li>\n<li><span class=\"userContent\"><span class=\"text_exposed_show\"><strong>Choices related to children are believed to be something that are best made after one has selected a life partner to discuss them with, and unilateral thinking is considered counterproductive</strong>: There's probably some truth to this. But the point could be made in reverse as well: in order to make sure one selects life partners well, it makes sense to think of one's choices and desires regarding children <em>before</em> one gets into a serious relationship so that one can check for compatibility on that front.</span></span></li>\n<li><span class=\"userContent\"><span class=\"text_exposed_show\"><strong>Teenagers aren't sufficiently mature (physically, mentally, emotionally) to think about childbearing and childrearing choices in a meaningful manner, and/or their views on the subject are much more likely to change, relative to their views on career choices</strong>.</span></span></li>\n<li><span class=\"userContent\"><span class=\"text_exposed_show\"><strong>People will have children anyway, because it's part of their biological instincts</strong>: This is sort of true, but not quite. Many countries in Europe and East Asia have fertility rates well below replacement. This includes some highly developed countries such as Singapore (TFR ~1.3), Germany and Japan (TFR ~1.4), as well as some middle-income countries such as China (TFR~1.6-1.8) and Eastern European countries. With the exception of Germany, most of these countries still have desired fertility greater than 2, but people aren't generally able to achieve their fertility desires -- partly because they keep delaying childrearing. Moreover, highly educated women often have similar or even higher reported ideal family size although their completed fertility is lower (see <a href=\"http://www.oeaw.ac.at/vid/download/edrp_2_2012.pdf%E2%80%8E\">this PDF</a> and <a href=\"http://demography.subwiki.org/wiki/Ideal_family_size\">the linked references here</a> for more).</span></span></li>\n<li><span class=\"userContent\"><span class=\"text_exposed_show\"><strong>Thinking about career instead of children signals high status</strong>: </span></span>This overlaps somewhat with the other points, but differs in that it's a more cynical take: perhaps<span class=\"userContent\"><span class=\"text_exposed_show\"> thinking about one's career as opposed to one's family signals one as high-status (by distinguishing oneself from demographic or socio-economic groups where people marry early or have out-of-wedlock teen pregnancies frequently). Something like \"while other women were dropping out of high school to have babies, I was baby working hard to get into Harvard so that I could then have a great career in finance after that.\" Or, \"while the other men were settling down for easy-to-get jobs and thinking of marrying to settle down and have kids, I'm working hard to fulfill my ambition to become a writer. I won't let thoughts of family distract me for now.\"<br /></span></span></li>\n</ol>\n<p>What do you think of these explanations? Any others I'm missing? Correctness of the explanations at a factual level? Importance as explanations?</p>\n<p>PS: Some of my other recent posts have been based on stuff I wrote up in connection with working for <a href=\"http://www.cognitomentoring.org\">Cognito Mentoring</a>, but this one isn't, though it's possible it might inform my later work for Cognito Mentoring.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tGpCRMtPMeM2hk8NA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.6100636234408873e-06, "legacy": true, "legacyId": "25769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}